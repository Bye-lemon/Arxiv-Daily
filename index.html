<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- PAGE settings -->
  <link rel="icon" href="https://templates.pingendo.com/assets/Pingendo_favicon.ico">
  <title>Arxiv Daily</title>
  <!-- CSS dependencies -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"
    type="text/css">
  <link rel="stylesheet" href="assets/wireframe.css">
</head>

<body class="bg-light">
  <div class="py-5">
    <div class="container">
      <div class="row">
        <div class="text-center col-md-7 mx-auto"> <i class="fa d-block fa-bullseye fa-5x mb-4 text-info"></i>
          <h2><b>Arxiv Daily - 2024-01-31</b></h2>
        </div>
      </div>
    </div>
  </div>
  <div class="py-2">
    <div class="container">
      <div class="row">
        <div class="col-md-12">
          <ul class="nav nav-tabs">
            
            <li class="nav-item"> <a href="" class="nav-link active"
                data-toggle="pill" data-target="#SLAM"><i class="fa fa-home"></i> SLAM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#SFM"><i class="fa fa-home"></i> SFM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#LLM"><i class="fa fa-home"></i> LLM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#SAM"><i class="fa fa-home"></i> SAM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#CLIP"><i class="fa fa-home"></i> CLIP</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#DINO"><i class="fa fa-home"></i> DINO</a>
            </li>
            
          </ul>
          <div class="tab-content mt-2">
            
            <div class="tab-pane fade active show" id="SLAM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2401_17061v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">OmniSCV: An Omnidirectional Synthetic Image Generator for Computer Vision</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17061v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17061v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17061v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Omnidirectional and 360{\deg} images are becoming widespread in industry and in consumer society, causing omnidirectional computer vision to gain attention. Their wide field of view allows the gathering of a great amount of information about the environment from only an image. However, the distortion of these images requires the development of specific algorithms for their treatment and interpretation. Moreover, a high number of images is essential for the correct training of computer vision algorithms based on learning. In this paper, we present a tool for generating datasets of omnidirectional images with semantic and depth information. These images are synthesized from a set of captures that are acquired in a realistic virtual environment for Unreal Engine 4 through an interface plugin. We gather a variety of well-known projection models such as equirectangular and cylindrical panoramas, different fish-eye lenses, catadioptric systems, and empiric models. Furthermore, we include in our tool photorealistic non-central-projection systems as non-central panoramas and non-central catadioptric systems. As far as we know, this is the first reported tool for generating photorealistic non-central images in the literature. Moreover, since the omnidirectional images are made virtually, we provide pixel-wise information about semantics and depth as well as perfect knowledge of the calibration parameters of the cameras. This allows the creation of ground-truth information with pixel precision for training learning algorithms and testing 3D vision approaches. To validate the proposed tool, different computer vision algorithms are tested as line extractions from dioptric and catadioptric central images, 3D Layout recovery and SLAM using equirectangular panoramas, and 3D reconstruction from non-central panoramas.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17061v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>全方位和360度图像在工业和消费社会中越来越普遍，引起了全方位计算机视觉的关注。它们的宽视场允许仅从图像中收集大量关于环境的信息。然而，这些图像的失真需要开发特定的算法来进行处理和解释。此外，大量的图像对于基于学习的计算机视觉算法的正确训练是必不可少的。在本文中，我们提出了一种用于生成具有语义和深度信息的全向图像数据集的工具。这些图像是从在虚幻引擎4的真实虚拟环境中通过接口插件获取的一组捕获中合成的。我们收集了各种著名的投影模型，如等矩形和圆柱形全景图、不同的鱼眼透镜、折反射系统和经验模型。此外，在我们的工具中，我们将真实感非中心投影系统包括为非中心全景和非中心折反射系统。据我们所知，这是文献中第一个报道的生成真实感非中心图像的工具。此外，由于全向图像是虚拟制作的，我们提供了关于语义和深度的像素信息，以及相机校准参数的完美知识。这允许创建具有像素精度的地面实况信息，用于训练学习算法和测试3D视觉方法。为了验证所提出的工具，测试了不同的计算机视觉算法，如从屈光和折反射中心图像中提取线，使用等矩形全景进行3D布局恢复和SLAM，以及从非中心全景进行3D重建。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17061v1" target="_blank">2401.17061v1</a>
                              </td>
                              <td>OmniSCV: An Omnidirectional Synthetic Image Generator for Computer Vision</td>
                              <td>Bruno Berenguel-Baeta</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17061v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17061v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/sbrunoberenguel/omniscv" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_16719v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">OptiState: State Estimation of Legged Robots using Gated Networks with Transformer-based Vision and Kalman Filtering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_16719v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_16719v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_16719v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>State estimation for legged robots is challenging due to their highly dynamic motion and limitations imposed by sensor accuracy. By integrating Kalman filtering, optimization, and learning-based modalities, we propose a hybrid solution that combines proprioception and exteroceptive information for estimating the state of the robot's trunk. Leveraging joint encoder and IMU measurements, our Kalman filter is enhanced through a single-rigid body model that incorporates ground reaction force control outputs from convex Model Predictive Control optimization. The estimation is further refined through Gated Recurrent Units, which also considers semantic insights and robot height from a Vision Transformer autoencoder applied on depth images. This framework not only furnishes accurate robot state estimates, including uncertainty evaluations, but can minimize the nonlinear errors that arise from sensor measurements and model simplifications through learning. The proposed methodology is evaluated in hardware using a quadruped robot on various terrains, yielding a 65% improvement on the Root Mean Squared Error compared to our VIO SLAM baseline. Code example: https://github.com/AlexS28/OptiState</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_16719v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于腿式机器人的高度动态运动和传感器精度的限制，其状态估计具有挑战性。通过集成卡尔曼滤波、优化和基于学习的模态，我们提出了一种混合解决方案，该解决方案结合本体感觉和外部感觉信息来估计机器人躯干的状态。利用联合编码器和IMU测量，我们的卡尔曼滤波器通过单个刚体模型得到增强，该模型结合了凸模型预测控制优化的地面反作用力控制输出。通过门控递归单元进一步细化了估计，该单元还考虑了应用于深度图像的视觉转换器自动编码器的语义见解和机器人高度。该框架不仅提供了准确的机器人状态估计，包括不确定性评估，而且可以通过学习将传感器测量和模型简化产生的非线性误差降至最低。使用四足机器人在各种地形上对所提出的方法进行了硬件评估，与我们的VIO SLAM基线相比，均方根误差提高了65%。代码示例：https://github.com/AlexS28/OptiState</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.16719v1" target="_blank">2401.16719v1</a>
                              </td>
                              <td>OptiState: State Estimation of Legged Robots using Gated Networks with Transformer-based Vision and Kalman Filtering</td>
                              <td>Alexander Schperberg</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_16719v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.16719v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/alexs28/optistate" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14857v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LIV-GaussMap: LiDAR-Inertial-Visual Fusion for Real-time 3D Radiance Field Map Rendering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14857v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14857v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14857v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce an integrated precise LiDAR, Inertial, and Visual (LIV) multi-modal sensor fused mapping system that builds on the differentiable surface splatting to improve the mapping fidelity, quality, and structural accuracy. Notably, this is also a novel form of tightly coupled map for LiDAR-visual-inertial sensor fusion.   This system leverages the complementary characteristics of LiDAR and visual data to capture the geometric structures of large-scale 3D scenes and restore their visual surface information with high fidelity. The initial poses for surface Gaussian scenes are obtained using a LiDAR-inertial system with size-adaptive voxels. Then, we optimized and refined the Gaussians by visual-derived photometric gradients to optimize the quality and density of LiDAR measurements.   Our method is compatible with various types of LiDAR, including solid-state and mechanical LiDAR, supporting both repetitive and non-repetitive scanning modes. bolstering structure construction through LiDAR and facilitating real-time generation of photorealistic renderings across diverse LIV datasets. It showcases notable resilience and versatility in generating real-time photorealistic scenes potentially for digital twins and virtual reality while also holding potential applicability in real-time SLAM and robotics domains.   We release our software and hardware and self-collected datasets on Github\footnote[3]{https://github.com/sheng00125/LIV-GaussMap} to benefit the community.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14857v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了一种集成的精确激光雷达、惯性和视觉（LIV）多模态传感器融合测绘系统，该系统建立在可微分表面飞溅的基础上，以提高测绘保真度、质量和结构精度。值得注意的是，这也是一种用于激光雷达视觉惯性传感器融合的新型紧耦合映射。该系统利用激光雷达和视觉数据的互补特性来捕捉大规模3D场景的几何结构，并高保真地恢复其视觉表面信息。表面高斯场景的初始姿态是使用具有尺寸自适应体素的激光雷达惯性系统获得的。然后，我们通过视觉推导的光度梯度优化和细化高斯，以优化激光雷达测量的质量和密度。我们的方法与各种类型的激光雷达兼容，包括固态和机械激光雷达，支持重复和非重复扫描模式。通过激光雷达支持结构构建，并促进在不同的LIV数据集上实时生成真实感渲染图。它在为数字双胞胎和虚拟现实生成实时真实感场景方面表现出了显著的弹性和多功能性，同时在实时SLAM和机器人领域也具有潜在的适用性。我们在Github上发布了我们的软件和硬件以及自行收集的数据集\脚注[3]{https://github.com/sheng00125/LIV-GaussMap}以造福社会。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14857v1" target="_blank">2401.14857v1</a>
                              </td>
                              <td>LIV-GaussMap: LiDAR-Inertial-Visual Fusion for Real-time 3D Radiance Field Map Rendering</td>
                              <td>Sheng Hong</td>
                              <td>2024-01-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14857v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14857v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_13877v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AscDAMs: Advanced SLAM-based channel detection and mapping system</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_13877v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_13877v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_13877v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Obtaining high-resolution, accurate channel topography and deposit conditions is the prior challenge for the study of channelized debris flow. Currently, wide-used mapping technologies including satellite imaging and drone photogrammetry struggle to precisely observe channel interior conditions of mountainous long-deep gullies, particularly those in the Wenchuan Earthquake region. SLAM is an emerging tech for 3D mapping; however, extremely rugged environment in long-deep gullies poses two major challenges even for the state-of-art SLAM: (1) Atypical features; (2) Violent swaying and oscillation of sensors. These issues result in large deviation and lots of noise for SLAM results. To improve SLAM mapping in such environments, we propose an advanced SLAM-based channel detection and mapping system, namely AscDAMs. It features three main enhancements to post-process SLAM results: (1) The digital orthophoto map aided deviation correction algorithm greatly eliminates the systematic error; (2) The point cloud smoothing algorithm substantially diminishes noises; (3) The cross section extraction algorithm enables the quantitative assessment of channel deposits and their changes. Two field experiments were conducted in Chutou Gully, Wenchuan County in China in February and November 2023, representing observations before and after the rainy season. We demonstrate the capability of AscDAMs to greatly improve SLAM results, promoting SLAM for mapping the specially challenging environment. The proposed method compensates for the insufficiencies of existing technologies in detecting debris flow channel interiors including detailed channel morphology, erosion patterns, deposit distinction, volume estimation and change detection. It serves to enhance the study of full-scale debris flow mechanisms, long-term post-seismic evolution, and hazard assessment.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_13877v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>获得高分辨率、准确的河道地形和沉积条件是渠化泥石流研究的首要挑战。目前，包括卫星成像和无人机摄影测量在内的广泛使用的测绘技术难以精确观测山区长深沟的河道内部条件，特别是汶川地震地区的深沟。SLAM是一种新兴的3D地图技术；然而，即使对最先进的SLAM来说，长深沟中极其崎岖的环境也带来了两大挑战：（1）非典型特征；（2） 传感器剧烈摆动和振荡。这些问题导致SLAM结果的大偏差和大量噪声。为了改进这种环境中的SLAM映射，我们提出了一种先进的基于SLAM的信道检测和映射系统，即AscDAM。它对后处理SLAM结果有三个主要改进：（1）数字正射影像图辅助偏差校正算法大大消除了系统误差；（2） 点云平滑算法大大减少了噪声；（3） 横截面提取算法能够对河道沉积物及其变化进行定量评估。2023年2月和11月，在中国汶川县楚头沟进行了两次野外实验，代表了雨季前后的观测结果。我们展示了AscDAM极大地提高SLAM结果的能力，促进了SLAM在特殊挑战环境中的映射。所提出的方法弥补了现有技术在检测泥石流通道内部方面的不足，包括详细的通道形态、侵蚀模式、沉积物区分、体积估计和变化检测。它有助于加强对全面泥石流机制、地震后长期演变和灾害评估的研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.13877v1" target="_blank">2401.13877v1</a>
                              </td>
                              <td>AscDAMs: Advanced SLAM-based channel detection and mapping system</td>
                              <td>Tengfei Wang</td>
                              <td>2024-01-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_13877v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.13877v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_13800v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-Object Navigation in real environments using hybrid policies</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_13800v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_13800v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_13800v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Navigation has been classically solved in robotics through the combination of SLAM and planning. More recently, beyond waypoint planning, problems involving significant components of (visual) high-level reasoning have been explored in simulated environments, mostly addressed with large-scale machine learning, in particular RL, offline-RL or imitation learning. These methods require the agent to learn various skills like local planning, mapping objects and querying the learned spatial representations. In contrast to simpler tasks like waypoint planning (PointGoal), for these more complex tasks the current state-of-the-art models have been thoroughly evaluated in simulation but, to our best knowledge, not yet in real environments.   In this work we focus on sim2real transfer. We target the challenging Multi-Object Navigation (Multi-ON) task and port it to a physical environment containing real replicas of the originally virtual Multi-ON objects. We introduce a hybrid navigation method, which decomposes the problem into two different skills: (1) waypoint navigation is addressed with classical SLAM combined with a symbolic planner, whereas (2) exploration, semantic mapping and goal retrieval are dealt with deep neural networks trained with a combination of supervised learning and RL. We show the advantages of this approach compared to end-to-end methods both in simulation and a real environment and outperform the SOTA for this task.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_13800v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>导航已经通过SLAM和规划的结合在机器人技术中得到了经典的解决。最近，除了航路点规划之外，还探索了在模拟环境中涉及（视觉）高级推理的重要组成部分的问题，主要通过大规模机器学习来解决，特别是RL、离线RL或模仿学习。这些方法需要代理学习各种技能，如局部规划、映射对象和查询所学习的空间表示。与航路点规划（PointGoal）等更简单的任务相比，对于这些更复杂的任务，目前最先进的模型已经在模拟中进行了全面评估，但据我们所知，还没有在真实环境中进行评估。在这项工作中，我们重点关注sim2real转移。我们针对具有挑战性的多对象导航（Multi-ON）任务，并将其移植到包含原始虚拟Multi-ON对象的真实副本的物理环境中。我们介绍了一种混合导航方法，该方法将问题分解为两种不同的技能：（1）航路点导航是用经典的SLAM与符号规划器相结合来解决的，而（2）探索、语义映射和目标检索是用监督学习和RL相结合来训练的深度神经网络来解决的。我们展示了与端到端方法相比，这种方法在模拟和真实环境中的优势，并在该任务中优于SOTA。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.13800v1" target="_blank">2401.13800v1</a>
                              </td>
                              <td>Multi-Object Navigation in real environments using hybrid policies</td>
                              <td>Assem Sadek</td>
                              <td>2024-01-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_13800v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.13800v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_14590v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">HeLiPR: Heterogeneous LiDAR Dataset for inter-LiDAR Place Recognition under Spatiotemporal Variations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_14590v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_14590v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_14590v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Place recognition is crucial for robot localization and loop closure in simultaneous localization and mapping (SLAM). Light Detection and Ranging (LiDAR), known for its robust sensing capabilities and measurement consistency even in varying illumination conditions, has become pivotal in various fields, surpassing traditional imaging sensors in certain applications. Among various types of LiDAR, spinning LiDARs are widely used, while non-repetitive scanning patterns have recently been utilized in robotics applications. Some LiDARs provide additional measurements such as reflectivity, Near Infrared (NIR), and velocity from Frequency modulated continuous wave (FMCW) LiDARs. Despite these advances, there is a lack of comprehensive datasets reflecting the broad spectrum of LiDAR configurations for place recognition. To tackle this issue, our paper proposes the HeLiPR dataset, curated especially for place recognition with heterogeneous LiDARs, embodying spatiotemporal variations. To the best of our knowledge, the HeLiPR dataset is the first heterogeneous LiDAR dataset supporting inter-LiDAR place recognition with both non-repetitive and spinning LiDARs, accommodating different field of view (FOV)s and varying numbers of rays. The dataset covers diverse environments, from urban cityscapes to high-dynamic freeways, over a month, enhancing adaptability and robustness across scenarios. Notably, HeLiPR includes trajectories parallel to MulRan sequences, making it valuable for research in heterogeneous LiDAR place recognition and long-term studies. The dataset is accessible at https://sites.google.com/view/heliprdataset .</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_14590v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在同步定位与映射（SLAM）中，位置识别对于机器人定位和闭环至关重要。光探测和测距（LiDAR）以其强大的传感能力和测量一致性而闻名，即使在不同的光照条件下也是如此，在各个领域已经成为关键，在某些应用中超过了传统的成像传感器。在各种类型的激光雷达中，旋转激光雷达被广泛使用，而非重复扫描模式最近被用于机器人应用。一些激光雷达提供额外的测量，如反射率、近红外（NIR）和调频连续波（FMCW）激光雷达的速度。尽管取得了这些进展，但缺乏全面的数据集来反映用于位置识别的激光雷达配置的广谱性。为了解决这个问题，我们的论文提出了HeLiPR数据集，该数据集专门用于异构激光雷达的位置识别，体现了时空变化。据我们所知，HeLiPR数据集是第一个支持非重复和旋转激光雷达的激光雷达位置识别的异构激光雷达数据集，可容纳不同的视场和不同数量的光线。该数据集在一个多月的时间里涵盖了从城市景观到高动态高速公路的各种环境，增强了跨场景的适应性和稳健性。值得注意的是，HeLiPR包括与MulRan序列平行的轨迹，这对于异质激光雷达位置识别和长期研究的研究具有价值。数据集可在访问https://sites.google.com/view/heliprdataset .</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.14590v2" target="_blank">2309.14590v2</a>
                              </td>
                              <td>HeLiPR: Heterogeneous LiDAR Dataset for inter-LiDAR Place Recognition under Spatiotemporal Variations</td>
                              <td>Minwoo Jung</td>
                              <td>2023-09-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_14590v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.14590v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_00928v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Quatro++: Robust Global Registration Exploiting Ground Segmentation for Loop Closing in LiDAR SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_00928v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_00928v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_00928v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Global registration is a fundamental task that estimates the relative pose between two viewpoints of 3D point clouds. However, there are two issues that degrade the performance of global registration in LiDAR SLAM: one is the sparsity issue and the other is degeneracy. The sparsity issue is caused by the sparse characteristics of the 3D point cloud measurements in a mechanically spinning LiDAR sensor. The degeneracy issue sometimes occurs because the outlier-rejection methods reject too many correspondences, leaving less than three inliers. These two issues have become more severe as the pose discrepancy between the two viewpoints of 3D point clouds becomes greater. To tackle these problems, we propose a robust global registration framework, called \textit{Quatro++}. Extending our previous work that solely focused on the global registration itself, we address the robust global registration in terms of the loop closing in LiDAR SLAM. To this end, ground segmentation is exploited to achieve robust global registration. Through the experiments, we demonstrate that our proposed method shows a higher success rate than the state-of-the-art global registration methods, overcoming the sparsity and degeneracy issues. In addition, we show that ground segmentation significantly helps to increase the success rate for the ground vehicles. Finally, we apply our proposed method to the loop closing module in LiDAR SLAM and confirm that the quality of the loop constraints is improved, showing more precise mapping results. Therefore, the experimental evidence corroborated the suitability of our method as an initial alignment in the loop closing. Our code is available at https://quatro-plusplus.github.io.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_00928v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>全局配准是估计三维点云的两个视点之间的相对姿态的基本任务。然而，有两个问题降低了激光雷达SLAM的全局配准性能：一个是稀疏性问题，另一个是退化性问题。稀疏性问题是由机械旋转的激光雷达传感器中的3D点云测量的稀疏特性引起的。退化问题有时会发生，因为异常值拒绝方法拒绝了太多的对应关系，只留下不到三个内部。随着3D点云的两个视点之间的姿态差异变得更大，这两个问题变得更加严重。为了解决这些问题，我们提出了一个强大的全局注册框架，称为\textit｛Quatro++｝。扩展我们之前仅专注于全球注册本身的工作，我们在激光雷达SLAM中解决了闭环方面的稳健全球注册问题。为此，利用地面分割来实现稳健的全局配准。通过实验，我们证明了我们提出的方法比最先进的全局配准方法具有更高的成功率，克服了稀疏性和退化性问题。此外，我们还表明，地面分割显著有助于提高地面车辆的成功率。最后，我们将我们提出的方法应用于激光雷达SLAM中的闭环模块，并证实了环路约束的质量得到了提高，显示出更精确的映射结果。因此，实验证据证实了我们的方法作为闭环初始对准的适用性。我们的代码可在https://quatro-plusplus.github.io.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.00928v2" target="_blank">2311.00928v2</a>
                              </td>
                              <td>Quatro++: Robust Global Registration Exploiting Ground Segmentation for Loop Closing in LiDAR SLAM</td>
                              <td>Hyungtae Lim</td>
                              <td>2023-11-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_00928v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.00928v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_10857v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Motion Consistency Loss for Monocular Visual Odometry with Attention-Based Deep Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_10857v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_10857v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_10857v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Deep learning algorithms have driven expressive progress in many complex tasks. The loss function is a core component of deep learning techniques, guiding the learning process of neural networks. This paper contributes by introducing a consistency loss for visual odometry with deep learning-based approaches. The motion consistency loss explores repeated motions that appear in consecutive overlapped video clips. Experimental results show that our approach increased the performance of a model on the KITTI odometry benchmark.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_10857v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>深度学习算法推动了许多复杂任务的表达进度。损失函数是深度学习技术的核心组成部分，指导神经网络的学习过程。本文通过引入基于深度学习的视觉里程计方法的一致性损失来做出贡献。运动一致性损失探索出现在连续重叠视频剪辑中的重复运动。实验结果表明，我们的方法提高了模型在KITTI里程计基准上的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.10857v1" target="_blank">2401.10857v1</a>
                              </td>
                              <td>Motion Consistency Loss for Monocular Visual Odometry with Attention-Based Deep Learning</td>
                              <td>André O. Françani</td>
                              <td>2024-01-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_10857v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.10857v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_10560v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">360ORB-SLAM: A Visual SLAM System for Panoramic Images with Depth Completion Network</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_10560v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_10560v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_10560v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>To enhance the performance and effect of AR/VR applications and visual assistance and inspection systems, visual simultaneous localization and mapping (vSLAM) is a fundamental task in computer vision and robotics. However, traditional vSLAM systems are limited by the camera's narrow field-of-view, resulting in challenges such as sparse feature distribution and lack of dense depth information. To overcome these limitations, this paper proposes a 360ORB-SLAM system for panoramic images that combines with a depth completion network. The system extracts feature points from the panoramic image, utilizes a panoramic triangulation module to generate sparse depth information, and employs a depth completion network to obtain a dense panoramic depth map. Experimental results on our novel panoramic dataset constructed based on Carla demonstrate that the proposed method achieves superior scale accuracy compared to existing monocular SLAM methods and effectively addresses the challenges of feature association and scale ambiguity. The integration of the depth completion network enhances system stability and mitigates the impact of dynamic elements on SLAM performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_10560v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>为了提高AR/VR应用程序以及视觉辅助和检测系统的性能和效果，视觉同步定位和映射（vSLAM）是计算机视觉和机器人技术中的一项基本任务。然而，传统的vSLAM系统受到相机狭窄视场的限制，导致了特征分布稀疏和缺乏密集深度信息等挑战。为了克服这些限制，本文提出了一种360ORB-SLAM全景图像系统，该系统与深度完成网络相结合。该系统从全景图像中提取特征点，利用全景三角测量模块生成稀疏的深度信息，并利用深度完成网络获得密集的全景深度图。在基于Carla构建的新全景数据集上的实验结果表明，与现有的单目SLAM方法相比，该方法实现了优越的尺度精度，并有效地解决了特征关联和尺度模糊的挑战。深度完井网络的集成增强了系统稳定性，并减轻了动态元素对SLAM性能的影响。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.10560v1" target="_blank">2401.10560v1</a>
                              </td>
                              <td>360ORB-SLAM: A Visual SLAM System for Panoramic Images with Depth Completion Network</td>
                              <td>Yichen Chen</td>
                              <td>2024-01-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_10560v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.10560v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_09388v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CognitiveDog: Large Multimodal Model Based System to Translate Vision and Language into Action of Quadruped Robot</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_09388v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_09388v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_09388v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper introduces CognitiveDog, a pioneering development of quadruped robot with Large Multi-modal Model (LMM) that is capable of not only communicating with humans verbally but also physically interacting with the environment through object manipulation. The system was realized on Unitree Go1 robot-dog equipped with a custom gripper and demonstrated autonomous decision-making capabilities, independently determining the most appropriate actions and interactions with various objects to fulfill user-defined tasks. These tasks do not necessarily include direct instructions, challenging the robot to comprehend and execute them based on natural language input and environmental cues. The paper delves into the intricacies of this system, dataset characteristics, and the software architecture. Key to this development is the robot's proficiency in navigating space using Visual-SLAM, effectively manipulating and transporting objects, and providing insightful natural language commentary during task execution. Experimental results highlight the robot's advanced task comprehension and adaptability, underscoring its potential in real-world applications. The dataset used to fine-tune the robot-dog behavior generation model is provided at the following link: huggingface.co/datasets/ArtemLykov/CognitiveDog_dataset</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_09388v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了CognitiveDog，这是一种具有大型多模态模型（LMM）的四足机器人的开创性发展，它不仅能够与人类进行言语交流，而且能够通过物体操纵与环境进行物理交互。该系统是在配备了自定义夹持器的Unitree Go1机器狗上实现的，并展示了自主决策能力，独立确定最合适的动作以及与各种对象的交互，以完成用户定义的任务。这些任务不一定包括直接指令，挑战机器人根据自然语言输入和环境线索理解和执行指令。本文深入研究了该系统的复杂性、数据集特性和软件体系结构。这一开发的关键是机器人熟练地使用Visual SLAM在空间中导航，有效地操纵和运输物体，并在任务执行过程中提供富有洞察力的自然语言评论。实验结果突出了机器人先进的任务理解能力和适应性，突出了其在现实世界应用中的潜力。用于微调机器狗行为生成模型的数据集在以下链接中提供：huggingface.co/datasets/ArtemLykov/CognitiveDog_dataset</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.09388v1" target="_blank">2401.09388v1</a>
                              </td>
                              <td>CognitiveDog: Large Multimodal Model Based System to Translate Vision and Language into Action of Quadruped Robot</td>
                              <td>Artem Lykov</td>
                              <td>2024-01-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_09388v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.09388v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_09331v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Event-Based Visual Odometry on Non-Holonomic Ground Vehicles</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_09331v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_09331v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_09331v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite the promise of superior performance under challenging conditions, event-based motion estimation remains a hard problem owing to the difficulty of extracting and tracking stable features from event streams. In order to robustify the estimation, it is generally believed that fusion with other sensors is a requirement. In this work, we demonstrate reliable, purely event-based visual odometry on planar ground vehicles by employing the constrained non-holonomic motion model of Ackermann steering platforms. We extend single feature n-linearities for regular frame-based cameras to the case of quasi time-continuous event-tracks, and achieve a polynomial form via variable degree Taylor expansions. Robust averaging over multiple event tracks is simply achieved via histogram voting. As demonstrated on both simulated and real data, our algorithm achieves accurate and robust estimates of the vehicle's instantaneous rotational velocity, and thus results that are comparable to the delta rotations obtained by frame-based sensors under normal conditions. We furthermore significantly outperform the more traditional alternatives in challenging illumination scenarios. The code is available at \url{https://github.com/gowanting/NHEVO}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_09331v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管在具有挑战性的条件下有望获得卓越的性能，但由于难以从事件流中提取和跟踪稳定特征，基于事件的运动估计仍然是一个难题。为了使估计具有鲁棒性，通常认为需要与其他传感器进行融合。在这项工作中，我们通过使用阿克曼转向平台的约束非完整运动模型，在平面地面车辆上演示了可靠的、纯基于事件的视觉里程计。我们将基于常规帧的相机的单特征n线性扩展到准时间连续事件轨迹的情况，并通过变阶泰勒展开实现多项式形式。通过直方图投票可以简单地实现对多个事件轨迹的稳健平均。如模拟数据和真实数据所示，我们的算法实现了对车辆瞬时转速的准确和稳健估计，从而获得了与正常条件下基于帧的传感器获得的德尔塔旋转相当的结果。此外，在具有挑战性的照明场景中，我们显著优于更传统的替代方案。代码位于\url{https://github.com/gowanting/NHEVO}.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.09331v1" target="_blank">2401.09331v1</a>
                              </td>
                              <td>Event-Based Visual Odometry on Non-Holonomic Ground Vehicles</td>
                              <td>Wanting Xu</td>
                              <td>2024-01-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_09331v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.09331v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/gowanting/nhevo" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_09322v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FIT-SLAM -- Fisher Information and Traversability estimation-based Active SLAM for exploration in 3D environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_09322v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_09322v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_09322v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Active visual SLAM finds a wide array of applications in GNSS-Denied sub-terrain environments and outdoor environments for ground robots. To achieve robust localization and mapping accuracy, it is imperative to incorporate the perception considerations in the goal selection and path planning towards the goal during an exploration mission. Through this work, we propose FIT-SLAM (Fisher Information and Traversability estimation-based Active SLAM), a new exploration method tailored for unmanned ground vehicles (UGVs) to explore 3D environments. This approach is devised with the dual objectives of sustaining an efficient exploration rate while optimizing SLAM accuracy. Initially, an estimation of a global traversability map is conducted, which accounts for the environmental constraints pertaining to traversability. Subsequently, we propose a goal candidate selection approach along with a path planning method towards this goal that takes into account the information provided by the landmarks used by the SLAM backend to achieve robust localization and successful path execution . The entire algorithm is tested and evaluated first in a simulated 3D world, followed by a real-world environment and is compared to pre-existing exploration methods. The results obtained during this evaluation demonstrate a significant increase in the exploration rate while effectively minimizing the localization covariance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_09322v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>主动视觉SLAM在拒绝全球导航卫星系统的地下环境和地面机器人的户外环境中有着广泛的应用。为了实现稳健的定位和测绘精度，在探索任务期间，必须将感知考虑因素纳入目标选择和目标路径规划中。通过这项工作，我们提出了FIT-SLAM（基于Fisher信息和遍历性估计的主动SLAM），这是一种为无人地面飞行器（UGV）探索3D环境量身定制的新探索方法。这种方法的设计具有双重目标，即在优化SLAM精度的同时保持有效的勘探速率。最初，对全球可穿越性地图进行估计，该地图考虑了与可穿越性相关的环境约束。随后，我们提出了一种目标候选选择方法以及实现该目标的路径规划方法，该方法考虑了SLAM后端使用的地标提供的信息，以实现稳健的定位和成功的路径执行。首先在模拟的3D世界中测试和评估整个算法，然后在真实世界环境中进行测试和评估，并将其与预先存在的探索方法进行比较。在该评估过程中获得的结果表明，在有效地最小化定位协方差的同时，勘探率显著提高。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.09322v1" target="_blank">2401.09322v1</a>
                              </td>
                              <td>FIT-SLAM -- Fisher Information and Traversability estimation-based Active SLAM for exploration in 3D environments</td>
                              <td>Suchetan Saravanan</td>
                              <td>2024-01-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_09322v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.09322v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_09160v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DK-SLAM: Monocular Visual SLAM with Deep Keypoints Adaptive Learning, Tracking and Loop-Closing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_09160v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_09160v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_09160v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Unreliable feature extraction and matching in handcrafted features undermine the performance of visual SLAM in complex real-world scenarios. While learned local features, leveraging CNNs, demonstrate proficiency in capturing high-level information and excel in matching benchmarks, they encounter challenges in continuous motion scenes, resulting in poor generalization and impacting loop detection accuracy. To address these issues, we present DK-SLAM, a monocular visual SLAM system with adaptive deep local features. MAML optimizes the training of these features, and we introduce a coarse-to-fine feature tracking approach. Initially, a direct method approximates the relative pose between consecutive frames, followed by a feature matching method for refined pose estimation. To counter cumulative positioning errors, a novel online learning binary feature-based online loop closure module identifies loop nodes within a sequence. Experimental results underscore DK-SLAM's efficacy, outperforms representative SLAM solutions, such as ORB-SLAM3 on publicly available datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_09160v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>手工特征中不可靠的特征提取和匹配会破坏视觉SLAM在复杂现实世界场景中的性能。虽然利用细胞神经网络学习局部特征，证明了它们在捕捉高级信息方面的熟练程度和在匹配基准方面的出色表现，但它们在连续运动场景中遇到了挑战，导致泛化能力差，并影响环路检测精度。为了解决这些问题，我们提出了DK-SLAM，一种具有自适应深度局部特征的单目视觉SLAM系统。MAML优化了这些特征的训练，并引入了一种从粗到细的特征跟踪方法。首先，直接方法近似连续帧之间的相对姿态，然后是用于精细姿态估计的特征匹配方法。为了应对累积的定位误差，一种新颖的基于二进制特征的在线学习回路闭合模块识别序列中的回路节点。实验结果强调了DK-SLAM的功效，在公开可用的数据集上优于代表性的SLAM解决方案，如ORB-SLAM3。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.09160v1" target="_blank">2401.09160v1</a>
                              </td>
                              <td>DK-SLAM: Monocular Visual SLAM with Deep Keypoints Adaptive Learning, Tracking and Loop-Closing</td>
                              <td>Hao Qu</td>
                              <td>2024-01-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_09160v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.09160v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_09101v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PIN-SLAM: LiDAR SLAM Using a Point-Based Implicit Neural Representation for Achieving Global Map Consistency</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_09101v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_09101v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_09101v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Accurate and robust localization and mapping are essential components for most autonomous robots. In this paper, we propose a SLAM system for building globally consistent maps, called PIN-SLAM, that is based on an elastic and compact point-based implicit neural map representation. Taking range measurements as input, our approach alternates between incremental learning of the local implicit signed distance field and the pose estimation given the current local map using a correspondence-free, point-to-implicit model registration. Our implicit map is based on sparse optimizable neural points, which are inherently elastic and deformable with the global pose adjustment when closing a loop. Loops are also detected using the neural point features. Extensive experiments validate that PIN-SLAM is robust to various environments and versatile to different range sensors such as LiDAR and RGB-D cameras. PIN-SLAM achieves pose estimation accuracy better or on par with the state-of-the-art LiDAR odometry or SLAM systems and outperforms the recent neural implicit SLAM approaches while maintaining a more consistent, and highly compact implicit map that can be reconstructed as accurate and complete meshes. Finally, thanks to the voxel hashing for efficient neural points indexing and the fast implicit map-based registration without closest point association, PIN-SLAM can run at the sensor frame rate on a moderate GPU. Codes will be available at: https://github.com/PRBonn/PIN_SLAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_09101v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>精确和稳健的定位和映射是大多数自主机器人的重要组成部分。在本文中，我们提出了一种用于构建全局一致映射的SLAM系统，称为PIN-SLAM，它基于弹性和紧凑的基于点的隐式神经映射表示。以距离测量作为输入，我们的方法在局部隐式符号距离场的增量学习和使用无对应的点到隐式模型配准的给定当前局部地图的姿态估计之间交替。我们的隐式映射基于稀疏的可优化神经点，这些神经点在闭合回路时具有固有的弹性和可变形性，并可通过全局姿态调整进行调整。还使用神经点特征来检测循环。大量实验验证了PIN-SLAM对各种环境具有鲁棒性，并适用于不同的距离传感器，如激光雷达和RGB-D相机。PIN-SLAM实现了更好或与最先进的LiDAR里程计或SLAM系统不相上下的姿态估计精度，并优于最近的神经隐式SLAM方法，同时保持了更一致、高度紧凑的隐式映射，可以重建为准确和完整的网格。最后，得益于用于高效神经点索引的体素哈希和无最近点关联的快速隐式基于映射的配准，PIN-SLAM可以在中等GPU上以传感器帧速率运行。代码将在以下位置提供：https://github.com/PRBonn/PIN_SLAM.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.09101v1" target="_blank">2401.09101v1</a>
                              </td>
                              <td>PIN-SLAM: LiDAR SLAM Using a Point-Based Implicit Neural Representation for Achieving Global Map Consistency</td>
                              <td>Yue Pan</td>
                              <td>2024-01-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_09101v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.09101v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/prbonn/pin_slam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2204_10993v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TerrainMesh: Metric-Semantic Terrain Reconstruction from Aerial Images Using Joint 2D-3D Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2204_10993v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2204_10993v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2204_10993v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper considers outdoor terrain mapping using RGB images obtained from an aerial vehicle. While feature-based localization and mapping techniques deliver real-time vehicle odometry and sparse keypoint depth reconstruction, a dense model of the environment geometry and semantics (vegetation, buildings, etc.) is usually recovered offline with significant computation and storage. This paper develops a joint 2D-3D learning approach to reconstruct a local metric-semantic mesh at each camera keyframe maintained by a visual odometry algorithm. Given the estimated camera trajectory, the local meshes can be assembled into a global environment model to capture the terrain topology and semantics during online operation. A local mesh is reconstructed using an initialization and refinement stage. In the initialization stage, we estimate the mesh vertex elevation by solving a least squares problem relating the vertex barycentric coordinates to the sparse keypoint depth measurements. In the refinement stage, we associate 2D image and semantic features with the 3D mesh vertices using camera projection and apply graph convolution to refine the mesh vertex spatial coordinates and semantic features based on joint 2D and 3D supervision. Quantitative and qualitative evaluation using real aerial images show the potential of our method to support environmental monitoring and surveillance applications.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2204_10993v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文考虑使用从飞行器获得的RGB图像进行户外地形测绘。虽然基于特征的定位和映射技术提供了实时车辆里程测量和稀疏关键点深度重建，但环境几何和语义（植被、建筑物等）的密集模型通常通过大量的计算和存储离线恢复。本文开发了一种2D-3D联合学习方法，以重建由视觉里程计算法维护的每个相机关键帧处的局部度量语义网格。给定估计的相机轨迹，可以将局部网格组装到全局环境模型中，以在在线操作期间捕捉地形拓扑和语义。使用初始化和细化阶段来重建局部网格。在初始化阶段，我们通过求解将顶点重心坐标与稀疏关键点深度测量值相关的最小二乘问题来估计网格顶点高程。在细化阶段，我们使用相机投影将二维图像和语义特征与三维网格顶点相关联，并基于二维和三维联合监督应用图卷积来细化网格顶点的空间坐标和语义特征。使用真实航空图像进行的定量和定性评估显示了我们的方法支持环境监测和监视应用的潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2204.10993v2" target="_blank">2204.10993v2</a>
                              </td>
                              <td>TerrainMesh: Metric-Semantic Terrain Reconstruction from Aerial Images Using Joint 2D-3D Learning</td>
                              <td>Qiaojun Feng</td>
                              <td>2022-04-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2204_10993v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2204.10993v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_08134v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">S3M: Semantic Segmentation Sparse Mapping for UAVs with RGB-D Camera</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_08134v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_08134v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_08134v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Unmanned Aerial Vehicles (UAVs) hold immense potential for critical applications, such as search and rescue operations, where accurate perception of indoor environments is paramount. However, the concurrent amalgamation of localization, 3D reconstruction, and semantic segmentation presents a notable hurdle, especially in the context of UAVs equipped with constrained power and computational resources. This paper presents a novel approach to address challenges in semantic information extraction and utilization within UAV operations. Our system integrates state-of-the-art visual SLAM to estimate a comprehensive 6-DoF pose and advanced object segmentation methods at the back end. To improve the computational and storage efficiency of the framework, we adopt a streamlined voxel-based 3D map representation - OctoMap to build a working system. Furthermore, the fusion algorithm is incorporated to obtain the semantic information of each frame from the front-end SLAM task, and the corresponding point. By leveraging semantic information, our framework enhances the UAV's ability to perceive and navigate through indoor spaces, addressing challenges in pose estimation accuracy and uncertainty reduction. Through Gazebo simulations, we validate the efficacy of our proposed system and successfully embed our approach into a Jetson Xavier AGX unit for real-world applications.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_08134v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>无人机在搜索和救援行动等关键应用中具有巨大潜力，在这些应用中，准确感知室内环境至关重要。然而，定位、3D重建和语义分割的同时融合是一个显著的障碍，尤其是在配备有限功率和计算资源的无人机的情况下。本文提出了一种新的方法来解决无人机作战中语义信息提取和利用方面的挑战。我们的系统集成了最先进的视觉SLAM来估计全面的6-DoF姿态，并在后端集成了先进的对象分割方法。为了提高框架的计算和存储效率，我们采用了一种简化的基于体素的三维地图表示——OctoMap来构建一个工作系统。此外，融合算法用于从前端SLAM任务中获得每个帧的语义信息和对应点。通过利用语义信息，我们的框架增强了无人机在室内空间感知和导航的能力，解决了姿态估计准确性和减少不确定性方面的挑战。通过Gazebo模拟，我们验证了我们提出的系统的有效性，并成功地将我们的方法嵌入到Jetson Xavier AGX单元中，用于真实世界的应用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.08134v1" target="_blank">2401.08134v1</a>
                              </td>
                              <td>S3M: Semantic Segmentation Sparse Mapping for UAVs with RGB-D Camera</td>
                              <td>Thanh Nguyen Canh</td>
                              <td>2024-01-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_08134v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.08134v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_08132v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Object-Oriented Semantic Mapping for Reliable UAVs Navigation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_08132v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_08132v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_08132v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>To autonomously navigate in real-world environments, special in search and rescue operations, Unmanned Aerial Vehicles (UAVs) necessitate comprehensive maps to ensure safety. However, the prevalent metric map often lacks semantic information crucial for holistic scene comprehension. In this paper, we proposed a system to construct a probabilistic metric map enriched with object information extracted from the environment from RGB-D images. Our approach combines a state-of-the-art YOLOv8-based object detection framework at the front end and a 2D SLAM method - CartoGrapher at the back end. To effectively track and position semantic object classes extracted from the front-end interface, we employ the innovative BoT-SORT methodology. A novel association method is introduced to extract the position of objects and then project it with the metric map. Unlike previous research, our approach takes into reliable navigating in the environment with various hollow bottom objects. The output of our system is a probabilistic map, which significantly enhances the map's representation by incorporating object-specific attributes, encompassing class distinctions, accurate positioning, and object heights. A number of experiments have been conducted to evaluate our proposed approach. The results show that the robot can effectively produce augmented semantic maps containing several objects (notably chairs and desks). Furthermore, our system is evaluated within an embedded computer - Jetson Xavier AGX unit to demonstrate the use case in real-world applications.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_08132v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>为了在现实世界环境中自主导航，特别是在搜救行动中，无人机需要全面的地图来确保安全。然而，流行的度量图往往缺乏对整体场景理解至关重要的语义信息。在本文中，我们提出了一种构建概率度量图的系统，该系统富含从RGB-D图像的环境中提取的对象信息。我们的方法在前端结合了最先进的基于YOLOv8的对象检测框架，在后端结合了2D SLAM方法CartoGraper。为了有效地跟踪和定位从前端接口提取的语义对象类，我们采用了创新的BoT SORT方法。引入了一种新的关联方法来提取物体的位置，然后将其与度量图进行投影。与之前的研究不同，我们的方法考虑了在各种中空底部物体的环境中进行可靠导航。我们系统的输出是一张概率地图，它通过结合特定对象的属性，包括类别区分、准确定位和对象高度，显著增强了地图的表示。已经进行了大量实验来评估我们提出的方法。结果表明，该机器人可以有效地生成包含多个对象（尤其是椅子和桌子）的增强语义图。此外，我们的系统在嵌入式计算机Jetson Xavier AGX单元中进行了评估，以展示真实应用中的用例。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.08132v1" target="_blank">2401.08132v1</a>
                              </td>
                              <td>Object-Oriented Semantic Mapping for Reliable UAVs Navigation</td>
                              <td>Thanh Nguyen Canh</td>
                              <td>2024-01-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_08132v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.08132v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_08043v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Cross-Modal Semi-Dense 6-DoF Tracking of an Event Camera in Challenging Conditions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_08043v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_08043v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_08043v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision-based localization is a cost-effective and thus attractive solution for many intelligent mobile platforms. However, its accuracy and especially robustness still suffer from low illumination conditions, illumination changes, and aggressive motion. Event-based cameras are bio-inspired visual sensors that perform well in HDR conditions and have high temporal resolution, and thus provide an interesting alternative in such challenging scenarios. While purely event-based solutions currently do not yet produce satisfying mapping results, the present work demonstrates the feasibility of purely event-based tracking if an alternative sensor is permitted for mapping. The method relies on geometric 3D-2D registration of semi-dense maps and events, and achieves highly reliable and accurate cross-modal tracking results. Practically relevant scenarios are given by depth camera-supported tracking or map-based localization with a semi-dense map prior created by a regular image-based visual SLAM or structure-from-motion system. Conventional edge-based 3D-2D alignment is extended by a novel polarity-aware registration that makes use of signed time-surface maps (STSM) obtained from event streams. We furthermore introduce a novel culling strategy for occluded points. Both modifications increase the speed of the tracker and its robustness against occlusions or large view-point variations. The approach is validated on many real datasets covering the above-mentioned challenging conditions, and compared against similar solutions realised with regular cameras.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_08043v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于视觉的定位是一种具有成本效益的解决方案，因此对许多智能移动平台具有吸引力。然而，其准确性和特别是鲁棒性仍然受到低照明条件、照明变化和侵略性运动的影响。基于事件的相机是受生物启发的视觉传感器，在HDR条件下表现良好，具有高时间分辨率，因此在这种具有挑战性的场景中提供了一种有趣的替代方案。虽然纯基于事件的解决方案目前还没有产生令人满意的映射结果，但目前的工作证明了如果允许替代传感器进行映射，则纯基于事件跟踪的可行性。该方法依赖于半密集地图和事件的几何3D-2D配准，并实现了高度可靠和准确的跨模态跟踪结果。实际相关场景由深度相机支持的跟踪或基于地图的定位给出，其中半密集地图由基于常规图像的视觉SLAM或来自运动系统的结构预先创建。传统的基于边缘的3D-2D对准通过利用从事件流获得的带符号时间表面图（STSM）的新颖的极性感知配准来扩展。此外，我们还介绍了一种新的遮挡点剔除策略。这两种修改都提高了跟踪器的速度及其对遮挡或大视点变化的鲁棒性。该方法在涵盖上述挑战性条件的许多真实数据集上进行了验证，并与使用常规相机实现的类似解决方案进行了比较。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.08043v1" target="_blank">2401.08043v1</a>
                              </td>
                              <td>Cross-Modal Semi-Dense 6-DoF Tracking of an Event Camera in Challenging Conditions</td>
                              <td>Yi-Fan Zuo</td>
                              <td>2024-01-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_08043v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.08043v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zyfff/canny-evt" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_07962v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Cesium Tiles for High-realism Simulation and Comparing SLAM Results in Corresponding Virtual and Real-world Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_07962v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_07962v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_07962v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This article discusses the use of a simulated environment to predict algorithm results in the real world. Simulators are crucial in allowing researchers to test algorithms, sensor integration, and navigation systems without deploying expensive hardware. This article examines how the AirSim simulator, Unreal Engine, and Cesium plugin can be used to generate simulated digital twin models of real-world locations. Several technical challenges in completing the analysis are discussed and the technical solutions are detailed in this article. Work investigates how to assess mapping results for a real-life experiment using Cesium Tiles provided by digital twins of the experimental location. This is accompanied by a description of a process for duplicating real-world flights in simulation. The performance of these methods is evaluated by analyzing real-life and experimental image telemetry with the Direct Sparse Odometry (DSO) mapping algorithm. Results indicate that Cesium Tiles environments can provide highly accurate models of ground truth geometry after careful alignment. Further, results from real-life and simulated telemetry analysis indicate that the virtual simulation results accurately predict real-life results. Findings indicate that the algorithm results in real life and in the simulated duplicate exhibited a high degree of similarity. This indicates that the use of Cesium Tiles environments as a virtual digital twin for real-life experiments will provide representative results for such algorithms. The impact of this can be significant, potentially allowing expansive virtual testing of robotic systems at specific deployment locations to develop solutions that are tailored to the environment and potentially outperforming solutions meant to work in completely generic environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_07962v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文讨论了在现实世界中使用模拟环境来预测算法结果。模拟器对于研究人员在不部署昂贵硬件的情况下测试算法、传感器集成和导航系统至关重要。本文研究了如何使用AirSim模拟器、虚幻引擎和Cesium插件来生成真实世界位置的模拟数字孪生模型。本文讨论了完成分析的几个技术挑战，并详细介绍了技术解决方案。这项工作调查了如何使用实验地点的数字双胞胎提供的铯砖来评估真实实验的映射结果。这还附带了一个在模拟中复制真实世界飞行的过程的描述。通过使用直接稀疏测距（DSO）映射算法分析真实和实验图像遥测，评估了这些方法的性能。结果表明，Cesium Tiles环境在仔细对准后可以提供高度准确的地面实况几何模型。此外，来自真实和模拟遥测分析的结果表明，虚拟模拟结果准确地预测了真实结果。研究结果表明，该算法在现实生活中的结果与模拟副本中的结果具有高度相似性。这表明，使用Cesium Tiles环境作为真实实验的虚拟数字孪生将为此类算法提供代表性结果。这可能会产生重大影响，有可能允许在特定部署位置对机器人系统进行广泛的虚拟测试，以开发适合环境的解决方案，并有可能优于在完全通用环境中工作的解决方案。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.07962v1" target="_blank">2401.07962v1</a>
                              </td>
                              <td>Cesium Tiles for High-realism Simulation and Comparing SLAM Results in Corresponding Virtual and Real-world Environments</td>
                              <td>Chris Beam</td>
                              <td>2024-01-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_07962v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.07962v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_14972v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Trade-off Analysis of Replacing Proprietary LLMs with Open Source SLMs in Production</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_14972v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_14972v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_14972v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Many companies rely on APIs of managed AI models such as OpenAI's GPT-4 to create AI-enabled experiences in their products. Along with the benefits of ease of use and shortened time to production, this reliance on proprietary APIs has downsides in terms of model control, performance reliability, up-time predictability, and cost. At the same time, there has been a flurry of open source small language models (SLMs) that have been made available for commercial use. However, their readiness to replace existing capabilities remains unclear, and a systematic approach to test these models is not readily available. In this paper, we present a systematic evaluation methodology for, and characterization of, modern open source SLMs and their trade-offs when replacing a proprietary LLM APIs for a real-world product feature. We have designed SLaM, an automated analysis tool that enables the quantitative and qualitative testing of product features utilizing arbitrary SLMs. Using SLaM, we examine both the quality and the performance characteristics of modern SLMs relative to an existing customer-facing OpenAI-based implementation. We find that across 9 SLMs and 29 variants, we observe competitive quality-of-results for our use case, significant performance consistency improvement, and a cost reduction of 5x-29x when compared to OpenAI GPT-4.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_14972v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>许多公司依靠OpenAI的GPT-4等托管人工智能模型的API在其产品中创建人工智能体验。除了易用性和缩短生产时间的好处外，这种对专有API的依赖在模型控制、性能可靠性、运行时间可预测性和成本方面也有缺点。与此同时，出现了一系列可供商业使用的开源小语言模型（SLM）。然而，它们取代现有能力的准备情况尚不清楚，而且还没有现成的系统方法来测试这些模型。在本文中，我们提出了现代开源SLM的系统评估方法和特征，以及在将专有LLM API替换为真实世界的产品功能时的权衡。我们设计了SLaM，这是一种自动化分析工具，能够利用任意SLM对产品特征进行定量和定性测试。使用SLaM，我们检查了现代SLM相对于现有面向客户的基于OpenAI的实现的质量和性能特征。我们发现，在9个SLM和29个变体中，与OpenAI GPT-4相比，我们观察到用例的结果质量具有竞争力，性能一致性显著提高，成本降低了5x-29x。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.14972v2" target="_blank">2312.14972v2</a>
                              </td>
                              <td>A Trade-off Analysis of Replacing Proprietary LLMs with Open Source SLMs in Production</td>
                              <td>Chandra Irugalbandara</td>
                              <td>2023-12-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_14972v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.14972v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_07658v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Robustness Evaluation of Localization Techniques for Autonomous Racing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_07658v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_07658v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_07658v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This work introduces SynPF, an MCL-based algorithm tailored for high-speed racing environments. Benchmarked against Cartographer, a state-of-the-art pose-graph SLAM algorithm, SynPF leverages synergies from previous particle-filtering methods and synthesizes them for the high-performance racing domain. Our extensive in-field evaluations reveal that while Cartographer excels under nominal conditions, it struggles when subjected to wheel-slip, a common phenomenon in a racing scenario due to varying grip levels and aggressive driving behaviour. Conversely, SynPF demonstrates robustness in these challenging conditions and a low-latency computation time of 1.25 ms on on-board computers without a GPU. Using the F1TENTH platform, a 1:10 scaled autonomous racing vehicle, this work not only highlights the vulnerabilities of existing algorithms in high-speed scenarios, tested up until 7.6 m/s, but also emphasizes the potential of SynPF as a viable alternative, especially in deteriorating odometry conditions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_07658v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>这项工作介绍了SynPF，一种基于MCL的算法，适用于高速比赛环境。SynPF以最先进的姿态图SLAM算法“制图器”为基准，利用了以前粒子滤波方法的协同作用，并将其合成为高性能的比赛领域。我们广泛的现场评估表明，虽然制图师在标称条件下表现出色，但在车轮打滑时却很吃力，这是比赛场景中的一种常见现象，原因是不同的抓地力水平和激进的驾驶行为。相反，SynPF在这些具有挑战性的条件下表现出了鲁棒性，并且在没有GPU的板载计算机上具有1.25ms的低延迟计算时间。这项工作使用F1TENTH平台，一种1:10比例的自动驾驶赛车，不仅突出了现有算法在高速场景中的漏洞，测试速度高达7.6米/秒，还强调了SynPF作为一种可行的替代方案的潜力，尤其是在里程计条件恶化的情况下。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.07658v1" target="_blank">2401.07658v1</a>
                              </td>
                              <td>Robustness Evaluation of Localization Techniques for Autonomous Racing</td>
                              <td>Tian Yi Lim</td>
                              <td>2024-01-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_07658v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.07658v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2110_15169v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multimotion Visual Odometry (MVO)</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2110_15169v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2110_15169v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2110_15169v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual motion estimation is a well-studied challenge in autonomous navigation. Recent work has focused on addressing multimotion estimation in highly dynamic environments. These environments not only comprise multiple, complex motions but also tend to exhibit significant occlusion.   Estimating third-party motions simultaneously with the sensor egomotion is difficult because an object's observed motion consists of both its true motion and the sensor motion. Most previous works in multimotion estimation simplify this problem by relying on appearance-based object detection or application-specific motion constraints. These approaches are effective in specific applications and environments but do not generalize well to the full multimotion estimation problem (MEP).   This paper presents Multimotion Visual Odometry (MVO), a multimotion estimation pipeline that estimates the full SE(3) trajectory of every motion in the scene, including the sensor egomotion, without relying on appearance-based information. MVO extends the traditional visual odometry (VO) pipeline with multimotion segmentation and tracking techniques. It uses physically founded motion priors to extrapolate motions through temporary occlusions and identify the reappearance of motions through motion closure. Evaluations on real-world data from the Oxford Multimotion Dataset (OMD) and the KITTI Vision Benchmark Suite demonstrate that MVO achieves good estimation accuracy compared to similar approaches and is applicable to a variety of multimotion estimation challenges.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2110_15169v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉运动估计是自主导航中一个研究得很好的挑战。最近的工作集中于解决高度动态环境中的多运动估计问题。这些环境不仅包括多个复杂的运动，而且往往表现出显著的遮挡。很难同时估计第三方运动和传感器自运动，因为物体的观测运动包括其真实运动和传感器运动。先前在多运动估计中的大多数工作通过依赖于基于外观的对象检测或特定于应用程序的运动约束来简化这个问题。这些方法在特定的应用程序和环境中是有效的，但不能很好地推广到完整的多运动估计问题（MEP）。本文介绍了Multimotion Visual Odometry（MVO），这是一种多运动估计管道，它估计场景中每个运动的完整SE（3）轨迹，包括传感器自身运动，而不依赖于基于外观的信息。MVO通过多运动分割和跟踪技术扩展了传统的视觉里程计（VO）管道。它使用物理建立的运动先验来推断通过临时遮挡的运动，并通过运动闭合来识别运动的再现。对牛津多运动数据集（OMD）和KITTI Vision Benchmark Suite的真实世界数据的评估表明，与类似方法相比，MVO实现了良好的估计精度，并适用于各种多运动估计挑战。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2110.15169v3" target="_blank">2110.15169v3</a>
                              </td>
                              <td>Multimotion Visual Odometry (MVO)</td>
                              <td>Kevin M. Judd</td>
                              <td>2021-10-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2110_15169v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2110.15169v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_06230v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Swarm-SLAM : Sparse Decentralized Collaborative Simultaneous Localization and Mapping Framework for Multi-Robot Systems</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_06230v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_06230v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_06230v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Collaborative Simultaneous Localization And Mapping (C-SLAM) is a vital component for successful multi-robot operations in environments without an external positioning system, such as indoors, underground or underwater. In this paper, we introduce Swarm-SLAM, an open-source C-SLAM system that is designed to be scalable, flexible, decentralized, and sparse, which are all key properties in swarm robotics. Our system supports inertial, lidar, stereo, and RGB-D sensing, and it includes a novel inter-robot loop closure prioritization technique that reduces communication and accelerates convergence. We evaluated our ROS-2 implementation on five different datasets, and in a real-world experiment with three robots communicating through an ad-hoc network. Our code is publicly available: https://github.com/MISTLab/Swarm-SLAM</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_06230v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在没有外部定位系统的环境中，如室内、地下或水下，协作式同步定位和测绘（C-SLAM）是多机器人成功操作的重要组成部分。在本文中，我们介绍了Swarm SLAM，这是一个开源的C-SLAM系统，旨在实现可扩展、灵活、分散和稀疏，这些都是群体机器人的关键特性。我们的系统支持惯性、激光雷达、立体声和RGB-D传感，并包括一种新颖的机器人间环路闭合优先级技术，该技术可以减少通信并加速收敛。我们在五个不同的数据集上评估了我们的ROS-2实现，并在三个机器人通过自组织网络通信的真实世界实验中进行了评估。我们的代码是公开的：https://github.com/MISTLab/Swarm-SLAM</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.06230v3" target="_blank">2301.06230v3</a>
                              </td>
                              <td>Swarm-SLAM : Sparse Decentralized Collaborative Simultaneous Localization and Mapping Framework for Multi-Robot Systems</td>
                              <td>Pierre-Yves Lajoie</td>
                              <td>2023-01-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_06230v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.06230v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/mistlab/swarm-slam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_06323v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Kimera2: Robust and Accurate Metric-Semantic SLAM in the Real World</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_06323v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_06323v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_06323v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present improvements to Kimera, an open-source metric-semantic visual-inertial SLAM library. In particular, we enhance Kimera-VIO, the visual-inertial odometry pipeline powering Kimera, to support better feature tracking, more efficient keyframe selection, and various input modalities (eg monocular, stereo, and RGB-D images, as well as wheel odometry). Additionally, Kimera-RPGO and Kimera-PGMO, Kimera's pose-graph optimization backends, are updated to support modern outlier rejection methods - specifically, Graduated-Non-Convexity - for improved robustness to spurious loop closures. These new features are evaluated extensively on a variety of simulated and real robotic platforms, including drones, quadrupeds, wheeled robots, and simulated self-driving cars. We present comparisons against several state-of-the-art visual-inertial SLAM pipelines and discuss strengths and weaknesses of the new release of Kimera. The newly added features have been released open-source at https://github.com/MIT-SPARK/Kimera.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_06323v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了对Kimera的改进，Kimera是一个开源的度量语义视觉惯性SLAM库。特别是，我们增强了Kimera VIO，这是为Kimera提供动力的视觉惯性里程计管道，以支持更好的特征跟踪、更高效的关键帧选择和各种输入模式（如单眼、立体和RGB-D图像，以及车轮里程计）。此外，Kimera的姿势图优化后端Kimera RPGO和Kimera PGMO也进行了更新，以支持现代异常值拒绝方法，特别是分级非凸性，从而提高对虚假环路闭合的鲁棒性。这些新功能在各种模拟和真实的机器人平台上进行了广泛评估，包括无人机、四足动物、轮式机器人和模拟自动驾驶汽车。我们将与几种最先进的视觉惯性SLAM管道进行比较，并讨论新版本Kimera的优势和劣势。新添加的功能已在上开源发布https://github.com/MIT-SPARK/Kimera.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.06323v1" target="_blank">2401.06323v1</a>
                              </td>
                              <td>Kimera2: Robust and Accurate Metric-Semantic SLAM in the Real World</td>
                              <td>Marcus Abate</td>
                              <td>2024-01-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_06323v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.06323v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_13182v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-Camera Visual-Inertial Simultaneous Localization and Mapping for Autonomous Valet Parking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_13182v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_13182v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_13182v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Localization and mapping are key capabilities for self-driving vehicles. In this paper, we build on Kimera and extend it to use multiple cameras as well as external (eg wheel) odometry sensors, to obtain accurate and robust odometry estimates in real-world problems. Additionally, we propose an effective scheme for closing loops that circumvents the drawbacks of common alternatives based on the Perspective-n-Point method and also works with a single monocular camera. Finally, we develop a method for dense 3D mapping of the free space that combines a segmentation network for free-space detection with a homography-based dense mapping technique. We test our system on photo-realistic simulations and on several real datasets collected on a car prototype developed by the Ford Motor Company, spanning both indoor and outdoor parking scenarios. Our multi-camera system is shown to outperform state-of-the art open-source visual-inertial-SLAM pipelines (Vins-Fusion, ORB-SLAM3), and exhibits an average trajectory error under 1% of the trajectory length across more than 8km of distance traveled (combined across all datasets). A video showcasing the system is available at: youtu.be/H8CpzDpXOI8.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_13182v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>定位和地图绘制是自动驾驶汽车的关键功能。在本文中，我们建立在Kimera的基础上，并将其扩展到使用多个摄像头以及外部（如车轮）里程计传感器，以在现实世界的问题中获得准确和稳健的里程计估计。此外，我们提出了一种有效的闭环方案，该方案绕过了基于透视n-Point方法的常见替代方案的缺点，也适用于单个单眼相机。最后，我们开发了一种用于自由空间的密集3D映射的方法，该方法将用于自由空间检测的分割网络与基于单应性的密集映射技术相结合。我们在逼真的照片模拟和福特汽车公司开发的汽车原型上收集的几个真实数据集上测试了我们的系统，涵盖了室内和室外停车场景。我们的多摄像头系统显示出优于最先进的开源视觉惯性SLAM管道（Vins Fusion，ORB-SLAM3），并且在超过8km的行进距离上（在所有数据集上组合），平均轨迹误差低于轨迹长度的1%。展示该系统的视频可在以下网址获取：youtu.be/H8CpzDpXOI8。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.13182v3" target="_blank">2304.13182v3</a>
                              </td>
                              <td>Multi-Camera Visual-Inertial Simultaneous Localization and Mapping for Autonomous Valet Parking</td>
                              <td>Marcus Abate</td>
                              <td>2023-04-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_13182v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.13182v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05836v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On State Estimation in Multi-Sensor Fusion Navigation: Optimization and Filtering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05836v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05836v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05836v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The essential of navigation, perception, and decision-making which are basic tasks for intelligent robots, is to estimate necessary system states. Among them, navigation is fundamental for other upper applications, providing precise position and orientation, by integrating measurements from multiple sensors. With observations of each sensor appropriately modelled, multi-sensor fusion tasks for navigation are reduced to the state estimation problem which can be solved by two approaches: optimization and filtering. Recent research has shown that optimization-based frameworks outperform filtering-based ones in terms of accuracy. However, both methods are based on maximum likelihood estimation (MLE) and should be theoretically equivalent with the same linearization points, observation model, measurements, and Gaussian noise assumption. In this paper, we deeply dig into the theories and existing strategies utilized in both optimization-based and filtering-based approaches. It is demonstrated that the two methods are equal theoretically, but this equivalence corrupts due to different strategies applied in real-time operation. By adjusting existing strategies of the filtering-based approaches, the Monte-Carlo simulation and vehicular ablation experiments based on visual odometry (VO) indicate that the strategy adjusted filtering strictly equals to optimization. Therefore, future research on sensor-fusion problems should concentrate on their own algorithms and strategies rather than state estimation approaches.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05836v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>导航、感知和决策是智能机器人的基本任务，其本质是估计必要的系统状态。其中，导航是其他上层应用程序的基础，通过集成来自多个传感器的测量，提供精确的位置和方向。通过对每个传感器的观测值进行适当的建模，将导航的多传感器融合任务简化为状态估计问题，该问题可以通过两种方法解决：优化和滤波。最近的研究表明，基于优化的框架在准确性方面优于基于过滤的框架。然而，这两种方法都是基于最大似然估计（MLE）的，并且在理论上应该与相同的线性化点、观测模型、测量和高斯噪声假设等效。在本文中，我们深入挖掘了基于优化和基于过滤的方法中使用的理论和现有策略。结果表明，这两种方法在理论上是相等的，但由于在实时操作中应用的策略不同，这种等价性会破坏。通过调整现有的基于滤波的方法的策略，基于视觉里程计（VO）的蒙特卡洛模拟和车载消融实验表明，策略调整后的滤波严格等于优化。因此，未来对传感器融合问题的研究应该集中在它们自己的算法和策略上，而不是状态估计方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05836v1" target="_blank">2401.05836v1</a>
                              </td>
                              <td>On State Estimation in Multi-Sensor Fusion Navigation: Optimization and Filtering</td>
                              <td>Feng Zhu</td>
                              <td>2024-01-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05836v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05836v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05152v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi S-Graphs: an Efficient Real-time Distributed Semantic-Relational Collaborative SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05152v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05152v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05152v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Collaborative Simultaneous Localization and Mapping (CSLAM) is critical to enable multiple robots to operate in complex environments. Most CSLAM techniques rely on raw sensor measurement or low-level features such as keyframe descriptors, which can lead to wrong loop closures due to the lack of deep understanding of the environment. Moreover, the exchange of these measurements and low-level features among the robots requires the transmission of a significant amount of data, which limits the scalability of the system. To overcome these limitations, we present Multi S-Graphs, a decentralized CSLAM system that utilizes high-level semantic-relational information embedded in the four-layered hierarchical and optimizable situational graphs for cooperative map generation and localization while minimizing the information exchanged between the robots. To support this, we present a novel room-based descriptor which, along with its connected walls, is used to perform inter-robot loop closures, addressing the challenges of multi-robot kidnapped problem initialization. Multiple experiments in simulated and real environments validate the improvement in accuracy and robustness of the proposed approach while reducing the amount of data exchanged between robots compared to other state-of-the-art approaches.   Software available within a docker image: https://github.com/snt-arg/multi_s_graphs_docker</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05152v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>协作式同时定位和映射（CSLAM）对于使多个机器人能够在复杂环境中操作至关重要。大多数CSLAM技术依赖于原始传感器测量或关键帧描述符等低级特征，由于缺乏对环境的深入了解，这可能导致错误的环路闭合。此外，在机器人之间交换这些测量值和低级特征需要传输大量数据，这限制了系统的可扩展性。为了克服这些限制，我们提出了Multi-S-Graphs，这是一种去中心化的CSLAM系统，它利用嵌入四层分层和可优化的态势图中的高级语义关系信息进行协作地图生成和定位，同时最大限度地减少机器人之间的信息交换。为了支持这一点，我们提出了一种新的基于房间的描述符，该描述符及其连接的墙用于执行机器人间环路闭合，解决了多机器人绑架问题初始化的挑战。在模拟和真实环境中进行的多次实验验证了所提出的方法在准确性和稳健性方面的改进，同时与其他最先进的方法相比，减少了机器人之间交换的数据量。docker映像中可用的软件：https://github.com/snt-arg/multi_s_graphs_docker</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05152v1" target="_blank">2401.05152v1</a>
                              </td>
                              <td>Multi S-Graphs: an Efficient Real-time Distributed Semantic-Relational Collaborative SLAM</td>
                              <td>Miguel Fernandez-Cortizas</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05152v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05152v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/snt-arg/multi_s_graphs_docker" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_01657v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Distributed Pose-graph Optimization with Multi-level Partitioning for Collaborative SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_01657v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_01657v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_01657v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The back-end module of Distributed Collaborative Simultaneous Localization and Mapping (DCSLAM) requires solving a nonlinear Pose Graph Optimization (PGO) under a distributed setting, also known as SE(d)-synchronization. Most existing distributed graph optimization algorithms employ a simple sequential partitioning scheme, which may result in unbalanced subgraph dimensions due to the different geographic locations of each robot, and hence imposes extra communication load. Moreover, the performance of current Riemannian optimization algorithms can be further accelerated. In this letter, we propose a novel distributed pose graph optimization algorithm combining multi-level partitioning with an accelerated Riemannian optimization method. Firstly, we employ the multi-level graph partitioning algorithm to preprocess the naive pose graph to formulate a balanced optimization problem. In addition, inspired by the accelerated coordinate descent method, we devise an Improved Riemannian Block Coordinate Descent (IRBCD) algorithm and the critical point obtained is globally optimal. Finally, we evaluate the effects of four common graph partitioning approaches on the correlation of the inter-subgraphs, and discover that the Highest scheme has the best partitioning performance. Also, we implement simulations to quantitatively demonstrate that our proposed algorithm outperforms the state-of-the-art distributed pose graph optimization protocols.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_01657v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分布式协同同步定位与映射（DCSLAM）的后端模块需要在分布式环境下求解非线性姿态图优化（PGO），也称为SE（d）-同步。大多数现有的分布式图优化算法都采用了简单的顺序划分方案，由于每个机器人的地理位置不同，这可能会导致子图维度不平衡，从而增加额外的通信负载。此外，当前黎曼优化算法的性能可以进一步提高。在这封信中，我们提出了一种新的分布式位姿图优化算法，该算法将多级划分与加速黎曼优化方法相结合。首先，我们采用多级图分割算法对初始姿态图进行预处理，以形成一个平衡优化问题。此外，受加速坐标下降法的启发，我们设计了一种改进的黎曼块坐标下降（IRBCD）算法，得到的临界点是全局最优的。最后，我们评估了四种常见的图划分方法对子图间相关性的影响，发现Highest方案具有最好的划分性能。此外，我们还进行了仿真，定量地证明了我们提出的算法优于最先进的分布式姿态图优化协议。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.01657v2" target="_blank">2401.01657v2</a>
                              </td>
                              <td>Distributed Pose-graph Optimization with Multi-level Partitioning for Collaborative SLAM</td>
                              <td>Cunhao Li</td>
                              <td>2024-01-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_01657v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.01657v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/tjcunhao/distributed-pose-graph" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_04791v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SOS-SLAM: Segmentation for Open-Set SLAM in Unstructured Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_04791v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_04791v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_04791v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present a novel framework for open-set Simultaneous Localization and Mapping (SLAM) in unstructured environments that uses segmentation to create a map of objects and geometric relationships between objects for localization. Our system consists of 1) a front-end mapping pipeline using a zero-shot segmentation model to extract object masks from images and track them across frames to generate an object-based map and 2) a frame alignment pipeline that uses the geometric consistency of objects to efficiently localize within maps taken in a variety of conditions. This approach is shown to be more robust to changes in lighting and appearance than traditional feature-based SLAM systems or global descriptor methods. This is established by evaluating SOS-SLAM on the Batvik seasonal dataset which includes drone flights collected over a coastal plot of southern Finland during different seasons and lighting conditions. Across flights during varying environmental conditions, our approach achieves higher recall than benchmark methods with precision of 1.0. SOS-SLAM localizes within a reference map up to 14x faster than other feature based approaches and has a map size less than 0.4% the size of the most compact other maps. When considering localization performance from varying viewpoints, our approach outperforms all benchmarks from the same viewpoint and most benchmarks from different viewpoints. SOS-SLAM is a promising new approach for SLAM in unstructured environments that is robust to changes in lighting and appearance and is more computationally efficient than other approaches. We release our code and datasets: https://acl.mit.edu/SOS-SLAM/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_04791v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种在非结构化环境中用于开放集同时定位和映射（SLAM）的新框架，该框架使用分割来创建用于定位的对象和对象之间的几何关系的映射。我们的系统由1）前端映射管道和2）帧对齐管道组成，前者使用零样本分割模型从图像中提取对象掩码，并在帧间跟踪它们以生成基于对象的地图，后者使用对象的几何一致性在各种条件下拍摄的地图内有效定位。与传统的基于特征的SLAM系统或全局描述符方法相比，这种方法对照明和外观的变化更具鲁棒性。这是通过评估巴特维克季节数据集上的SOS-SLAM来建立的，该数据集包括在不同季节和光照条件下在芬兰南部沿海地区收集的无人机飞行。在不同环境条件下的飞行中，我们的方法实现了比基准方法更高的召回率，精度为1.0。SOS-SLAM在参考地图内的定位速度比其他基于特征的方法快14倍，并且地图大小小于最紧凑的其他地图大小的0.4%。当从不同的角度考虑本地化性能时，我们的方法优于来自同一角度的所有基准测试和来自不同角度的大多数基准测试。SOS-SLAM是非结构化环境中SLAM的一种很有前途的新方法，它对光照和外观的变化具有鲁棒性，并且在计算上比其他方法更高效。我们发布我们的代码和数据集：https://acl.mit.edu/SOS-SLAM/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.04791v1" target="_blank">2401.04791v1</a>
                              </td>
                              <td>SOS-SLAM: Segmentation for Open-Set SLAM in Unstructured Environments</td>
                              <td>Jouko Kinnari</td>
                              <td>2024-01-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_04791v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.04791v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_03398v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Amplifying robotics capacities with a human touch: An immersive low-latency panoramic remote system</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_03398v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_03398v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_03398v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>AI and robotics technologies have witnessed remarkable advancements in the past decade, revolutionizing work patterns and opportunities in various domains. The application of these technologies has propelled society towards an era of symbiosis between humans and machines. To facilitate efficient communication between humans and intelligent robots, we propose the "Avatar" system, an immersive low-latency panoramic human-robot interaction platform. We have designed and tested a prototype of a rugged mobile platform integrated with edge computing units, panoramic video capture devices, power batteries, robot arms, and network communication equipment. Under favorable network conditions, we achieved a low-latency high-definition panoramic visual experience with a delay of 357ms. Operators can utilize VR headsets and controllers for real-time immersive control of robots and devices. The system enables remote control over vast physical distances, spanning campuses, provinces, countries, and even continents (New York to Shenzhen). Additionally, the system incorporates visual SLAM technology for map and trajectory recording, providing autonomous navigation capabilities. We believe that this intuitive system platform can enhance efficiency and situational experience in human-robot collaboration, and with further advancements in related technologies, it will become a versatile tool for efficient and symbiotic cooperation between AI and humans.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_03398v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人工智能和机器人技术在过去十年中取得了显著进步，改变了各个领域的工作模式和机会。这些技术的应用将社会推向了一个人与机器共生的时代。为了促进人类与智能机器人之间的高效通信，我们提出了“阿凡达”系统，这是一个沉浸式低延迟全景人机交互平台。我们设计并测试了一个坚固的移动平台原型，该平台集成了边缘计算单元、全景视频捕获设备、动力电池、机械臂和网络通信设备。在良好的网络条件下，我们实现了延迟357ms的低延迟高清全景视觉体验。操作员可以利用VR耳机和控制器对机器人和设备进行实时沉浸式控制。该系统能够实现跨越校园、省份、国家甚至大洲（纽约到深圳）的远距离远程控制。此外，该系统结合了用于地图和轨迹记录的视觉SLAM技术，提供了自主导航功能。我们相信，这个直观的系统平台可以提高人机协作的效率和情景体验，随着相关技术的进一步进步，它将成为人工智能与人类高效共生合作的通用工具。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.03398v2" target="_blank">2401.03398v2</a>
                              </td>
                              <td>Amplifying robotics capacities with a human touch: An immersive low-latency panoramic remote system</td>
                              <td>Junjie Li</td>
                              <td>2024-01-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_03398v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.03398v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_11598v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Neural Implicit through Volume Rendering with Attentive Depth Fusion Priors</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_11598v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_11598v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_11598v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Learning neural implicit representations has achieved remarkable performance in 3D reconstruction from multi-view images. Current methods use volume rendering to render implicit representations into either RGB or depth images that are supervised by multi-view ground truth. However, rendering a view each time suffers from incomplete depth at holes and unawareness of occluded structures from the depth supervision, which severely affects the accuracy of geometry inference via volume rendering. To resolve this issue, we propose to learn neural implicit representations from multi-view RGBD images through volume rendering with an attentive depth fusion prior. Our prior allows neural networks to perceive coarse 3D structures from the Truncated Signed Distance Function (TSDF) fused from all depth images available for rendering. The TSDF enables accessing the missing depth at holes on one depth image and the occluded parts that are invisible from the current view. By introducing a novel attention mechanism, we allow neural networks to directly use the depth fusion prior with the inferred occupancy as the learned implicit function. Our attention mechanism works with either a one-time fused TSDF that represents a whole scene or an incrementally fused TSDF that represents a partial scene in the context of Simultaneous Localization and Mapping (SLAM). Our evaluations on widely used benchmarks including synthetic and real-world scans show our superiority over the latest neural implicit methods. Project page: https://machineperceptionlab.github.io/Attentive_DF_Prior/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_11598v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>学习神经隐式表示在多视图图像的三维重建中取得了显著的性能。当前的方法使用体积渲染将隐式表示渲染为RGB或深度图像，这些图像由多视图地面实况监督。然而，每次渲染视图都会遇到孔的深度不完整以及深度监督对遮挡结构的不了解，这严重影响了通过体绘制进行几何推断的准确性。为了解决这个问题，我们建议通过具有注意深度融合先验的体绘制，从多视图RGBD图像中学习神经隐式表示。我们的先验允许神经网络从从可用于渲染的所有深度图像中融合的截断有符号距离函数（TSDF）中感知粗略的3D结构。TSDF能够访问一个深度图像上孔的缺失深度以及当前视图中不可见的遮挡部分。通过引入一种新的注意力机制，我们允许神经网络直接使用具有推断占用率的深度融合先验作为学习的隐函数。我们的注意力机制与表示整个场景的一次性融合TSDF或表示同步定位和映射（SLAM）上下文中的部分场景的增量融合TSDF一起工作。我们对广泛使用的基准（包括合成扫描和真实世界扫描）的评估表明，我们优于最新的神经隐式方法。项目页面：https://machineperceptionlab.github.io/Attentive_DF_Prior/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.11598v2" target="_blank">2310.11598v2</a>
                              </td>
                              <td>Learning Neural Implicit through Volume Rendering with Attentive Depth Fusion Priors</td>
                              <td>Pengchong Hu</td>
                              <td>2023-10-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_11598v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.11598v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_03604v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Amirkabir campus dataset: Real-world challenges and scenarios of Visual Inertial Odometry (VIO) for visually impaired people</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_03604v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_03604v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_03604v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual Inertial Odometry (VIO) algorithms estimate the accurate camera trajectory by using camera and Inertial Measurement Unit (IMU) sensors. The applications of VIO span a diverse range, including augmented reality and indoor navigation. VIO algorithms hold the potential to facilitate navigation for visually impaired individuals in both indoor and outdoor settings. Nevertheless, state-of-the-art VIO algorithms encounter substantial challenges in dynamic environments, particularly in densely populated corridors. Existing VIO datasets, e.g., ADVIO, typically fail to effectively exploit these challenges. In this paper, we introduce the Amirkabir campus dataset (AUT-VI) to address the mentioned problem and improve the navigation systems. AUT-VI is a novel and super-challenging dataset with 126 diverse sequences in 17 different locations. This dataset contains dynamic objects, challenging loop-closure/map-reuse, different lighting conditions, reflections, and sudden camera movements to cover all extreme navigation scenarios. Moreover, in support of ongoing development efforts, we have released the Android application for data capture to the public. This allows fellow researchers to easily capture their customized VIO dataset variations. In addition, we evaluate state-of-the-art Visual Inertial Odometry (VIO) and Visual Odometry (VO) methods on our dataset, emphasizing the essential need for this challenging dataset.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_03604v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉惯性里程计（VIO）算法通过使用相机和惯性测量单元（IMU）传感器来估计精确的相机轨迹。VIO的应用范围广泛，包括增强现实和室内导航。VIO算法有可能促进视障人士在室内和室外环境中的导航。然而，最先进的VIO算法在动态环境中，特别是在人口稠密的走廊中，遇到了巨大的挑战。现有的VIO数据集，例如ADVIO，通常无法有效利用这些挑战。在本文中，我们引入了Amirkabir校园数据集（AUT-VI）来解决上述问题并改进导航系统。AUT-VI是一个新颖且极具挑战性的数据集，包含17个不同位置的126个不同序列。该数据集包含动态对象、具有挑战性的回路闭合/地图重用、不同的照明条件、反射和相机突然移动，以覆盖所有极端导航场景。此外，为了支持正在进行的开发工作，我们向公众发布了用于数据捕获的Android应用程序。这使得其他研究人员能够轻松地捕捉他们定制的VIO数据集变体。此外，我们在数据集上评估了最先进的视觉惯性里程计（VIO）和视觉里程计（VO）方法，强调了对这一具有挑战性的数据集的必要性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.03604v1" target="_blank">2401.03604v1</a>
                              </td>
                              <td>Amirkabir campus dataset: Real-world challenges and scenarios of Visual Inertial Odometry (VIO) for visually impaired people</td>
                              <td>Ali Samadzadeh</td>
                              <td>2024-01-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_03604v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.03604v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/A3DV/VIRec" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02816v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Comparative Evaluation of RGB-D SLAM Methods for Humanoid Robot Localization and Mapping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02816v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02816v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02816v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we conducted a comparative evaluation of three RGB-D SLAM (Simultaneous Localization and Mapping) algorithms: RTAB-Map, ORB-SLAM3, and OpenVSLAM for SURENA-V humanoid robot localization and mapping. Our test involves the robot to follow a full circular pattern, with an Intel RealSense D435 RGB-D camera installed on its head. In assessing localization accuracy, ORB-SLAM3 outperformed the others with an ATE of 0.1073, followed by RTAB-Map at 0.1641 and OpenVSLAM at 0.1847. However, it should be noted that both ORB-SLAM3 and OpenVSLAM faced challenges in maintaining accurate odometry when the robot encountered a wall with limited feature points. Nevertheless, OpenVSLAM demonstrated the ability to detect loop closures and successfully relocalize itself within the map when the robot approached its initial location. The investigation also extended to mapping capabilities, where RTAB-Map excelled by offering diverse mapping outputs, including dense, OctoMap, and occupancy grid maps. In contrast, both ORB-SLAM3 and OpenVSLAM provided only sparse maps.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02816v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们对用于SURENA-V人形机器人定位和映射的三种RGB-D SLAM（同时定位和映射）算法：RTAB-Map、ORB-SLAM3和OpenVSLAM进行了比较评估。我们的测试涉及机器人遵循全圆形模式，其头部安装了Intel RealSense D435 RGB-D相机。在评估定位精度方面，ORB-SLAM3的ATE为0.1073，优于其他公司，其次是RTAB-Map，为0.1641，OpenVSLAM为0.1847。然而，应该注意的是，当机器人遇到具有有限特征点的墙壁时，ORB-SLAM3和OpenVSLAM在保持准确的里程测量方面都面临挑战。尽管如此，OpenVSLAM展示了检测环路闭合的能力，并在机器人接近其初始位置时成功地在地图内重新定位自己。调查还扩展到了地图功能，RTAB Map在地图功能方面表现出色，提供了多种地图输出，包括密集、OctoMap和占用网格地图。相比之下，ORB-SLAM3和OpenVSLAM都只提供了稀疏映射。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02816v1" target="_blank">2401.02816v1</a>
                              </td>
                              <td>Comparative Evaluation of RGB-D SLAM Methods for Humanoid Robot Localization and Mapping</td>
                              <td>Amirhosein Vedadi</td>
                              <td>2024-01-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02816v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02816v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_01081v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PLE-SLAM: A Visual-Inertial SLAM Based on Point-Line Features and Efficient IMU Initialization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_01081v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_01081v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_01081v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual-inertial SLAM is crucial in various fields, such as aerial vehicles, industrial robots, and autonomous driving. The fusion of camera and inertial measurement unit (IMU) makes up for the shortcomings of a signal sensor, which significantly improves the accuracy and robustness of localization in challenging environments. This article presents PLE-SLAM, an accurate and real-time visual-inertial SLAM algorithm based on point-line features and efficient IMU initialization. First, we use parallel computing methods to extract features and compute descriptors to ensure real-time performance. Adjacent short line segments are merged into long line segments, and isolated short line segments are directly deleted. Second, a rotation-translation-decoupled initialization method is extended to use both points and lines. Gyroscope bias is optimized by tightly coupling IMU measurements and image observations. Accelerometer bias and gravity direction are solved by an analytical method for efficiency. To improve the system's intelligence in handling complex environments, a scheme of leveraging semantic information and geometric constraints to eliminate dynamic features and A solution for loop detection and closed-loop frame pose estimation using CNN and GNN are integrated into the system. All networks are accelerated to ensure real-time performance. The experiment results on public datasets illustrate that PLE-SLAM is one of the state-of-the-art visual-inertial SLAM systems.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_01081v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉惯性SLAM在飞行器、工业机器人和自动驾驶等各个领域都至关重要。相机和惯性测量单元（IMU）的融合弥补了信号传感器的不足，显著提高了在具有挑战性的环境中定位的准确性和鲁棒性。本文提出了一种基于点线特征和有效IMU初始化的精确实时视觉惯性SLAM算法PLE-SLAM。首先，我们使用并行计算方法来提取特征并计算描述符，以确保实时性能。相邻的短线段合并为长线段，孤立的短线段直接删除。其次，将旋转-平移解耦初始化方法扩展为同时使用点和线。陀螺仪偏置通过紧密耦合IMU测量和图像观测进行优化。加速度计偏置和重力方向通过效率分析方法求解。为了提高系统在处理复杂环境中的智能性，将一种利用语义信息和几何约束来消除动态特征的方案以及一种使用CNN和GNN的环路检测和闭环帧姿态估计的解决方案集成到系统中。所有网络都被加速以确保实时性能。在公共数据集上的实验结果表明，PLE-SLAM是最先进的视觉惯性SLAM系统之一。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.01081v2" target="_blank">2401.01081v2</a>
                              </td>
                              <td>PLE-SLAM: A Visual-Inertial SLAM Based on Point-Line Features and Efficient IMU Initialization</td>
                              <td>Jiaming He</td>
                              <td>2024-01-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_01081v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.01081v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/hjmgarmin/ple-slam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11700v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11700v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11700v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11700v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we introduce $\textbf{GS-SLAM}$ that first utilizes 3D Gaussian representation in the Simultaneous Localization and Mapping (SLAM) system. It facilitates a better balance between efficiency and accuracy. Compared to recent SLAM methods employing neural implicit representations, our method utilizes a real-time differentiable splatting rendering pipeline that offers significant speedup to map optimization and RGB-D re-rendering. Specifically, we propose an adaptive expansion strategy that adds new or deletes noisy 3D Gaussian in order to efficiently reconstruct new observed scene geometry and improve the mapping of previously observed areas. This strategy is essential to extend 3D Gaussian representation to reconstruct the whole scene rather than synthesize a static object in existing methods. Moreover, in the pose tracking process, an effective coarse-to-fine technique is designed to select reliable 3D Gaussian representations to optimize camera pose, resulting in runtime reduction and robust estimation. Our method achieves competitive performance compared with existing state-of-the-art real-time methods on the Replica, TUM-RGBD datasets. The source code will be released soon.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11700v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们介绍了$\textbf{GS-SLAM}$，它首先在同步定位和映射（SLAM）系统中使用3D高斯表示。它有助于更好地平衡效率和准确性。与最近使用神经隐式表示的SLAM方法相比，我们的方法使用了实时可微分的飞溅渲染管道，大大加快了地图优化和RGB-D重新渲染的速度。具体而言，我们提出了一种自适应扩展策略，该策略添加新的或删除有噪声的3D高斯，以有效地重建新的观测场景几何结构并改进先前观测区域的映射。该策略对于扩展3D高斯表示以重建整个场景而不是在现有方法中合成静态对象至关重要。此外，在姿态跟踪过程中，设计了一种有效的从粗到细的技术来选择可靠的3D高斯表示来优化相机姿态，从而减少了运行时间并实现了稳健的估计。与Replica、TUM-RGBD数据集上现有的最先进的实时方法相比，我们的方法实现了具有竞争力的性能。源代码将很快发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11700v3" target="_blank">2311.11700v3</a>
                              </td>
                              <td>GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting</td>
                              <td>Chi Yan</td>
                              <td>2023-11-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11700v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11700v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_01887v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LEAP-VO: Long-term Effective Any Point Tracking for Visual Odometry</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_01887v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_01887v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_01887v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual odometry estimates the motion of a moving camera based on visual input. Existing methods, mostly focusing on two-view point tracking, often ignore the rich temporal context in the image sequence, thereby overlooking the global motion patterns and providing no assessment of the full trajectory reliability. These shortcomings hinder performance in scenarios with occlusion, dynamic objects, and low-texture areas. To address these challenges, we present the Long-term Effective Any Point Tracking (LEAP) module. LEAP innovatively combines visual, inter-track, and temporal cues with mindfully selected anchors for dynamic track estimation. Moreover, LEAP's temporal probabilistic formulation integrates distribution updates into a learnable iterative refinement module to reason about point-wise uncertainty. Based on these traits, we develop LEAP-VO, a robust visual odometry system adept at handling occlusions and dynamic scenes. Our mindful integration showcases a novel practice by employing long-term point tracking as the front-end. Extensive experiments demonstrate that the proposed pipeline significantly outperforms existing baselines across various visual odometry benchmarks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_01887v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉里程计基于视觉输入来估计移动的相机的运动。现有的方法大多侧重于双视点跟踪，往往忽略了图像序列中丰富的时间上下文，从而忽略了全局运动模式，并且没有提供对完整轨迹可靠性的评估。这些缺点阻碍了在具有遮挡、动态对象和低纹理区域的场景中的性能。为了应对这些挑战，我们提出了长期有效的任意点跟踪（LEAP）模块。LEAP创新地将视觉、轨迹间和时间线索与精心选择的锚点相结合，用于动态轨迹估计。此外，LEAP的时间概率公式将分布更新集成到可学习的迭代精化模块中，以推理逐点不确定性。基于这些特点，我们开发了LEAP-VO，这是一种强大的视觉里程计系统，擅长处理遮挡和动态场景。我们的专注集成展示了一种新颖的做法，将长期点跟踪作为前端。大量实验表明，在各种视觉里程计基准测试中，所提出的管道显著优于现有基线。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.01887v1" target="_blank">2401.01887v1</a>
                              </td>
                              <td>LEAP-VO: Long-term Effective Any Point Tracking for Visual Odometry</td>
                              <td>Weirong Chen</td>
                              <td>2024-01-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_01887v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.01887v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_01545v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DDN-SLAM: Real-time Dense Dynamic Neural Implicit SLAM with Joint Semantic Encoding</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_01545v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_01545v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_01545v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose DDN-SLAM, a real-time dense neural implicit semantic SLAM system designed for dynamic scenes. While existing neural implicit SLAM systems perform well in static scenes, they often encounter challenges in real-world environments with dynamic interferences, leading to ineffective tracking and mapping. DDN-SLAM utilizes the priors provided by the deep semantic system, combined with conditional probability fields, for segmentation.By constructing depth-guided static masks and employing joint multi-resolution hashing encoding, we ensure fast hole filling and high-quality mapping while mitigating the effects of dynamic information interference. To enhance tracking robustness, we utilize sparse feature points validated with optical flow and keyframes, enabling loop closure detection and global bundle optimization. Furthermore, DDN-SLAM supports monocular, stereo, and RGB-D inputs, operating robustly at a frequency of 20-30Hz. Extensive experiments on 6 virtual/real datasets demonstrate that our method outperforms state-of-the-art approaches in both dynamic and static scenes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_01545v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了DDN-SLAM，一个针对动态场景设计的实时密集神经隐式语义SLAM系统。虽然现有的神经隐式SLAM系统在静态场景中表现良好，但它们在具有动态干扰的真实世界环境中经常遇到挑战，导致跟踪和映射无效。DDN-SLAM利用深度语义系统提供的先验，结合条件概率场进行分割。通过构建深度引导的静态掩模并采用联合多分辨率哈希编码，我们确保了快速的孔洞填充和高质量的映射，同时减轻了动态信息干扰的影响。为了增强跟踪鲁棒性，我们利用经过光流和关键帧验证的稀疏特征点，实现闭环检测和全局束优化。此外，DDN-SLAM支持单眼、立体声和RGB-D输入，在20-30Hz的频率下稳定工作。在6个虚拟/真实数据集上进行的大量实验表明，我们的方法在动态和静态场景中都优于最先进的方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.01545v1" target="_blank">2401.01545v1</a>
                              </td>
                              <td>DDN-SLAM: Real-time Dense Dynamic Neural Implicit SLAM with Joint Semantic Encoding</td>
                              <td>Mingrui Li</td>
                              <td>2024-01-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_01545v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.01545v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_01189v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NID-SLAM: Neural Implicit Representation-based RGB-D SLAM in dynamic environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_01189v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_01189v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_01189v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural implicit representations have been explored to enhance visual SLAM algorithms, especially in providing high-fidelity dense map. Existing methods operate robustly in static scenes but struggle with the disruption caused by moving objects. In this paper we present NID-SLAM, which significantly improves the performance of neural SLAM in dynamic environments. We propose a new approach to enhance inaccurate regions in semantic masks, particularly in marginal areas. Utilizing the geometric information present in depth images, this method enables accurate removal of dynamic objects, thereby reducing the probability of camera drift. Additionally, we introduce a keyframe selection strategy for dynamic scenes, which enhances camera tracking robustness against large-scale objects and improves the efficiency of mapping. Experiments on publicly available RGB-D datasets demonstrate that our method outperforms competitive neural SLAM approaches in tracking accuracy and mapping quality in dynamic environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_01189v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>已经探索了神经隐式表示来增强视觉SLAM算法，特别是在提供高保真密集地图方面。现有的方法在静态场景中运行稳健，但难以应对移动对象造成的干扰。在本文中，我们提出了NID-SLAM，它显著提高了神经SLAM在动态环境中的性能。我们提出了一种新的方法来增强语义掩码中的不准确区域，特别是在边缘区域。利用深度图像中存在的几何信息，该方法能够准确地去除动态对象，从而降低相机漂移的概率。此外，我们还引入了一种动态场景的关键帧选择策略，该策略增强了相机对大型对象的跟踪鲁棒性，并提高了映射效率。在公开的RGB-D数据集上的实验表明，我们的方法在动态环境中的跟踪精度和映射质量方面优于竞争性的神经SLAM方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.01189v1" target="_blank">2401.01189v1</a>
                              </td>
                              <td>NID-SLAM: Neural Implicit Representation-based RGB-D SLAM in dynamic environments</td>
                              <td>Ziheng Xu</td>
                              <td>2024-01-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_01189v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.01189v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07607v4_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SubT-MRS Dataset: Pushing SLAM Towards All-weather Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07607v4_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07607v4_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07607v4_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localization and mapping (SLAM) is a fundamental task for numerous applications such as autonomous navigation and exploration. Despite many SLAM datasets have been released, current SLAM solutions still struggle to have sustained and resilient performance. One major issue is the absence of high-quality datasets including diverse all-weather conditions and a reliable metric for assessing robustness. This limitation significantly restricts the scalability and generalizability of SLAM technologies, impacting their development, validation, and deployment. To address this problem, we present SubT-MRS, an extremely challenging real-world dataset designed to push SLAM towards all-weather environments to pursue the most robust SLAM performance. It contains multi-degraded environments including over 30 diverse scenes such as structureless corridors, varying lighting conditions, and perceptual obscurants like smoke and dust; multimodal sensors such as LiDAR, fisheye camera, IMU, and thermal camera; and multiple locomotions like aerial, legged, and wheeled robots. We develop accuracy and robustness evaluation tracks for SLAM and introduced novel robustness metrics. Comprehensive studies are performed, revealing new observations, challenges, and opportunities for future research.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07607v4_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和测绘（SLAM）是许多应用（如自主导航和勘探）的基本任务。尽管已经发布了许多SLAM数据集，但当前的SLAM解决方案仍难以获得持续和有弹性的性能。一个主要问题是缺乏高质量的数据集，包括不同的全天候条件和评估稳健性的可靠指标。这种限制极大地限制了SLAM技术的可扩展性和可推广性，影响了它们的开发、验证和部署。为了解决这个问题，我们提出了SubT-MRS，这是一个极具挑战性的真实世界数据集，旨在将SLAM推向全天候环境，以追求最稳健的SLAM性能。它包含了多种退化环境，包括30多个不同的场景，如无结构走廊、不同的照明条件和烟雾和灰尘等感知障碍物；多模式传感器，如激光雷达、鱼眼相机、IMU和热像仪；以及多种运动方式，如空中机器人、腿式机器人和轮式机器人。我们为SLAM开发了准确性和稳健性评估跟踪，并引入了新的稳健性度量。进行了全面的研究，揭示了新的观察结果、挑战和未来研究的机会。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07607v4" target="_blank">2307.07607v4</a>
                              </td>
                              <td>SubT-MRS Dataset: Pushing SLAM Towards All-weather Environments</td>
                              <td>Shibo Zhao</td>
                              <td>2023-07-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07607v4_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07607v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_17110v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Toward Semantic Scene Understanding for Fine-Grained 3D Modeling of Plants</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_17110v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_17110v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_17110v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Agricultural robotics is an active research area due to global population growth and expectations of food and labor shortages. Robots can potentially help with tasks such as pruning, harvesting, phenotyping, and plant modeling. However, agricultural automation is hampered by the difficulty in creating high resolution 3D semantic maps in the field that would allow for safe manipulation and navigation. In this paper, we build toward solutions for this issue and showcase how the use of semantics and environmental priors can help in constructing accurate 3D maps for the target application of sorghum. Specifically, we 1) use sorghum seeds as semantic landmarks to build a visual Simultaneous Localization and Mapping (SLAM) system that enables us to map 78\\% of a sorghum range on average, compared to 38% with ORB-SLAM2; and 2) use seeds as semantic features to improve 3D reconstruction of a full sorghum panicle from images taken by a robotic in-hand camera.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_17110v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于全球人口增长以及对粮食和劳动力短缺的预期，农业机器人是一个活跃的研究领域。机器人可能有助于完成修剪、收割、表型分析和植物建模等任务。然而，农业自动化受到阻碍，因为难以在该领域创建高分辨率3D语义地图，从而实现安全操作和导航。在本文中，我们致力于解决这一问题，并展示了语义和环境先验的使用如何帮助为高粱的目标应用构建准确的3D地图。具体而言，我们1）使用高粱种子作为语义地标来构建视觉同步定位和映射（SLAM）系统，该系统使我们能够平均映射78%的高粱范围，而ORB-SLAM2的映射率为38%；和2）使用种子作为语义特征来改进由机器人手持相机拍摄的图像对完整高粱穗的3D重建。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.17110v1" target="_blank">2312.17110v1</a>
                              </td>
                              <td>Toward Semantic Scene Understanding for Fine-Grained 3D Modeling of Plants</td>
                              <td>Mohamad Qadri</td>
                              <td>2023-12-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_17110v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.17110v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_16800v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SR-LIVO: LiDAR-Inertial-Visual Odometry and Mapping with Sweep Reconstruction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_16800v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_16800v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_16800v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Existing LiDAR-inertial-visual odometry and mapping (LIV-SLAM) systems mainly utilize the LiDAR-inertial odometry (LIO) module for structure reconstruction and the visual-inertial odometry (VIO) module for color rendering. However, the accuracy of VIO is often compromised by photometric changes, weak textures and motion blur, unlike the more robust LIO. This paper introduces SR-LIVO, an advanced and novel LIV-SLAM system employing sweep reconstruction to align reconstructed sweeps with image timestamps. This allows the LIO module to accurately determine states at all imaging moments, enhancing pose accuracy and processing efficiency. Experimental results on two public datasets demonstrate that: 1) our SRLIVO outperforms existing state-of-the-art LIV-SLAM systems in both pose accuracy and time efficiency; 2) our LIO-based pose estimation prove more accurate than VIO-based ones in several mainstream LIV-SLAM systems (including ours). We have released our source code to contribute to the community development in this field.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_16800v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现有的激光雷达惯性视觉里程计和测绘（LIV-SLAM）系统主要利用激光雷达惯性里程计（LIO）模块进行结构重建和视觉惯性里程计模块进行颜色渲染。然而，与更稳健的LIO不同，VIO的准确性经常受到光度变化、弱纹理和运动模糊的影响。本文介绍了SR-LIVO，这是一种先进新颖的LIV-SLAM系统，它采用扫描重建来将重建的扫描与图像时间戳对齐。这使得LIO模块能够准确地确定所有成像时刻的状态，从而提高姿态精度和处理效率。在两个公共数据集上的实验结果表明：1）我们的SRLIVO在姿态精度和时间效率方面都优于现有最先进的LIV-SLAM系统；2） 在几个主流的LIV-SLAM系统（包括我们的系统）中，我们基于LIO的姿态估计被证明比基于VIO的姿态估算更准确。我们已经发布了我们的源代码，为该领域的社区开发做出贡献。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.16800v1" target="_blank">2312.16800v1</a>
                              </td>
                              <td>SR-LIVO: LiDAR-Inertial-Visual Odometry and Mapping with Sweep Reconstruction</td>
                              <td>Zikang Yuan</td>
                              <td>2023-12-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_16800v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.16800v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/ZikangYuan/sr_livo" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_06141v5_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Active Semantic Localization with Graph Neural Embedding</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_06141v5_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_06141v5_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_06141v5_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Semantic localization, i.e., robot self-localization with semantic image modality, is critical in recently emerging embodied AI applications (e.g., point-goal navigation, object-goal navigation, vision language navigation) and topological mapping applications (e.g., graph neural SLAM, ego-centric topological map). However, most existing works on semantic localization focus on passive vision tasks without viewpoint planning, or rely on additional rich modalities (e.g., depth measurements). Thus, the problem is largely unsolved. In this work, we explore a lightweight, entirely CPU-based, domain-adaptive semantic localization framework, called graph neural localizer. Our approach is inspired by two recently emerging technologies: (1) Scene graph, which combines the viewpoint- and appearance- invariance of local and global features; (2) Graph neural network, which enables direct learning/recognition of graph data (i.e., non-vector data). Specifically, a graph convolutional neural network is first trained as a scene graph classifier for passive vision, and then its knowledge is transferred to a reinforcement-learning planner for active vision. Experiments on two scenarios, self-supervised learning and unsupervised domain adaptation, using a photo-realistic Habitat simulator validate the effectiveness of the proposed method.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_06141v5_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语义定位，即具有语义图像模态的机器人自我定位，在最近出现的嵌入式人工智能应用（如点目标导航、对象目标导航、视觉语言导航）和拓扑映射应用（如图神经SLAM、以自我为中心的拓扑图）中至关重要。然而，大多数现有的语义定位工作都集中在被动视觉任务上，而没有视点规划，或者依赖于额外的丰富模式（例如，深度测量）。因此，这个问题在很大程度上没有得到解决。在这项工作中，我们探索了一种轻量级的、完全基于CPU的、领域自适应的语义定位框架，称为图神经定位器。我们的方法受到了两种最近出现的技术的启发：（1）场景图，它结合了局部和全局特征的视点和外观不变性；（2） 图形神经网络，能够直接学习/识别图形数据（即非矢量数据）。具体来说，首先将图卷积神经网络训练为被动视觉的场景图分类器，然后将其知识转移到主动视觉的强化学习规划器中。在自监督学习和无监督领域自适应两种场景下，使用照片逼真的Habitat模拟器进行实验，验证了所提出方法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.06141v5" target="_blank">2305.06141v5</a>
                              </td>
                              <td>Active Semantic Localization with Graph Neural Embedding</td>
                              <td>Mitsuki Yoshida</td>
                              <td>2023-05-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_06141v5_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.06141v5" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07763v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Tightly-Coupled LiDAR-Visual SLAM Based on Geometric Features for Mobile Agents</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07763v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07763v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07763v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The mobile robot relies on SLAM (Simultaneous Localization and Mapping) to provide autonomous navigation and task execution in complex and unknown environments. However, it is hard to develop a dedicated algorithm for mobile robots due to dynamic and challenging situations, such as poor lighting conditions and motion blur. To tackle this issue, we propose a tightly-coupled LiDAR-visual SLAM based on geometric features, which includes two sub-systems (LiDAR and monocular visual SLAM) and a fusion framework. The fusion framework associates the depth and semantics of the multi-modal geometric features to complement the visual line landmarks and to add direction optimization in Bundle Adjustment (BA). This further constrains visual odometry. On the other hand, the entire line segment detected by the visual subsystem overcomes the limitation of the LiDAR subsystem, which can only perform the local calculation for geometric features. It adjusts the direction of linear feature points and filters out outliers, leading to a higher accurate odometry system. Finally, we employ a module to detect the subsystem's operation, providing the LiDAR subsystem's output as a complementary trajectory to our system while visual subsystem tracking fails. The evaluation results on the public dataset M2DGR, gathered from ground robots across various indoor and outdoor scenarios, show that our system achieves more accurate and robust pose estimation compared to current state-of-the-art multi-modal methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07763v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>移动机器人依靠SLAM（同步定位和映射）在复杂和未知的环境中提供自主导航和任务执行。然而，由于动态和具有挑战性的情况，例如较差的照明条件和运动模糊，很难为移动机器人开发专用算法。为了解决这个问题，我们提出了一种基于几何特征的紧密耦合激光雷达视觉SLAM，它包括两个子系统（激光雷达和单目视觉SLAM）和一个融合框架。融合框架将多模态几何特征的深度和语义相关联，以补充视觉线地标，并在束调整（BA）中添加方向优化。这进一步限制了视觉里程计。另一方面，视觉子系统检测到的整个线段克服了激光雷达子系统只能对几何特征进行局部计算的局限性。它调整线性特征点的方向并过滤掉异常值，从而实现更高精度的里程计系统。最后，我们使用一个模块来检测子系统的操作，在视觉子系统跟踪失败时，将激光雷达子系统的输出作为我们系统的补充轨迹。从各种室内和室外场景的地面机器人收集的公共数据集M2DGR的评估结果表明，与当前最先进的多模态方法相比，我们的系统实现了更准确、更稳健的姿态估计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07763v3" target="_blank">2307.07763v3</a>
                              </td>
                              <td>Tightly-Coupled LiDAR-Visual SLAM Based on Geometric Features for Mobile Agents</td>
                              <td>Ke Cao</td>
                              <td>2023-07-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07763v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07763v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_15679v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BDIS-SLAM: A lightweight CPU-based dense stereo SLAM for surgery</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_15679v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_15679v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_15679v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Purpose: Common dense stereo Simultaneous Localization and Mapping (SLAM) approaches in Minimally Invasive Surgery (MIS) require high-end parallel computational resources for real-time implementation. Yet, it is not always feasible since the computational resources should be allocated to other tasks like segmentation, detection, and tracking. To solve the problem of limited parallel computational power, this research aims at a lightweight dense stereo SLAM system that works on a single-core CPU and achieves real-time performance (more than 30 Hz in typical scenarios). Methods: A new dense stereo mapping module is integrated with the ORB-SLAM2 system and named BDIS-SLAM. Our new dense stereo mapping module includes stereo matching and 3D dense depth mosaic methods. Stereo matching is achieved with the recently proposed CPU-level real-time matching algorithm Bayesian Dense Inverse Searching (BDIS). A BDIS-based shape recovery and a depth mosaic strategy are integrated as a new thread and coupled with the backbone ORB-SLAM2 system for real-time stereo shape recovery. Results: Experiments on in-vivo data sets show that BDIS-SLAM runs at over 30 Hz speed on modern single-core CPU in typical endoscopy/colonoscopy scenarios. BDIS-SLAM only consumes around an additional 12% time compared with the backbone ORB-SLAM2. Although our lightweight BDIS-SLAM simplifies the process by ignoring deformation and fusion procedures, it can provide a usable dense mapping for modern MIS on computationally constrained devices. Conclusion: The proposed BDIS-SLAM is a lightweight stereo dense SLAM system for MIS. It achieves 30 Hz on a modern single-core CPU in typical endoscopy/colonoscopy scenarios (image size around 640*480). BDIS-SLAM provides a low-cost solution for dense mapping in MIS and has the potential to be applied in surgical robots and AR systems.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_15679v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>目的：微创外科（MIS）中常见的密集立体同时定位和映射（SLAM）方法需要高端并行计算资源才能实时实现。然而，这并不总是可行的，因为计算资源应该分配给其他任务，如分割、检测和跟踪。为了解决并行计算能力有限的问题，本研究旨在开发一种在单核CPU上工作并实现实时性能（在典型场景中超过30 Hz）的轻量级密集立体声SLAM系统。方法：将一种新的稠密立体映射模块与ORB-SLAM2系统集成，命名为BDIS-SLAM。我们新的密集立体映射模块包括立体匹配和3D密集深度镶嵌方法。最近提出的CPU级实时匹配算法贝叶斯密集逆搜索（BDIS）实现了立体匹配。基于BDIS的形状恢复和深度镶嵌策略被集成为一个新的线程，并与骨干ORB-SLAM2系统耦合，用于实时立体形状恢复。结果：在体内数据集上的实验表明，在典型的内窥镜/结肠镜检查场景中，BDIS-SLAM在现代单核CPU上以超过30Hz的速度运行。与骨干ORB-SLAM2相比，BDIS-SLAM仅额外消耗约12%的时间。尽管我们的轻量级BDIS-SLAM通过忽略变形和融合过程简化了过程，但它可以在计算受限的设备上为现代MIS提供可用的密集映射。结论：BDIS-SLAM是一种用于MIS的轻量级立体密集SLAM系统。在典型的内窥镜/结肠镜检查场景中，它在现代单核CPU上实现了30 Hz（图像大小约为640*480）。BDIS-SLAM为MIS中的密集映射提供了一种低成本的解决方案，并有可能应用于外科机器人和AR系统。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.15679v1" target="_blank">2312.15679v1</a>
                              </td>
                              <td>BDIS-SLAM: A lightweight CPU-based dense stereo SLAM for surgery</td>
                              <td>Jingwei Song</td>
                              <td>2023-12-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_15679v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.15679v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/jingweisong/bdis-slam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_11310v4_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Twilight SLAM: Navigating Low-Light Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_11310v4_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_11310v4_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_11310v4_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a detailed examination of low-light visual Simultaneous Localization and Mapping (SLAM) pipelines, focusing on the integration of state-of-the-art (SOTA) low-light image enhancement algorithms with standard and contemporary SLAM frameworks. The primary objective of our work is to address a pivotal question: Does illuminating visual input significantly improve localization accuracy in both semi-dark and dark environments? In contrast to previous works that primarily address partially dim-lit datasets, we comprehensively evaluate various low-light SLAM pipelines across obscurely-lit environments. Employing a meticulous experimental approach, we qualitatively and quantitatively assess different combinations of image enhancers and SLAM frameworks, identifying the best-performing combinations for feature-based visual SLAM. The findings advance low-light SLAM by highlighting the practical implications of enhancing visual input for improved localization accuracy in challenging lighting conditions. This paper also offers valuable insights, encouraging further exploration of visual enhancement strategies for enhanced SLAM performance in real-world scenarios.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_11310v4_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文详细研究了微光视觉同步定位和映射（SLAM）管道，重点是将最先进的微光图像增强算法与标准和现代SLAM框架相结合。我们工作的主要目标是解决一个关键问题：在半暗和暗环境中，照明视觉输入是否能显著提高定位精度？与之前主要处理部分昏暗数据集的工作相比，我们全面评估了昏暗环境中的各种微光SLAM管道。采用细致的实验方法，我们定性和定量地评估了图像增强器和SLAM框架的不同组合，确定了基于特征的视觉SLAM的最佳组合。该发现通过强调在具有挑战性的照明条件下增强视觉输入以提高定位精度的实际意义，推进了微光SLAM。本文还提供了有价值的见解，鼓励进一步探索在现实世界场景中增强SLAM性能的视觉增强策略。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.11310v4" target="_blank">2304.11310v4</a>
                              </td>
                              <td>Twilight SLAM: Navigating Low-Light Environments</td>
                              <td>Surya Pratap Singh</td>
                              <td>2023-04-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_11310v4_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.11310v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_13332v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Ternary-type Opacity and Hybrid Odometry for RGB-only NeRF-SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_13332v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_13332v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_13332v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The opacity of rigid 3D scenes with opaque surfaces is considered to be of a binary type. However, we observed that this property is not followed by the existing RGB-only NeRF-SLAM. Therefore, we are motivated to introduce this prior into the RGB-only NeRF-SLAM pipeline. Unfortunately, the optimization through the volumetric rendering function does not facilitate easy integration of the desired prior. Instead, we observed that the opacity of ternary-type (TT) is well supported. In this work, we study why ternary-type opacity is well-suited and desired for the task at hand. In particular, we provide theoretical insights into the process of jointly optimizing radiance and opacity through the volumetric rendering process. Through exhaustive experiments on benchmark datasets, we validate our claim and provide insights into the optimization process, which we believe will unleash the potential of RGB-only NeRF-SLAM. To foster this line of research, we also propose a simple yet novel visual odometry scheme that uses a hybrid combination of volumetric and warping-based image renderings. More specifically, the proposed hybrid odometry (HO) additionally uses image warping-based coarse odometry, leading up to an order of magnitude final speed-up. Furthermore, we show that the proposed TT and HO well complement each other, offering state-of-the-art results on benchmark datasets in terms of both speed and accuracy.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_13332v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>具有不透明表面的刚性3D场景的不透明度被认为是二元类型。然而，我们观察到，现有的仅RGB的NeRF SLAM并没有遵循这一特性。因此，我们有动机将此先验引入仅RGB的NeRF SLAM流水线。不幸的是，通过体积渲染函数的优化不利于所需先验的容易集成。相反，我们观察到三元类型（TT）的不透明性得到了很好的支持。在这项工作中，我们研究了为什么三元型不透明性非常适合并期望用于手头的任务。特别是，我们为通过体积渲染过程联合优化辐射和不透明度的过程提供了理论见解。通过在基准数据集上进行详尽的实验，我们验证了我们的说法，并深入了解了优化过程，我们相信这将释放仅RGB的NeRF SLAM的潜力。为了促进这一研究方向，我们还提出了一种简单而新颖的视觉里程计方案，该方案使用体积和基于扭曲的图像渲染的混合组合。更具体地说，所提出的混合里程计（HO）还使用了基于图像扭曲的粗略里程计，导致了一个数量级的最终加速。此外，我们还表明，所提出的TT和HO很好地相互补充，在速度和准确性方面，在基准数据集上提供了最先进的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.13332v2" target="_blank">2312.13332v2</a>
                              </td>
                              <td>Ternary-type Opacity and Hybrid Odometry for RGB-only NeRF-SLAM</td>
                              <td>Junru Lin</td>
                              <td>2023-12-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_13332v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.13332v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_13802v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Dense Subframe-based SLAM Framework with Side-scan Sonar</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_13802v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_13802v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_13802v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Side-scan sonar (SSS) is a lightweight acoustic sensor that is commonly deployed on autonomous underwater vehicles (AUVs) to provide high-resolution seafloor images. However, leveraging side-scan images for simultaneous localization and mapping (SLAM) presents a notable challenge, primarily due to the difficulty of establishing sufficient amount of accurate correspondences between these images. To address this, we introduce a novel subframe-based dense SLAM framework utilizing side-scan sonar data, enabling effective dense matching in overlapping regions of paired side-scan images. With each image being evenly divided into subframes, we propose a robust estimation pipeline to estimate the relative pose between each paired subframes, by using a good inlier set identified from dense correspondences. These relative poses are then integrated as edge constraints in a factor graph to optimize the AUV pose trajectory.   The proposed framework is evaluated on three real datasets collected by a Hugin AUV. Among one of them includes manually-annotated keypoint correspondences as ground truth and is used for evaluation of pose trajectory. We also present a feasible way of evaluating mapping quality against multi-beam echosounder (MBES) data without the influence of pose. Experimental results demonstrate that our approach effectively mitigates drift from the dead-reckoning (DR) system and enables quasi-dense bathymetry reconstruction. An open-source implementation of this work is available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_13802v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>侧扫声纳（SSS）是一种轻型声学传感器，通常部署在自主水下航行器（AUV）上，以提供高分辨率海底图像。然而，利用侧扫描图像进行同时定位和映射（SLAM）是一个显著的挑战，主要是由于难以在这些图像之间建立足够数量的精确对应关系。为了解决这一问题，我们引入了一种新的基于子帧的密集SLAM框架，该框架利用侧扫声纳数据，能够在成对侧扫图像的重叠区域进行有效的密集匹配。在将每个图像均匀地划分为子帧的情况下，我们提出了一种稳健的估计流水线，通过使用从密集对应中识别出的良好内部集合来估计每个成对子帧之间的相对姿态。然后将这些相对姿态作为边缘约束集成到因子图中，以优化AUV姿态轨迹。在Hugin AUV收集的三个真实数据集上对所提出的框架进行了评估。其中一个包括手动注释的关键点对应关系作为基本事实，并用于姿态轨迹的评估。我们还提出了一种在不受姿态影响的情况下根据多波束回声测深仪（MBES）数据评估测绘质量的可行方法。实验结果表明，我们的方法有效地缓解了航位推算（DR）系统的漂移，并实现了准密集水深重建。这项工作的开源实现是可用的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>46</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.13802v1" target="_blank">2312.13802v1</a>
                              </td>
                              <td>A Dense Subframe-based SLAM Framework with Side-scan Sonar</td>
                              <td>Jun Zhang</td>
                              <td>2023-12-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_13802v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.13802v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="SFM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2304_07250v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fusing Structure from Motion and Simulation-Augmented Pose Regression from Optical Flow for Challenging Indoor Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_07250v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_07250v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_07250v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The localization of objects is a crucial task in various applications such as robotics, virtual and augmented reality, and the transportation of goods in warehouses. Recent advances in deep learning have enabled the localization using monocular visual cameras. While structure from motion (SfM) predicts the absolute pose from a point cloud, absolute pose regression (APR) methods learn a semantic understanding of the environment through neural networks. However, both fields face challenges caused by the environment such as motion blur, lighting changes, repetitive patterns, and feature-less structures. This study aims to address these challenges by incorporating additional information and regularizing the absolute pose using relative pose regression (RPR) methods. RPR methods suffer under different challenges, i.e., motion blur. The optical flow between consecutive images is computed using the Lucas-Kanade algorithm, and the relative pose is predicted using an auxiliary small recurrent convolutional network. The fusion of absolute and relative poses is a complex task due to the mismatch between the global and local coordinate systems. State-of-the-art methods fusing absolute and relative poses use pose graph optimization (PGO) to regularize the absolute pose predictions using relative poses. In this work, we propose recurrent fusion networks to optimally align absolute and relative pose predictions to improve the absolute pose prediction. We evaluate eight different recurrent units and construct a simulation environment to pre-train the APR and RPR networks for better generalized training. Additionally, we record a large database of different scenarios in a challenging large-scale indoor environment that mimics a warehouse with transportation robots. We conduct hyperparameter searches and experiments to show the effectiveness of our recurrent fusion method compared to PGO.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_07250v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在机器人、虚拟和增强现实以及仓库中的货物运输等各种应用中，物体的定位是一项至关重要的任务。深度学习的最新进展使得能够使用单目视觉相机进行定位。虽然运动结构（SfM）从点云预测绝对姿态，但绝对姿态回归（APR）方法通过神经网络学习对环境的语义理解。然而，这两个领域都面临着环境带来的挑战，如运动模糊、照明变化、重复模式和无特征结构。本研究旨在通过结合额外信息和使用相对姿态回归（RPR）方法规范绝对姿态来应对这些挑战。RPR方法面临不同的挑战，即运动模糊。使用Lucas Kanade算法计算连续图像之间的光流，并使用辅助小递归卷积网络预测相对姿态。由于全局坐标系和局部坐标系之间的不匹配，绝对姿态和相对姿态的融合是一项复杂的任务。融合绝对姿态和相对姿态的现有技术方法使用姿态图优化（PGO）来使用相对姿态正则化绝对姿态预测。在这项工作中，我们提出了递归融合网络来优化绝对和相对姿态预测，以改进绝对姿态预测。我们评估了八个不同的递归单元，并构建了一个模拟环境来预训练APR和RPR网络，以便更好地进行广义训练。此外，我们在具有挑战性的大型室内环境中记录了不同场景的大型数据库，该环境模拟了带有运输机器人的仓库。我们进行了超参数搜索和实验，以显示与PGO相比，我们的递归融合方法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.07250v3" target="_blank">2304.07250v3</a>
                              </td>
                              <td>Fusing Structure from Motion and Simulation-Augmented Pose Regression from Optical Flow for Challenging Indoor Environments</td>
                              <td>Felix Ott</td>
                              <td>2023-04-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_07250v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.07250v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_15667v4_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_15667v4_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_15667v4_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_15667v4_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Camera pose estimation is a long-standing computer vision problem that to date often relies on classical methods, such as handcrafted keypoint matching, RANSAC and bundle adjustment. In this paper, we propose to formulate the Structure from Motion (SfM) problem inside a probabilistic diffusion framework, modelling the conditional distribution of camera poses given input images. This novel view of an old problem has several advantages. (i) The nature of the diffusion framework mirrors the iterative procedure of bundle adjustment. (ii) The formulation allows a seamless integration of geometric constraints from epipolar geometry. (iii) It excels in typically difficult scenarios such as sparse views with wide baselines. (iv) The method can predict intrinsics and extrinsics for an arbitrary amount of images. We demonstrate that our method PoseDiffusion significantly improves over the classic SfM pipelines and the learned approaches on two real-world datasets. Finally, it is observed that our method can generalize across datasets without further training. Project page: https://posediffusion.github.io/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_15667v4_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>相机姿态估计是一个长期存在的计算机视觉问题，迄今为止，它通常依赖于经典的方法，如手工关键点匹配、RANSAC和束调整。在本文中，我们建议在概率扩散框架内公式化运动结构（SfM）问题，对给定输入图像的相机姿态的条件分布进行建模。这种对老问题的新颖看法有几个优点。（i） 扩散框架的性质反映了束调整的迭代过程。（ii）该公式允许来自核极几何的几何约束的无缝集成。（iii）它在典型的困难场景中表现出色，例如具有宽基线的稀疏视图。（iv）该方法可以预测任意数量的图像的内在和外在。我们在两个真实世界的数据集上证明了我们的方法PoseDiffusion显著优于经典的SfM管道和学习的方法。最后，我们观察到，我们的方法可以在不需要进一步训练的情况下在数据集之间进行推广。项目页面：https://posediffusion.github.io/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.15667v4" target="_blank">2306.15667v4</a>
                              </td>
                              <td>PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment</td>
                              <td>Jianyuan Wang</td>
                              <td>2023-06-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_15667v4_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.15667v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14289v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Speech foundation models on intelligibility prediction for hearing-impaired listeners</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14289v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14289v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14289v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Speech foundation models (SFMs) have been benchmarked on many speech processing tasks, often achieving state-of-the-art performance with minimal adaptation. However, the SFM paradigm has been significantly less explored for applications of interest to the speech perception community. In this paper we present a systematic evaluation of 10 SFMs on one such application: Speech intelligibility prediction. We focus on the non-intrusive setup of the Clarity Prediction Challenge 2 (CPC2), where the task is to predict the percentage of words correctly perceived by hearing-impaired listeners from speech-in-noise recordings. We propose a simple method that learns a lightweight specialized prediction head on top of frozen SFMs to approach the problem. Our results reveal statistically significant differences in performance across SFMs. Our method resulted in the winning submission in the CPC2, demonstrating its promise for speech perception applications.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14289v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语音基础模型（SFM）已经在许多语音处理任务上进行了基准测试，通常以最小的适应度实现最先进的性能。然而，对于语音感知社区感兴趣的应用，SFM范式的探索明显较少。在本文中，我们对10个SFM的一个应用进行了系统评估：语音可懂度预测。我们专注于清晰度预测挑战2（CPC2）的非侵入性设置，其中的任务是预测听障听众从噪声记录中的语音中正确感知的单词的百分比。我们提出了一种简单的方法，在冻结的SFM上学习轻量级的专业预测头来解决这个问题。我们的研究结果显示，SFM之间的性能存在统计学上的显著差异。我们的方法在CPC2中获胜，证明了它在语音感知应用中的前景。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14289v1" target="_blank">2401.14289v1</a>
                              </td>
                              <td>Speech foundation models on intelligibility prediction for hearing-impaired listeners</td>
                              <td>Santiago Cuervo</td>
                              <td>2024-01-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14289v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14289v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_11711v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">HG3-NeRF: Hierarchical Geometric, Semantic, and Photometric Guided Neural Radiance Fields for Sparse View Inputs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_11711v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_11711v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_11711v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural Radiance Fields (NeRF) have garnered considerable attention as a paradigm for novel view synthesis by learning scene representations from discrete observations. Nevertheless, NeRF exhibit pronounced performance degradation when confronted with sparse view inputs, consequently curtailing its further applicability. In this work, we introduce Hierarchical Geometric, Semantic, and Photometric Guided NeRF (HG3-NeRF), a novel methodology that can address the aforementioned limitation and enhance consistency of geometry, semantic content, and appearance across different views. We propose Hierarchical Geometric Guidance (HGG) to incorporate the attachment of Structure from Motion (SfM), namely sparse depth prior, into the scene representations. Different from direct depth supervision, HGG samples volume points from local-to-global geometric regions, mitigating the misalignment caused by inherent bias in the depth prior. Furthermore, we draw inspiration from notable variations in semantic consistency observed across images of different resolutions and propose Hierarchical Semantic Guidance (HSG) to learn the coarse-to-fine semantic content, which corresponds to the coarse-to-fine scene representations. Experimental results demonstrate that HG3-NeRF can outperform other state-of-the-art methods on different standard benchmarks and achieve high-fidelity synthesis results for sparse view inputs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_11711v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经辐射场（NeRF）作为一种通过从离散观测中学习场景表示的新型视图合成范式，已经引起了人们的极大关注。然而，当面对稀疏视图输入时，NeRF表现出明显的性能退化，从而限制了其进一步的适用性。在这项工作中，我们介绍了层次几何、语义和光度引导的NeRF（HG3-NeRF），这是一种新的方法，可以解决上述限制，并增强不同视图中几何、语义内容和外观的一致性。我们提出了分层几何制导（HGG），将运动结构的附加（SfM），即稀疏深度先验，纳入场景表示中。与直接深度监督不同，HGG从局部几何区域到全局几何区域对体积点进行采样，减轻了深度先验中固有偏差引起的偏差。此外，我们从不同分辨率的图像中观察到的语义一致性的显著变化中获得了灵感，并提出了层次语义引导（HSG）来学习粗到细的语义内容，该内容对应于粗到细场景表示。实验结果表明，HG3-NeRF可以在不同的标准基准上优于其他最先进的方法，并实现稀疏视图输入的高保真度合成结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.11711v1" target="_blank">2401.11711v1</a>
                              </td>
                              <td>HG3-NeRF: Hierarchical Geometric, Semantic, and Photometric Guided Neural Radiance Fields for Sparse View Inputs</td>
                              <td>Zelin Gao</td>
                              <td>2024-01-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_11711v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.11711v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_10886v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SCENES: Subpixel Correspondence Estimation With Epipolar Supervision</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_10886v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_10886v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_10886v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Extracting point correspondences from two or more views of a scene is a fundamental computer vision problem with particular importance for relative camera pose estimation and structure-from-motion. Existing local feature matching approaches, trained with correspondence supervision on large-scale datasets, obtain highly-accurate matches on the test sets. However, they do not generalise well to new datasets with different characteristics to those they were trained on, unlike classic feature extractors. Instead, they require finetuning, which assumes that ground-truth correspondences or ground-truth camera poses and 3D structure are available. We relax this assumption by removing the requirement of 3D structure, e.g., depth maps or point clouds, and only require camera pose information, which can be obtained from odometry. We do so by replacing correspondence losses with epipolar losses, which encourage putative matches to lie on the associated epipolar line. While weaker than correspondence supervision, we observe that this cue is sufficient for finetuning existing models on new data. We then further relax the assumption of known camera poses by using pose estimates in a novel bootstrapping approach. We evaluate on highly challenging datasets, including an indoor drone dataset and an outdoor smartphone camera dataset, and obtain state-of-the-art results without strong supervision.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_10886v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从场景的两个或多个视图中提取点对应关系是一个基本的计算机视觉问题，对于从运动中估计相对相机姿态和结构具有特别重要的意义。现有的局部特征匹配方法，在大规模数据集上进行对应监督训练，可以在测试集上获得高度准确的匹配。然而，与经典的特征提取器不同，它们不能很好地推广到与训练对象具有不同特征的新数据集。相反，它们需要微调，这假设地面实况对应关系或地面实况摄像机姿态和3D结构是可用的。我们通过取消3D结构（例如深度图或点云）的要求来放松这一假设，并且只需要相机姿态信息，这些信息可以从里程计中获得。我们通过用核极损失代替对应损失来做到这一点，这鼓励假定的匹配位于相关的核极线上。虽然弱于对应监督，但我们观察到，这一提示足以在新数据上微调现有模型。然后，我们通过在一种新颖的自举方法中使用姿势估计，进一步放宽了已知相机姿势的假设。我们在极具挑战性的数据集上进行评估，包括室内无人机数据集和室外智能手机相机数据集，并在没有强有力监督的情况下获得最先进的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.10886v1" target="_blank">2401.10886v1</a>
                              </td>
                              <td>SCENES: Subpixel Correspondence Estimation With Epipolar Supervision</td>
                              <td>Dominik A. Kloepfer</td>
                              <td>2024-01-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_10886v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.10886v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_09252v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D Scene Geometry Estimation from 360$^\circ$ Imagery: A Survey</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_09252v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_09252v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_09252v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper provides a comprehensive survey on pioneer and state-of-the-art 3D scene geometry estimation methodologies based on single, two, or multiple images captured under the omnidirectional optics. We first revisit the basic concepts of the spherical camera model, and review the most common acquisition technologies and representation formats suitable for omnidirectional (also called 360$^\circ$, spherical or panoramic) images and videos. We then survey monocular layout and depth inference approaches, highlighting the recent advances in learning-based solutions suited for spherical data. The classical stereo matching is then revised on the spherical domain, where methodologies for detecting and describing sparse and dense features become crucial. The stereo matching concepts are then extrapolated for multiple view camera setups, categorizing them among light fields, multi-view stereo, and structure from motion (or visual simultaneous localization and mapping). We also compile and discuss commonly adopted datasets and figures of merit indicated for each purpose and list recent results for completeness. We conclude this paper by pointing out current and future trends.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_09252v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文对基于在全向光学系统下捕获的单个、两个或多个图像的先驱和最先进的3D场景几何估计方法进行了全面的综述。我们首先回顾了球面相机模型的基本概念，并回顾了适用于全向（也称为360$^\circ$，球面或全景）图像和视频的最常见的采集技术和表示格式。然后，我们调查了单目布局和深度推理方法，重点介绍了适用于球形数据的基于学习的解决方案的最新进展。然后在球面域上修改经典的立体匹配，其中检测和描述稀疏和密集特征的方法变得至关重要。然后，将立体匹配概念外推到多视图相机设置中，将其分类为光场、多视图立体和运动结构（或视觉同时定位和映射）。我们还汇编和讨论了常用的数据集和针对每种目的指出的优缺点，并列出了最新的结果以确保完整性。最后，我们指出了当前和未来的趋势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.09252v1" target="_blank">2401.09252v1</a>
                              </td>
                              <td>3D Scene Geometry Estimation from 360$^\circ$ Imagery: A Survey</td>
                              <td>Thiago Lopes Trugillo da Silveira</td>
                              <td>2024-01-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_09252v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.09252v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_08937v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_08937v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_08937v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_08937v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural Radiance Fields (NeRF) exhibit remarkable performance for Novel View Synthesis (NVS) given a set of 2D images. However, NeRF training requires accurate camera pose for each input view, typically obtained by Structure-from-Motion (SfM) pipelines. Recent works have attempted to relax this constraint, but they still often rely on decent initial poses which they can refine. Here we aim at removing the requirement for pose initialization. We present Incremental CONfidence (ICON), an optimization procedure for training NeRFs from 2D video frames. ICON only assumes smooth camera motion to estimate initial guess for poses. Further, ICON introduces ``confidence": an adaptive measure of model quality used to dynamically reweight gradients. ICON relies on high-confidence poses to learn NeRF, and high-confidence 3D structure (as encoded by NeRF) to learn poses. We show that ICON, without prior pose initialization, achieves superior performance in both CO3D and HO3D versus methods which use SfM pose.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_08937v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在给定一组2D图像的情况下，神经辐射场（NeRF）在新视图合成（NVS）中表现出显著的性能。然而，NeRF训练需要每个输入视图的精确相机姿势，通常通过运动结构（SfM）管道获得。最近的作品试图放松这种限制，但它们仍然经常依赖于可以改进的体面的初始姿势。在这里，我们旨在消除姿势初始化的要求。我们提出了增量置信（ICON），这是一种从2D视频帧中训练NeRF的优化过程。ICON仅假设相机运动平滑，以估计姿势的初始猜测。此外，ICON引入了“置信度”：一种用于动态重加权梯度的模型质量自适应度量。ICON依赖于高置信度姿势来学习NeRF，并依赖于高置信度3D结构（由NeRF编码）来学习姿势。我们表明，与使用SfM姿势的方法相比，ICON在没有预先初始化姿势的情况下，在CO3D和HO3D中都实现了卓越的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.08937v1" target="_blank">2401.08937v1</a>
                              </td>
                              <td>ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization</td>
                              <td>Weiyao Wang</td>
                              <td>2024-01-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_08937v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.08937v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_08043v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Cross-Modal Semi-Dense 6-DoF Tracking of an Event Camera in Challenging Conditions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_08043v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_08043v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_08043v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision-based localization is a cost-effective and thus attractive solution for many intelligent mobile platforms. However, its accuracy and especially robustness still suffer from low illumination conditions, illumination changes, and aggressive motion. Event-based cameras are bio-inspired visual sensors that perform well in HDR conditions and have high temporal resolution, and thus provide an interesting alternative in such challenging scenarios. While purely event-based solutions currently do not yet produce satisfying mapping results, the present work demonstrates the feasibility of purely event-based tracking if an alternative sensor is permitted for mapping. The method relies on geometric 3D-2D registration of semi-dense maps and events, and achieves highly reliable and accurate cross-modal tracking results. Practically relevant scenarios are given by depth camera-supported tracking or map-based localization with a semi-dense map prior created by a regular image-based visual SLAM or structure-from-motion system. Conventional edge-based 3D-2D alignment is extended by a novel polarity-aware registration that makes use of signed time-surface maps (STSM) obtained from event streams. We furthermore introduce a novel culling strategy for occluded points. Both modifications increase the speed of the tracker and its robustness against occlusions or large view-point variations. The approach is validated on many real datasets covering the above-mentioned challenging conditions, and compared against similar solutions realised with regular cameras.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_08043v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于视觉的定位是一种具有成本效益的解决方案，因此对许多智能移动平台具有吸引力。然而，其准确性和特别是鲁棒性仍然受到低照明条件、照明变化和侵略性运动的影响。基于事件的相机是受生物启发的视觉传感器，在HDR条件下表现良好，具有高时间分辨率，因此在这种具有挑战性的场景中提供了一种有趣的替代方案。虽然纯基于事件的解决方案目前还没有产生令人满意的映射结果，但目前的工作证明了如果允许替代传感器进行映射，则纯基于事件跟踪的可行性。该方法依赖于半密集地图和事件的几何3D-2D配准，并实现了高度可靠和准确的跨模态跟踪结果。实际相关场景由深度相机支持的跟踪或基于地图的定位给出，其中半密集地图由基于常规图像的视觉SLAM或来自运动系统的结构预先创建。传统的基于边缘的3D-2D对准通过利用从事件流获得的带符号时间表面图（STSM）的新颖的极性感知配准来扩展。此外，我们还介绍了一种新的遮挡点剔除策略。这两种修改都提高了跟踪器的速度及其对遮挡或大视点变化的鲁棒性。该方法在涵盖上述挑战性条件的许多真实数据集上进行了验证，并与使用常规相机实现的类似解决方案进行了比较。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.08043v1" target="_blank">2401.08043v1</a>
                              </td>
                              <td>Cross-Modal Semi-Dense 6-DoF Tracking of an Event Camera in Challenging Conditions</td>
                              <td>Yi-Fan Zuo</td>
                              <td>2024-01-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_08043v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.08043v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zyfff/canny-evt" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_08422v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A vision-based autonomous UAV inspection framework for unknown tunnel construction sites with dynamic obstacles</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_08422v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_08422v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_08422v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Tunnel construction using the drill-and-blast method requires the 3D measurement of the excavation front to evaluate underbreak locations. Considering the inspection and measurement task's safety, cost, and efficiency, deploying lightweight autonomous robots, such as unmanned aerial vehicles (UAV), becomes more necessary and popular. Most of the previous works use a prior map for inspection viewpoint determination and do not consider dynamic obstacles. To maximally increase the level of autonomy, this paper proposes a vision-based UAV inspection framework for dynamic tunnel environments without using a prior map. Our approach utilizes a hierarchical planning scheme, decomposing the inspection problem into different levels. The high-level decision maker first determines the task for the robot and generates the target point. Then, the mid-level path planner finds the waypoint path and optimizes the collision-free static trajectory. Finally, the static trajectory will be fed into the low-level local planner to avoid dynamic obstacles and navigate to the target point. Besides, our framework contains a novel dynamic map module that can simultaneously track dynamic obstacles and represent static obstacles based on an RGB-D camera. After inspection, the Structure-from-Motion (SfM) pipeline is applied to generate the 3D shape of the target. To our best knowledge, this is the first time autonomous inspection has been realized in unknown and dynamic tunnel environments. Our flight experiments in a real tunnel prove that our method can autonomously inspect the tunnel excavation front surface. Our software is available on GitHub as an open-source ROS package.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_08422v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用钻爆法进行隧道施工需要对开挖前沿进行三维测量，以评估欠挖位置。考虑到检测和测量任务的安全性、成本和效率，部署无人机等轻型自主机器人变得更加必要和流行。以前的大多数工作都使用先验图来确定检查视点，并且没有考虑动态障碍物。为了最大限度地提高自主性水平，本文提出了一种基于视觉的无人机动态隧道环境检测框架，无需使用先验地图。我们的方法使用分层规划方案，将检查问题分解为不同的级别。高级决策者首先确定机器人的任务并生成目标点。然后，中级路径规划器找到航路点路径并优化无碰撞静态轨迹。最后，静态轨迹将被输入到低级别的局部规划器中，以避开动态障碍并导航到目标点。此外，我们的框架包含一个新的动态地图模块，该模块可以基于RGB-D相机同时跟踪动态障碍物和表示静态障碍物。检查后，应用运动结构（SfM）管道生成目标的3D形状。据我们所知，这是首次在未知和动态的隧道环境中实现自主检测。我们在实际隧道中的飞行实验证明，我们的方法可以自主检测隧道开挖前表面。我们的软件在GitHub上作为开源ROS包提供。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.08422v3" target="_blank">2301.08422v3</a>
                              </td>
                              <td>A vision-based autonomous UAV inspection framework for unknown tunnel construction sites with dynamic obstacles</td>
                              <td>Zhefan Xu</td>
                              <td>2023-01-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_08422v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.08422v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zhefan-xu/cerlab-uav-autonomy" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_03930v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Photometric Correction for Infrared Sensors</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_03930v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_03930v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_03930v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Infrared thermography has been widely used in several domains to capture and measure temperature distributions across surfaces and objects. This methodology can be further expanded to 3D applications if the spatial distribution of the temperature distribution is available. Structure from Motion (SfM) is a photometric range imaging technique that makes it possible to obtain 3D renderings from a cloud of 2D images. To explore the possibility of 3D reconstruction via SfM from infrared images, this article proposes a photometric correction model for infrared sensors based on temperature constancy. Photometric correction is accomplished by estimating the scene irradiance as the values from the solution to a differential equation for microbolometer pixel excitation with unknown coefficients and initial conditions. The model was integrated into an SfM framework and experimental evaluations demonstrate the contribution of the photometric correction for improving the estimates of both the camera motion and the scene structure. Further, experiments show that the reconstruction quality from the corrected infrared imagery achieves performance on par with state-of-the-art reconstruction using RGB sensors.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_03930v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>红外热成像已广泛应用于多个领域，用于捕捉和测量表面和物体的温度分布。如果温度分布的空间分布可用，则该方法可以进一步扩展到3D应用。运动结构（SfM）是一种光度范围成像技术，可以从2D图像云中获得3D渲染。为了探索通过SfM从红外图像中进行三维重建的可能性，本文提出了一种基于温度恒定性的红外传感器光度校正模型。光度校正是通过将场景辐照度估计为具有未知系数和初始条件的微测辐射热计像素激发的微分方程的解的值来实现的。该模型被集成到SfM框架中，实验评估证明了光度校正对改善相机运动和场景结构估计的贡献。此外，实验表明，校正后的红外图像的重建质量达到了与使用RGB传感器的最先进重建相当的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.03930v2" target="_blank">2304.03930v2</a>
                              </td>
                              <td>Photometric Correction for Infrared Sensors</td>
                              <td>Jincheng Zhang</td>
                              <td>2023-04-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_03930v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.03930v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05236v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Structure from Duplicates: Neural Inverse Graphics from a Pile of Objects</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05236v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05236v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05236v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Our world is full of identical objects (\emphe.g., cans of coke, cars of same model). These duplicates, when seen together, provide additional and strong cues for us to effectively reason about 3D. Inspired by this observation, we introduce Structure from Duplicates (SfD), a novel inverse graphics framework that reconstructs geometry, material, and illumination from a single image containing multiple identical objects. SfD begins by identifying multiple instances of an object within an image, and then jointly estimates the 6DoF pose for all instances.An inverse graphics pipeline is subsequently employed to jointly reason about the shape, material of the object, and the environment light, while adhering to the shared geometry and material constraint across instances. Our primary contributions involve utilizing object duplicates as a robust prior for single-image inverse graphics and proposing an in-plane rotation-robust Structure from Motion (SfM) formulation for joint 6-DoF object pose estimation. By leveraging multi-view cues from a single image, SfD generates more realistic and detailed 3D reconstructions, significantly outperforming existing single image reconstruction models and multi-view reconstruction approaches with a similar or greater number of observations.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05236v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们的世界充满了相同的物体（例如，可乐罐、同一型号的汽车）。当这些重复出现在一起时，为我们有效地推理3D提供了额外而有力的线索。受这一观察结果的启发，我们引入了“重复结构”（SfD），这是一种新颖的逆图形框架，可以从包含多个相同对象的单个图像中重建几何体、材料和照明。SfD首先识别图像中对象的多个实例，然后联合估计所有实例的6DoF姿势。随后使用反向图形管道来联合推理对象的形状、材质和环境光，同时遵守实例之间的共享几何图形和材质约束。我们的主要贡献包括利用对象副本作为单图像逆图形的鲁棒先验，并提出用于联合6-DoF对象姿态估计的平面内旋转鲁棒运动结构（SfM）公式。通过利用来自单个图像的多视图线索，SfD生成了更真实、更详细的3D重建，显著优于具有相似或更多观测值的现有单个图像重建模型和多视图重建方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05236v1" target="_blank">2401.05236v1</a>
                              </td>
                              <td>Structure from Duplicates: Neural Inverse Graphics from a Pile of Objects</td>
                              <td>Tianhang Cheng</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05236v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05236v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/tianhang-cheng/sfd" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_03450v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Classification of Critical Configurations for any Number of Projective Views</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_03450v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_03450v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_03450v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Structure from motion is the process of recovering information about cameras and 3D scene from a set of images. Generally, in a noise-free setting, all information can be uniquely recovered if enough images and image points are provided. There are, however, certain cases where unique recovery is impossible, even in theory; these are called critical configurations. We use a recently developed algebraic approach to classify all critical configurations for any number of projective cameras. We show that they form well-known algebraic varieties, such as quadric surfaces and curves of degree at most 4. This paper also improves upon earlier results both by finding previously unknown critical configurations and by showing that some configurations previously believed to be critical are in fact not.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_03450v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>运动结构是从一组图像中恢复有关相机和3D场景的信息的过程。通常，在无噪声设置中，如果提供了足够的图像和图像点，则可以唯一地恢复所有信息。然而，在某些情况下，即使在理论上，也不可能进行独特的恢复；这些被称为关键配置。我们使用最近开发的代数方法对任意数量的投影相机的所有关键配置进行分类。我们证明了它们形成了众所周知的代数变体，如二次曲面和次数最多为4的曲线。本文还改进了早期的结果，发现了以前未知的关键构型，并表明一些以前认为是关键的构型实际上不是。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.03450v1" target="_blank">2401.03450v1</a>
                              </td>
                              <td>A Classification of Critical Configurations for any Number of Projective Views</td>
                              <td>Martin Bråtelund</td>
                              <td>2024-01-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_03450v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.03450v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/mabraate/critical-configurations" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_11153v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Research on Multilingual Natural Scene Text Detection Algorithm</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_11153v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_11153v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_11153v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Natural scene text detection is a significant challenge in computer vision, with tremendous potential applications in multilingual, diverse, and complex text scenarios. We propose a multilingual text detection model to address the issues of low accuracy and high difficulty in detecting multilingual text in natural scenes. In response to the challenges posed by multilingual text images with multiple character sets and various font styles, we introduce the SFM Swin Transformer feature extraction network to enhance the model's robustness in detecting characters and fonts across different languages. Dealing with the considerable variation in text scales and complex arrangements in natural scene text images, we present the AS-HRFPN feature fusion network by incorporating an Adaptive Spatial Feature Fusion module and a Spatial Pyramid Pooling module. The feature fusion network improvements enhance the model's ability to detect text sizes and orientations. Addressing diverse backgrounds and font variations in multilingual scene text images is a challenge for existing methods. Limited local receptive fields hinder detection performance. To overcome this, we propose a Global Semantic Segmentation Branch, extracting and preserving global features for more effective text detection, aligning with the need for comprehensive information. In this study, we collected and built a real-world multilingual natural scene text image dataset and conducted comprehensive experiments and analyses. The experimental results demonstrate that the proposed algorithm achieves an F-measure of 85.02\%, which is 4.71\% higher than the baseline model. We also conducted extensive cross-dataset validation on MSRA-TD500, ICDAR2017MLT, and ICDAR2015 datasets to verify the generality of our approach. The code and dataset can be found at https://github.com/wangmelon/CEMLT.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_11153v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自然场景文本检测是计算机视觉中的一个重大挑战，在多语言、多样化和复杂的文本场景中具有巨大的潜在应用。我们提出了一种多语言文本检测模型，以解决在自然场景中检测多语言文本的准确性低和难度高的问题。为了应对具有多个字符集和各种字体样式的多语言文本图像带来的挑战，我们引入了SFM Swin Transformer特征提取网络，以增强模型在检测不同语言的字符和字体时的鲁棒性。针对自然场景文本图像中文本尺度的巨大变化和复杂排列，我们结合自适应空间特征融合模块和空间金字塔池模块，提出了AS-HRFPN特征融合网络。特征融合网络的改进增强了模型检测文本大小和方向的能力。解决多语言场景文本图像中的不同背景和字体变化是对现有方法的挑战。有限的局部感受野阻碍了检测性能。为了克服这一点，我们提出了一个全局语义分割分支，提取并保留全局特征，以实现更有效的文本检测，从而满足对全面信息的需求。在本研究中，我们收集并构建了一个真实世界的多语言自然场景文本图像数据集，并进行了全面的实验和分析。实验结果表明，该算法的F测度为85.02%，比基线模型高4.71%。我们还对MSRA-TD500、ICDAR2017MLT和ICDAR2015数据集进行了广泛的跨数据集验证，以验证我们方法的通用性。代码和数据集位于https://github.com/wangmelon/CEMLT.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.11153v2" target="_blank">2312.11153v2</a>
                              </td>
                              <td>Research on Multilingual Natural Scene Text Detection Algorithm</td>
                              <td>Tao Wang</td>
                              <td>2023-12-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_11153v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.11153v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09012v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Yes, we CANN: Constrained Approximate Nearest Neighbors for local feature-based visual localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09012v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09012v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09012v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large-scale visual localization systems continue to rely on 3D point clouds built from image collections using structure-from-motion. While the 3D points in these models are represented using local image features, directly matching a query image's local features against the point cloud is challenging due to the scale of the nearest-neighbor search problem. Many recent approaches to visual localization have thus proposed a hybrid method, where first a global (per image) embedding is used to retrieve a small subset of database images, and local features of the query are matched only against those. It seems to have become common belief that global embeddings are critical for said image-retrieval in visual localization, despite the significant downside of having to compute two feature types for each query image. In this paper, we take a step back from this assumption and propose Constrained Approximate Nearest Neighbors (CANN), a joint solution of k-nearest-neighbors across both the geometry and appearance space using only local features. We first derive the theoretical foundation for k-nearest-neighbor retrieval across multiple metrics and then showcase how CANN improves visual localization. Our experiments on public localization benchmarks demonstrate that our method significantly outperforms both state-of-the-art global feature-based retrieval and approaches using local feature aggregation schemes. Moreover, it is an order of magnitude faster in both index and query time than feature aggregation schemes for these datasets. Code: \url{https://github.com/google-research/google-research/tree/master/cann}</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09012v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大规模视觉定位系统继续依赖于使用来自运动的结构从图像集合构建的3D点云。虽然这些模型中的3D点是使用局部图像特征来表示的，但由于最近邻搜索问题的规模，将查询图像的局部特征与点云直接匹配是具有挑战性的。因此，许多最近的视觉定位方法提出了一种混合方法，其中首先使用全局（每图像）嵌入来检索数据库图像的一小部分，并且仅针对查询的局部特征进行匹配。人们似乎已经普遍认为，全局嵌入对于视觉定位中的所述图像检索至关重要，尽管必须为每个查询图像计算两种特征类型是显著的缺点。在本文中，我们从这一假设后退一步，提出了约束近似最近邻（CANN），这是一种仅使用局部特征在几何和外观空间上的k个最近邻的联合解。我们首先推导了跨多个度量的k近邻检索的理论基础，然后展示了CANN如何改进视觉定位。我们在公共定位基准上的实验表明，我们的方法显著优于最先进的基于全局特征的检索和使用局部特征聚合方案的方法。此外，它在索引和查询时间上都比这些数据集的特征聚合方案快一个数量级。代码：\url{https://github.com/google-research/google-research/tree/master/cann}</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09012v3" target="_blank">2306.09012v3</a>
                              </td>
                              <td>Yes, we CANN: Constrained Approximate Nearest Neighbors for local feature-based visual localization</td>
                              <td>Dror Aiger</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09012v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09012v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/google-research/google-research" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_03704v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Pose-Free Generalizable Rendering Transformer</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_03704v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_03704v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_03704v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In the field of novel-view synthesis, the necessity of knowing camera poses (e.g., via Structure from Motion) before rendering has been a common practice. However, the consistent acquisition of accurate camera poses remains elusive, and errors in pose extraction can adversely impact the view synthesis process. To address this challenge, we introduce PF-GRT, a new Pose-Free framework for Generalizable Rendering Transformer, eliminating the need for pre-computed camera poses and instead leveraging feature-matching learned directly from data. PF-GRT is parameterized using a local relative coordinate system, where one of the source images is set as the origin. An OmniView Transformer is designed for fusing multi-view cues under the pose-free setting, where unposed-view fusion and origin-centric aggregation are performed. The 3D point feature along target ray is sampled by projecting onto the selected origin plane. The final pixel intensities are modulated and decoded using another Transformer. PF-GRT demonstrates an impressive ability to generalize to new scenes that were not encountered during the training phase, without the need of pre-computing camera poses. Our experiments with zero-shot rendering on the LLFF, RealEstate-10k, Shiny, and Blender datasets reveal that it produces superior quality in generating photo-realistic images. Moreover, it demonstrates robustness against noise in test camera poses. Code is available at https://zhiwenfan.github.io/PF-GRT/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_03704v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在新视图合成领域，在渲染之前了解相机姿势（例如，通过运动结构）的必要性已经成为一种常见的做法。然而，准确的相机姿势的一致获取仍然难以捉摸，姿势提取中的错误可能会对视图合成过程产生不利影响。为了应对这一挑战，我们引入了PF-GRT，这是一种用于通用渲染转换器的新的无姿势框架，无需预先计算相机姿势，而是利用直接从数据中学习的特征匹配。PF-GRT使用局部相对坐标系进行参数化，其中一个源图像被设置为原点。OmniView Transformer设计用于在无姿势设置下融合多视图线索，其中执行未融合的视图融合和以原点为中心的聚合。通过投影到选定的原点平面上，对沿目标射线的三维点特征进行采样。使用另一个Transformer对最终像素强度进行调制和解码。PF-GRT展示了一种令人印象深刻的能力，可以推广到训练阶段没有遇到的新场景，而无需预先计算相机姿势。我们在LLFF、RealEstate-10k、Shiny和Blender数据集上进行的零样本渲染实验表明，它在生成照片真实感图像时产生了卓越的质量。此外，它还展示了在测试相机姿态时对噪声的鲁棒性。代码位于https://zhiwenfan.github.io/PF-GRT/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.03704v3" target="_blank">2310.03704v3</a>
                              </td>
                              <td>Pose-Free Generalizable Rendering Transformer</td>
                              <td>Zhiwen Fan</td>
                              <td>2023-10-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_03704v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.03704v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zhiwenfan/DragView" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_15471v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Residual Learning for Image Point Descriptors</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_15471v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_15471v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_15471v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Local image feature descriptors have had a tremendous impact on the development and application of computer vision methods. It is therefore unsurprising that significant efforts are being made for learning-based image point descriptors. However, the advantage of learned methods over handcrafted methods in real applications is subtle and more nuanced than expected. Moreover, handcrafted descriptors such as SIFT and SURF still perform better point localization in Structure-from-Motion (SfM) compared to many learned counterparts. In this paper, we propose a very simple and effective approach to learning local image descriptors by using a hand-crafted detector and descriptor. Specifically, we choose to learn only the descriptors, supported by handcrafted descriptors while discarding the point localization head. We optimize the final descriptor by leveraging the knowledge already present in the handcrafted descriptor. Such an approach of optimization allows us to discard learning knowledge already present in non-differentiable functions such as the hand-crafted descriptors and only learn the residual knowledge in the main network branch. This offers 50X convergence speed compared to the standard baseline architecture of SuperPoint while at inference the combined descriptor provides superior performance over the learned and hand-crafted descriptors. This is done with minor increase in the computations over the baseline learned descriptor. Our approach has potential applications in ensemble learning and learning with non-differentiable functions. We perform experiments in matching, camera localization and Structure-from-Motion in order to showcase the advantages of our approach.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_15471v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>局部图像特征描述符对计算机视觉方法的发展和应用产生了巨大的影响。因此，对基于学习的图像点描述符做出重大努力并不令人惊讶。然而，在实际应用中，学习方法相对于手工制作方法的优势是微妙的，而且比预期的更微妙。此外，与许多学习的描述符相比，手工制作的描述符（如SIFT和SURF）在运动结构（SfM）中仍然执行更好的点定位。在本文中，我们提出了一种非常简单有效的方法，通过使用手工制作的检测器和描述符来学习局部图像描述符。具体来说，我们选择只学习描述符，由手工制作的描述符支持，同时丢弃点定位头。我们通过利用手工制作的描述符中已经存在的知识来优化最终描述符。这种优化方法允许我们丢弃已经存在于不可微函数中的学习知识，例如手工制作的描述符，并且只学习主网络分支中的剩余知识。与SuperPoint的标准基线架构相比，这提供了50倍的收敛速度，而在推理时，组合描述符提供了优于学习和手工制作的描述符的性能。这是在计算量比基线学习描述符略有增加的情况下完成的。我们的方法在集成学习和具有不可微函数的学习中具有潜在的应用。我们在匹配、相机定位和运动结构方面进行了实验，以展示我们方法的优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.15471v1" target="_blank">2312.15471v1</a>
                              </td>
                              <td>Residual Learning for Image Point Descriptors</td>
                              <td>Rashik Shrestha</td>
                              <td>2023-12-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_15471v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.15471v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_13977v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NeuSurf: On-Surface Priors for Neural Surface Reconstruction from Sparse Input Views</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_13977v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_13977v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_13977v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, neural implicit functions have demonstrated remarkable results in the field of multi-view reconstruction. However, most existing methods are tailored for dense views and exhibit unsatisfactory performance when dealing with sparse views. Several latest methods have been proposed for generalizing implicit reconstruction to address the sparse view reconstruction task, but they still suffer from high training costs and are merely valid under carefully selected perspectives. In this paper, we propose a novel sparse view reconstruction framework that leverages on-surface priors to achieve highly faithful surface reconstruction. Specifically, we design several constraints on global geometry alignment and local geometry refinement for jointly optimizing coarse shapes and fine details. To achieve this, we train a neural network to learn a global implicit field from the on-surface points obtained from SfM and then leverage it as a coarse geometric constraint. To exploit local geometric consistency, we project on-surface points onto seen and unseen views, treating the consistent loss of projected features as a fine geometric constraint. The experimental results with DTU and BlendedMVS datasets in two prevalent sparse settings demonstrate significant improvements over the state-of-the-art methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_13977v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，神经隐函数在多视图重建领域取得了显著的成果。然而，大多数现有的方法都是为密集视图量身定制的，并且在处理稀疏视图时表现出不令人满意的性能。已经提出了几种最新的方法来推广隐式重建以解决稀疏视图重建任务，但它们仍然存在较高的训练成本，并且仅在精心选择的视角下有效。在本文中，我们提出了一种新的稀疏视图重建框架，该框架利用表面先验来实现高度忠实的表面重建。具体而言，我们设计了全局几何对齐和局部几何细化的几个约束条件，以共同优化粗略形状和精细细节。为了实现这一点，我们训练神经网络从SfM获得的表面点学习全局隐式场，然后将其作为粗略的几何约束。为了利用局部几何一致性，我们将表面上的点投影到可见和不可见的视图上，将投影特征的一致丢失视为精细的几何约束。DTU和BlendedMVS数据集在两种流行的稀疏设置中的实验结果表明，与最先进的方法相比，有了显著的改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.13977v2" target="_blank">2312.13977v2</a>
                              </td>
                              <td>NeuSurf: On-Surface Priors for Neural Surface Reconstruction from Sparse Input Views</td>
                              <td>Han Huang</td>
                              <td>2023-12-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_13977v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.13977v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_10529v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Transformers in Unsupervised Structure-from-Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_10529v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_10529v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_10529v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Transformers have revolutionized deep learning based computer vision with improved performance as well as robustness to natural corruptions and adversarial attacks. Transformers are used predominantly for 2D vision tasks, including image classification, semantic segmentation, and object detection. However, robots and advanced driver assistance systems also require 3D scene understanding for decision making by extracting structure-from-motion (SfM). We propose a robust transformer-based monocular SfM method that learns to predict monocular pixel-wise depth, ego vehicle's translation and rotation, as well as camera's focal length and principal point, simultaneously. With experiments on KITTI and DDAD datasets, we demonstrate how to adapt different vision transformers and compare them against contemporary CNN-based methods. Our study shows that transformer-based architecture, though lower in run-time efficiency, achieves comparable performance while being more robust against natural corruptions, as well as untargeted and targeted attacks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_10529v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Transformers通过提高性能以及对自然腐蚀和对抗性攻击的鲁棒性，彻底改变了基于深度学习的计算机视觉。转换器主要用于2D视觉任务，包括图像分类、语义分割和对象检测。然而，机器人和先进的驾驶员辅助系统也需要3D场景理解，以便通过从运动中提取结构（SfM）来进行决策。我们提出了一种基于稳健变换器的单目SfM方法，该方法可以同时学习预测单目像素深度、自我车辆的平移和旋转以及相机的焦距和主点。通过在KITTI和DDAD数据集上的实验，我们展示了如何适应不同的视觉变换器，并将其与当代基于CNN的方法进行比较。我们的研究表明，基于转换器的体系结构虽然运行时效率较低，但可以实现相当的性能，同时对自然损坏以及无目标和有针对性的攻击更具鲁棒性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.10529v1" target="_blank">2312.10529v1</a>
                              </td>
                              <td>Transformers in Unsupervised Structure-from-Motion</td>
                              <td>Hemang Chawla</td>
                              <td>2023-12-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_10529v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.10529v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/neurai-lab/mt-sfmlearner" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_10109v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Enlighten-Your-Voice: When Multimodal Meets Zero-shot Low-light Image Enhancement</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_10109v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_10109v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_10109v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Low-light image enhancement is a crucial visual task, and many unsupervised methods tend to overlook the degradation of visible information in low-light scenes, which adversely affects the fusion of complementary information and hinders the generation of satisfactory results. To address this, our study introduces ``Enlighten-Your-Voice'', a multimodal enhancement framework that innovatively enriches user interaction through voice and textual commands. This approach does not merely signify a technical leap but also represents a paradigm shift in user engagement. Our model is equipped with a Dual Collaborative Attention Module (DCAM) that meticulously caters to distinct content and color discrepancies, thereby facilitating nuanced enhancements. Complementarily, we introduce a Semantic Feature Fusion (SFM) plug-and-play module that synergizes semantic context with low-light enhancement operations, sharpening the algorithm's efficacy. Crucially, ``Enlighten-Your-Voice'' showcases remarkable generalization in unsupervised zero-shot scenarios. The source code can be accessed from https://github.com/zhangbaijin/Enlighten-Your-Voice</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_10109v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>弱光图像增强是一项至关重要的视觉任务，许多无监督方法往往忽略了弱光场景中可见信息的退化，这对互补信息的融合产生了不利影响，并阻碍了令人满意的结果的产生。为了解决这一问题，我们的研究引入了“启发你的声音”，这是一个多模式增强框架，通过语音和文本命令创新地丰富了用户交互。这种方法不仅意味着技术上的飞跃，而且代表着用户参与度的范式转变。我们的模型配备了双协作注意力模块（DCAM），该模块可精心处理不同的内容和颜色差异，从而促进细微的增强。作为补充，我们引入了一个语义特征融合（SFM）即插即用模块，该模块将语义上下文与弱光增强操作协同，提高了算法的有效性。至关重要的是，“启蒙青年之声”在无监督的零样本场景中表现出了显著的泛化能力。源代码可以从https://github.com/zhangbaijin/Enlighten-Your-Voice</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.10109v1" target="_blank">2312.10109v1</a>
                              </td>
                              <td>Enlighten-Your-Voice: When Multimodal Meets Zero-shot Low-light Image Enhancement</td>
                              <td>Xiaofeng Zhang</td>
                              <td>2023-12-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_10109v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.10109v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zhangbaijin/enlighten-your-voice" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_08863v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">HeadRecon: High-Fidelity 3D Head Reconstruction from Monocular Video</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_08863v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_08863v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_08863v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, the reconstruction of high-fidelity 3D head models from static portrait image has made great progress. However, most methods require multi-view or multi-illumination information, which therefore put forward high requirements for data acquisition. In this paper, we study the reconstruction of high-fidelity 3D head models from arbitrary monocular videos. Non-rigid structure from motion (NRSFM) methods have been widely used to solve such problems according to the two-dimensional correspondence between different frames. However, the inaccurate correspondence caused by high-complex hair structures and various facial expression changes would heavily influence the reconstruction accuracy. To tackle these problems, we propose a prior-guided dynamic implicit neural network. Specifically, we design a two-part dynamic deformation field to transform the current frame space to the canonical one. We further model the head geometry in the canonical space with a learnable signed distance field (SDF) and optimize it using the volumetric rendering with the guidance of two-main head priors to improve the reconstruction accuracy and robustness. Extensive ablation studies and comparisons with state-of-the-art methods demonstrate the effectiveness and robustness of our proposed method.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_08863v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，从静态人像图像重建高保真三维头部模型取得了很大进展。然而，大多数方法都需要多视图或多照明信息，因此对数据采集提出了很高的要求。在本文中，我们研究了从任意单目视频中重建高保真3D头部模型。根据不同框架之间的二维对应关系，非刚性运动结构（NRSFM）方法已被广泛用于解决这些问题。然而，高度复杂的头发结构和各种面部表情变化导致的不准确对应将严重影响重建的准确性。为了解决这些问题，我们提出了一种先验引导的动态隐式神经网络。具体来说，我们设计了一个由两部分组成的动态变形场，将当前帧空间转换为规范帧空间。我们进一步用可学习的符号距离场（SDF）在正则空间中对头部几何结构进行建模，并在两个主要头部先验的指导下使用体积渲染对其进行优化，以提高重建精度和鲁棒性。广泛的消融研究和与最先进方法的比较证明了我们提出的方法的有效性和稳健性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.08863v1" target="_blank">2312.08863v1</a>
                              </td>
                              <td>HeadRecon: High-Fidelity 3D Head Reconstruction from Monocular Video</td>
                              <td>Xueying Wang</td>
                              <td>2023-12-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_08863v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.08863v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_08760v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CF-NeRF: Camera Parameter Free Neural Radiance Fields with Incremental Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_08760v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_08760v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_08760v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural Radiance Fields (NeRF) have demonstrated impressive performance in novel view synthesis. However, NeRF and most of its variants still rely on traditional complex pipelines to provide extrinsic and intrinsic camera parameters, such as COLMAP. Recent works, like NeRFmm, BARF, and L2G-NeRF, directly treat camera parameters as learnable and estimate them through differential volume rendering. However, these methods work for forward-looking scenes with slight motions and fail to tackle the rotation scenario in practice. To overcome this limitation, we propose a novel \underline{c}amera parameter \underline{f}ree neural radiance field (CF-NeRF), which incrementally reconstructs 3D representations and recovers the camera parameters inspired by incremental structure from motion (SfM). Given a sequence of images, CF-NeRF estimates the camera parameters of images one by one and reconstructs the scene through initialization, implicit localization, and implicit optimization. To evaluate our method, we use a challenging real-world dataset NeRFBuster which provides 12 scenes under complex trajectories. Results demonstrate that CF-NeRF is robust to camera rotation and achieves state-of-the-art results without providing prior information and constraints.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_08760v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经辐射场（NeRF）在新的视图合成中表现出了令人印象深刻的性能。然而，NeRF及其大多数变体仍然依赖于传统的复杂管道来提供外在和内在的相机参数，如COLMAP。最近的工作，如NeRFmm、BARF和L2G NeRF，直接将相机参数视为可学习的，并通过差分体绘制进行估计。然而，这些方法适用于具有轻微运动的前瞻性场景，在实践中无法解决旋转场景。为了克服这一限制，我们提出了一种新颖的下划线{c}amera参数\下划线{f}ree神经辐射场（CF NeRF），其增量重建3D表示并从运动中恢复受增量结构启发的相机参数（SfM）。给定一系列图像，CF-NeRF逐个估计图像的相机参数，并通过初始化、隐式定位和隐式优化重建场景。为了评估我们的方法，我们使用了一个具有挑战性的真实世界数据集NeRFBuster，该数据集提供了复杂轨迹下的12个场景。结果表明，CF-NeRF对相机旋转具有鲁棒性，并且在不提供先验信息和约束的情况下实现了最先进的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.08760v1" target="_blank">2312.08760v1</a>
                              </td>
                              <td>CF-NeRF: Camera Parameter Free Neural Radiance Fields with Incremental Learning</td>
                              <td>Qingsong Yan</td>
                              <td>2023-12-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_08760v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.08760v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_07504v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">COLMAP-Free 3D Gaussian Splatting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_07504v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_07504v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_07504v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While neural rendering has led to impressive advances in scene reconstruction and novel view synthesis, it relies heavily on accurately pre-computed camera poses. To relax this constraint, multiple efforts have been made to train Neural Radiance Fields (NeRFs) without pre-processed camera poses. However, the implicit representations of NeRFs provide extra challenges to optimize the 3D structure and camera poses at the same time. On the other hand, the recently proposed 3D Gaussian Splatting provides new opportunities given its explicit point cloud representations. This paper leverages both the explicit geometric representation and the continuity of the input video stream to perform novel view synthesis without any SfM preprocessing. We process the input frames in a sequential manner and progressively grow the 3D Gaussians set by taking one input frame at a time, without the need to pre-compute the camera poses. Our method significantly improves over previous approaches in view synthesis and camera pose estimation under large motion changes. Our project page is https://oasisyang.github.io/colmap-free-3dgs</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_07504v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>虽然神经渲染在场景重建和新颖的视图合成方面取得了令人印象深刻的进展，但它在很大程度上依赖于精确预计算的相机姿态。为了放松这一限制，已经做出了多项努力来训练神经辐射场（NeRF），而不需要预处理相机姿势。然而，NeRF的隐式表示为同时优化3D结构和相机姿态提供了额外的挑战。另一方面，鉴于其明确的点云表示，最近提出的3D高斯飞溅提供了新的机会。本文利用输入视频流的显式几何表示和连续性来执行新的视图合成，而无需任何SfM预处理。我们以顺序的方式处理输入帧，并通过一次获取一个输入帧来逐步增长3D高斯集，而无需预先计算相机姿势。在大的运动变化下，我们的方法在视图合成和相机姿态估计方面比以前的方法有了显著的改进。我们的项目页面是https://oasisyang.github.io/colmap-free-3dgs</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.07504v1" target="_blank">2312.07504v1</a>
                              </td>
                              <td>COLMAP-Free 3D Gaussian Splatting</td>
                              <td>Yang Fu</td>
                              <td>2023-12-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_07504v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.07504v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_06865v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Keypoint-based Stereophotoclinometry for Characterizing and Navigating Small Bodies: A Factor Graph Approach</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_06865v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_06865v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_06865v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper proposes the incorporation of techniques from stereophotoclinometry (SPC) into a keypoint-based structure-from-motion (SfM) system to estimate the surface normal and albedo at detected landmarks to improve autonomous surface and shape characterization of small celestial bodies from in-situ imagery. In contrast to the current state-of-the-practice method for small body shape reconstruction, i.e., SPC, which relies on human-in-the-loop verification and high-fidelity a priori information to achieve accurate results, we forego the expensive maplet estimation step and instead leverage dense keypoint measurements and correspondences from an autonomous keypoint detection and matching method based on deep learning to provide the necessary photogrammetric constraints. Moreover, we develop a factor graph-based approach allowing for simultaneous optimization of the spacecraft's pose, landmark positions, Sun-relative direction, and surface normals and albedos via fusion of Sun sensor measurements and image keypoint measurements. The proposed framework is validated on real imagery of the Cornelia crater on Asteroid 4 Vesta, along with pose estimation and mapping comparison against an SPC reconstruction, where we demonstrate precise alignment to the SPC solution without relying on any a priori camera pose and topography information or humans-in-the-loop</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_06865v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文建议将立体摄影测斜（SPC）技术纳入基于关键点的运动结构（SfM）系统，以估计探测到的地标的表面法线和反照率，从而从原位图像中改进小型天体的自主表面和形状特征。与小体型重建的实践方法（即SPC）的当前状态相反，SPC依赖于人在环验证和高保真度先验信息来实现准确的结果，我们放弃了昂贵的maplet估计步骤，而是利用基于深度学习的自主关键点检测和匹配方法的密集关键点测量和对应关系来提供必要的摄影测量约束。此外，我们开发了一种基于因子图的方法，通过融合太阳传感器测量和图像关键点测量，可以同时优化航天器的姿态、地标位置、太阳相对方向以及表面法线和反照率。所提出的框架在小行星4灶神星上科妮利亚陨石坑的真实图像上得到了验证，同时还进行了姿态估计和与SPC重建的映射比较，在SPC重建中，我们展示了与SPC解决方案的精确对准，而不依赖于任何先验的相机姿态和地形信息或环中的人类</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.06865v1" target="_blank">2312.06865v1</a>
                              </td>
                              <td>Keypoint-based Stereophotoclinometry for Characterizing and Navigating Small Bodies: A Factor Graph Approach</td>
                              <td>Travis Driver</td>
                              <td>2023-12-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_06865v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.06865v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_06741v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Gaussian Splatting SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_06741v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_06741v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_06741v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present the first application of 3D Gaussian Splatting to incremental 3D reconstruction using a single moving monocular or RGB-D camera. Our Simultaneous Localisation and Mapping (SLAM) method, which runs live at 3fps, utilises Gaussians as the only 3D representation, unifying the required representation for accurate, efficient tracking, mapping, and high-quality rendering. Several innovations are required to continuously reconstruct 3D scenes with high fidelity from a live camera. First, to move beyond the original 3DGS algorithm, which requires accurate poses from an offline Structure from Motion (SfM) system, we formulate camera tracking for 3DGS using direct optimisation against the 3D Gaussians, and show that this enables fast and robust tracking with a wide basin of convergence. Second, by utilising the explicit nature of the Gaussians, we introduce geometric verification and regularisation to handle the ambiguities occurring in incremental 3D dense reconstruction. Finally, we introduce a full SLAM system which not only achieves state-of-the-art results in novel view synthesis and trajectory estimation, but also reconstruction of tiny and even transparent objects.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_06741v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们首次将3D高斯散射应用于使用单个移动单目或RGB-D相机的增量3D重建。我们的同步定位和映射（SLAM）方法以3fps实时运行，使用高斯作为唯一的3D表示，统一了所需的表示，以实现准确、高效的跟踪、映射和高质量渲染。需要一些创新来从现场摄像机连续重建具有高保真度的3D场景。首先，为了超越最初的3DGS算法，该算法需要来自离线运动结构（SfM）系统的精确姿态，我们使用针对3D高斯的直接优化来制定3DGS的相机跟踪，并表明这能够实现快速而稳健的跟踪，并具有广泛的收敛范围。其次，通过利用高斯的显式性质，我们引入了几何验证和正则化来处理增量三维密集重建中出现的模糊性。最后，我们介绍了一个完整的SLAM系统，它不仅在新的视图合成和轨迹估计方面取得了最先进的结果，而且还重建了微小甚至透明的物体。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.06741v1" target="_blank">2312.06741v1</a>
                              </td>
                              <td>Gaussian Splatting SLAM</td>
                              <td>Hidenobu Matsuki</td>
                              <td>2023-12-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_06741v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.06741v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/spla-tam/SplaTAM" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_08479v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DeDoDe: Detect, Don't Describe -- Describe, Don't Detect for Local Feature Matching</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_08479v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_08479v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_08479v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Keypoint detection is a pivotal step in 3D reconstruction, whereby sets of (up to) K points are detected in each view of a scene. Crucially, the detected points need to be consistent between views, i.e., correspond to the same 3D point in the scene. One of the main challenges with keypoint detection is the formulation of the learning objective. Previous learning-based methods typically jointly learn descriptors with keypoints, and treat the keypoint detection as a binary classification task on mutual nearest neighbours. However, basing keypoint detection on descriptor nearest neighbours is a proxy task, which is not guaranteed to produce 3D-consistent keypoints. Furthermore, this ties the keypoints to a specific descriptor, complicating downstream usage. In this work, we instead learn keypoints directly from 3D consistency. To this end, we train the detector to detect tracks from large-scale SfM. As these points are often overly sparse, we derive a semi-supervised two-view detection objective to expand this set to a desired number of detections. To train a descriptor, we maximize the mutual nearest neighbour objective over the keypoints with a separate network. Results show that our approach, DeDoDe, achieves significant gains on multiple geometry benchmarks. Code is provided at https://github.com/Parskatt/DeDoDe</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_08479v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>关键点检测是3D重建中的关键步骤，通过该步骤可以在场景的每个视图中检测到（最多）K个点的集合。至关重要的是，检测到的点需要在视图之间保持一致，即对应于场景中的同一3D点。关键点检测的主要挑战之一是学习目标的制定。以前基于学习的方法通常将描述符与关键点联合学习，并将关键点检测视为对相互最近邻居的二元分类任务。然而，基于描述符最近邻居的关键点检测是一项代理任务，不能保证产生3D一致的关键点。此外，这将关键点与特定描述符联系在一起，使下游使用变得复杂。在这项工作中，我们直接从3D一致性中学习关键点。为此，我们训练检测器来检测大规模SfM中的轨道。由于这些点往往过于稀疏，我们推导出一个半监督的双视图检测目标，以将该集扩展到所需的检测数量。为了训练描述符，我们使用单独的网络在关键点上最大化相互最近邻目标。结果表明，我们的方法DeDoDe在多个几何基准上实现了显著的增益。代码提供于https://github.com/Parskatt/DeDoDe</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.08479v3" target="_blank">2308.08479v3</a>
                              </td>
                              <td>DeDoDe: Detect, Don't Describe -- Describe, Don't Detect for Local Feature Matching</td>
                              <td>Johan Edstedt</td>
                              <td>2023-08-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_08479v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.08479v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/Parskatt/DeDoDe" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/parskatt/dedode" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_05889v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SuperPrimitive: Scene Reconstruction at a Primitive Level</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_05889v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_05889v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_05889v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Joint camera pose and dense geometry estimation from a set of images or a monocular video remains a challenging problem due to its computational complexity and inherent visual ambiguities. Most dense incremental reconstruction systems operate directly on image pixels and solve for their 3D positions using multi-view geometry cues. Such pixel-level approaches suffer from ambiguities or violations of multi-view consistency (e.g. caused by textureless or specular surfaces).   We address this issue with a new image representation which we call a SuperPrimitive. SuperPrimitives are obtained by splitting images into semantically correlated local regions and enhancing them with estimated surface normal directions, both of which are predicted by state-of-the-art single image neural networks. This provides a local geometry estimate per SuperPrimitive, while their relative positions are adjusted based on multi-view observations.   We demonstrate the versatility of our new representation by addressing three 3D reconstruction tasks: depth completion, few-view structure from motion, and monocular dense visual odometry.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_05889v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于其计算复杂性和固有的视觉模糊性，从一组图像或单目视频中进行联合相机姿态和密集几何估计仍然是一个具有挑战性的问题。大多数密集增量重建系统直接对图像像素进行操作，并使用多视图几何提示来求解其3D位置。这种像素级方法存在模糊性或违反多视图一致性的问题（例如，由无纹理或镜面引起）。我们用一种新的图像表示来解决这个问题，我们称之为超原始。超基元是通过将图像分割成语义相关的局部区域并用估计的表面法线方向对其进行增强来获得的，这两者都是由最先进的单图像神经网络预测的。这提供了每个SuperPrimitive的局部几何体估计，而它们的相对位置是基于多视图观测进行调整的。我们通过解决三个3D重建任务来展示我们新表示的多功能性：深度完成、运动中的少量视图结构和单目密集视觉里程计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.05889v1" target="_blank">2312.05889v1</a>
                              </td>
                              <td>SuperPrimitive: Scene Reconstruction at a Primitive Level</td>
                              <td>Kirill Mazur</td>
                              <td>2023-12-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_05889v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.05889v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_17245v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_17245v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_17245v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_17245v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advancements in real-time neural rendering using point-based techniques have paved the way for the widespread adoption of 3D representations. However, foundational approaches like 3D Gaussian Splatting come with a substantial storage overhead caused by growing the SfM points to millions, often demanding gigabyte-level disk space for a single unbounded scene, posing significant scalability challenges and hindering the splatting efficiency.   To address this challenge, we introduce LightGaussian, a novel method designed to transform 3D Gaussians into a more efficient and compact format. Drawing inspiration from the concept of Network Pruning, LightGaussian identifies Gaussians that are insignificant in contributing to the scene reconstruction and adopts a pruning and recovery process, effectively reducing redundancy in Gaussian counts while preserving visual effects. Additionally, LightGaussian employs distillation and pseudo-view augmentation to distill spherical harmonics to a lower degree, allowing knowledge transfer to more compact representations while maintaining reflectance. Furthermore, we propose a hybrid scheme, VecTree Quantization, to quantize all attributes, resulting in lower bitwidth representations with minimal accuracy losses.   In summary, LightGaussian achieves an averaged compression rate over 15x while boosting the FPS from 139 to 215, enabling an efficient representation of complex scenes on Mip-NeRF 360, Tank and Temple datasets.   Project website: https://lightgaussian.github.io/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_17245v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用基于点的技术进行实时神经渲染的最新进展为3D表示的广泛采用铺平了道路。然而，像3D高斯飞溅这样的基础方法会带来大量的存储开销，这是由于SfM点增长到数百万，通常需要千兆字节级别的磁盘空间才能用于单个无边界场景，这对可扩展性提出了重大挑战，并阻碍了飞溅效率。为了应对这一挑战，我们引入了LightGaussian，这是一种新的方法，旨在将3D高斯变换为更高效、更紧凑的格式。LightGaussian从网络修剪的概念中汲取灵感，识别出对场景重建贡献不大的高斯，并采用修剪和恢复过程，有效地减少了高斯计数的冗余，同时保留了视觉效果。此外，LightGaussian采用蒸馏和伪视图增强来提取较低程度的球面谐波，允许将知识转移到更紧凑的表示中，同时保持反射率。此外，我们提出了一种混合方案，VecTree量化，来量化所有属性，从而以最小的精度损失获得较低的位宽表示。总之，LightGaussian实现了超过15倍的平均压缩率，同时将FPS从139提高到215，从而能够在Mip-NeRF 360、Tank和Temple数据集上高效地表示复杂场景。项目网站：https://lightgaussian.github.io/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.17245v3" target="_blank">2311.17245v3</a>
                              </td>
                              <td>LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS</td>
                              <td>Zhiwen Fan</td>
                              <td>2023-11-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_17245v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.17245v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/VITA-Group/LightGaussian" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/VITA-Group/LightGaussian" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04563v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Visual Geometry Grounded Deep Structure From Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04563v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04563v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04563v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Structure-from-motion (SfM) is a long-standing problem in the computer vision community, which aims to reconstruct the camera poses and 3D structure of a scene from a set of unconstrained 2D images. Classical frameworks solve this problem in an incremental manner by detecting and matching keypoints, registering images, triangulating 3D points, and conducting bundle adjustment. Recent research efforts have predominantly revolved around harnessing the power of deep learning techniques to enhance specific elements (e.g., keypoint matching), but are still based on the original, non-differentiable pipeline. Instead, we propose a new deep pipeline VGGSfM, where each component is fully differentiable and thus can be trained in an end-to-end manner. To this end, we introduce new mechanisms and simplifications. First, we build on recent advances in deep 2D point tracking to extract reliable pixel-accurate tracks, which eliminates the need for chaining pairwise matches. Furthermore, we recover all cameras simultaneously based on the image and track features instead of gradually registering cameras. Finally, we optimise the cameras and triangulate 3D points via a differentiable bundle adjustment layer. We attain state-of-the-art performance on three popular datasets, CO3D, IMC Phototourism, and ETH3D.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04563v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>运动结构（SfM）是计算机视觉界的一个长期问题，其目的是从一组不受约束的2D图像中重建场景的相机姿态和3D结构。经典框架通过检测和匹配关键点、配准图像、三角测量3D点和进行束调整，以增量的方式解决了这个问题。最近的研究工作主要围绕着利用深度学习技术的力量来增强特定元素（例如，关键点匹配），但仍基于原始的、不可微分的管道。相反，我们提出了一种新的深度流水线VGGSfM，其中每个组件都是完全可微的，因此可以以端到端的方式进行训练。为此，我们引入了新的机制和简化。首先，我们在深度2D点跟踪的最新进展的基础上提取可靠的像素精确轨迹，这消除了对成对匹配进行链接的需要。此外，我们根据图像和跟踪特征同时恢复所有相机，而不是逐渐注册相机。最后，我们优化相机，并通过可微分束调整层对3D点进行三角测量。我们在三个流行的数据集上获得了最先进的性能，即CO3D、IMC Phototourism和ETH3D。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04563v1" target="_blank">2312.04563v1</a>
                              </td>
                              <td>Visual Geometry Grounded Deep Structure From Motion</td>
                              <td>Jianyuan Wang</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04563v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04563v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_15984v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Structure-from-Motion with Graph Attention Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_15984v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_15984v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_15984v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we tackle the problem of learning Structure-from-Motion (SfM) through the use of graph attention networks. SfM is a classic computer vision problem that is solved though iterative minimization of reprojection errors, referred to as Bundle Adjustment (BA), starting from a good initialization. In order to obtain a good enough initialization to BA, conventional methods rely on a sequence of sub-problems (such as pairwise pose estimation, pose averaging or triangulation) which provides an initial solution that can then be refined using BA. In this work we replace these sub-problems by learning a model that takes as input the 2D keypoints detected across multiple views, and outputs the corresponding camera poses and 3D keypoint coordinates. Our model takes advantage of graph neural networks to learn SfM-specific primitives, and we show that it can be used for fast inference of the reconstruction for new and unseen sequences. The experimental results show that the proposed model outperforms competing learning-based methods, and challenges COLMAP while having lower runtime.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_15984v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们通过使用图注意力网络来解决从运动中学习结构（SfM）的问题。SfM是一个经典的计算机视觉问题，通过迭代最小化重投影误差来解决，称为束调整（BA），从良好的初始化开始。为了获得足够好的BA初始化，传统方法依赖于一系列子问题（如成对姿态估计、姿态平均或三角测量），这些子问题提供了一个初始解决方案，然后可以使用BA进行细化。在这项工作中，我们通过学习一个模型来替换这些子问题，该模型将在多个视图中检测到的2D关键点作为输入，并输出相应的相机姿态和3D关键点坐标。我们的模型利用图神经网络来学习SfM特定的基元，并表明它可以用于新的和看不见的序列的重建的快速推理。实验结果表明，所提出的模型优于竞争的基于学习的方法，并在具有较低运行时间的同时挑战了COLMAP。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.15984v2" target="_blank">2308.15984v2</a>
                              </td>
                              <td>Learning Structure-from-Motion with Graph Attention Networks</td>
                              <td>Lucas Brynte</td>
                              <td>2023-08-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_15984v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.15984v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00451v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00451v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00451v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00451v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Novel view synthesis from limited observations remains an important and persistent task. However, high efficiency in existing NeRF-based few-shot view synthesis is often compromised to obtain an accurate 3D representation. To address this challenge, we propose a few-shot view synthesis framework based on 3D Gaussian Splatting that enables real-time and photo-realistic view synthesis with as few as three training views. The proposed method, dubbed FSGS, handles the extremely sparse initialized SfM points with a thoughtfully designed Gaussian Unpooling process. Our method iteratively distributes new Gaussians around the most representative locations, subsequently infilling local details in vacant areas. We also integrate a large-scale pre-trained monocular depth estimator within the Gaussians optimization process, leveraging online augmented views to guide the geometric optimization towards an optimal solution. Starting from sparse points observed from limited input viewpoints, our FSGS can accurately grow into unseen regions, comprehensively covering the scene and boosting the rendering quality of novel views. Overall, FSGS achieves state-of-the-art performance in both accuracy and rendering efficiency across diverse datasets, including LLFF, Mip-NeRF360, and Blender. Project website: https://zehaozhu.github.io/FSGS/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00451v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从有限的观测中合成新的观点仍然是一项重要而持久的任务。然而，为了获得准确的3D表示，现有的基于NeRF的少镜头视图合成中的高效率经常受到损害。为了应对这一挑战，我们提出了一种基于3D高斯散射的多镜头视图合成框架，该框架能够在只有三个训练视图的情况下进行实时和照片逼真的视图合成。所提出的方法被称为FSGS，通过精心设计的高斯去极化过程来处理极稀疏的初始化SfM点。我们的方法迭代地将新的高斯分布在最具代表性的位置周围，随后在空置区域填充局部细节。我们还在Gaussians优化过程中集成了一个大规模的预训练单目深度估计器，利用在线增强视图来引导几何优化走向最优解。从有限输入视点观察到的稀疏点开始，我们的FSGS可以准确地生长到看不见的区域，全面覆盖场景，提高新视图的渲染质量。总体而言，FSGS在各种数据集（包括LLFF、Mip-NeRF360和Blender）的准确性和渲染效率方面都达到了最先进的性能。项目网站：https://zehaozhu.github.io/FSGS/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00451v1" target="_blank">2312.00451v1</a>
                              </td>
                              <td>FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting</td>
                              <td>Zehao Zhu</td>
                              <td>2023-12-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00451v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00451v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/VITA-Group/FSGS" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_18801v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Distributed Global Structure-from-Motion with a Deep Front-End</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_18801v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_18801v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_18801v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While initial approaches to Structure-from-Motion (SfM) revolved around both global and incremental methods, most recent applications rely on incremental systems to estimate camera poses due to their superior robustness. Though there has been tremendous progress in SfM `front-ends' powered by deep models learned from data, the state-of-the-art (incremental) SfM pipelines still rely on classical SIFT features, developed in 2004. In this work, we investigate whether leveraging the developments in feature extraction and matching helps global SfM perform on par with the SOTA incremental SfM approach (COLMAP). To do so, we design a modular SfM framework that allows us to easily combine developments in different stages of the SfM pipeline. Our experiments show that while developments in deep-learning based two-view correspondence estimation do translate to improvements in point density for scenes reconstructed with global SfM, none of them outperform SIFT when comparing with incremental SfM results on a range of datasets. Our SfM system is designed from the ground up to leverage distributed computation, enabling us to parallelize computation on multiple machines and scale to large scenes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_18801v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>虽然最初的运动结构（SfM）方法围绕着全局和增量方法，但由于其优越的鲁棒性，最近的应用依赖于增量系统来估计相机姿态。尽管通过从数据中学习的深度模型在SfM“前端”方面取得了巨大进展，但最先进的（增量）SfM管道仍然依赖于2004年开发的经典SIFT特征。在这项工作中，我们研究了利用特征提取和匹配的发展是否有助于全局SfM与SOTA增量SfM方法（COLMAP）不相上下。为此，我们设计了一个模块化的SfM框架，使我们能够轻松地将SfM管道不同阶段的开发结合起来。我们的实验表明，虽然基于深度学习的两视图对应性估计的发展确实转化为用全局SfM重建的场景的点密度的提高，但与一系列数据集上的增量SfM结果相比，它们都没有优于SIFT。我们的SfM系统是从头开始设计的，以利用分布式计算，使我们能够在多台机器上并行计算并扩展到大型场景。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.18801v1" target="_blank">2311.18801v1</a>
                              </td>
                              <td>Distributed Global Structure-from-Motion with a Deep Front-End</td>
                              <td>Ayush Baid</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_18801v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.18801v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/borglab/gtsfm" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_11702v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SACReg: Scene-Agnostic Coordinate Regression for Visual Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_11702v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_11702v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_11702v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Scene coordinates regression (SCR), i.e., predicting 3D coordinates for every pixel of a given image, has recently shown promising potential. However, existing methods remain limited to small scenes memorized during training, and thus hardly scale to realistic datasets and scenarios. In this paper, we propose a generalized SCR model trained once to be deployed in new test scenes, regardless of their scale, without any finetuning. Instead of encoding the scene coordinates into the network weights, our model takes as input a database image with some sparse 2D pixel to 3D coordinate annotations, extracted from e.g. off-the-shelf Structure-from-Motion or RGB-D data, and a query image for which are predicted a dense 3D coordinate map and its confidence, based on cross-attention. At test time, we rely on existing off-the-shelf image retrieval systems and fuse the predictions from a shortlist of relevant database images w.r.t. the query. Afterwards camera pose is obtained using standard Perspective-n-Point (PnP). Starting from selfsupervised CroCo pretrained weights, we train our model on diverse datasets to ensure generalizabilty across various scenarios, and significantly outperform other scene regression approaches, including scene-specific models, on multiple visual localization benchmarks. Finally, we show that the database representation of images and their 2D-3D annotations can be highly compressed with negligible loss of localization performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_11702v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>场景坐标回归（SCR），即预测给定图像的每个像素的3D坐标，最近显示出了很有前途的潜力。然而，现有的方法仍然局限于训练期间记忆的小场景，因此很难扩展到真实的数据集和场景。在本文中，我们提出了一个经过一次训练的广义SCR模型，该模型将部署在新的测试场景中，无论其规模如何，而无需任何微调。我们的模型不是将场景坐标编码到网络权重中，而是将具有一些稀疏的2D像素到3D坐标注释的数据库图像作为输入，该数据库图像是从例如现成的运动结构或RGB-D数据中提取的，以及查询图像，基于交叉关注，对其预测密集的3D坐标图及其置信度。在测试时，我们依赖现有的现成图像检索系统，并将相关数据库图像的短名单中的预测与查询相融合。然后，使用标准透视n-Point（PnP）来获得相机姿势。从自监督CroCo预训练的权重开始，我们在不同的数据集上训练我们的模型，以确保在各种场景中的可推广性，并在多个视觉定位基准上显著优于其他场景回归方法，包括场景特定模型。最后，我们证明了图像的数据库表示及其2D-3D注释可以被高度压缩，而定位性能的损失可以忽略不计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.11702v3" target="_blank">2307.11702v3</a>
                              </td>
                              <td>SACReg: Scene-Agnostic Coordinate Regression for Visual Localization</td>
                              <td>Jerome Revaud</td>
                              <td>2023-07-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_11702v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.11702v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11808v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Robot Hand-Eye Calibration using Structure-from-Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11808v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11808v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11808v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we propose a new flexible method for hand-eye calibration. The vast majority of existing hand-eye calibration techniques requires a calibration rig which is used in conjunction with camera pose estimation methods. Instead, we combine structure-from-motion with known robot motions and we show that the solution can be obtained in linear form. The latter solves for both the hand-eye parameters and for the unknown scale factor inherent with structure-from-motion methods. The algebraic analysis that is made possible with such a linear formulation allows to investigate not only the well known case of general screw motions but also such singular motions as pure translations, pure rotations, and planar motions. In essence, the robot-mounted camera looks to an unknown rigid layout, tracks points over an image sequence and estimates the camera-to-robot relationship. Such a self calibration process is relevant for unmanned vehicles, robots working in remote places, and so forth. We conduct a large number of experiments which validate the quality of the method by comparing it with existing ones.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11808v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了一种新的灵活的手眼校准方法。绝大多数现有的手眼校准技术需要与相机姿态估计方法结合使用的校准装置。相反，我们将运动中的结构与已知的机器人运动相结合，证明了该解可以以线性形式获得。后者同时求解手眼参数和运动中结构方法固有的未知比例因子。用这种线性公式进行代数分析不仅可以研究一般螺杆运动的已知情况，还可以研究纯平移、纯旋转和平面运动等奇异运动。本质上，安装在机器人上的相机观察未知的刚性布局，跟踪图像序列上的点，并估计相机与机器人的关系。这种自校准过程与无人车、在偏远地区工作的机器人等相关。我们进行了大量的实验，通过与现有方法的比较验证了该方法的质量。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11808v2" target="_blank">2311.11808v2</a>
                              </td>
                              <td>Robot Hand-Eye Calibration using Structure-from-Motion</td>
                              <td>Nicolas Andreff</td>
                              <td>2023-11-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11808v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11808v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11171v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LOSTU: Fast, Scalable, and Uncertainty-Aware Triangulation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11171v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11171v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11171v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Triangulation algorithms often aim to minimize the reprojection ($L_2$) error, but this only provides the maximum likelihood estimate when there are no errors in the camera parameters or camera poses. Although recent advancements have yielded techniques to estimate camera parameters accounting for 3D point uncertainties, most structure from motion (SfM) pipelines still use older triangulation algorithms. This work leverages recent discoveries to provide a fast, scalable, and statistically optimal way to triangulate called LOSTU. Results show that LOSTU consistently produces lower 3D reconstruction errors than conventional $L_2$ triangulation methods -- often allowing LOSTU to successfully triangulate more points. Moreover, in addition to providing a better 3D reconstruction, LOSTU can be substantially faster than Levenberg-Marquardt (or similar) optimization schemes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11171v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三角测量算法通常旨在最小化重投影（$L_2$）误差，但这仅在相机参数或相机姿态没有误差时提供最大似然估计。尽管最近的进步已经产生了估计相机参数的技术，考虑到3D点的不确定性，但大多数运动结构（SfM）管道仍然使用旧的三角测量算法。这项工作利用最近的发现，提供了一种快速、可扩展和统计优化的三角测量方法，称为LOSTU。结果表明，与传统的$L_2$三角测量方法相比，LOSTU始终产生较低的三维重建误差——通常允许LOSTU成功地对更多的点进行三角测量。此外，除了提供更好的3D重建外，LOSTU可以比Levenberg-Marquardt（或类似）优化方案快得多。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11171v1" target="_blank">2311.11171v1</a>
                              </td>
                              <td>LOSTU: Fast, Scalable, and Uncertainty-Aware Triangulation</td>
                              <td>Sébastien Henry</td>
                              <td>2023-11-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11171v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11171v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_10582v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Human motion trajectory prediction using the Social Force Model for real-time and low computational cost applications</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_10582v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_10582v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_10582v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Human motion trajectory prediction is a very important functionality for human-robot collaboration, specifically in accompanying, guiding, or approaching tasks, but also in social robotics, self-driving vehicles, or security systems. In this paper, a novel trajectory prediction model, Social Force Generative Adversarial Network (SoFGAN), is proposed. SoFGAN uses a Generative Adversarial Network (GAN) and Social Force Model (SFM) to generate different plausible people trajectories reducing collisions in a scene. Furthermore, a Conditional Variational Autoencoder (CVAE) module is added to emphasize the destination learning. We show that our method is more accurate in making predictions in UCY or BIWI datasets than most of the current state-of-the-art models and also reduces collisions in comparison to other approaches. Through real-life experiments, we demonstrate that the model can be used in real-time without GPU's to perform good quality predictions with a low computational cost.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_10582v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人类运动轨迹预测是人机协作的一个非常重要的功能，特别是在伴随、引导或接近任务时，也在社交机器人、自动驾驶车辆或安全系统中。本文提出了一种新的轨迹预测模型——社会力量生成对抗网络（SoFGAN）。SoFGAN使用生成对抗网络（GAN）和社会力量模型（SFM）来生成不同的看似合理的人的轨迹，从而减少场景中的碰撞。此外，增加了条件变分自动编码器（CVAE）模块，以强调目的地学习。我们表明，与当前大多数最先进的模型相比，我们的方法在UCY或BIWI数据集中进行预测时更准确，并且与其他方法相比，还减少了碰撞。通过真实的实验，我们证明了该模型可以在没有GPU的情况下实时使用，以低计算成本执行高质量的预测。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.10582v1" target="_blank">2311.10582v1</a>
                              </td>
                              <td>Human motion trajectory prediction using the Social Force Model for real-time and low computational cost applications</td>
                              <td>Oscar Gil</td>
                              <td>2023-11-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_10582v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.10582v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_03426v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Convex Minimization with Integer Minima in $\widetilde O(n^4)$ Time</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_03426v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_03426v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_03426v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Given a convex function $f$ on $\mathbb{R}^n$ with an integer minimizer, we show how to find an exact minimizer of $f$ using $O(n^2 \log n)$ calls to a separation oracle and $O(n^4 \log n)$ time. The previous best polynomial time algorithm for this problem given in [Jiang, SODA 2021, JACM 2022] achieves $O(n^2\log\log n/\log n)$ oracle complexity. However, the overall runtime of Jiang's algorithm is at least $\widetilde{\Omega}(n^8)$, due to expensive sub-routines such as the Lenstra-Lenstra-Lov\'asz (LLL) algorithm [Lenstra, Lenstra, Lov\'asz, Math. Ann. 1982] and random walk based cutting plane method [Bertsimas, Vempala, JACM 2004]. Our significant speedup is obtained by a nontrivial combination of a faster version of the LLL algorithm due to [Neumaier, Stehl\'e, ISSAC 2016] that gives similar guarantees, the volumetric center cutting plane method (CPM) by [Vaidya, FOCS 1989] and its fast implementation given in [Jiang, Lee, Song, Wong, STOC 2020].   For the special case of submodular function minimization (SFM), our result implies a strongly polynomial time algorithm for this problem using $O(n^3 \log n)$ calls to an evaluation oracle and $O(n^4 \log n)$ additional arithmetic operations. Both the oracle complexity and the number of arithmetic operations of our more general algorithm are better than the previous best-known runtime algorithms for this specific problem given in [Lee, Sidford, Wong, FOCS 2015] and [Dadush, V\'egh, Zambelli, SODA 2018, MOR 2021].</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_03426v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>给定$\mathbb｛R｝^n$上具有整数极小值的凸函数$f$，我们展示了如何使用对分离预言机的$O（n^2\logn）$调用和$O（n ^4\logn）$时间来找到$f$的精确极小值。[Jiang，SODA 2021，JACM 2022]中给出的该问题的先前最佳多项式时间算法实现了$O（n^2 \log\logn/\logn）$oracle复杂度。然而，由于昂贵的子程序，如Lenstra-Lenstra-Lov'asz（LLL）算法[Lenstra，Lenstra，Lov'asz，Math.Ann.1982]和基于随机行走的切割平面方法[Bertsimas，Vempala，JACM 2004]，姜算法的总体运行时间至少为$\widetilde｛\Omega｝（n^8）$。我们的显著加速是通过[Neumaier，Stehl\'e，ISSAC 2016]的LLL算法的更快版本、[Vaidya，FOCS 1989]的体积中心切割平面法（CPM）及其在[Jiang，Lee，Song，Wong，STOC 2020]中给出的快速实现的非平凡组合而获得的。对于子模函数最小化（SFM）的特殊情况，我们的结果暗示了该问题的强多项式时间算法，使用$O（n^3\logn）$对评估预言机的调用和$O（n ^4\logn。对于这一特定问题，我们更通用的算法的预言机复杂性和算术运算次数都优于[Lee，Sidford，Wong，FOCS 2015]和[Dadush，V\'egh，Zambelli，SODA 2018，MOR 2021]中给出的以前最著名的运行时算法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.03426v2" target="_blank">2304.03426v2</a>
                              </td>
                              <td>Convex Minimization with Integer Minima in $\widetilde O(n^4)$ Time</td>
                              <td>Haotian Jiang</td>
                              <td>2023-04-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_03426v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.03426v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_06137v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MonoProb: Self-Supervised Monocular Depth Estimation with Interpretable Uncertainty</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_06137v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_06137v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_06137v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised monocular depth estimation methods aim to be used in critical applications such as autonomous vehicles for environment analysis. To circumvent the potential imperfections of these approaches, a quantification of the prediction confidence is crucial to guide decision-making systems that rely on depth estimation. In this paper, we propose MonoProb, a new unsupervised monocular depth estimation method that returns an interpretable uncertainty, which means that the uncertainty reflects the expected error of the network in its depth predictions. We rethink the stereo or the structure-from-motion paradigms used to train unsupervised monocular depth models as a probabilistic problem. Within a single forward pass inference, this model provides a depth prediction and a measure of its confidence, without increasing the inference time. We then improve the performance on depth and uncertainty with a novel self-distillation loss for which a student is supervised by a pseudo ground truth that is a probability distribution on depth output by a teacher. To quantify the performance of our models we design new metrics that, unlike traditional ones, measure the absolute performance of uncertainty predictions. Our experiments highlight enhancements achieved by our method on standard depth and uncertainty metrics as well as on our tailored metrics. https://github.com/CEA-LIST/MonoProb</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_06137v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督单目深度估计方法旨在用于关键应用，如用于环境分析的自动驾驶汽车。为了避免这些方法的潜在缺陷，预测置信度的量化对于指导依赖深度估计的决策系统至关重要。在本文中，我们提出了MonoProb，这是一种新的无监督单目深度估计方法，它返回可解释的不确定性，这意味着不确定性反映了网络在深度预测中的预期误差。我们将用于训练无监督单目深度模型的运动范式中的立体或结构重新思考为一个概率问题。在单次前向推理中，该模型提供深度预测及其置信度的测量，而不增加推理时间。然后，我们通过一种新颖的自蒸馏损失来提高深度和不确定性方面的性能，对于这种损失，学生受到伪基本真理的监督，伪基本真理是教师输出的深度上的概率分布。为了量化我们模型的性能，我们设计了新的指标，与传统指标不同，这些指标衡量不确定性预测的绝对性能。我们的实验强调了我们的方法在标准深度和不确定性指标以及我们定制的指标上实现的增强。https://github.com/CEA-LIST/MonoProb</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.06137v1" target="_blank">2311.06137v1</a>
                              </td>
                              <td>MonoProb: Self-Supervised Monocular Depth Estimation with Interpretable Uncertainty</td>
                              <td>Rémi Marsal</td>
                              <td>2023-11-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_06137v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.06137v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/cea-list/monoprob" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_05323v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Spatial Attention-based Distribution Integration Network for Human Pose Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_05323v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_05323v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_05323v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, human pose estimation has made significant progress through the implementation of deep learning techniques. However, these techniques still face limitations when confronted with challenging scenarios, including occlusion, diverse appearances, variations in illumination, and overlap. To cope with such drawbacks, we present the Spatial Attention-based Distribution Integration Network (SADI-NET) to improve the accuracy of localization in such situations. Our network consists of three efficient models: the receptive fortified module (RFM), spatial fusion module (SFM), and distribution learning module (DLM). Building upon the classic HourglassNet architecture, we replace the basic block with our proposed RFM. The RFM incorporates a dilated residual block and attention mechanism to expand receptive fields while enhancing sensitivity to spatial information. In addition, the SFM incorporates multi-scale characteristics by employing both global and local attention mechanisms. Furthermore, the DLM, inspired by residual log-likelihood estimation (RLE), reconfigures a predicted heatmap using a trainable distribution weight. For the purpose of determining the efficacy of our model, we conducted extensive experiments on the MPII and LSP benchmarks. Particularly, our model obtained a remarkable $92.10\%$ percent accuracy on the MPII test dataset, demonstrating significant improvements over existing models and establishing state-of-the-art performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_05323v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，通过深度学习技术的实施，人体姿态估计取得了重大进展。然而，当面临具有挑战性的场景时，这些技术仍然面临限制，包括遮挡、不同的外观、照明的变化和重叠。为了解决这些缺点，我们提出了基于空间注意力的分布集成网络（SADI-NET）来提高这种情况下的定位精度。我们的网络由三个有效的模型组成：接受强化模块（RFM）、空间融合模块（SFM）和分布学习模块（DLM）。在经典HourglassNet架构的基础上，我们用我们提出的RFM取代了基本块。RFM结合了扩张的残差块和注意力机制，以扩大感受野，同时增强对空间信息的敏感性。此外，通过采用全局和局部注意力机制，SFM融合了多尺度特征。此外，受残差对数似然估计（RLE）的启发，DLM使用可训练分布权重重新配置预测热图。为了确定我们的模型的有效性，我们在MPII和LSP基准上进行了广泛的实验。特别是，我们的模型在MPII测试数据集上获得了92.10%$%的显著准确率，证明了与现有模型相比的显著改进，并建立了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.05323v1" target="_blank">2311.05323v1</a>
                              </td>
                              <td>Spatial Attention-based Distribution Integration Network for Human Pose Estimation</td>
                              <td>Sihan Gao</td>
                              <td>2023-11-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_05323v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.05323v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_04634v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">VET: Visual Error Tomography for Point Cloud Completion and High-Quality Neural Rendering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_04634v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_04634v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_04634v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In the last few years, deep neural networks opened the doors for big advances in novel view synthesis. Many of these approaches are based on a (coarse) proxy geometry obtained by structure from motion algorithms. Small deficiencies in this proxy can be fixed by neural rendering, but larger holes or missing parts, as they commonly appear for thin structures or for glossy regions, still lead to distracting artifacts and temporal instability. In this paper, we present a novel neural-rendering-based approach to detect and fix such deficiencies. As a proxy, we use a point cloud, which allows us to easily remove outlier geometry and to fill in missing geometry without complicated topological operations. Keys to our approach are (i) a differentiable, blending point-based renderer that can blend out redundant points, as well as (ii) the concept of Visual Error Tomography (VET), which allows us to lift 2D error maps to identify 3D-regions lacking geometry and to spawn novel points accordingly. Furthermore, (iii) by adding points as nested environment maps, our approach allows us to generate high-quality renderings of the surroundings in the same pipeline. In our results, we show that our approach can improve the quality of a point cloud obtained by structure from motion and thus increase novel view synthesis quality significantly. In contrast to point growing techniques, the approach can also fix large-scale holes and missing thin structures effectively. Rendering quality outperforms state-of-the-art methods and temporal stability is significantly improved, while rendering is possible at real-time frame rates.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_04634v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在过去的几年里，深度神经网络为新视图合成的巨大进步打开了大门。这些方法中的许多都是基于通过结构从运动算法获得的（粗略）代理几何结构。这种代理中的小缺陷可以通过神经渲染来修复，但较大的孔洞或缺失部分，通常出现在薄结构或光滑区域，仍然会导致分散注意力的伪影和时间不稳定。在本文中，我们提出了一种新的基于神经渲染的方法来检测和修复这些缺陷。作为代理，我们使用点云，这使我们能够轻松删除异常几何体并填充缺失的几何体，而无需复杂的拓扑操作。我们方法的关键是（i）一种可微分的、基于混合点的渲染器，它可以混合掉多余的点，以及（ii）视觉误差层析成像（VET）的概念，它允许我们提升2D误差图，以识别缺乏几何结构的3D区域，并相应地生成新的点。此外，（iii）通过添加点作为嵌套的环境贴图，我们的方法使我们能够在同一管道中生成高质量的周围环境渲染图。在我们的结果中，我们表明我们的方法可以提高由结构从运动中获得的点云的质量，从而显著提高新视图合成的质量。与点生长技术相比，该方法还可以有效地修复大规模孔洞和缺失的薄结构。渲染质量优于最先进的方法，时间稳定性显著提高，同时可以以实时帧速率进行渲染。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.04634v1" target="_blank">2311.04634v1</a>
                              </td>
                              <td>VET: Visual Error Tomography for Point Cloud Completion and High-Quality Neural Rendering</td>
                              <td>Linus Franke</td>
                              <td>2023-11-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_04634v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.04634v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/lfranke/vet" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03404v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RGB-D Mapping and Tracking in a Plenoxel Radiance Field</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03404v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03404v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03404v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The widespread adoption of Neural Radiance Fields (NeRFs) have ensured significant advances in the domain of novel view synthesis in recent years. These models capture a volumetric radiance field of a scene, creating highly convincing, dense, photorealistic models through the use of simple, differentiable rendering equations. Despite their popularity, these algorithms suffer from severe ambiguities in visual data inherent to the RGB sensor, which means that although images generated with view synthesis can visually appear very believable, the underlying 3D model will often be wrong. This considerably limits the usefulness of these models in practical applications like Robotics and Extended Reality (XR), where an accurate dense 3D reconstruction otherwise would be of significant value. In this paper, we present the vital differences between view synthesis models and 3D reconstruction models. We also comment on why a depth sensor is essential for modeling accurate geometry in general outward-facing scenes using the current paradigm of novel view synthesis methods. Focusing on the structure-from-motion task, we practically demonstrate this need by extending the Plenoxel radiance field model: Presenting an analytical differential approach for dense mapping and tracking with radiance fields based on RGB-D data without a neural network. Our method achieves state-of-the-art results in both mapping and tracking tasks, while also being faster than competing neural network-based approaches. The code is available at: https://github.com/ysus33/RGB-D_Plenoxel_Mapping_Tracking.git</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03404v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，神经辐射场（NeRF）的广泛采用确保了新视图合成领域的重大进展。这些模型捕捉场景的体积辐射场，通过使用简单的、可微分的渲染方程创建高度令人信服的、密集的照片真实感模型。尽管这些算法很受欢迎，但RGB传感器固有的视觉数据存在严重的模糊性，这意味着尽管通过视图合成生成的图像在视觉上看起来非常可信，但底层的3D模型往往是错误的。这大大限制了这些模型在机器人和扩展现实（XR）等实际应用中的有用性，否则，精确的密集3D重建将具有重要价值。在本文中，我们介绍了视图合成模型和三维重建模型之间的重要区别。我们还评论了为什么深度传感器对于使用当前新型视图合成方法的范式在一般面向外的场景中建模精确的几何体至关重要。专注于运动任务中的结构，我们通过扩展Plenoxel辐射场模型来实际证明这一需求：在没有神经网络的情况下，基于RGB-D数据，提出了一种用于辐射场密集映射和跟踪的解析微分方法。我们的方法在映射和跟踪任务中都取得了最先进的结果，同时也比竞争对手的基于神经网络的方法更快。代码位于：https://github.com/ysus33/RGB-D_Plenoxel_Mapping_Tracking.git</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03404v2" target="_blank">2307.03404v2</a>
                              </td>
                              <td>RGB-D Mapping and Tracking in a Plenoxel Radiance Field</td>
                              <td>Andreas L. Teigen</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03404v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03404v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/ysus33/rgb-d_plenoxel_mapping_tracking" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_14364v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Quantitative Evaluation of Dense 3D Reconstruction of Sinus Anatomy from Monocular Endoscopic Video</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_14364v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_14364v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_14364v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Generating accurate 3D reconstructions from endoscopic video is a promising avenue for longitudinal radiation-free analysis of sinus anatomy and surgical outcomes. Several methods for monocular reconstruction have been proposed, yielding visually pleasant 3D anatomical structures by retrieving relative camera poses with structure-from-motion-type algorithms and fusion of monocular depth estimates. However, due to the complex properties of the underlying algorithms and endoscopic scenes, the reconstruction pipeline may perform poorly or fail unexpectedly. Further, acquiring medical data conveys additional challenges, presenting difficulties in quantitatively benchmarking these models, understanding failure cases, and identifying critical components that contribute to their precision. In this work, we perform a quantitative analysis of a self-supervised approach for sinus reconstruction using endoscopic sequences paired with optical tracking and high-resolution computed tomography acquired from nine ex-vivo specimens. Our results show that the generated reconstructions are in high agreement with the anatomy, yielding an average point-to-mesh error of 0.91 mm between reconstructions and CT segmentations. However, in a point-to-point matching scenario, relevant for endoscope tracking and navigation, we found average target registration errors of 6.58 mm. We identified that pose and depth estimation inaccuracies contribute equally to this error and that locally consistent sequences with shorter trajectories generate more accurate reconstructions. These results suggest that achieving global consistency between relative camera poses and estimated depths with the anatomy is essential. In doing so, we can ensure proper synergy between all components of the pipeline for improved reconstructions that will facilitate clinical application of this innovative technology.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_14364v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从内窥镜视频中生成准确的3D重建是对鼻窦解剖结构和手术结果进行纵向无辐射分析的一种很有前途的途径。已经提出了几种单目重建方法，通过从运动类型算法中检索具有结构的相对相机姿态并融合单目深度估计，产生视觉上令人愉快的3D解剖结构。然而，由于底层算法和内窥镜场景的复杂特性，重建管道可能表现不佳或意外失败。此外，获取医疗数据带来了额外的挑战，在定量基准测试这些模型、了解故障案例和确定有助于其准确性的关键组件方面存在困难。在这项工作中，我们对自监督鼻窦重建方法进行了定量分析，该方法使用内窥镜序列与从9个离体标本中采集的光学跟踪和高分辨率计算机断层扫描相结合。我们的结果表明，生成的重建与解剖结构高度一致，在重建和CT分割之间产生0.91mm的平均点到网格误差。然而，在与内窥镜跟踪和导航相关的点对点匹配场景中，我们发现平均目标配准误差为6.58 mm。我们发现，姿态和深度估计的不准确度对该误差的贡献相同，并且轨迹较短的局部一致序列会产生更准确的重建。这些结果表明，实现相对相机姿态和估计深度与解剖结构之间的全局一致性至关重要。通过这样做，我们可以确保管道的所有组成部分之间的适当协同作用，以改进重建，从而促进这项创新技术的临床应用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.14364v1" target="_blank">2310.14364v1</a>
                              </td>
                              <td>A Quantitative Evaluation of Dense 3D Reconstruction of Sinus Anatomy from Monocular Endoscopic Video</td>
                              <td>Jan Emily Mangulabnan</td>
                              <td>2023-10-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_14364v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.14364v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_13605v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FMRT: Learning Accurate Feature Matching with Reconciliatory Transformer</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_13605v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_13605v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_13605v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Local Feature Matching, an essential component of several computer vision tasks (e.g., structure from motion and visual localization), has been effectively settled by Transformer-based methods. However, these methods only integrate long-range context information among keypoints with a fixed receptive field, which constrains the network from reconciling the importance of features with different receptive fields to realize complete image perception, hence limiting the matching accuracy. In addition, these methods utilize a conventional handcrafted encoding approach to integrate the positional information of keypoints into the visual descriptors, which limits the capability of the network to extract reliable positional encoding message. In this study, we propose Feature Matching with Reconciliatory Transformer (FMRT), a novel Transformer-based detector-free method that reconciles different features with multiple receptive fields adaptively and utilizes parallel networks to realize reliable positional encoding. Specifically, FMRT proposes a dedicated Reconciliatory Transformer (RecFormer) that consists of a Global Perception Attention Layer (GPAL) to extract visual descriptors with different receptive fields and integrate global context information under various scales, Perception Weight Layer (PWL) to measure the importance of various receptive fields adaptively, and Local Perception Feed-forward Network (LPFFN) to extract deep aggregated multi-scale local feature representation. Extensive experiments demonstrate that FMRT yields extraordinary performance on multiple benchmarks, including pose estimation, visual localization, homography estimation, and image matching.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_13605v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>局部特征匹配是计算机视觉任务（如运动结构和视觉定位）的重要组成部分，已通过基于Transformer的方法得到有效解决。然而，这些方法只将关键点之间的长程上下文信息与固定的感受野相结合，这限制了网络协调特征与不同感受野的重要性以实现完整的图像感知，从而限制了匹配精度。此外，这些方法利用传统的手工编码方法将关键点的位置信息集成到视觉描述符中，这限制了网络提取可靠位置编码消息的能力。在这项研究中，我们提出了具有协调变换器的特征匹配（FMRT），这是一种新的基于变换器的无检测器方法，它自适应地协调不同特征与多个感受野，并利用并行网络实现可靠的位置编码。具体而言，FMRT提出了一种专用的协调转换器（RecFormer），该转换器由全局感知注意力层（GPAL）组成，用于提取具有不同感受野的视觉描述符并整合各种尺度下的全局上下文信息，感知权重层（PWL）用于自适应地测量各种感受野的重要性，以及局部感知前馈网络（LPFFN）来提取深度聚合的多尺度局部特征表示。大量实验表明，FMRT在多个基准上产生了非凡的性能，包括姿态估计、视觉定位、单应性估计和图像匹配。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.13605v1" target="_blank">2310.13605v1</a>
                              </td>
                              <td>FMRT: Learning Accurate Feature Matching with Reconciliatory Transformer</td>
                              <td>Xinyu Zhang</td>
                              <td>2023-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_13605v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.13605v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09109v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09109v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09109v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09109v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in neural reconstruction enable high-quality 3D object reconstruction from casually captured image collections. Current techniques mostly analyze their progress on relatively simple image collections where Structure-from-Motion (SfM) techniques can provide ground-truth (GT) camera poses. We note that SfM techniques tend to fail on in-the-wild image collections such as image search results with varying backgrounds and illuminations. To enable systematic research progress on 3D reconstruction from casual image captures, we propose NAVI: a new dataset of category-agnostic image collections of objects with high-quality 3D scans along with per-image 2D-3D alignments providing near-perfect GT camera parameters. These 2D-3D alignments allow us to extract accurate derivative annotations such as dense pixel correspondences, depth and segmentation maps. We demonstrate the use of NAVI image collections on different problem settings and show that NAVI enables more thorough evaluations that were not possible with existing datasets. We believe NAVI is beneficial for systematic research progress on 3D reconstruction and correspondence estimation. Project page: https://navidataset.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09109v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经重建的最新进展使得能够从随意捕获的图像集合中进行高质量的3D对象重建。当前的技术主要分析它们在相对简单的图像采集上的进展，其中运动结构（SfM）技术可以提供地面实况（GT）相机姿态。我们注意到，SfM技术在野生图像集合中往往失败，例如具有不同背景和照明的图像搜索结果。为了实现从随意图像捕获的3D重建的系统研究进展，我们提出了NAVI：一个具有高质量3D扫描的对象的类别不可知图像集合的新数据集，以及提供近乎完美的GT相机参数的每张图像2D-3D对齐。这些2D-3D比对允许我们提取精确的导数注释，例如密集像素对应、深度和分割图。我们展示了NAVI图像集合在不同问题设置中的使用，并表明NAVI能够实现现有数据集无法实现的更彻底的评估。我们认为NAVI有利于三维重建和对应关系估计的系统研究进展。项目页面：https://navidataset.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09109v2" target="_blank">2306.09109v2</a>
                              </td>
                              <td>NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations</td>
                              <td>Varun Jampani</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09109v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09109v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_05504v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Colmap-PCD: An Open-source Tool for Fine Image-to-point cloud Registration</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_05504v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_05504v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_05504v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>State-of-the-art techniques for monocular camera reconstruction predominantly rely on the Structure from Motion (SfM) pipeline. However, such methods often yield reconstruction outcomes that lack crucial scale information, and over time, accumulation of images leads to inevitable drift issues. In contrast, mapping methods based on LiDAR scans are popular in large-scale urban scene reconstruction due to their precise distance measurements, a capability fundamentally absent in visual-based approaches. Researchers have made attempts to utilize concurrent LiDAR and camera measurements in pursuit of precise scaling and color details within mapping outcomes. However, the outcomes are subject to extrinsic calibration and time synchronization precision. In this paper, we propose a novel cost-effective reconstruction pipeline that utilizes a pre-established LiDAR map as a fixed constraint to effectively address the inherent scale challenges present in monocular camera reconstruction. To our knowledge, our method is the first to register images onto the point cloud map without requiring synchronous capture of camera and LiDAR data, granting us the flexibility to manage reconstruction detail levels across various areas of interest. To facilitate further research in this domain, we have released Colmap-PCD${^{3}}$, an open-source tool leveraging the Colmap algorithm, that enables precise fine-scale registration of images to the point cloud map.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_05504v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>用于单目相机重建的现有技术主要依赖于运动结构（SfM）流水线。然而，这种方法往往会产生缺乏关键尺度信息的重建结果，随着时间的推移，图像的积累会导致不可避免的漂移问题。相比之下，基于激光雷达扫描的地图绘制方法由于其精确的距离测量而在大规模城市场景重建中很受欢迎，而这在基于视觉的方法中根本不具备。研究人员试图利用激光雷达和相机的同时测量，在地图绘制结果中追求精确的缩放和颜色细节。然而，结果受到外部校准和时间同步精度的影响。在本文中，我们提出了一种新的具有成本效益的重建管道，该管道利用预先建立的激光雷达图作为固定约束，以有效解决单目相机重建中存在的固有规模挑战。据我们所知，我们的方法是第一个将图像配准到点云图上，而不需要同步捕获相机和激光雷达数据，这使我们能够灵活地管理各个感兴趣区域的重建细节水平。为了促进该领域的进一步研究，我们发布了Colmap PCD$｛^｛3｝｝$，这是一款利用Colmap算法的开源工具，可以将图像精确地精细配准到点云地图上。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.05504v1" target="_blank">2310.05504v1</a>
                              </td>
                              <td>Colmap-PCD: An Open-source Tool for Fine Image-to-point cloud Registration</td>
                              <td>Chunge Bai</td>
                              <td>2023-10-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_05504v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.05504v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/xiaobaiiiiii/colmap-pcd" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="LLM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2401_17268v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Weaver: Foundation Models for Creative Writing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17268v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17268v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17268v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This work introduces Weaver, our first family of large language models (LLMs) dedicated to content creation. Weaver is pre-trained on a carefully selected corpus that focuses on improving the writing capabilities of large language models. We then fine-tune Weaver for creative and professional writing purposes and align it to the preference of professional writers using a suit of novel methods for instruction data synthesis and LLM alignment, making it able to produce more human-like texts and follow more diverse instructions for content creation. The Weaver family consists of models of Weaver Mini (1.8B), Weaver Base (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for different applications and can be dynamically dispatched by a routing agent according to query complexity to balance response quality and computation cost. Evaluation on a carefully curated benchmark for assessing the writing capabilities of LLMs shows Weaver models of all sizes outperform generalist LLMs several times larger than them. Notably, our most-capable Weaver Ultra model surpasses GPT-4, a state-of-the-art generalist LLM, on various writing scenarios, demonstrating the advantage of training specialized LLMs for writing purposes. Moreover, Weaver natively supports retrieval-augmented generation (RAG) and function calling (tool usage). We present various use cases of these abilities for improving AI-assisted writing systems, including integration of external knowledge bases, tools, or APIs, and providing personalized writing assistance. Furthermore, we discuss and summarize a guideline and best practices for pre-training and fine-tuning domain-specific LLMs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17268v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>这部作品介绍了Weaver，这是我们第一个致力于内容创建的大型语言模型（LLM）家族。Weaver是在精心挑选的语料库上进行预训练的，该语料库侧重于提高大型语言模型的写作能力。然后，我们对Weaver进行微调，以实现创造性和专业写作目的，并使用一套新颖的教学数据合成和LLM对齐方法，使其符合专业作家的偏好，使其能够生成更人性化的文本，并遵循更多样的内容创作指示。Weaver系列由Weaver Mini（1.8B）、Weaver Base（6B）、Wea弗Pro（14B）和Weaver Ultra（34B）尺寸的模型组成，适用于不同的应用程序，并且可以由路由代理根据查询复杂性动态调度，以平衡响应质量和计算成本。对评估LLM写作能力的精心策划的基准进行的评估表明，各种规模的Weaver模型都优于比它们大几倍的多面手LLM。值得注意的是，我们最强大的Weaver Ultra模型在各种写作场景上都超过了GPT-4，这是一种最先进的多面手LLM，展示了为写作目的训练专业LLM的优势。此外，Weaver本机支持检索增强生成（RAG）和函数调用（工具使用）。我们介绍了这些能力的各种用例，用于改进人工智能辅助写作系统，包括集成外部知识库、工具或API，以及提供个性化写作辅助。此外，我们讨论并总结了预训练和微调特定领域LLM的指导方针和最佳实践。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17268v1" target="_blank">2401.17268v1</a>
                              </td>
                              <td>Weaver: Foundation Models for Creative Writing</td>
                              <td>Tiannan Wang</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17268v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17268v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17267v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ReacLLaMA: Merging chemical and textual information in chemical reactivity AI models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17267v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17267v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17267v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Chemical reactivity models are developed to predict chemical reaction outcomes in the form of classification (success/failure) or regression (product yield) tasks. The vast majority of the reported models are trained solely on chemical information such as reactants, products, reagents, and solvents, but not on the details of a synthetic protocol. Herein incorporation of procedural text with the aim to augment the Graphormer reactivity model and improve its accuracy is presented. Two major approaches are used: training an adapter Graphormer model that is provided with a GPT-2-derived latent representation of the text procedure (ReacLLaMA-Adapter) and labeling an unlabeled part of a dataset with the LLaMA 2 model followed by training the Graphormer on an extended dataset (Zero-Shot Labeling ReacLLaMA). Both methodologies enhance the discernment of unpromising reactions, thereby providing more accurate models with improved specificity.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17267v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>开发化学反应性模型，以分类（成功/失败）或回归（产品产量）任务的形式预测化学反应结果。绝大多数报道的模型仅根据化学信息（如反应物、产物、试剂和溶剂）进行训练，而不根据合成方案的细节进行训练。本文引入程序文本，旨在增强Graphormer反应性模型并提高其准确性。使用两种主要方法：训练适配器Graphormer模型，该模型提供了文本过程的GPT-2衍生的潜在表示（ReacLLaMA-adapter），并用LLaMA 2模型标记数据集的未标记部分，然后在扩展数据集上训练Graphormers（零样本标记ReacLLaMA）。这两种方法都能增强对无希望反应的辨别力，从而提供更准确、特异性更强的模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17267v1" target="_blank">2401.17267v1</a>
                              </td>
                              <td>ReacLLaMA: Merging chemical and textual information in chemical reactivity AI models</td>
                              <td>Aline Hartgers</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17267v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17267v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_17227v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_17227v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_17227v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_17227v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Can we avoid wars at the crossroads of history? This question has been pursued by individuals, scholars, policymakers, and organizations throughout human history. In this research, we attempt to answer the question based on the recent advances of Artificial Intelligence (AI) and Large Language Models (LLMs). We propose \textbf{WarAgent}, an LLM-powered multi-agent AI system, to simulate the participating countries, their decisions, and the consequences, in historical international conflicts, including the World War I (WWI), the World War II (WWII), and the Warring States Period (WSP) in Ancient China. By evaluating the simulation effectiveness, we examine the advancements and limitations of cutting-edge AI systems' abilities in studying complex collective human behaviors such as international conflicts under diverse settings. In these simulations, the emergent interactions among agents also offer a novel perspective for examining the triggers and conditions that lead to war. Our findings offer data-driven and AI-augmented insights that can redefine how we approach conflict resolution and peacekeeping strategies. The implications stretch beyond historical analysis, offering a blueprint for using AI to understand human history and possibly prevent future international conflicts. Code and data are available at \url{https://github.com/agiresearch/WarAgent}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_17227v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们能在历史的十字路口避免战争吗？在整个人类历史上，个人、学者、决策者和组织都在探讨这个问题。在这项研究中，我们试图根据人工智能（AI）和大型语言模型（LLM）的最新进展来回答这个问题。我们提出\textbf{WarAgent}，这是一个LLM驱动的多智能体人工智能系统，用于模拟历史国际冲突中的参与国及其决策和后果，包括第一次世界大战、第二次世界大战和中国古代的战国时期。通过评估模拟的有效性，我们考察了尖端人工智能系统在研究复杂的人类集体行为（如不同环境下的国际冲突）方面的能力的进步和局限性。在这些模拟中，代理人之间的紧急互动也为研究导致战争的触发因素和条件提供了一个新的视角。我们的研究结果提供了数据驱动和人工智能增强的见解，可以重新定义我们如何处理冲突解决和维和战略。其影响超越了历史分析，为使用人工智能了解人类历史并可能防止未来的国际冲突提供了蓝图。代码和数据位于\url{https://github.com/agiresearch/WarAgent}.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.17227v2" target="_blank">2311.17227v2</a>
                              </td>
                              <td>War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars</td>
                              <td>Wenyue Hua</td>
                              <td>2023-11-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_17227v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.17227v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/agiresearch/waragent" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17256v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Weak-to-Strong Jailbreaking on Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17256v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17256v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17256v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Although significant efforts have been dedicated to aligning large language models (LLMs), red-teaming reports suggest that these carefully aligned LLMs could still be jailbroken through adversarial prompts, tuning, or decoding. Upon examining the jailbreaking vulnerability of aligned LLMs, we observe that the decoding distributions of jailbroken and aligned models differ only in the initial generations. This observation motivates us to propose the weak-to-strong jailbreaking attack, where adversaries can utilize smaller unsafe/aligned LLMs (e.g., 7B) to guide jailbreaking against significantly larger aligned LLMs (e.g., 70B). To jailbreak, one only needs to additionally decode two smaller LLMs once, which involves minimal computation and latency compared to decoding the larger LLMs. The efficacy of this attack is demonstrated through experiments conducted on five models from three different organizations. Our study reveals a previously unnoticed yet efficient way of jailbreaking, exposing an urgent safety issue that needs to be considered when aligning LLMs. As an initial attempt, we propose a defense strategy to protect against such attacks, but creating more advanced defenses remains challenging. The code for replicating the method is available at https://github.com/XuandongZhao/weak-to-strong</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17256v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管在对齐大型语言模型（LLM）方面做出了重大努力，但红队报告表明，这些精心对齐的LLM仍可能通过对抗性提示、调整或解码而被破解。在检查对齐LLM的越狱漏洞后，我们观察到越狱模型和对齐模型的解码分布仅在最初几代中不同。这一观察结果促使我们提出从弱到强的越狱攻击，其中对手可以利用较小的不安全/一致LLM（例如，7B）来指导针对明显较大的一致LLM的越狱（例如，70B）。要越狱，只需要额外解码两个较小的LLM一次，与解码较大的LLM相比，这涉及最小的计算和延迟。通过对来自三个不同组织的五个模型进行的实验，证明了这种攻击的有效性。我们的研究揭示了一种以前未被注意但有效的越狱方式，暴露了一个在调整LLM时需要考虑的紧迫安全问题。作为初步尝试，我们提出了一种防御策略来抵御此类攻击，但创建更先进的防御仍然具有挑战性。用于复制该方法的代码位于https://github.com/XuandongZhao/weak-to-strong</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17256v1" target="_blank">2401.17256v1</a>
                              </td>
                              <td>Weak-to-Strong Jailbreaking on Large Language Models</td>
                              <td>Xuandong Zhao</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17256v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17256v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/xuandongzhao/weak-to-strong" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17244v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17244v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17244v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17244v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Reducing hallucination of Large Language Models (LLMs) is imperative for use in the sciences where reproducibility is crucial. However, LLMs inherently lack long-term memory, making it a nontrivial, ad hoc, and inevitably biased task to fine-tune them on domain-specific literature and data. Here we introduce LLaMP, a multimodal retrieval-augmented generation (RAG) framework of multiple data-aware reasoning-and-acting (ReAct) agents that dynamically interact with computational and experimental data on Materials Project (MP). Without fine-tuning, LLaMP demonstrates an ability to comprehend and integrate various modalities of materials science concepts, fetch relevant data stores on the fly, process higher-order data (such as crystal structures and elastic tensors), and summarize multi-step procedures for solid-state synthesis. We show that LLaMP effectively corrects errors in GPT-3.5's intrinsic knowledge, reducing a 5.21% MAPE on frequently-documented bandgaps and a significant 1103.54% MAPE on formation energies -- errors that GPT-3.5 seems to derive from mixed data sources. Additionally, LLaMP substantially reduces the hallucinated volumetric strain in a diamond cubic silicon structure from 66.3% to 0. The proposed framework offers an intuitive and nearly hallucination-free approach to exploring materials informatics and establishes a pathway for knowledge distillation and fine-tuning other language models. We envision the framework as a valuable component for scientific hypotheses and a foundation for future autonomous laboratories where multiple LLM agents communicate and cooperate with robotics to drive material synthesis and chemical reactions without hard-coded human logic and intervention.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17244v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在再现性至关重要的科学中，减少大型语言模型（LLM）的幻觉是必不可少的。然而，LLM天生缺乏长期记忆，因此根据特定领域的文献和数据对其进行微调是一项非平凡的、临时的、不可避免的有偏见的任务。在这里，我们介绍LLaMP，这是一种多模式检索增强生成（RAG）框架，由多个数据感知推理和行动（ReAct）代理组成，可与材料项目（MP）的计算和实验数据动态交互。在没有微调的情况下，LLaMP展示了理解和集成材料科学概念的各种模式、动态获取相关数据存储、处理高阶数据（如晶体结构和弹性张量）以及总结固态合成多步骤程序的能力。我们表明，LLaMP有效地纠正了GPT-3.5内在知识中的错误，在频繁记录的带隙上减少了5.21%的MAPE，在形成能上减少了1103.54%的MAPE——GPT-3.5的错误似乎来自混合数据源。此外，LLaMP将金刚石立方硅结构中的幻觉体积应变从66.3%显著降低到0。所提出的框架为探索材料信息学提供了一种直观且几乎没有幻觉的方法，并为知识提炼和微调其他语言模型建立了一条途径。我们设想该框架是科学假设的一个有价值的组成部分，也是未来自主实验室的基础，在未来的自主实验室中，多个LLM代理与机器人通信和合作，在没有硬编码的人类逻辑和干预的情况下驱动材料合成和化学反应。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17244v1" target="_blank">2401.17244v1</a>
                              </td>
                              <td>LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation</td>
                              <td>Yuan Chiang</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17244v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17244v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_11863v4_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Scaling laws for language encoding models in fMRI</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_11863v4_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_11863v4_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_11863v4_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Representations from transformer-based unidirectional language models are known to be effective at predicting brain responses to natural language. However, most studies comparing language models to brains have used GPT-2 or similarly sized language models. Here we tested whether larger open-source models such as those from the OPT and LLaMA families are better at predicting brain responses recorded using fMRI. Mirroring scaling results from other contexts, we found that brain prediction performance scales logarithmically with model size from 125M to 30B parameter models, with ~15% increased encoding performance as measured by correlation with a held-out test set across 3 subjects. Similar logarithmic behavior was observed when scaling the size of the fMRI training set. We also characterized scaling for acoustic encoding models that use HuBERT, WavLM, and Whisper, and we found comparable improvements with model size. A noise ceiling analysis of these large, high-performance encoding models showed that performance is nearing the theoretical maximum for brain areas such as the precuneus and higher auditory cortex. These results suggest that increasing scale in both models and data will yield incredibly effective models of language processing in the brain, enabling better scientific understanding as well as applications such as decoding.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_11863v4_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于转换器的单向语言模型的表示在预测大脑对自然语言的反应方面是有效的。然而，大多数将语言模型与大脑进行比较的研究都使用了GPT-2或类似大小的语言模型。在这里，我们测试了更大的开源模型，如OPT和LLaMA家族的模型，是否更善于预测使用fMRI记录的大脑反应。反映了其他情况下的缩放结果，我们发现大脑预测性能与模型大小成对数比例，从125M到30B参数模型，通过与3名受试者的测试集的相关性测量，编码性能提高了约15%。当缩放fMRI训练集的大小时，观察到类似的对数行为。我们还对使用HuBERT、WavLM和Whisper的声学编码模型的缩放进行了表征，并发现了与模型大小相当的改进。对这些大型高性能编码模型的噪声上限分析表明，大脑前叶和高级听觉皮层等区域的性能接近理论最大值。这些结果表明，增加模型和数据的规模将产生令人难以置信的有效的大脑语言处理模型，从而实现更好的科学理解以及解码等应用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.11863v4" target="_blank">2305.11863v4</a>
                              </td>
                              <td>Scaling laws for language encoding models in fMRI</td>
                              <td>Richard Antonello</td>
                              <td>2023-05-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_11863v4_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.11863v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/huthlab/encoding-model-scaling-laws" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_07900v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Ambiguity-Aware In-Context Learning with Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_07900v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_07900v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_07900v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In-context learning (ICL) i.e. showing LLMs only a few task-specific demonstrations has led to downstream gains with no task-specific fine-tuning required. However, LLMs are sensitive to the choice of prompts, and therefore a crucial research question is how to select good demonstrations for ICL. One effective strategy is leveraging semantic similarity between the ICL demonstrations and test inputs by using a text retriever, which however is sub-optimal as that does not consider the LLM's existing knowledge about that task. From prior work (Lyu et al., 2023), we already know that labels paired with the demonstrations bias the model predictions. This leads us to our hypothesis whether considering LLM's existing knowledge about the task, especially with respect to the output label space can help in a better demonstration selection strategy. Through extensive experimentation on three text classification tasks, we find that it is beneficial to not only choose semantically similar ICL demonstrations but also to choose those demonstrations that help resolve the inherent label ambiguity surrounding the test example. Interestingly, we find that including demonstrations that the LLM previously mis-classified and also fall on the test example's decision boundary, brings the most performance gain.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_07900v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在上下文学习（ICL）中，即只显示少数特定任务的演示，导致了下游收益，而不需要特定任务的微调。然而，LLM对提示的选择很敏感，因此一个关键的研究问题是如何为ICL选择好的演示。一种有效的策略是通过使用文本检索器来利用ICL演示和测试输入之间的语义相似性，然而，这是次优的，因为它不考虑LLM关于该任务的现有知识。从之前的工作（Lyu et al.，2023）中，我们已经知道，与演示配对的标签会使模型预测产生偏差。这就引出了我们的假设，即考虑LLM关于任务的现有知识，特别是关于输出标签空间的知识，是否有助于更好的演示选择策略。通过对三个文本分类任务的广泛实验，我们发现不仅选择语义相似的ICL演示是有益的，而且选择那些有助于解决测试示例周围固有的标签歧义的演示也是有益的。有趣的是，我们发现，包括LLM之前错误分类的演示，以及落在测试示例的决策边界上的演示，带来了最大的性能增益。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.07900v2" target="_blank">2309.07900v2</a>
                              </td>
                              <td>Ambiguity-Aware In-Context Learning with Large Language Models</td>
                              <td>Lingyu Gao</td>
                              <td>2023-09-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_07900v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.07900v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17221v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MouSi: Poly-Visual-Expert Vision-Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17221v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17221v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17221v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Current large vision-language models (VLMs) often encounter challenges such as insufficient capabilities of a single visual component and excessively long visual tokens. These issues can limit the model's effectiveness in accurately interpreting complex visual information and over-lengthy contextual information. Addressing these challenges is crucial for enhancing the performance and applicability of VLMs. This paper proposes the use of ensemble experts technique to synergizes the capabilities of individual visual encoders, including those skilled in image-text matching, OCR, image segmentation, etc. This technique introduces a fusion network to unify the processing of outputs from different visual experts, while bridging the gap between image encoders and pre-trained LLMs. In addition, we explore different positional encoding schemes to alleviate the waste of positional encoding caused by lengthy image feature sequences, effectively addressing the issue of position overflow and length limitations. For instance, in our implementation, this technique significantly reduces the positional occupancy in models like SAM, from a substantial 4096 to a more efficient and manageable 64 or even down to 1. Experimental results demonstrate that VLMs with multiple experts exhibit consistently superior performance over isolated visual encoders and mark a significant performance boost as more experts are integrated. We have open-sourced the training code used in this report. All of these resources can be found on our project website.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17221v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当前的大型视觉语言模型（VLM）经常遇到诸如单个视觉组件能力不足和视觉标记过长等挑战。这些问题可能会限制模型准确解释复杂视觉信息和过长上下文信息的有效性。应对这些挑战对于提高VLM的性能和适用性至关重要。本文提出使用集成专家技术来协同各个视觉编码器的能力，包括那些擅长图像文本匹配、OCR、图像分割等的编码器。该技术引入了一个融合网络来统一处理不同视觉专家的输出，同时弥合图像编码器和预训练LLM之间的差距。此外，我们探索了不同的位置编码方案，以减轻图像特征序列过长造成的位置编码浪费，有效地解决了位置溢出和长度限制的问题。例如，在我们的实现中，该技术显著降低了SAM等模型中的位置占用率，从相当大的4096降低到更高效和可管理的64，甚至降低到1。实验结果表明，与孤立的视觉编码器相比，具有多个专家的VLM始终表现出优异的性能，并且随着更多专家的集成，性能显著提高。我们已经开源了本报告中使用的培训代码。所有这些资源都可以在我们的项目网站上找到。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17221v1" target="_blank">2401.17221v1</a>
                              </td>
                              <td>MouSi: Poly-Visual-Expert Vision-Language Models</td>
                              <td>Xiaoran Fan</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17221v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17221v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17217v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GazeGPT: Augmenting Human Capabilities using Gaze-contingent Contextual AI for Smart Eyewear</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17217v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17217v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17217v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multimodal large language models (LMMs) excel in world knowledge and problem-solving abilities. Through the use of a world-facing camera and contextual AI, emerging smart accessories aim to provide a seamless interface between humans and LMMs. Yet, these wearable computing systems lack an understanding of the user's attention. We introduce GazeGPT as a new user interaction paradigm for contextual AI. GazeGPT uses eye tracking to help the LMM understand which object in the world-facing camera view a user is paying attention to. Using extensive user evaluations, we show that this gaze-contingent mechanism is a faster and more accurate pointing mechanism than alternatives; that it augments human capabilities by significantly improving their accuracy in a dog-breed classification task; and that it is consistently ranked as more natural than head- or body-driven selection mechanisms for contextual AI. Moreover, we prototype a variety of application scenarios that suggest GazeGPT could be of significant value to users as part of future AI-driven personal assistants.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17217v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多模式大型语言模型（LMM）在世界知识和解决问题的能力方面表现出色。通过使用面向世界的摄像头和上下文AI，新兴的智能配件旨在提供人类和LMM之间的无缝接口。然而，这些可穿戴计算系统缺乏对用户注意力的理解。我们引入GazeGPT作为上下文人工智能的一种新的用户交互范式。GazeGPT使用眼动追踪来帮助LMM了解用户正在关注世界上面向相机的视图中的哪个对象。通过广泛的用户评估，我们表明这种基于凝视的机制是一种比其他机制更快、更准确的指向机制；它通过显著提高犬种分类任务的准确性来增强人类的能力；并且它一直被认为比头部或身体驱动的上下文人工智能选择机制更自然。此外，我们对各种应用场景进行了原型设计，这些场景表明GazeGPT作为未来人工智能驱动的个人助理的一部分，对用户可能具有重要价值。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17217v1" target="_blank">2401.17217v1</a>
                              </td>
                              <td>GazeGPT: Augmenting Human Capabilities using Gaze-contingent Contextual AI for Smart Eyewear</td>
                              <td>Robert Konrad</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17217v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17217v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17197v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Data-efficient Fine-tuning for LLM-based Recommendation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17197v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17197v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17197v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Leveraging Large Language Models (LLMs) for recommendation has recently garnered considerable attention, where fine-tuning plays a key role in LLMs' adaptation. However, the cost of fine-tuning LLMs on rapidly expanding recommendation data limits their practical application. To address this challenge, few-shot fine-tuning offers a promising approach to quickly adapt LLMs to new recommendation data. We propose the task of data pruning for efficient LLM-based recommendation, aimed at identifying representative samples tailored for LLMs' few-shot fine-tuning. While coreset selection is closely related to the proposed task, existing coreset selection methods often rely on suboptimal heuristic metrics or entail costly optimization on large-scale recommendation data.   To tackle these issues, we introduce two objectives for the data pruning task in the context of LLM-based recommendation: 1) high accuracy aims to identify the influential samples that can lead to high overall performance; and 2) high efficiency underlines the low costs of the data pruning process. To pursue the two objectives, we propose a novel data pruning method based on two scores, i.e., influence score and effort score, to efficiently identify the influential samples. Particularly, the influence score is introduced to accurately estimate the influence of sample removal on the overall performance. To achieve low costs of the data pruning process, we use a small-sized surrogate model to replace LLMs to obtain the influence score. Considering the potential gap between the surrogate model and LLMs, we further propose an effort score to prioritize some hard samples specifically for LLMs. Empirical results on three real-world datasets validate the effectiveness of our proposed method. In particular, the proposed method uses only 2% samples to surpass the full data fine-tuning, reducing time costs by 97%.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17197v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>利用大型语言模型（LLM）进行推荐最近引起了相当大的关注，其中微调在LLM的适应中起着关键作用。然而，在快速扩展的推荐数据上微调LLM的成本限制了它们的实际应用。为了应对这一挑战，少镜头微调提供了一种很有前途的方法，可以快速调整LLM以适应新的推荐数据。我们提出了基于LLM的高效推荐的数据修剪任务，旨在识别为LLM的少量微调量身定制的代表性样本。虽然核心集选择与所提出的任务密切相关，但现有的核心集选择方法往往依赖于次优启发式指标，或者需要对大规模推荐数据进行代价高昂的优化。为了解决这些问题，我们在基于LLM的推荐的背景下引入了数据修剪任务的两个目标：1）高精度旨在识别能够导致高整体性能的有影响力的样本；以及2）高效率强调了数据修剪过程的低成本。为了实现这两个目标，我们提出了一种新的基于两个得分（即影响得分和努力得分）的数据修剪方法，以有效地识别有影响的样本。特别地，引入影响分数来准确估计样本去除对整体性能的影响。为了实现数据修剪过程的低成本，我们使用小型代理模型来代替LLM来获得影响分数。考虑到代理模型和LLM之间的潜在差距，我们进一步提出了一个努力得分，以优先考虑一些专门针对LLM的硬样本。在三个真实世界数据集上的实验结果验证了我们提出的方法的有效性。特别地，所提出的方法仅使用2%的样本来超过全数据微调，将时间成本降低了97%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17197v1" target="_blank">2401.17197v1</a>
                              </td>
                              <td>Data-efficient Fine-tuning for LLM-based Recommendation</td>
                              <td>Xinyu Lin</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17197v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17197v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17181v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Transfer Learning for Text Diffusion Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17181v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17181v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17181v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this report, we explore the potential for text diffusion to replace autoregressive (AR) decoding for the training and deployment of large language models (LLMs). We are particularly interested to see whether pretrained AR models can be transformed into text diffusion models through a lightweight adaptation procedure we call ``AR2Diff''. We begin by establishing a strong baseline setup for training text diffusion models. Comparing across multiple architectures and pretraining objectives, we find that training a decoder-only model with a prefix LM objective is best or near-best across several tasks. Building on this finding, we test various transfer learning setups for text diffusion models. On machine translation, we find that text diffusion underperforms the standard AR approach. However, on code synthesis and extractive QA, we find diffusion models trained from scratch outperform AR models in many cases. We also observe quality gains from AR2Diff -- adapting AR models to use diffusion decoding. These results are promising given that text diffusion is relatively underexplored and can be significantly faster than AR decoding for long text generation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17181v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本报告中，我们探讨了文本扩散取代自回归（AR）解码用于大型语言模型（LLM）的训练和部署的潜力。我们特别感兴趣的是，通过我们称之为“AR2Diff”的轻量级自适应过程，是否可以将预训练的AR模型转换为文本扩散模型。我们首先为训练文本扩散模型建立一个强大的基线设置。通过在多个体系结构和预训练目标之间进行比较，我们发现在几个任务中，训练具有前缀LM目标的仅解码器模型是最好的或接近最好的。基于这一发现，我们测试了文本扩散模型的各种迁移学习设置。在机器翻译方面，我们发现文本扩散不如标准的AR方法。然而，在代码合成和提取QA方面，我们发现从头开始训练的扩散模型在许多情况下优于AR模型。我们还观察到AR2Diff的质量增益——将AR模型调整为使用扩散解码。这些结果是有希望的，因为文本扩散相对未被充分开发，并且对于长文本生成来说，可以明显快于AR解码。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17181v1" target="_blank">2401.17181v1</a>
                              </td>
                              <td>Transfer Learning for Text Diffusion Models</td>
                              <td>Kehang Han</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17181v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17181v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17169v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Conditional and Modal Reasoning in Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17169v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17169v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17169v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The reasoning abilities of large language models (LLMs) are the topic of a growing body of research in artificial intelligence and cognitive science. In this paper, we probe the extent to which a dozen LLMs are able to distinguish logically correct inferences from logically fallacious ones. We focus on inference patterns involving conditionals (e.g., 'If Ann has a queen, then Bob has a jack') and epistemic modals (e.g., 'Ann might have an ace', 'Bob must have a king'). These inference patterns have been of special interest to logicians, philosophers, and linguists, since they plausibly play a central role in human reasoning. Assessing LLMs on these inference patterns is thus highly relevant to the question of how much the reasoning abilities of LLMs match those of humans. Among the LLMs we tested, all but GPT-4 often make basic mistakes with conditionals. Moreover, even GPT-4 displays logically inconsistent judgments across inference patterns involving epistemic modals.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17169v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）的推理能力是人工智能和认知科学领域越来越多研究的主题。在本文中，我们探讨了十几个LLM能够在多大程度上区分逻辑正确的推论和逻辑错误的推论。我们专注于涉及条件句（例如，“如果安有女王，那么鲍勃就有杰克”）和认知模态（例如，”安可能有王牌“，”鲍勃必须有国王“）的推理模式。逻辑学家、哲学家和语言学家对这些推理模式特别感兴趣，因为它们似乎在人类推理中发挥着核心作用。因此，根据这些推理模式评估LLM与LLM的推理能力在多大程度上与人类的推理能力相匹配的问题高度相关。在我们测试的LLM中，除了GPT-4之外，所有LLM都经常在条件句方面犯基本错误。此外，甚至GPT-4在涉及认知模态的推理模式中也显示出逻辑上不一致的判断。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17169v1" target="_blank">2401.17169v1</a>
                              </td>
                              <td>Conditional and Modal Reasoning in Large Language Models</td>
                              <td>Wesley H. Holliday</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17169v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17169v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17167v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool Utilization in Real-World Complex Scenarios</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17167v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17167v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17167v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The recent trend of using Large Language Models (LLMs) as intelligent agents in real-world applications underscores the necessity for comprehensive evaluations of their capabilities, particularly in complex scenarios involving planning, creating, and using tools. However, existing benchmarks typically focus on simple synthesized queries that do not reflect real-world complexity, thereby offering limited perspectives in evaluating tool utilization. To address this issue, we present UltraTool, a novel benchmark designed to improve and evaluate LLMs' ability in tool utilization within real-world scenarios. UltraTool focuses on the entire process of using tools - from planning and creating to applying them in complex tasks. It emphasizes real-world complexities, demanding accurate, multi-step planning for effective problem-solving. A key feature of UltraTool is its independent evaluation of planning with natural language, which happens before tool usage and simplifies the task solving by mapping out the intermediate steps. Thus, unlike previous work, it eliminates the restriction of pre-defined toolset during planning. Through extensive experiments on various LLMs, we offer novel insights into the evaluation of capabilities of LLMs in tool utilization, thereby contributing a fresh perspective to this rapidly evolving field. The benchmark is publicly available at https://github.com/JoeYing1019/UltraTool.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17167v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近在现实世界应用中使用大型语言模型（LLM）作为智能代理的趋势强调了对其能力进行全面评估的必要性，特别是在涉及规划、创建和使用工具的复杂场景中。然而，现有的基准测试通常侧重于简单的综合查询，这些查询不能反映现实世界的复杂性，从而在评估工具利用率方面提供了有限的视角。为了解决这个问题，我们提出了UltraTool，这是一种新颖的基准测试，旨在提高和评估LLM在现实世界场景中的工具利用能力。UltraTool专注于使用工具的整个过程——从规划和创建到将其应用于复杂任务。它强调现实世界的复杂性，要求准确的多步骤规划以有效解决问题。UltraTool的一个关键功能是它使用自然语言对规划进行独立评估，这发生在工具使用之前，并通过规划中间步骤简化了任务解决。因此，与之前的工作不同，它消除了规划过程中预定义工具集的限制。通过对各种LLM的广泛实验，我们对LLM在工具利用方面的能力评估提供了新的见解，从而为这一快速发展的领域提供了一个新的视角。该基准可在https://github.com/JoeYing1019/UltraTool.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17167v1" target="_blank">2401.17167v1</a>
                              </td>
                              <td>Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool Utilization in Real-World Complex Scenarios</td>
                              <td>Shijue Huang</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17167v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17167v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17163v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo Chat</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17163v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17163v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17163v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) have the potential to fundamentally change the way people engage in computer programming. Agent-based modeling (ABM) has become ubiquitous in natural and social sciences and education, yet no prior studies have explored the potential of LLMs to assist it. We designed NetLogo Chat to support the learning and practice of NetLogo, a programming language for ABM. To understand how users perceive, use, and need LLM-based interfaces, we interviewed 30 participants from global academia, industry, and graduate schools. Experts reported more perceived benefits than novices and were more inclined to adopt LLMs in their workflow. We found significant differences between experts and novices in their perceptions, behaviors, and needs for human-AI collaboration. We surfaced a knowledge gap between experts and novices as a possible reason for the benefit gap. We identified guidance, personalization, and integration as major needs for LLM-based interfaces to support the programming of ABM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17163v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）有可能从根本上改变人们参与计算机编程的方式。基于代理的建模（ABM）在自然科学、社会科学和教育中已经无处不在，但之前没有研究探索LLM的潜力来帮助它。我们设计了NetLogo聊天来支持NetLogo的学习和实践，这是一种ABM的编程语言。为了了解用户如何感知、使用和需要基于LLM的界面，我们采访了来自全球学术界、工业界和研究生院的30名参与者。专家们报告说，与新手相比，他们感知到的好处更多，并且更倾向于在工作流程中采用LLM。我们发现，专家和新手在感知、行为和人类人工智能协作需求方面存在显著差异。我们发现专家和新手之间的知识差距可能是造成利益差距的原因。我们将指导、个性化和集成确定为基于LLM的接口的主要需求，以支持ABM的编程。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17163v1" target="_blank">2401.17163v1</a>
                              </td>
                              <td>Learning Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo Chat</td>
                              <td>John Chen</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17163v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17163v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_12585v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SLANG: New Concept Comprehension of Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_12585v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_12585v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_12585v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The dynamic nature of language, particularly evident in the realm of slang and memes on the Internet, poses serious challenges to the adaptability of large language models (LLMs). Traditionally anchored to static datasets, these models often struggle to keep up with the rapid linguistic evolution characteristic of online communities. This research addresses the critical need to bridge this gap, aiming to enhance LLMs' comprehension of the evolving new concepts on the internet, without the high cost of continual retraining. To address this issue, we propose a new benchmark $\textbf{SLANG}$, which can autonomously integrates novel data to stay dataset up-to-date, to assess LLMs' capability in comprehending emerging concepts and an approach $\textbf{FOCUS}$, which uses causal inference to enhance LLMs to understand new phrases and their colloquial context. This benchmark and approach involves digesting real-world instances of linguistic shifts, serving as contextual beacons, to form more precise and contextually relevant connections between newly emerging expressions and their intended meanings. The empirical analysis shows that our causal inference-based approach outperforms the traditional models in terms of precision and relevance in the interpretation of internet slang and memes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_12585v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语言的动态性，尤其是在互联网上的俚语和模因领域，对大型语言模型的适应性提出了严重挑战。传统上，这些模型以静态数据集为基础，往往难以跟上在线社区的快速语言进化特征。这项研究解决了弥合这一差距的关键需求，旨在提高LLM对互联网上不断发展的新概念的理解，而无需持续再培训的高昂成本。为了解决这个问题，我们提出了一个新的基准$\textbf｛SLANG｝$，它可以自主集成新数据以保持数据集的最新状态，以评估LLM理解新兴概念的能力，以及一种方法$\textbf｛FOCUS｝，它使用因果推理来增强LLM理解新短语及其口语上下文的能力。这种基准和方法包括消化现实世界中的语言变化实例，作为上下文信标，在新出现的表达及其预期含义之间形成更精确和上下文相关的联系。实证分析表明，我们基于因果推理的方法在解释网络俚语和模因的准确性和相关性方面优于传统模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.12585v2" target="_blank">2401.12585v2</a>
                              </td>
                              <td>SLANG: New Concept Comprehension of Large Language Models</td>
                              <td>Lingrui Mei</td>
                              <td>2024-01-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_12585v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.12585v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17139v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Large Language Model Evaluation via Matrix Entropy</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17139v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17139v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17139v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have revolutionized the field of natural language processing, extending their strong capabilities into multi-modal domains. Thus, it is vital to define proper and diversified metrics for the evaluation of LLMs.   In this paper, we introduce matrix entropy, a novel metric rooted in information theory and geometry principles to quantify the data compression proficiency in LLMs. It reflects the model's ability to extract relevant information and eliminate unnecessary elements, thereby providing insight into the language model's intrinsic capability. Specifically, we demonstrate its applicability in both single-modal (language) and multi-modal settings. For language models, our findings reveal that the matrix entropy of representations follows a scaling law type reduction when the model scales up, serving as a complement to the traditional loss scaling law. For the multi-modal setting, we also propose an evaluation method based on matrix entropy for assessing alignment quality and we find that modern large multi-modal models exhibit great alignment performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17139v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）已经彻底改变了自然语言处理领域，将其强大的功能扩展到多模态领域。因此，为LLM的评估定义适当和多样化的指标至关重要。在本文中，我们引入了矩阵熵，这是一种植根于信息论和几何原理的新度量，用于量化LLM中的数据压缩能力。它反映了模型提取相关信息和消除不必要元素的能力，从而深入了解语言模型的内在能力。具体来说，我们展示了它在单模态（语言）和多模态设置中的适用性。对于语言模型，我们的研究结果表明，当模型向上扩展时，表示的矩阵熵遵循比例律类型的约简，这是对传统损失比例律的补充。对于多模态环境，我们还提出了一种基于矩阵熵的评估方法来评估对准质量，我们发现现代大型多模态模型表现出良好的对准性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17139v1" target="_blank">2401.17139v1</a>
                              </td>
                              <td>Large Language Model Evaluation via Matrix Entropy</td>
                              <td>Lai Wei</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17139v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17139v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_12570v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Creativity Support in the Age of Large Language Models: An Empirical Study Involving Emerging Writers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_12570v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_12570v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_12570v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The development of large language models (LLMs) capable of following instructions and engaging in conversational interactions sparked increased interest in their utilization across various support tools. We investigate the utility of modern LLMs in assisting professional writers via an empirical user study (n=30). The design of our collaborative writing interface is grounded in the cognitive process model of writing that views writing as a goal-oriented thinking process encompassing non-linear cognitive activities: planning, translating, and reviewing. Participants are asked to submit a post-completion survey to provide feedback on the potential and pitfalls of LLMs as writing collaborators. Upon analyzing the writer-LLM interactions, we find that while writers seek LLM's help across all three types of cognitive activities, they find LLMs more helpful in translation and reviewing. Our findings from analyzing both the interactions and the survey responses highlight future research directions in creative writing assistance using LLMs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_12570v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>能够遵循指令并参与对话交互的大型语言模型（LLM）的开发引发了人们对其在各种支持工具中的使用越来越感兴趣。我们通过实证用户研究（n=30）调查了现代LLM在帮助专业作家方面的效用。我们合作写作界面的设计基于写作的认知过程模型，该模型将写作视为一个目标导向的思维过程，包括非线性认知活动：计划、翻译和复习。参与者被要求提交一份完成后调查，就LLM作为写作合作者的潜力和陷阱提供反馈。通过分析作家与LLM的互动，我们发现，当作家在所有三种类型的认知活动中寻求LLM的帮助时，他们发现LLM在翻译和评论中更有帮助。我们分析互动和调查结果的结果突出了使用LLM进行创造性写作辅助的未来研究方向。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.12570v3" target="_blank">2309.12570v3</a>
                              </td>
                              <td>Creativity Support in the Age of Large Language Models: An Empirical Study Involving Emerging Writers</td>
                              <td>Tuhin Chakrabarty</td>
                              <td>2023-09-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_12570v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.12570v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17120v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PlantoGraphy: Incorporating Iterative Design Process into Generative Artificial Intelligence for Landscape Rendering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17120v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17120v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17120v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Landscape renderings are realistic images of landscape sites, allowing stakeholders to perceive better and evaluate design ideas. While recent advances in Generative Artificial Intelligence (GAI) enable automated generation of landscape renderings, the end-to-end methods are not compatible with common design processes, leading to insufficient alignment with design idealizations and limited cohesion of iterative landscape design. Informed by a formative study for comprehending design requirements, we present PlantoGraphy, an iterative design system that allows for interactive configuration of GAI models to accommodate human-centered design practice. A two-stage pipeline is incorporated: first, concretization module transforms conceptual ideas into concrete scene layouts with a domain-oriented large language model; and second, illustration module converts scene layouts into realistic landscape renderings using a fine-tuned low-rank adaptation diffusion model. PlantoGraphy has undergone a series of performance evaluations and user studies, demonstrating its effectiveness in landscape rendering generation and the high recognition of its interactive functionality.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17120v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>景观效果图是景观现场的真实图像，使利益相关者能够更好地感知和评估设计理念。虽然生成人工智能（GAI）的最新进展使景观效果图的自动生成成为可能，但端到端的方法与常见的设计流程不兼容，导致与设计理想化的一致性不足，迭代景观设计的凝聚力有限。在理解设计需求的形成性研究的基础上，我们提出了PlantoGrapy，这是一个迭代设计系统，允许GAI模型的交互式配置，以适应以人为中心的设计实践。具体化模块采用面向领域的大型语言模型，将概念概念转化为具体的场景布局；其次，插图模块使用微调的低阶自适应扩散模型将场景布局转换为逼真的景观渲染。PlantoGraphy经过了一系列性能评估和用户研究，证明了其在景观渲染生成方面的有效性及其交互功能的高度认可。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17120v1" target="_blank">2401.17120v1</a>
                              </td>
                              <td>PlantoGraphy: Incorporating Iterative Design Process into Generative Artificial Intelligence for Landscape Rendering</td>
                              <td>Rong Huang</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17120v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17120v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_16160v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_16160v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_16160v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_16160v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Instruction finetuning on a variety of image-text instruction data is the key to obtaining a versatile Multimodal Large Language Model (MLLM), and different configurations of the instruction data can lead to finetuned models with different capabilities. However, we have discovered that data conflicts are inevitable when mixing instruction data from distinct domains, which can result in performance drops for tasks of a specific domain. To address this issue, we propose to apply an efficient Mixture of Experts (MoE) design, which is a sparse Mixture of LoRA Experts (MoLE) for instruction finetuning MLLMs. Within the Transformer layers, we extend the popular Low-Rank Adaption (LoRA) method by creating a set of LoRA experts specifically for the MLP layer, and route each token to the top-1 expert based on a routing function, allowing adaptive choices for tokens from different domains. Since the LoRA experts are sparsely activated, the training and inference cost are kept roughly constant compared to the original LoRA method. By replacing the plain-LoRA of LLaVA-1.5 with our MoE design, our final model is named LLaVA-MoLE. Extensive experiments proved that LLaVA-MoLE effectively mitigates the data conflict issue when mixing multiple distinct instruction datasets with various configurations, and achieves consistent performance gains over the strong plain-LoRA baselines. Most importantly, on the mixed datasets, LLaVA-MoLE can even outperform the plain-LoRA baseline trained with twice the samples.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_16160v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对各种图像文本指令数据的指令微调是获得通用的多模式大型语言模型（MLLM）的关键，而指令数据的不同配置可以导致具有不同能力的微调模型。然而，我们发现，当混合来自不同域的指令数据时，数据冲突是不可避免的，这可能会导致特定域的任务性能下降。为了解决这个问题，我们建议应用一种有效的专家混合（MoE）设计，这是一种用于指令微调MLLMs的LoRA专家稀疏混合（MoLE）。在Transformer层中，我们通过创建一组专门针对MLP层的LoRA专家来扩展流行的低秩自适应（LoRA）方法，并根据路由函数将每个令牌路由到排名前1的专家，从而允许对来自不同域的令牌进行自适应选择。由于LoRA专家很少被激活，因此与原始的LoRA方法相比，训练和推理成本大致保持不变。通过用我们的MoE设计取代LLaVA-1.5的普通LoRA，我们的最终模型被命名为LLaVA-MoLE。大量实验证明，当将多个不同的指令数据集与各种配置混合时，LLaVA-MoLE有效地缓解了数据冲突问题，并在强纯LoRA基线上实现了一致的性能提升。最重要的是，在混合数据集上，LLaVA-MoLE甚至可以优于用两倍样本训练的普通LoRA基线。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.16160v2" target="_blank">2401.16160v2</a>
                              </td>
                              <td>LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs</td>
                              <td>Shaoxiang Chen</td>
                              <td>2024-01-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_16160v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.16160v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14673v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Generative Expressive Robot Behaviors using Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14673v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14673v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14673v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>People employ expressive behaviors to effectively communicate and coordinate their actions with others, such as nodding to acknowledge a person glancing at them or saying "excuse me" to pass people in a busy corridor. We would like robots to also demonstrate expressive behaviors in human-robot interaction. Prior work proposes rule-based methods that struggle to scale to new communication modalities or social situations, while data-driven methods require specialized datasets for each social situation the robot is used in. We propose to leverage the rich social context available from large language models (LLMs) and their ability to generate motion based on instructions or user preferences, to generate expressive robot motion that is adaptable and composable, building upon each other. Our approach utilizes few-shot chain-of-thought prompting to translate human language instructions into parametrized control code using the robot's available and learned skills. Through user studies and simulation experiments, we demonstrate that our approach produces behaviors that users found to be competent and easy to understand. Supplementary material can be found at https://generative-expressive-motion.github.io/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14673v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人们采用富有表现力的行为来有效地与他人沟通和协调自己的行为，比如点头向瞥了他们一眼的人表示感谢，或者在繁忙的走廊里与人擦肩而过时说“对不起”。我们希望机器人也能在人机交互中表现出富有表现力的行为。先前的工作提出了基于规则的方法，这些方法很难扩展到新的通信模式或社交环境，而数据驱动的方法需要机器人所使用的每种社交环境的专门数据集。我们建议利用大型语言模型（LLM）中丰富的社交环境及其基于指令或用户偏好生成运动的能力，生成可适应和组合的富有表现力的机器人动作，相互构建。我们的方法利用机器人现有的和学到的技能，利用少量的思维链提示将人类语言指令翻译成参数化的控制代码。通过用户研究和模拟实验，我们证明了我们的方法产生了用户认为有能力且易于理解的行为。补充材料可在https://generative-expressive-motion.github.io/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14673v2" target="_blank">2401.14673v2</a>
                              </td>
                              <td>Generative Expressive Robot Behaviors using Large Language Models</td>
                              <td>Karthik Mahadevan</td>
                              <td>2024-01-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14673v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14673v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_09003v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Augmenting Math Word Problems via Iterative Question Composing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_09003v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_09003v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_09003v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite the advancements in large language models (LLMs) for mathematical reasoning, solving competition-level math problems remains a significant challenge, especially for open-source LLMs without external tools. We introduce the MMIQC dataset, comprising a mixture of processed web data and synthetic question-response pairs, aimed at enhancing the mathematical reasoning capabilities of base language models. Models fine-tuned on MMIQC consistently surpass their counterparts in performance on the MATH benchmark across various model sizes. Notably, Qwen-72B-MMIQC achieves a 45.0% accuracy, exceeding the previous open-source state-of-the-art by 8.2% and outperforming the initial version GPT-4 released in 2023. Extensive evaluation results on Hungarian high school finals suggest that such improvement can generalize to unseen data. Our ablation study on MMIQC reveals that a large part of the improvement can be attributed to our novel augmentation method, Iterative Question Composing (IQC), which involves iteratively composing new questions from seed problems using an LLM and applying rejection sampling through another LLM. The MMIQC dataset is available on the HuggingFace hub at https://huggingface.co/datasets/Vivacem/MMIQC. Our code is available at https://github.com/iiis-ai/IterativeQuestionComposing.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_09003v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管数学推理的大型语言模型（LLM）取得了进步，但解决竞争级别的数学问题仍然是一个重大挑战，尤其是对于没有外部工具的开源LLM来说。我们介绍了MMIQC数据集，该数据集由处理过的网络数据和合成的问答对组成，旨在增强基本语言模型的数学推理能力。在MMIQC上微调的型号在各种型号的MATH基准测试中的性能始终超过同行。值得注意的是，Qwen-7B-MMIQC的准确率为45.0%，比之前的开源技术高出8.2%，并优于2023年发布的初始版本GPT-4。对匈牙利高中期末考试的广泛评估结果表明，这种改进可以推广到看不见的数据。我们对MMIQC的消融研究表明，很大一部分改进可归因于我们的新增强方法，迭代问题合成（IQC），该方法包括使用LLM从种子问题迭代合成新问题，并通过另一个LLM应用拒绝采样。MMIQC数据集可在HuggingFace集线器上获得，网址为https://huggingface.co/datasets/Vivacem/MMIQC.我们的代码可在https://github.com/iiis-ai/IterativeQuestionComposing.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.09003v3" target="_blank">2401.09003v3</a>
                              </td>
                              <td>Augmenting Math Word Problems via Iterative Question Composing</td>
                              <td>Haoxiong Liu</td>
                              <td>2024-01-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_09003v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.09003v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/iiis-ai/iterativequestioncomposing" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17093v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17093v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17093v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17093v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>To leverage LLMs for visual synthesis, traditional methods convert raster image information into discrete grid tokens through specialized visual modules, while disrupting the model's ability to capture the true semantic representation of visual scenes. This paper posits that an alternative representation of images, vector graphics, can effectively surmount this limitation by enabling a more natural and semantically coherent segmentation of the image information. Thus, we introduce StrokeNUWA, a pioneering work exploring a better visual representation ''stroke tokens'' on vector graphics, which is inherently visual semantics rich, naturally compatible with LLMs, and highly compressed. Equipped with stroke tokens, StrokeNUWA can significantly surpass traditional LLM-based and optimization-based methods across various metrics in the vector graphic generation task. Besides, StrokeNUWA achieves up to a 94x speedup in inference over the speed of prior methods with an exceptional SVG code compression ratio of 6.9%.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17093v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>为了利用LLM进行视觉合成，传统方法通过专门的视觉模块将光栅图像信息转换为离散的网格标记，同时破坏了模型捕捉视觉场景真实语义表示的能力。本文假设，图像的另一种表示方式，矢量图形，可以通过对图像信息进行更自然和语义连贯的分割，有效地克服这一限制。因此，我们介绍了StrokeNUWA，这是一项探索矢量图形上更好的视觉表示“笔划标记”的开创性工作，它天生具有丰富的视觉语义，与LLM自然兼容，并高度压缩。配备了笔划标记，StrokeNUWA可以在矢量图形生成任务中的各种指标上显著超过传统的基于LLM和基于优化的方法。此外，StrokeNUWA在推理速度上比现有方法提高了94倍，SVG代码压缩率高达6.9%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17093v1" target="_blank">2401.17093v1</a>
                              </td>
                              <td>StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis</td>
                              <td>Zecheng Tang</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17093v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17093v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17072v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SemScore: Automated Evaluation of Instruction-Tuned LLMs based on Semantic Textual Similarity</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17072v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17072v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17072v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Instruction-tuned Large Language Models (LLMs) have recently showcased remarkable advancements in their ability to generate fitting responses to natural language instructions. However, many current works rely on manual evaluation to judge the quality of generated responses. Since such manual evaluation is time-consuming, it does not easily scale to the evaluation of multiple models and model variants. In this short paper, we propose a straightforward but remarkably effective evaluation metric called SemScore, in which we directly compare model outputs to gold target responses using semantic textual similarity (STS). We conduct a comparative evaluation of the model outputs of 12 prominent instruction-tuned LLMs using 8 widely-used evaluation metrics for text generation. We find that our proposed SemScore metric outperforms all other, in many cases more complex, evaluation metrics in terms of correlation to human evaluation. These findings indicate the utility of our proposed metric for the evaluation of instruction-tuned LLMs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17072v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>调整指令的大型语言模型（LLM）最近在生成对自然语言指令的拟合响应的能力方面取得了显著进步。然而，目前的许多工作都依赖于手动评估来判断生成的响应的质量。由于这种手动评估是耗时的，它不容易扩展到多个模型和模型变体的评估。在这篇简短的论文中，我们提出了一种简单但非常有效的评估指标，称为SemScore，其中我们使用语义文本相似性（STS）直接将模型输出与黄金目标响应进行比较。我们使用8个广泛使用的文本生成评估指标，对12个突出的指令调优LLM的模型输出进行了比较评估。我们发现，就与人类评估的相关性而言，我们提出的SemScore指标优于所有其他（在许多情况下更复杂）评估指标。这些发现表明了我们提出的指标对教学调整LLM评估的效用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17072v1" target="_blank">2401.17072v1</a>
                              </td>
                              <td>SemScore: Automated Evaluation of Instruction-Tuned LLMs based on Semantic Textual Similarity</td>
                              <td>Ansar Aynetdinov</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17072v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17072v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17043v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17043v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17043v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17043v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by incorporating external knowledge sources. This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate "hallucinated" content. However, the evaluation of RAG systems is challenging, as existing benchmarks are limited in scope and diversity. Most of the current benchmarks predominantly assess question-answering applications, overlooking the broader spectrum of situations where RAG could prove advantageous. Moreover, they only evaluate the performance of the LLM component of the RAG pipeline in the experiments, and neglect the influence of the retrieval component and the external knowledge database. To address these issues, this paper constructs a large-scale and more comprehensive benchmark, and evaluates all the components of RAG systems in various RAG application scenarios. Specifically, we have categorized the range of RAG applications into four distinct types-Create, Read, Update, and Delete (CRUD), each representing a unique use case. "Create" refers to scenarios requiring the generation of original, varied content. "Read" involves responding to intricate questions in knowledge-intensive situations. "Update" focuses on revising and rectifying inaccuracies or inconsistencies in pre-existing texts. "Delete" pertains to the task of summarizing extensive texts into more concise forms. For each of these CRUD categories, we have developed comprehensive datasets to evaluate the performance of RAG systems. We also analyze the effects of various components of the RAG system, such as the retriever, the context length, the knowledge base construction, and the LLM. Finally, we provide useful insights for optimizing the RAG technology for different scenarios.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17043v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>检索增强生成（RAG）是一种通过引入外部知识源来增强大型语言模型（LLM）能力的技术。这种方法解决了LLM的常见局限性，包括过时的信息和产生不准确“幻觉”内容的趋势。然而，RAG系统的评估具有挑战性，因为现有基准的范围和多样性有限。目前的大多数基准主要评估问答应用程序，忽略了RAG可能被证明是有利的更广泛的情况。此外，他们在实验中只评估了RAG管道的LLM组件的性能，而忽略了检索组件和外部知识数据库的影响。为了解决这些问题，本文构建了一个大规模、更全面的基准，并在各种RAG应用场景中评估了RAG系统的所有组件。具体来说，我们将RAG应用程序的范围分为四种不同的类型——创建、读取、更新和删除（CRUD），每种类型都代表一个独特的用例。“创建”是指需要生成原创、多样化内容的场景。“阅读”包括在知识密集的情况下回答复杂的问题。“更新”侧重于修订和纠正已有文本中的不准确或不一致之处。“删除”是指将大量文本概括成更简洁的形式。对于这些CRUD类别中的每一个，我们都开发了全面的数据集来评估RAG系统的性能。我们还分析了RAG系统的各个组成部分的影响，如检索器、上下文长度、知识库构建和LLM。最后，我们为针对不同场景优化RAG技术提供了有用的见解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17043v1" target="_blank">2401.17043v1</a>
                              </td>
                              <td>CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models</td>
                              <td>Yuanjie Lyu</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17043v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17043v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00506v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Generative AI enhances individual creativity but reduces the collective diversity of novel content</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00506v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00506v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00506v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Creativity is core to being human. Generative artificial intelligence (GenAI) -- including ever more powerful large language models (LLMs) -- holds promise for humans to be more creative by offering new ideas, or less creative by anchoring on GenAI ideas. We study the causal impact of GenAI ideas on the production of a short story in an online experimental study where some writers could obtain story ideas from a GenAI platform. We find that access to GenAI ideas causes stories to be evaluated as more creative, better written, and more enjoyable, especially among less creative writers. However, GenAI-enabled stories are more similar to each other than stories by humans alone. These results point to an increase in individual creativity at the risk of losing collective novelty. This dynamic resembles a social dilemma: with GenAI, individual writers are better off, but collectively a narrower scope of novel content may be produced. Our results have implications for researchers, policy-makers and practitioners interested in bolstering creativity.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00506v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>创造力是做人的核心。生成人工智能（GenAI）——包括越来越强大的大型语言模型（LLM）——有望让人类通过提供新想法来提高创造力，或者通过锚定GenAI想法来降低创造力。我们在一项在线实验研究中研究了GenAI思想对短篇小说制作的因果影响，一些作家可以从GenAI平台获得故事思想。我们发现，接触GenAI思想会使故事被评价为更具创造性、写得更好、更令人愉快，尤其是在创造力较低的作家中。然而，与人类单独的故事相比，GenAI支持的故事彼此更相似。这些结果表明，在失去集体新颖性的风险下，个人创造力会增加。这种动态类似于一种社会困境：有了GenAI，个体作家的境况会更好，但总体而言，可能会产生范围更窄的小说内容。我们的研究结果对有兴趣增强创造力的研究人员、决策者和从业者都有启示。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00506v2" target="_blank">2312.00506v2</a>
                              </td>
                              <td>Generative AI enhances individual creativity but reduces the collective diversity of novel content</td>
                              <td>Anil R. Doshi</td>
                              <td>2023-12-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00506v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00506v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_08406v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_08406v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_08406v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_08406v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not seen much penetration of AI, and we study a potentially disruptive application - what if we could provide location-specific insights to a farmer? Our results show the effectiveness of our dataset generation pipeline in capturing geographic-specific knowledge, and the quantitative and qualitative benefits of RAG and fine-tuning. We see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further. In one particular experiment, we also demonstrate that the fine-tuned model leverages information from across geographies to answer specific questions, increasing answer similarity from 47% to 72%. Overall, the results point to how systems built using LLMs can be adapted to respond and incorporate knowledge across a dimension that is critical for a specific industry, paving the way for further applications of LLMs in other industrial domains.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_08406v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>开发人员在构建大型语言模型（LLM）应用程序时，有两种常见的方式来合并专有数据和特定领域的数据：检索增强生成（RAG）和微调。RAG使用外部数据增强提示，而微调则将额外的知识融入模型本身。然而，这两种方法的利弊并没有得到很好的理解。在本文中，我们提出了一种用于微调和RAG的管道，并对多种流行的LLM（包括Llama2-13B、GPT-3.5和GPT-4）进行了权衡。我们的管道由多个阶段组成，包括从PDF中提取信息、生成问题和答案、使用它们进行微调，以及利用GPT-4评估结果。我们提出了评估RAG和微调管道不同阶段性能的指标。我们对农业数据集进行了深入研究。农业作为一个行业还没有看到人工智能的太多渗透，我们研究了一个潜在的颠覆性应用——如果我们能为农民提供特定地点的见解呢？我们的结果显示了我们的数据集生成管道在获取特定地理知识方面的有效性，以及RAG和微调的数量和质量效益。我们看到，当微调模型时，精度增加了6个百分点以上，这与RAG是累积的，它进一步增加了5个百分点的精度。在一个特定的实验中，我们还证明了微调模型利用来自不同地理位置的信息来回答特定问题，将答案的相似性从47%提高到72%。总体而言，研究结果表明，如何调整使用LLM构建的系统，以响应并整合对特定行业至关重要的知识，为LLM在其他工业领域的进一步应用铺平道路。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.08406v3" target="_blank">2401.08406v3</a>
                              </td>
                              <td>RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture</td>
                              <td>Angels Balaguer</td>
                              <td>2024-01-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_08406v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.08406v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17019v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Generating Executable Metamorphic Relations Using Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17019v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17019v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17019v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Metamorphic testing (MT) has proven to be a successful solution to automating testing and addressing the oracle problem. However, it entails manually deriving metamorphic relations (MRs) and converting them into an executable form; these steps are time-consuming and may prevent the adoption of MT. In this paper, we propose an approach for automatically deriving executable MRs (EMRs) from requirements using large language models (LLMs). Instead of merely asking the LLM to produce EMRs, our approach relies on a few-shot prompting strategy to instruct the LLM to perform activities in the MT process, by providing requirements and API specifications, as one would do with software engineers. To assess the feasibility of our approach, we conducted a questionnaire-based survey in collaboration with Siemens Industry Software, focusing on four of their software applications. Additionally, we evaluated the accuracy of the generated EMRs for a web application. The outcomes of our study are highly promising, as they demonstrate the capability of our approach to generate MRs and EMRs that are both comprehensible and pertinent for testing purposes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17019v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>变形测试（MT）已被证明是自动化测试和解决预言机问题的成功解决方案。然而，它需要手动推导变形关系（MR），并将其转换为可执行的形式；这些步骤很耗时，可能会阻碍MT的采用。在本文中，我们提出了一种使用大型语言模型（LLM）从需求中自动派生可执行MR（EMR）的方法。我们的方法不只是要求LLM生产EMR，而是依赖于一种少热提示策略，通过提供需求和API规范来指导LLM执行MT过程中的活动，就像对待软件工程师一样。为了评估我们的方法的可行性，我们与西门子工业软件公司合作进行了一项基于问卷的调查，重点关注他们的四个软件应用程序。此外，我们还评估了为web应用程序生成的电子病历的准确性。我们的研究结果非常有希望，因为它们证明了我们的方法生成可理解且与测试目的相关的核磁共振和电子病历的能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17019v1" target="_blank">2401.17019v1</a>
                              </td>
                              <td>Towards Generating Executable Metamorphic Relations Using Large Language Models</td>
                              <td>Seung Yeob Shin</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17019v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17019v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17010v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Finetuning Large Language Models for Vulnerability Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17010v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17010v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17010v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents the results of finetuning large language models (LLMs) for the task of detecting vulnerabilities in source code. We leverage WizardCoder, a recent improvement of the state-of-the-art LLM StarCoder, and adapt it for vulnerability detection through further finetuning. To accelerate training, we modify WizardCoder's training procedure, also we investigate optimal training regimes. For the imbalanced dataset with many more negative examples than positive, we also explore different techniques to improve classification performance. The finetuned WizardCoder model achieves improvement in ROC AUC and F1 measures on balanced and imbalanced vulnerability datasets over CodeBERT-like model, demonstrating the effectiveness of adapting pretrained LLMs for vulnerability detection in source code. The key contributions are finetuning the state-of-the-art code LLM, WizardCoder, increasing its training speed without the performance harm, optimizing the training procedure and regimes, handling class imbalance, and improving performance on difficult vulnerability detection datasets. This demonstrates the potential for transfer learning by finetuning large pretrained language models for specialized source code analysis tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17010v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了为检测源代码中的漏洞而对大型语言模型（LLM）进行微调的结果。我们利用最先进的LLM StarCoder的最新改进WizardCoder，并通过进一步的微调将其用于漏洞检测。为了加速训练，我们修改了WizardCoder的训练程序，还研究了最佳训练机制。对于负面例子比正面例子多的不平衡数据集，我们还探索了不同的技术来提高分类性能。与类CodeBERT模型相比，经过微调的WizardCoder模型在平衡和不平衡的漏洞数据集上实现了ROC AUC和F1测量的改进，证明了将预训练的LLM用于源代码中的漏洞检测的有效性。关键贡献是微调最先进的代码LLM，即WizardCoder，在不损害性能的情况下提高其训练速度，优化训练过程和机制，处理类不平衡，以及提高在困难的漏洞检测数据集上的性能。这证明了通过为专门的源代码分析任务微调大型预训练语言模型来进行迁移学习的潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17010v1" target="_blank">2401.17010v1</a>
                              </td>
                              <td>Finetuning Large Language Models for Vulnerability Detection</td>
                              <td>Alexey Shestov</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17010v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17010v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17005v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAL-PIM: A Subarray-level Processing-in-Memory Architecture with LUT-based Linear Interpolation for Transformer-based Text Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17005v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17005v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17005v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Text generation is a compelling sub-field of natural language processing, aiming to generate human-readable text from input words. In particular, the decoder-only generative models, such as generative pre-trained transformer (GPT), are widely used for text generation, with two major computational stages: summarization and generation. Unlike the summarization stage, which can process the input tokens in parallel, the generation stage is difficult to accelerate due to its sequential generation of output tokens through iteration. Moreover, each iteration requires reading a whole model with little data reuse opportunity. Therefore, the workload of transformer-based text generation is severely memory-bound, making the external memory bandwidth system bottleneck. In this paper, we proposed a subarray-level processing-in-memory architecture named SAL-PIM, HBM-based PIM architecture for the end-to-end acceleration of transformer-based text generation. The SAL-PIM architecture includes three architectural features. First, the SAL-PIM architecture utilizes higher internal bandwidth by integrating multiple subarray-level arithmetic logic units with optimized data mapping schemes. Second, the SAL-PIM architecture adopts LUT-based linear interpolation to perform complex non-linear functions in PIM. Third, the SAL-PIM architecture accelerates end-to-end inference on PIM in text generation. Furthermore, to validate the SAL-PIM architecture, we built cycle-accurate simulator and implemented the SAL-PIM's logic units in 28-nm CMOS technology. As a result, when the input size is from 32 to 128 and the output size is from 1 to 256, SAL-PIM achieves a maximum of 4.72 times speedup and an average of 1.83 times speedup for the text generation based on the GPT-2 medium model compared to the server-level GPU.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17005v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文本生成是自然语言处理的一个引人注目的子领域，旨在从输入单词中生成人类可读的文本。特别是，仅解码器的生成模型，如生成预训练变换器（GPT），被广泛用于文本生成，具有两个主要的计算阶段：摘要和生成。与可以并行处理输入令牌的摘要阶段不同，生成阶段由于通过迭代顺序生成输出令牌，因此难以加速。此外，每次迭代都需要读取整个模型，几乎没有数据重用的机会。因此，基于transformer的文本生成的工作量严重受限于内存，使外部内存带宽系统成为瓶颈。在本文中，我们提出了一种名为SAL-PIM的子阵列级存储器处理架构，即基于HBM的PIM架构，用于基于转换器的文本生成的端到端加速。SAL-PIM体系结构包括三个体系结构特征。首先，SAL-PIM体系结构通过集成多个子阵列级算术逻辑单元和优化的数据映射方案，利用了更高的内部带宽。其次，SAL-PIM体系结构采用基于LUT的线性插值来执行PIM中的复杂非线性函数。第三，SAL-PIM体系结构加速了文本生成中PIM的端到端推理。此外，为了验证SAL-PIM体系结构，我们建立了周期精确模拟器，并在28nm CMOS技术中实现了SAL-PIM的逻辑单元。结果，当输入大小从32到128并且输出大小从1到256时，与服务器级GPU相比，SAL-PIM实现了基于GPT-2介质模型的文本生成的最大4.72倍加速和平均1.83倍加速。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17005v1" target="_blank">2401.17005v1</a>
                              </td>
                              <td>SAL-PIM: A Subarray-level Processing-in-Memory Architecture with LUT-based Linear Interpolation for Transformer-based Text Generation</td>
                              <td>Wontak Han</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17005v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17005v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_00595v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">State of What Art? A Call for Multi-Prompt LLM Evaluation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_00595v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_00595v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_00595v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in large language models (LLMs) have led to the development of various evaluation benchmarks. These benchmarks typically rely on a single instruction template for evaluating all LLMs on a specific task. In this paper, we comprehensively analyze the brittleness of results obtained via single-prompt evaluations across 6.5M instances, involving 20 different LLMs and 39 tasks from 3 benchmarks. To improve robustness of the analysis, we propose to evaluate LLMs with a set of diverse prompts instead. We discuss tailored evaluation metrics for specific use cases (e.g., LLM developers vs. developers interested in a specific downstream task), ensuring a more reliable and meaningful assessment of LLM capabilities. We then implement these criteria and conduct evaluations of multiple models, providing insights into the true strengths and limitations of current LLMs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_00595v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）的最新进展导致了各种评估基准的发展。这些基准测试通常依赖于单个指令模板来评估特定任务中的所有LLM。在本文中，我们全面分析了通过6.5M个实例的单次即时评估获得的结果的脆性，这些实例涉及20个不同的LLM和来自3个基准的39个任务。为了提高分析的稳健性，我们建议使用一组不同的提示来评估LLM。我们讨论了针对特定用例的定制评估指标（例如，LLM开发人员与对特定下游任务感兴趣的开发人员），以确保对LLM功能进行更可靠和有意义的评估。然后，我们实施这些标准，并对多个模型进行评估，深入了解当前LLM的真正优势和局限性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.00595v2" target="_blank">2401.00595v2</a>
                              </td>
                              <td>State of What Art? A Call for Multi-Prompt LLM Evaluation</td>
                              <td>Moran Mizrahi</td>
                              <td>2023-12-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_00595v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.00595v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/slab-nlp/multi-prompt-llm-evaluation" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_15688v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_15688v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_15688v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_15688v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite significant advancements in text-to-image models for generating high-quality images, these methods still struggle to ensure the controllability of text prompts over images in the context of complex text prompts, especially when it comes to retaining object attributes and relationships. In this paper, we propose CompAgent, a training-free approach for compositional text-to-image generation, with a large language model (LLM) agent as its core. The fundamental idea underlying CompAgent is premised on a divide-and-conquer methodology. Given a complex text prompt containing multiple concepts including objects, attributes, and relationships, the LLM agent initially decomposes it, which entails the extraction of individual objects, their associated attributes, and the prediction of a coherent scene layout. These individual objects can then be independently conquered. Subsequently, the agent performs reasoning by analyzing the text, plans and employs the tools to compose these isolated objects. The verification and human feedback mechanism is finally incorporated into our agent to further correct the potential attribute errors and refine the generated images. Guided by the LLM agent, we propose a tuning-free multi-concept customization model and a layout-to-image generation model as the tools for concept composition, and a local image editing method as the tool to interact with the agent for verification. The scene layout controls the image generation process among these tools to prevent confusion among multiple objects. Extensive experiments demonstrate the superiority of our approach for compositional text-to-image generation: CompAgent achieves more than 10\% improvement on T2I-CompBench, a comprehensive benchmark for open-world compositional T2I generation. The extension to various related tasks also illustrates the flexibility of our CompAgent for potential applications.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_15688v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管在生成高质量图像的文本到图像模型方面取得了重大进展，但在复杂的文本提示背景下，这些方法仍然难以确保文本提示相对于图像的可控性，尤其是在保留对象属性和关系方面。在本文中，我们提出了CompAgent，这是一种无需训练的合成文本到图像生成方法，以大型语言模型（LLM）代理为核心。CompAgent的基本思想是以分而治之的方法论为前提的。给定包含多个概念（包括对象、属性和关系）的复杂文本提示，LLM代理首先对其进行分解，这需要提取单个对象及其相关属性，并预测连贯的场景布局。然后可以独立地征服这些单独的物体。随后，代理通过分析文本进行推理，计划并使用工具来组成这些孤立的对象。验证和人类反馈机制最终被纳入我们的代理中，以进一步纠正潜在的属性错误并细化生成的图像。在LLM代理的指导下，我们提出了一个无调整的多概念定制模型和布局到图像生成模型作为概念合成的工具，并提出了一种局部图像编辑方法作为与代理交互进行验证的工具。场景布局控制这些工具之间的图像生成过程，以防止多个对象之间的混淆。大量实验证明了我们的合成文本到图像生成方法的优越性：CompAgent在T2I CompBench（开放世界合成T2I生成的综合基准）上实现了10%以上的改进。对各种相关任务的扩展也说明了CompAgent在潜在应用中的灵活性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.15688v2" target="_blank">2401.15688v2</a>
                              </td>
                              <td>Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation</td>
                              <td>Zhenyu Wang</td>
                              <td>2024-01-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_15688v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.15688v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_16969v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Taxonomy of Mathematical Plagiarism</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_16969v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_16969v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_16969v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Plagiarism is a pressing concern, even more so with the availability of large language models. Existing plagiarism detection systems reliably find copied and moderately reworded text but fail for idea plagiarism, especially in mathematical science, which heavily uses formal mathematical notation. We make two contributions. First, we establish a taxonomy of mathematical content reuse by annotating potentially plagiarised 122 scientific document pairs. Second, we analyze the best-performing approaches to detect plagiarism and mathematical content similarity on the newly established taxonomy. We found that the best-performing methods for plagiarism and math content similarity achieve an overall detection score (PlagDet) of 0.06 and 0.16, respectively. The best-performing methods failed to detect most cases from all seven newly established math similarity types. Outlined contributions will benefit research in plagiarism detection systems, recommender systems, question-answering systems, and search engines. We make our experiment's code and annotated dataset available to the community: https://github.com/gipplab/Taxonomy-of-Mathematical-Plagiarism</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_16969v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>剽窃是一个紧迫的问题，随着大型语言模型的出现，情况更是如此。现有的剽窃检测系统可以可靠地找到复制的和适度改写的文本，但不能检测出思想剽窃，尤其是在大量使用正式数学符号的数学科学中。我们有两个贡献。首先，我们通过注释122个可能被剽窃的科学文档对，建立了数学内容重用的分类法。其次，我们分析了在新建立的分类法上检测剽窃和数学内容相似性的最佳方法。我们发现，剽窃和数学内容相似性表现最好的方法的总体检测得分（PlagDet）分别为0.06和0.16。性能最好的方法未能从所有七种新建立的数学相似性类型中检测到大多数情况。概述的贡献将有利于剽窃检测系统、推荐系统、问答系统和搜索引擎的研究。我们向社区提供实验的代码和注释数据集：https://github.com/gipplab/Taxonomy-of-Mathematical-Plagiarism</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.16969v1" target="_blank">2401.16969v1</a>
                              </td>
                              <td>Taxonomy of Mathematical Plagiarism</td>
                              <td>Ankit Satpute</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_16969v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.16969v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/gipplab/taxonomy-of-mathematical-plagiarism" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_16960v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Two Heads Are Better Than One: Integrating Knowledge from Knowledge Graphs and Large Language Models for Entity Alignment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_16960v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_16960v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_16960v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Entity alignment, which is a prerequisite for creating a more comprehensive Knowledge Graph (KG), involves pinpointing equivalent entities across disparate KGs. Contemporary methods for entity alignment have predominantly utilized knowledge embedding models to procure entity embeddings that encapsulate various similarities-structural, relational, and attributive. These embeddings are then integrated through attention-based information fusion mechanisms. Despite this progress, effectively harnessing multifaceted information remains challenging due to inherent heterogeneity. Moreover, while Large Language Models (LLMs) have exhibited exceptional performance across diverse downstream tasks by implicitly capturing entity semantics, this implicit knowledge has yet to be exploited for entity alignment. In this study, we propose a Large Language Model-enhanced Entity Alignment framework (LLMEA), integrating structural knowledge from KGs with semantic knowledge from LLMs to enhance entity alignment. Specifically, LLMEA identifies candidate alignments for a given entity by considering both embedding similarities between entities across KGs and edit distances to a virtual equivalent entity. It then engages an LLM iteratively, posing multiple multi-choice questions to draw upon the LLM's inference capability. The final prediction of the equivalent entity is derived from the LLM's output. Experiments conducted on three public datasets reveal that LLMEA surpasses leading baseline models. Additional ablation studies underscore the efficacy of our proposed framework.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_16960v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>实体对齐是创建更全面的知识图（KG）的先决条件，包括在不同的知识图中精确定位等效实体。当代的实体对齐方法主要利用知识嵌入模型来获取实体嵌入，这些实体嵌入封装了各种相似性——结构、关系和属性。然后通过基于注意力的信息融合机制将这些嵌入进行集成。尽管取得了这些进展，但由于固有的异质性，有效利用多方面信息仍然具有挑战性。此外，尽管大型语言模型（LLM）通过隐式捕获实体语义，在不同的下游任务中表现出了卓越的性能，但这种隐式知识尚未被用于实体对齐。在这项研究中，我们提出了一个大型语言模型增强的实体对齐框架（LLMEA），将来自KGs的结构知识与来自LLM的语义知识相结合，以增强实体对齐。具体而言，LLMEA通过考虑跨KGs的实体之间的嵌入相似性和编辑到虚拟等效实体的距离来识别给定实体的候选比对。然后，它迭代地使用LLM，提出多个多选问题，以利用LLM的推理能力。等效实体的最终预测是从LLM的输出中导出的。在三个公共数据集上进行的实验表明，LLMEA超过了领先的基线模型。其他消融研究强调了我们提出的框架的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.16960v1" target="_blank">2401.16960v1</a>
                              </td>
                              <td>Two Heads Are Better Than One: Integrating Knowledge from Knowledge Graphs and Large Language Models for Entity Alignment</td>
                              <td>Linyao Yang</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_16960v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.16960v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_12141v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Locating Factual Knowledge in Large Language Models: Exploring the Residual Stream and Analyzing Subvalues in Vocabulary Space</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_12141v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_12141v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_12141v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We find the location of factual knowledge in large language models by exploring the residual stream and analyzing subvalues in vocabulary space. We find the reason why subvalues have human-interpretable concepts when projecting into vocabulary space. The before-softmax values of subvalues are added by an addition function, thus the probability of top tokens in vocabulary space will increase. Based on this, we find using log probability increase to compute the significance of layers and subvalues is better than probability increase, since the curve of log probability increase has a linear monotonically increasing shape. Moreover, we calculate the inner products to evaluate how much a feed-forward network (FFN) subvalue is activated by previous layers. Base on our methods, we find where factual knowledge <France, capital, Paris> is stored. Specifically, attention layers store "Paris is related to France". FFN layers store "Paris is a capital/city", activated by attention subvalues related to "capital". We leverage our method on Baevski-18, GPT2 medium, Llama-7B and Llama-13B. Overall, we provide a new method for understanding the mechanism of transformers. We will release our code on github.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_12141v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们通过探索剩余流和分析词汇空间中的子值来找到事实知识在大型语言模型中的位置。我们发现了子值在投影到词汇空间时具有人类可解释概念的原因。子值的前softmax值通过加法函数相加，因此词汇表空间中顶部标记的概率将增加。基于此，我们发现使用对数概率增加来计算层和子值的显著性优于概率增加，因为对数概率增加的曲线具有线性单调增加的形状。此外，我们计算内积，以评估前馈网络（FFN）子值被前几层激活的程度。根据我们的方法，我们找到了事实知识<法国，首都，巴黎>的存储位置。具体来说，关注层商店“巴黎与法国有关”。FFN分层存储“巴黎是一个首都/城市”，由与“首都”相关的注意力子值激活。我们在Baevski-18、GPT2培养基、Llama-7B和Llama-13B上利用我们的方法。总之，我们为理解变压器的机理提供了一种新的方法。我们将在github上发布我们的代码。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.12141v2" target="_blank">2312.12141v2</a>
                              </td>
                              <td>Locating Factual Knowledge in Large Language Models: Exploring the Residual Stream and Analyzing Subvalues in Vocabulary Space</td>
                              <td>Zeping Yu</td>
                              <td>2023-12-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_12141v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.12141v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_11489v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Give Us the Facts: Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Modeling</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_11489v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_11489v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_11489v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, ChatGPT, a representative large language model (LLM), has gained considerable attention due to its powerful emergent abilities. Some researchers suggest that LLMs could potentially replace structured knowledge bases like knowledge graphs (KGs) and function as parameterized knowledge bases. However, while LLMs are proficient at learning probabilistic language patterns based on large corpus and engaging in conversations with humans, they, like previous smaller pre-trained language models (PLMs), still have difficulty in recalling facts while generating knowledge-grounded contents. To overcome these limitations, researchers have proposed enhancing data-driven PLMs with knowledge-based KGs to incorporate explicit factual knowledge into PLMs, thus improving their performance to generate texts requiring factual knowledge and providing more informed responses to user queries. This paper reviews the studies on enhancing PLMs with KGs, detailing existing knowledge graph enhanced pre-trained language models (KGPLMs) as well as their applications. Inspired by existing studies on KGPLM, this paper proposes to enhance LLMs with KGs by developing knowledge graph-enhanced large language models (KGLLMs). KGLLM provides a solution to enhance LLMs' factual reasoning ability, opening up new avenues for LLM research.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_11489v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，具有代表性的大型语言模型（LLM）ChatGPT由于其强大的涌现能力而备受关注。一些研究人员认为，LLM有可能取代知识图等结构化知识库，并作为参数化知识库发挥作用。然而，尽管LLM精通基于大型语料库的概率语言模式学习和与人类的对话，但与以前较小的预训练语言模型（PLM）一样，它们在生成基于知识的内容时，仍难以回忆事实。为了克服这些局限性，研究人员提出用基于知识的KGs增强数据驱动的PLM，将明确的事实知识纳入PLM，从而提高其生成需要事实知识的文本的性能，并为用户查询提供更明智的响应。本文综述了用知识图增强预训练语言模型（KGPLM）的研究进展，详细介绍了现有的知识图增强预先训练语言模型及其应用。受现有KGPLM研究的启发，本文提出通过开发知识图增强的大型语言模型（KGLLM）来增强KGs的LLM。KGLLM为提高LLM的事实推理能力提供了一种解决方案，为LLM研究开辟了新的途径。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.11489v2" target="_blank">2306.11489v2</a>
                              </td>
                              <td>Give Us the Facts: Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Modeling</td>
                              <td>Linyao Yang</td>
                              <td>2023-06-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_11489v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.11489v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_06071v4_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GroundingGPT:Language Enhanced Multi-modal Grounding Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_06071v4_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_06071v4_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_06071v4_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multi-modal large language models have demonstrated impressive performance across various tasks in different modalities. However, existing multi-modal models primarily emphasize capturing global information within each modality while neglecting the importance of perceiving local information across modalities. Consequently, these models lack the ability to effectively understand the fine-grained details of input data, limiting their performance in tasks that require a more nuanced understanding. To address this limitation, there is a compelling need to develop models that enable fine-grained understanding across multiple modalities, thereby enhancing their applicability to a wide range of tasks. In this paper, we propose GroundingGPT, a language enhanced multi-modal grounding model. Beyond capturing global information like other multi-modal models, our proposed model excels at tasks demanding a detailed understanding of local information within the input. It demonstrates precise identification and localization of specific regions in images or moments in videos. To achieve this objective, we design a diversified dataset construction pipeline, resulting in a multi-modal, multi-granularity dataset for model training. The code, dataset, and demo of our model can be found at https: //github.com/lzw-lzw/GroundingGPT.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_06071v4_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多模态大型语言模型在不同模态的各种任务中表现出了令人印象深刻的性能。然而，现有的多模态模型主要强调在每个模态内捕获全局信息，而忽略了跨模态感知局部信息的重要性。因此，这些模型缺乏有效理解输入数据细粒度细节的能力，限制了它们在需要更细致理解的任务中的性能。为了解决这一限制，迫切需要开发能够跨多种模式进行细粒度理解的模型，从而增强其对广泛任务的适用性。在本文中，我们提出了GroundingGPT，这是一种语言增强的多模态grounding模型。除了像其他多模态模型一样捕获全局信息外，我们提出的模型还擅长于要求详细了解输入中的局部信息的任务。它展示了图像或视频中特定区域的精确识别和定位。为了实现这一目标，我们设计了一个多样化的数据集构建管道，从而产生了一个用于模型训练的多模式、多粒度数据集。我们模型的代码、数据集和演示可以在https://github.com/lzw-lzw/GroundingGPT上找到。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.06071v4" target="_blank">2401.06071v4</a>
                              </td>
                              <td>GroundingGPT:Language Enhanced Multi-modal Grounding Model</td>
                              <td>Zhaowei Li</td>
                              <td>2024-01-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_06071v4_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.06071v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/lzw-lzw/groundinggpt" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_05934v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_05934v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_05934v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_05934v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their ability to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of LLMs on previously seen information poses a significant challenge. In this study, we compare two common approaches: unsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate both approaches on a variety of knowledge-intensive tasks across different topics. Our findings reveal that while unsupervised fine-tuning offers some improvement, RAG consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge. Moreover, we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing them to numerous variations of the same fact during training could alleviate this problem.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_05934v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）在其预先训练的权重中封装了大量的事实信息，其回答不同领域的不同问题的能力证明了这一点。然而，这种知识本身是有限的，严重依赖于训练数据的特征。因此，使用外部数据集来整合新信息或完善LLM对先前看到的信息的能力是一个重大挑战。在这项研究中，我们比较了两种常见的方法：无监督微调和检索增强生成（RAG）。我们在不同主题的各种知识密集型任务中评估了这两种方法。我们的研究结果表明，虽然无监督微调提供了一些改进，但无论是在训练过程中遇到的现有知识还是全新知识方面，RAG都始终优于它。此外，我们发现LLM很难通过无监督的微调来学习新的事实信息，并且在训练期间将它们暴露在同一事实的许多变化中可以缓解这个问题。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.05934v3" target="_blank">2312.05934v3</a>
                              </td>
                              <td>Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs</td>
                              <td>Oded Ovadia</td>
                              <td>2023-12-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_05934v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.05934v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14424v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Discovering Mathematical Formulas from Data via GPT-guided Monte Carlo Tree Search</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14424v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14424v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14424v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Finding a concise and interpretable mathematical formula that accurately describes the relationship between each variable and the predicted value in the data is a crucial task in scientific research, as well as a significant challenge in artificial intelligence. This problem is referred to as symbolic regression, which is an NP-hard problem. In the previous year, a novel symbolic regression methodology utilizing Monte Carlo Tree Search (MCTS) was advanced, achieving state-of-the-art results on a diverse range of datasets. although this algorithm has shown considerable improvement in recovering target expressions compared to previous methods, the lack of guidance during the MCTS process severely hampers its search efficiency. Recently, some algorithms have added a pre-trained policy network to guide the search of MCTS, but the pre-trained policy network generalizes poorly. To optimize the trade-off between efficiency and versatility, we introduce SR-GPT, a novel algorithm for symbolic regression that integrates Monte Carlo Tree Search (MCTS) with a Generative Pre-Trained Transformer (GPT). By using GPT to guide the MCTS, the search efficiency of MCTS is significantly improved. Next, we utilize the MCTS results to further refine the GPT, enhancing its capabilities and providing more accurate guidance for the MCTS. MCTS and GPT are coupled together and optimize each other until the target expression is successfully determined. We conducted extensive evaluations of SR-GPT using 222 expressions sourced from over 10 different symbolic regression datasets. The experimental results demonstrate that SR-GPT outperforms existing state-of-the-art algorithms in accurately recovering symbolic expressions both with and without added noise.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14424v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>找到一个简洁、可解释的数学公式，准确描述数据中每个变量与预测值之间的关系，是科学研究的一项关键任务，也是人工智能的一项重大挑战。这个问题被称为符号回归，是一个NP难题。在前一年，提出了一种利用蒙特卡洛树搜索（MCTS）的新型符号回归方法，在各种数据集上取得了最先进的结果。尽管与以前的方法相比，该算法在恢复目标表达式方面表现出相当大的改进，但在MCTS过程中缺乏指导严重阻碍了其搜索效率。最近，一些算法增加了一个预训练的策略网络来指导MCTS的搜索，但预训练的政策网络泛化能力较差。为了优化效率和多功能性之间的权衡，我们引入了SR-GPT，这是一种新的符号回归算法，它将蒙特卡洛树搜索（MCTS）与生成预训练变换器（GPT）相结合。通过使用GPT来引导MCTS，显著提高了MCTS的搜索效率。接下来，我们利用MCTS的结果进一步完善GPT，增强其能力，并为MCTS提供更准确的指导。MCTS和GPT耦合在一起并相互优化，直到成功确定目标表达。我们使用来自10多个不同符号回归数据集的222个表达式对SR-GPT进行了广泛的评估。实验结果表明，在添加和不添加噪声的情况下，SR-GPT在准确恢复符号表达式方面优于现有的最先进的算法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14424v3" target="_blank">2401.14424v3</a>
                              </td>
                              <td>Discovering Mathematical Formulas from Data via GPT-guided Monte Carlo Tree Search</td>
                              <td>Yanjie Li</td>
                              <td>2024-01-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14424v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14424v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_09499v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_09499v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_09499v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_09499v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Various Large Language Models(LLMs) from the Generative Pretrained Transformer~(GPT) family have achieved outstanding performances in a wide range of text generation tasks. However, the enormous model sizes have hindered their practical use in real-world applications due to high inference latency. Therefore, improving the efficiencies of LLMs through quantization, pruning, and other means has been a key issue in LLM studies. In this work, we propose a method based on Hessian sensitivity-aware mixed sparsity pruning to prune LLMs to at least 50\% sparsity without the need of any retraining. It allocates sparsity adaptively based on sensitivity, allowing us to reduce pruning-induced error while maintaining the overall sparsity level. The advantages of the proposed method exhibit even more when the sparsity is extremely high. Furthermore, our method is compatible with quantization, enabling further compression of LLMs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_09499v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>来自生成预训练转换器（GPT）家族的各种大型语言模型（LLM）在广泛的文本生成任务中取得了卓越的性能。然而，由于高推理延迟，巨大的模型大小阻碍了它们在现实世界应用中的实际使用。因此，通过量化、修剪和其他手段提高LLM的效率一直是LLM研究的关键问题。在这项工作中，我们提出了一种基于Hessian敏感性感知混合稀疏性修剪的方法，将LLM修剪到至少50%的稀疏性，而不需要任何再训练。它基于灵敏度自适应地分配稀疏性，使我们能够在保持总体稀疏性水平的同时减少修剪引起的误差。当稀疏性极高时，所提出的方法的优点表现得更加明显。此外，我们的方法与量化兼容，能够进一步压缩LLM。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.09499v2" target="_blank">2310.09499v2</a>
                              </td>
                              <td>One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models</td>
                              <td>Hang Shao</td>
                              <td>2023-10-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_09499v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.09499v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_10545v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_10545v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_10545v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_10545v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This study explores the nuanced capabilities and inherent biases of Recommender Systems using Large Language Models (RecLLMs), with a focus on ChatGPT-based systems. It studies into the contrasting behaviors of generative models and traditional collaborative filtering models in movie recommendations. The research primarily investigates prompt design strategies and their impact on various aspects of recommendation quality, including accuracy, provider fairness, diversity, stability, genre dominance, and temporal freshness (recency).   Our experimental analysis reveals that the introduction of specific 'system roles' and 'prompt strategies' in RecLLMs significantly influences their performance. For instance, role-based prompts enhance fairness and diversity in recommendations, mitigating popularity bias. We find that while GPT-based models do not always match the performance of CF baselines, they exhibit a unique tendency to recommend newer and more diverse movie genres. Notably, GPT-based models tend to recommend more recent films, particularly those released post-2000, and show a preference for genres like \sq{Drama} and Comedy, and Romance (compared to CF Action, Adventure) presumably due to the RecLLMs' training on varied data sets, which allows them to capture recent trends and discussions more effectively than CF models. Interestingly, our results demonstrate that the 'Simple' and 'Chain of Thought (COT)' paradigms yield the highest accuracy. These findings imply the potential of combining these strategies with scenarios that favor more recent content, thereby offering a more balanced and up-to-date recommendation experience. This study contributes significantly to the understanding of emerging RecLLMs, particularly in the context of harms and biases within these systems.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_10545v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本研究探讨了使用大型语言模型（RecLLM）的推荐系统的细微功能和固有偏见，重点是基于ChatGPT的系统。研究了生成模型与传统协同过滤模型在电影推荐中的对比行为。本研究主要调查即时设计策略及其对推荐质量各个方面的影响，包括准确性、提供者公平性、多样性、稳定性、类型优势和时间新鲜度（最近度）。我们的实验分析表明，在RecLLM中引入特定的“系统角色”和“即时策略”会显著影响其性能。例如，基于角色的提示增强了推荐的公平性和多样性，减轻了受欢迎程度的偏见。我们发现，虽然基于GPT的模型并不总是与CF基线的性能相匹配，但它们表现出推荐更新和更多样化的电影类型的独特趋势。值得注意的是，基于GPT的模型倾向于推荐更新的电影，特别是2000年后上映的电影，并显示出对戏剧、喜剧和浪漫主义（与CF动作、冒险相比）等类型的偏好，这可能是因为RecLLM对各种数据集的训练，这使他们能够比CF模型更有效地捕捉最近的趋势和讨论。有趣的是，我们的结果表明，“简单”和“思想链（COT）”范式产生了最高的准确性。这些发现意味着将这些策略与有利于更新内容的场景相结合的潜力，从而提供更平衡和最新的推荐体验。这项研究有助于理解新出现的RecLLM，特别是在这些系统中的危害和偏见的背景下。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.10545v2" target="_blank">2401.10545v2</a>
                              </td>
                              <td>Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency</td>
                              <td>Yashar Deldjoo</td>
                              <td>2024-01-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_10545v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.10545v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_16822v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">EarthGPT: A Universal Multi-modal Large Language Model for Multi-sensor Image Comprehension in Remote Sensing Domain</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_16822v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_16822v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_16822v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multi-modal large language models (MLLMs) have demonstrated remarkable success in vision and visual-language tasks within the natural image domain. Owing to the significant diversities between the natural image and RS image hinder the development of MLLMs in the remote sensing (RS) domain. Currently, the unified and powerful MLLM capable of various RS visual tasks is still under-explored. To fill the gap, a pioneer MLLM called EarthGPT is proposed for universal RS image comprehension, which integrates various multi-sensor RS interpretation tasks uniformly. More importantly, a large-scale multi-sensor multi-modal RS instruction-following dataset named MMRS is carefully constructed, which comprises 1005.842k image-text pairs based on 34 existing diverse RS datasets and includes multi-sensor images such as optical, synthetic aperture radar (SAR), and infrared. The MMRS addresses the issue of MLLMs lacking RS expert knowledge and stimulates the development of MMLMs in the RS domain. Extensive experiments demonstrate the EarthGPT's superior performance in various RS visual interpretation tasks compared with the other specialist models and MLLMs, which proves the effectiveness of the proposed EarthGPT and provides a versatile paradigm for open-set reasoning tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_16822v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多模态大语言模型在自然图像领域的视觉和视觉语言任务中取得了显著的成功。由于自然图像和遥感图像之间的显著差异，阻碍了MLLMs在遥感领域的发展。目前，能够执行各种RS视觉任务的统一且强大的MLLM仍在探索中。为了填补这一空白，提出了一种称为EarthGPT的通用遥感图像理解先驱MLLM，它将各种多传感器遥感解释任务统一集成在一起。更重要的是，我们精心构建了一个名为MMRS的大规模多传感器多模态RS指令跟随数据集，该数据集基于现有的34个不同的RS数据集，包括1005.842k的图像-文本对，包括光学、合成孔径雷达和红外等多传感器图像。MMRS解决了缺乏RS专家知识的MLLMs问题，并促进了MMLMs在RS领域的发展。大量实验证明，与其他专业模型和MLLMs相比，EarthGPT在各种RS视觉解释任务中具有优越的性能，这证明了所提出的EarthGPT的有效性，并为开集推理任务提供了一种通用的范式。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.16822v1" target="_blank">2401.16822v1</a>
                              </td>
                              <td>EarthGPT: A Universal Multi-modal Large Language Model for Multi-sensor Image Comprehension in Remote Sensing Domain</td>
                              <td>Wei Zhang</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_16822v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.16822v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_16820v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Provably Robust Multi-bit Watermarking for AI-generated Text via Error Correction Code</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_16820v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_16820v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_16820v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) have been widely deployed for their remarkable capability to generate texts resembling human language. However, they could be misused by criminals to create deceptive content, such as fake news and phishing emails, which raises ethical concerns. Watermarking is a key technique to mitigate the misuse of LLMs, which embeds a watermark (e.g., a bit string) into a text generated by a LLM. Consequently, this enables the detection of texts generated by a LLM as well as the tracing of generated texts to a specific user. The major limitation of existing watermark techniques is that they cannot accurately or efficiently extract the watermark from a text, especially when the watermark is a long bit string. This key limitation impedes their deployment for real-world applications, e.g., tracing generated texts to a specific user.   This work introduces a novel watermarking method for LLM-generated text grounded in \textbf{error-correction codes} to address this challenge. We provide strong theoretical analysis, demonstrating that under bounded adversarial word/token edits (insertion, deletion, and substitution), our method can correctly extract watermarks, offering a provable robustness guarantee. This breakthrough is also evidenced by our extensive experimental results. The experiments show that our method substantially outperforms existing baselines in both accuracy and robustness on benchmark datasets. For instance, when embedding a bit string of length 12 into a 200-token generated text, our approach attains an impressive match rate of $98.4\%$, surpassing the performance of Yoo et al. (state-of-the-art baseline) at $85.6\%$. When subjected to a copy-paste attack involving the injection of 50 tokens to generated texts with 200 words, our method maintains a substantial match rate of $90.8\%$, while the match rate of Yoo et al. diminishes to below $65\%$.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_16820v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）由于其生成类似人类语言的文本的卓越能力而被广泛部署。然而，它们可能被犯罪分子滥用，以创建欺骗性内容，如假新闻和网络钓鱼电子邮件，这引发了道德问题。水印技术是减少LLM滥用的关键技术，它将水印（例如，比特串）嵌入LLM生成的文本中。因此，这使得能够检测LLM生成的文本以及将生成的文本追踪到特定用户。现有水印技术的主要局限性在于，它们不能准确或有效地从文本中提取水印，尤其是当水印是长比特串时。这一关键限制阻碍了它们在现实世界应用程序中的部署，例如，将生成的文本跟踪到特定用户。这项工作介绍了一种基于\textbf｛纠错码｝的LLM生成文本的新水印方法，以应对这一挑战。我们提供了强有力的理论分析，证明在有界对抗性单词/令牌编辑（插入、删除和替换）下，我们的方法可以正确提取水印，提供了可证明的鲁棒性保证。我们广泛的实验结果也证明了这一突破。实验表明，我们的方法在基准数据集上的准确性和稳健性都大大优于现有的基线。例如，当将长度为12的比特串嵌入到200个令牌生成的文本中时，我们的方法获得了令人印象深刻的98.4\%$的匹配率，超过了Yoo等人的性能。（最先进的基线）$85.6\%$。当受到复制粘贴攻击时，包括向生成的200个单词的文本注入50个令牌，而Yoo等人的匹配率降低到低于$65\%$。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.16820v1" target="_blank">2401.16820v1</a>
                              </td>
                              <td>Provably Robust Multi-bit Watermarking for AI-generated Text via Error Correction Code</td>
                              <td>Wenjie Qu</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_16820v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.16820v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_16818v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">H2O-Danube-1.8B Technical Report</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_16818v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_16818v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_16818v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present H2O-Danube-1.8B, a 1.8B language model trained on 1T tokens following the core principles of LLama 2 and Mistral. We leverage and refine various techniques for pre-training large language models. Although our model is trained on significantly fewer total tokens compared to reference models of similar size, it exhibits highly competitive metrics across a multitude of benchmarks. We additionally release a chat model trained with supervised fine-tuning followed by direct preference optimization. We make H2O-Danube-1.8B openly available under Apache 2.0 license further democratizing LLMs to a wider audience economically.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_16818v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了H2O-Danube-1.8B，这是一个在1T令牌上训练的1.8B语言模型，遵循LLama 2和Mistral的核心原则。我们利用和完善各种技术来预训练大型语言模型。尽管与类似规模的参考模型相比，我们的模型在更少的总代币上进行训练，但它在众多基准中表现出了高度竞争力的指标。我们还发布了一个聊天模型，该模型经过监督微调和直接偏好优化训练。我们在Apache 2.0许可证下公开提供H2O-Danube-1.8B，进一步使LLM在经济上向更广泛的受众民主化。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.16818v1" target="_blank">2401.16818v1</a>
                              </td>
                              <td>H2O-Danube-1.8B Technical Report</td>
                              <td>Philipp Singer</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_16818v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.16818v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_16807v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Detecting LLM-Assisted Writing in Scientific Communication: Are We There Yet?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_16807v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_16807v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_16807v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs), exemplified by ChatGPT, have significantly reshaped text generation, particularly in the realm of writing assistance. While ethical considerations underscore the importance of transparently acknowledging LLM use, especially in scientific communication, genuine acknowledgment remains infrequent. A potential avenue to encourage accurate acknowledging of LLM-assisted writing involves employing automated detectors. Our evaluation of four cutting-edge LLM-generated text detectors reveals their suboptimal performance compared to a simple ad-hoc detector designed to identify abrupt writing style changes around the time of LLM proliferation. We contend that the development of specialized detectors exclusively dedicated to LLM-assisted writing detection is necessary. Such detectors could play a crucial role in fostering more authentic recognition of LLM involvement in scientific communication, addressing the current challenges in acknowledgment practices.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_16807v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>以ChatGPT为例的大型语言模型（LLM）极大地重塑了文本生成，尤其是在写作辅助领域。尽管伦理考虑强调了透明地承认LLM使用的重要性，特别是在科学传播中，但真正的承认仍然很少。一种鼓励准确承认LLM辅助写作的潜在途径包括使用自动检测器。我们对四个尖端LLM生成的文本检测器的评估显示，与一个简单的专门检测器相比，它们的性能并不理想，该检测器旨在识别LLM激增期间的突然写作风格变化。我们认为，开发专门用于LLM辅助写入检测的专用检测器是必要的。这种探测器可以在促进对LLM参与科学传播的更真实的认识方面发挥关键作用，解决目前在承认实践中的挑战。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.16807v1" target="_blank">2401.16807v1</a>
                              </td>
                              <td>Detecting LLM-Assisted Writing in Scientific Communication: Are We There Yet?</td>
                              <td>Teddy Lazebnik</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_16807v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.16807v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_16267v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Novel Preprocessing Technique for Data Embedding in Engineering Code Generation Using Large Language Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_16267v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_16267v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_16267v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present four main contributions to enhance the performance of Large Language Models (LLMs) in generating domain-specific code: (i) utilizing LLM-based data splitting and data renovation techniques to improve the semantic representation of embeddings' space; (ii) introducing the Chain of Density for Renovation Credibility (CoDRC), driven by LLMs, and the Adaptive Text Renovation (ATR) algorithm for assessing data renovation reliability; (iii) developing the Implicit Knowledge Expansion and Contemplation (IKEC) Prompt technique; and (iv) effectively refactoring existing scripts to generate new and high-quality scripts with LLMs. By using engineering simulation software RedHawk-SC as a case study, we demonstrate the effectiveness of our data pre-processing method for expanding and categorizing scripts. When combined with IKEC, these techniques enhance the Retrieval-Augmented Generation (RAG) method in retrieving more relevant information, ultimately achieving a 73.33% "Percentage of Correct Lines" for code generation problems in MapReduce applications.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_16267v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了四个主要贡献来提高大型语言模型（LLM）在生成领域特定代码方面的性能：（i）利用基于LLM的数据拆分和数据更新技术来改进嵌入空间的语义表示；（ii）引入由LLM驱动的更新可信度密度链（CoDRC）和用于评估数据更新可靠性的自适应文本更新（ATR）算法；（iii）开发内隐知识扩展和思考（IKEC）提示技术；以及（iv）有效地重构现有脚本，以生成具有LLM的新的高质量脚本。通过使用工程仿真软件RedHawk SC作为案例研究，我们证明了我们的数据预处理方法对脚本进行扩展和分类的有效性。当与IKEC相结合时，这些技术增强了检索增强生成（RAG）方法检索更多相关信息的能力，最终在MapReduce应用程序中实现了73.33%的代码生成问题“正确行百分比”。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.16267v2" target="_blank">2311.16267v2</a>
                              </td>
                              <td>Novel Preprocessing Technique for Data Embedding in Engineering Code Generation Using Large Language Model</td>
                              <td>Yu-Chen Lin</td>
                              <td>2023-11-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_16267v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.16267v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_16797v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Enhancing Compiler Transformation Robustness with Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_16797v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_16797v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_16797v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a framework that integrates Large Language Models (LLMs) into translation validation, targeting LLVM compiler transformations where formal verification tools are insufficient. Our framework first utilizes existing formal verification frameworks for translation validation. In this work, we use Alive2, a well-known tool in LLVM compiler verification, as an example. When formal verification frameworks are unable to confirm a transformation's soundness, our framework employs fine-tuned LLMs for prediction. It applies fuzzing to transformations predicted as potentially unsound by the LLMs due to return value or memory inconsistencies, aiming to find counterexamples. In cases where transformations are unsound for other reasons or sound, or if no counterexamples emerge, the framework directly reports these outcomes without further fuzzing. This methodology has shown effectiveness in complex areas like deep-learning accelerator design, where traditional tools struggle.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_16797v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一个将大型语言模型（LLM）集成到翻译验证中的框架，针对形式验证工具不足的LLVM编译器转换。我们的框架首先利用现有的正式验证框架进行翻译验证。在这项工作中，我们使用Alive2作为示例，Alive2是LLVM编译器验证中的一个著名工具。当正式的验证框架无法确认转换的合理性时，我们的框架会使用经过微调的LLM进行预测。它将模糊化应用于LLM预测的由于返回值或内存不一致而可能不健全的转换，旨在找到反例。在转换因其他原因而不健全或合理的情况下，或者如果没有反例出现，框架会直接报告这些结果，而不会进一步模糊。这种方法在传统工具难以解决的深度学习加速器设计等复杂领域显示出了有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.16797v1" target="_blank">2401.16797v1</a>
                              </td>
                              <td>Enhancing Compiler Transformation Robustness with Large Language Models</td>
                              <td>Yanzhao Wang</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_16797v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.16797v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14423v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Prompt Design and Engineering: Introduction and Advanced Methods</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14423v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14423v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14423v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Prompt design and engineering has become an important discipline in just the past few months. In this paper, we provide an introduction to the main concepts and design approaches. We also provide more advanced techniques all the way to those needed to design LLM-based agents. We finish by providing a list of existing tools for prompt engineering.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14423v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在过去的几个月里，快速设计和工程已经成为一门重要的学科。在本文中，我们介绍了主要概念和设计方法。我们还提供了更先进的技术，一直到设计基于LLM的代理所需的技术。最后，我们提供了一份现有工具的列表，用于快速工程设计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>46</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14423v2" target="_blank">2401.14423v2</a>
                              </td>
                              <td>Prompt Design and Engineering: Introduction and Advanced Methods</td>
                              <td>Xavier Amatriain</td>
                              <td>2024-01-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14423v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14423v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_16788v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_16788v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_16788v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_16788v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite the utility of Large Language Models (LLMs) across a wide range of tasks and scenarios, developing a method for reliably evaluating LLMs across varied contexts continues to be challenging. Modern evaluation approaches often use LLMs to assess responses generated by LLMs. However, the meta-evaluation conducted to assess the effectiveness of these LLMs as evaluators is typically constrained by the coverage of existing benchmarks or requires extensive human annotation. This underscores the urgency of methods for scalable meta-evaluation that can effectively, reliably, and efficiently evaluate the performance of LLMs as evaluators across diverse tasks and scenarios, particularly in potentially new, user-defined scenarios. To fill this gap, we propose ScaleEval, an agent-debate-assisted meta-evaluation framework that leverages the capabilities of multiple communicative LLM agents. This framework supports multi-round discussions to assist human annotators in discerning the most capable LLMs as evaluators, which significantly eases their workload in cases that used to require large-scale annotations during meta-evaluation. We release the code for our framework, which is publicly available at: \url{https://github.com/GAIR-NLP/scaleeval}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_16788v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管大型语言模型（LLM）在各种任务和场景中都很有用，但开发一种在不同背景下可靠评估LLM的方法仍然具有挑战性。现代评估方法通常使用LLM来评估LLM产生的反应。然而，为评估这些LLM作为评估者的有效性而进行的元评估通常受到现有基准覆盖范围的限制，或者需要大量的人工注释。这突出了可扩展元评估方法的紧迫性，该方法可以有效、可靠和高效地评估LLM作为评估者在不同任务和场景中的性能，特别是在潜在的新的用户定义场景中。为了填补这一空白，我们提出了ScaleEval，这是一个代理辩论辅助的元评估框架，利用了多个交际LLM代理的能力。该框架支持多轮讨论，以帮助注释人员将最有能力的LLM识别为评估者，这大大减轻了他们在元评估期间需要大规模注释的情况下的工作量。我们发布了框架的代码，可在以下网址公开获取：\url{https://github.com/GAIR-NLP/scaleeval}.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>47</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.16788v1" target="_blank">2401.16788v1</a>
                              </td>
                              <td>Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate</td>
                              <td>Steffi Chern</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_16788v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.16788v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02777v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02777v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02777v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02777v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper introduces RAISE (Reasoning and Acting through Scratchpad and Examples), an advanced architecture enhancing the integration of Large Language Models (LLMs) like GPT-4 into conversational agents. RAISE, an enhancement of the ReAct framework, incorporates a dual-component memory system, mirroring human short-term and long-term memory, to maintain context and continuity in conversations. It entails a comprehensive agent construction scenario, including phases like Conversation Selection, Scene Extraction, CoT Completion, and Scene Augmentation, leading to the LLMs Training phase. This approach appears to enhance agent controllability and adaptability in complex, multi-turn dialogues. Our preliminary evaluations in a real estate sales context suggest that RAISE has some advantages over traditional agents, indicating its potential for broader applications. This work contributes to the AI field by providing a robust framework for developing more context-aware and versatile conversational agents.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02777v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了RAISE（Reasoning and Acting through Scratchpad and Examples），这是一种高级架构，用于增强GPT-4等大型语言模型（LLM）与会话代理的集成。RAISE是ReAct框架的增强，它包含了一个双成分记忆系统，反映了人类的短期和长期记忆，以保持对话的上下文和连续性。它需要一个全面的代理构建场景，包括会话选择、场景提取、CoT完成和场景增强等阶段，从而进入LLM训练阶段。这种方法似乎增强了智能体在复杂、多回合对话中的可控性和适应性。我们在房地产销售背景下的初步评估表明，RAISE与传统代理商相比具有一些优势，这表明其具有更广泛的应用潜力。这项工作为开发更多上下文感知和通用的会话代理提供了一个强大的框架，为人工智能领域做出了贡献。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>48</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02777v2" target="_blank">2401.02777v2</a>
                              </td>
                              <td>From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models</td>
                              <td>Na Liu</td>
                              <td>2024-01-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02777v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02777v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="SAM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2401_17221v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MouSi: Poly-Visual-Expert Vision-Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17221v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17221v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17221v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Current large vision-language models (VLMs) often encounter challenges such as insufficient capabilities of a single visual component and excessively long visual tokens. These issues can limit the model's effectiveness in accurately interpreting complex visual information and over-lengthy contextual information. Addressing these challenges is crucial for enhancing the performance and applicability of VLMs. This paper proposes the use of ensemble experts technique to synergizes the capabilities of individual visual encoders, including those skilled in image-text matching, OCR, image segmentation, etc. This technique introduces a fusion network to unify the processing of outputs from different visual experts, while bridging the gap between image encoders and pre-trained LLMs. In addition, we explore different positional encoding schemes to alleviate the waste of positional encoding caused by lengthy image feature sequences, effectively addressing the issue of position overflow and length limitations. For instance, in our implementation, this technique significantly reduces the positional occupancy in models like SAM, from a substantial 4096 to a more efficient and manageable 64 or even down to 1. Experimental results demonstrate that VLMs with multiple experts exhibit consistently superior performance over isolated visual encoders and mark a significant performance boost as more experts are integrated. We have open-sourced the training code used in this report. All of these resources can be found on our project website.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17221v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当前的大型视觉语言模型（VLM）经常遇到诸如单个视觉组件能力不足和视觉标记过长等挑战。这些问题可能会限制模型准确解释复杂视觉信息和过长上下文信息的有效性。应对这些挑战对于提高VLM的性能和适用性至关重要。本文提出使用集成专家技术来协同各个视觉编码器的能力，包括那些擅长图像文本匹配、OCR、图像分割等的编码器。该技术引入了一个融合网络来统一处理不同视觉专家的输出，同时弥合图像编码器和预训练LLM之间的差距。此外，我们探索了不同的位置编码方案，以减轻图像特征序列过长造成的位置编码浪费，有效地解决了位置溢出和长度限制的问题。例如，在我们的实现中，该技术显著降低了SAM等模型中的位置占用率，从相当大的4096降低到更高效和可管理的64，甚至降低到1。实验结果表明，与孤立的视觉编码器相比，具有多个专家的VLM始终表现出优异的性能，并且随着更多专家的集成，性能显著提高。我们已经开源了本报告中使用的培训代码。所有这些资源都可以在我们的项目网站上找到。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17221v1" target="_blank">2401.17221v1</a>
                              </td>
                              <td>MouSi: Poly-Visual-Expert Vision-Language Models</td>
                              <td>Xiaoran Fan</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17221v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17221v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17083v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Online Robot Navigation and and Manipulation with Distilled Vision-Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17083v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17083v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17083v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Autonomous robot navigation within the dynamic unknown environment is of crucial significance for mobile robotic applications including robot navigation in last-mile delivery and robot-enabled automated supplies in industrial and hospital delivery applications. Current solutions still suffer from limitations, such as the robot cannot recognize unknown objects in real time and cannot navigate freely in a dynamic, narrow, and complex environment. We propose a complete software framework for autonomous robot perception and navigation within very dense obstacles and dense human crowds. First, we propose a framework that accurately detects and segments open-world object categories in a zero-shot manner, which overcomes the over-segmentation limitation of the current SAM model. Second, we proposed the distillation strategy to distill the knowledge to segment the free space of the walkway for robot navigation without the label. In the meantime, we design the trimming strategy that works collaboratively with distillation to enable lightweight inference to deploy the neural network on edge devices such as NVIDIA-TX2 or Xavier NX during autonomous navigation. Integrated into the robot navigation system, extensive experiments demonstrate that our proposed framework has achieved superior performance in terms of both accuracy and efficiency in robot scene perception and autonomous robot navigation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17083v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>动态未知环境中的自主机器人导航对移动机器人应用具有至关重要的意义，包括最后一英里配送中的机器人导航以及工业和医院配送应用中的机器人自动化供应。目前的解决方案仍然存在局限性，例如机器人无法实时识别未知物体，也无法在动态、狭窄和复杂的环境中自由导航。我们提出了一个完整的软件框架，用于在非常密集的障碍物和密集的人群中进行自主机器人感知和导航。首先，我们提出了一个框架，以零样本的方式准确地检测和分割开放世界对象类别，这克服了当前SAM模型的过度分割限制。其次，我们提出了提取知识的提取策略，以在没有标签的情况下分割机器人导航通道的自由空间。与此同时，我们设计了与蒸馏协同工作的微调策略，以实现轻量级推理，从而在自主导航期间将神经网络部署在NVIDIA-TX2或Xavier NX等边缘设备上。集成到机器人导航系统中，大量实验表明，我们提出的框架在机器人场景感知和自主机器人导航方面的准确性和效率都取得了优异的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17083v1" target="_blank">2401.17083v1</a>
                              </td>
                              <td>Online Robot Navigation and and Manipulation with Distilled Vision-Language Models</td>
                              <td>Kangcheng Liu</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17083v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17083v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_15266v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAM-based instance segmentation models for the automation of structural damage detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_15266v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_15266v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_15266v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Automating visual inspection for capturing defects based on civil structures appearance is crucial due to its currently labour-intensive and time-consuming nature. An important aspect of automated inspection is image acquisition, which is rapid and cost-effective considering the pervasive developments in both software and hardware computing in recent years. Previous studies largely focused on concrete and asphalt, with less attention to masonry cracks. The latter also lacks publicly available datasets. In this paper, we first present a corresponding data set for instance segmentation with 1,300 annotated images (640 pixels x 640 pixels), named as MCrack1300, covering bricks, broken bricks, and cracks. We then test several leading algorithms for benchmarking, including the latest large-scale model, the prompt-based Segment Anything Model (SAM). We fine-tune the encoder using Low-Rank Adaptation (LoRA) and proposed two novel methods for automation of SAM execution. The first method involves abandoning the prompt encoder and connecting the SAM encoder to other decoders, while the second method introduces a learnable self-generating prompter. In order to ensure the seamless integration of the two proposed methods with SAM encoder section, we redesign the feature extractor. Both proposed methods exceed state-of-the-art performance, surpassing the best benchmark by approximately 3% for all classes and around 6% for cracks specifically. Based on successful detection, we propose a method based on a monocular camera and the Hough Line Transform to automatically transform images into orthographic projection maps. By incorporating known real sizes of brick units, we accurately estimate crack dimensions, with the results differing by less than 10% from those obtained by laser scanning. Overall, we address important research gaps in automated masonry crack detection and size estimation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_15266v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于土木结构外观的缺陷自动视觉检测至关重要，因为其目前劳动密集且耗时。自动检测的一个重要方面是图像采集，考虑到近年来软件和硬件计算的普遍发展，图像采集快速且具有成本效益。以前的研究主要集中在混凝土和沥青上，很少关注砌体裂缝。后者也缺乏公开的数据集。在本文中，我们首先提出了一个相应的数据集，例如具有1300个注释图像（640像素x 640像素）的分割，称为MCrack1300，覆盖砖块、碎砖和裂缝。然后，我们测试了几种领先的基准测试算法，包括最新的大规模模型，基于提示的分段任意模型（SAM）。我们使用低秩自适应（LoRA）对编码器进行微调，并提出了两种新的SAM执行自动化方法。第一种方法包括放弃提示编码器并将SAM编码器连接到其他解码器，而第二种方法引入了可学习的自生成提示器。为了确保所提出的两种方法与SAM编码器部分的无缝集成，我们重新设计了特征提取器。两种提出的方法都超过了最先进的性能，所有类别都超过了最佳基准约3%，特别是裂纹超过了约6%。在成功检测的基础上，我们提出了一种基于单眼相机和霍夫线变换的方法，将图像自动转换为正交投影图。通过结合已知的砖单元实际尺寸，我们准确地估计了裂缝尺寸，结果与激光扫描结果相差不到10%。总的来说，我们解决了自动化砌体裂缝检测和尺寸估计方面的重要研究空白。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.15266v2" target="_blank">2401.15266v2</a>
                              </td>
                              <td>SAM-based instance segmentation models for the automation of structural damage detection</td>
                              <td>Zehao Ye</td>
                              <td>2024-01-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_15266v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.15266v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_16741v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MESA: Matching Everything by Segmenting Anything</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_16741v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_16741v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_16741v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Feature matching is a crucial task in the field of computer vision, which involves finding correspondences between images. Previous studies achieve remarkable performance using learning-based feature comparison. However, the pervasive presence of matching redundancy between images gives rise to unnecessary and error-prone computations in these methods, imposing limitations on their accuracy. To address this issue, we propose MESA, a novel approach to establish precise area (or region) matches for efficient matching redundancy reduction. MESA first leverages the advanced image understanding capability of SAM, a state-of-the-art foundation model for image segmentation, to obtain image areas with implicit semantic. Then, a multi-relational graph is proposed to model the spatial structure of these areas and construct their scale hierarchy. Based on graphical models derived from the graph, the area matching is reformulated as an energy minimization task and effectively resolved. Extensive experiments demonstrate that MESA yields substantial precision improvement for multiple point matchers in indoor and outdoor downstream tasks, e.g. +13.61% for DKM in indoor pose estimation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_16741v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>特征匹配是计算机视觉领域的一项关键任务，它涉及到寻找图像之间的对应关系。先前的研究使用基于学习的特征比较实现了显著的性能。然而，图像之间普遍存在的匹配冗余导致了这些方法中不必要且容易出错的计算，从而限制了它们的准确性。为了解决这个问题，我们提出了MESA，这是一种新的方法来建立精确的区域（或区域）匹配，以有效地减少匹配冗余。MESA首先利用SAM的高级图像理解能力，这是一种最先进的图像分割基础模型，以获得具有隐含语义的图像区域。然后，提出了一种多关系图来对这些区域的空间结构进行建模，并构建它们的尺度层次。基于从图中导出的图形模型，将区域匹配重新表述为能量最小化任务，并有效地解决了该问题。大量实验表明，MESA在室内和室外下游任务中为多点匹配器带来了显著的精度提高，例如，在室内姿态估计中，DKM的精度提高了13.61%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.16741v1" target="_blank">2401.16741v1</a>
                              </td>
                              <td>MESA: Matching Everything by Segmenting Anything</td>
                              <td>Yesheng Zhang</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_16741v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.16741v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_02245v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAM3D: Zero-Shot 3D Object Detection via Segment Anything Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_02245v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_02245v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_02245v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With the development of large language models, many remarkable linguistic systems like ChatGPT have thrived and achieved astonishing success on many tasks, showing the incredible power of foundation models. In the spirit of unleashing the capability of foundation models on vision tasks, the Segment Anything Model (SAM), a vision foundation model for image segmentation, has been proposed recently and presents strong zero-shot ability on many downstream 2D tasks. However, whether SAM can be adapted to 3D vision tasks has yet to be explored, especially 3D object detection. With this inspiration, we explore adapting the zero-shot ability of SAM to 3D object detection in this paper. We propose a SAM-powered BEV processing pipeline to detect objects and get promising results on the large-scale Waymo open dataset. As an early attempt, our method takes a step toward 3D object detection with vision foundation models and presents the opportunity to unleash their power on 3D vision tasks. The code is released at https://github.com/DYZhang09/SAM3D.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_02245v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着大型语言模型的发展，像ChatGPT这样的许多卓越的语言系统蓬勃发展，并在许多任务上取得了惊人的成功，显示了基础模型令人难以置信的力量。本着释放基础模型在视觉任务上的能力的精神，最近提出了用于图像分割的视觉基础模型Segment Anything Model（SAM），并在许多下游2D任务上表现出强大的零样本能力。然而，SAM是否能够适应3D视觉任务还有待探索，尤其是3D对象检测。在这种启发下，我们探索将SAM的零样本能力应用于三维物体检测。我们提出了一种SAM驱动的BEV处理管道来检测对象，并在大规模Waymo开放数据集上获得有希望的结果。作为早期的尝试，我们的方法向使用视觉基础模型进行3D对象检测迈出了一步，并提供了在3D视觉任务中释放其力量的机会。代码发布于https://github.com/DYZhang09/SAM3D.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.02245v2" target="_blank">2306.02245v2</a>
                              </td>
                              <td>SAM3D: Zero-Shot 3D Object Detection via Segment Anything Model</td>
                              <td>Dingyuan Zhang</td>
                              <td>2023-06-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_02245v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.02245v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/dyzhang09/sam3d" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_12665v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_12665v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_12665v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_12665v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, foundational models such as CLIP and SAM have shown promising performance for the task of Zero-Shot Anomaly Segmentation (ZSAS). However, either CLIP-based or SAM-based ZSAS methods still suffer from non-negligible key drawbacks: 1) CLIP primarily focuses on global feature alignment across different inputs, leading to imprecise segmentation of local anomalous parts; 2) SAM tends to generate numerous redundant masks without proper prompt constraints, resulting in complex post-processing requirements. In this work, we innovatively propose a CLIP and SAM collaboration framework called ClipSAM for ZSAS. The insight behind ClipSAM is to employ CLIP's semantic understanding capability for anomaly localization and rough segmentation, which is further used as the prompt constraints for SAM to refine the anomaly segmentation results. In details, we introduce a crucial Unified Multi-scale Cross-modal Interaction (UMCI) module for interacting language with visual features at multiple scales of CLIP to reason anomaly positions. Then, we design a novel Multi-level Mask Refinement (MMR) module, which utilizes the positional information as multi-level prompts for SAM to acquire hierarchical levels of masks and merges them. Extensive experiments validate the effectiveness of our approach, achieving the optimal segmentation performance on the MVTec-AD and VisA datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_12665v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，CLIP和SAM等基础模型在零样本异常分割（ZSAS）任务中表现出了良好的性能。然而，基于CLIP或基于SAM的ZSAS方法仍然存在不可忽略的关键缺点：1）CLIP主要关注不同输入的全局特征对齐，导致局部异常部分的分割不精确；2） SAM倾向于在没有适当提示约束的情况下生成大量冗余掩码，从而导致复杂的后处理要求。在这项工作中，我们创新性地为ZSAS提出了一个名为ClipSAM的CLIP和SAM合作框架。ClipSAM背后的见解是利用CLIP的语义理解能力进行异常定位和粗略分割，这进一步被用作SAM细化异常分割结果的提示约束。详细地说，我们介绍了一个关键的统一多尺度跨模态交互（UMCI）模块，用于在CLIP的多个尺度上与视觉特征进行交互，以确定异常位置。然后，我们设计了一个新的多级掩码细化（MMR）模块，该模块利用位置信息作为SAM的多级提示来获取分层掩码并将其合并。大量实验验证了我们方法的有效性，在MVTec AD和VisA数据集上实现了最佳分割性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.12665v2" target="_blank">2401.12665v2</a>
                              </td>
                              <td>ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation</td>
                              <td>Shengze Li</td>
                              <td>2024-01-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_12665v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.12665v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/lszcoding/clipsam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_11787v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">HypBO: Accelerating Black-Box Scientific Experiments Using Experts' Hypotheses</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_11787v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_11787v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_11787v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Robotics and automation offer massive accelerations for solving intractable, multivariate scientific problems such as materials discovery, but the available search spaces can be dauntingly large. Bayesian optimization (BO) has emerged as a popular sample-efficient optimization engine, thriving in tasks where no analytic form of the target function/property is known. Here, we exploit expert human knowledge in the form of hypotheses to direct Bayesian searches more quickly to promising regions of chemical space. Previous methods have used underlying distributions derived from existing experimental measurements, which is unfeasible for new, unexplored scientific tasks. Also, such distributions cannot capture intricate hypotheses. Our proposed method, which we call HypBO, uses expert human hypotheses to generate improved seed samples. Unpromising seeds are automatically discounted, while promising seeds are used to augment the surrogate model data, thus achieving better-informed sampling. This process continues in a global versus local search fashion, organized in a bilevel optimization framework. We validate the performance of our method on a range of synthetic functions and demonstrate its practical utility on a real chemical design task where the use of expert hypotheses accelerates the search performance significantly.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_11787v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>机器人和自动化为解决材料发现等棘手的多变量科学问题提供了巨大的加速，但可用的搜索空间可能大得惊人。贝叶斯优化（BO）已成为一种流行的样本高效优化引擎，在目标函数/属性的分析形式未知的任务中蓬勃发展。在这里，我们以假设的形式利用人类专家知识，将贝叶斯搜索更快地引导到化学空间的有希望的区域。以前的方法使用了从现有实验测量中得出的潜在分布，这对于新的、未经探索的科学任务来说是不可行的。此外，这样的分布无法捕捉到复杂的假设。我们提出的方法，我们称之为HypBO，使用人类专家的假设来生成改进的种子样本。没有希望的种子会自动贴现，而有希望的种子则用于增加代理模型数据，从而实现更好的知情采样。这一过程以全局搜索与局部搜索的方式继续，在双层优化框架中组织。我们验证了我们的方法在一系列合成函数上的性能，并在实际的化学设计任务中证明了它的实用性，在该任务中，专家假设的使用显著加快了搜索性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.11787v3" target="_blank">2308.11787v3</a>
                              </td>
                              <td>HypBO: Accelerating Black-Box Scientific Experiments Using Experts' Hypotheses</td>
                              <td>Abdoulatif Cisse</td>
                              <td>2023-08-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_11787v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.11787v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/luinardi/hypermapper" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_15282v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GEM: Boost Simple Network for Glass Surface Segmentation via Segment Anything Model and Data Synthesis</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_15282v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_15282v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_15282v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Detecting glass regions is a challenging task due to the ambiguity of their transparency and reflection properties. These transparent glasses share the visual appearance of both transmitted arbitrary background scenes and reflected objects, thus having no fixed patterns.Recent visual foundation models, which are trained on vast amounts of data, have manifested stunning performance in terms of image perception and image generation. To segment glass surfaces with higher accuracy, we make full use of two visual foundation models: Segment Anything (SAM) and Stable Diffusion.Specifically, we devise a simple glass surface segmentor named GEM, which only consists of a SAM backbone, a simple feature pyramid, a discerning query selection module, and a mask decoder. The discerning query selection can adaptively identify glass surface features, assigning them as initialized queries in the mask decoder. We also propose a Synthetic but photorealistic large-scale Glass Surface Detection dataset dubbed S-GSD via diffusion model with four different scales, which contain 1x, 5x, 10x, and 20x of the original real data size. This dataset is a feasible source for transfer learning. The scale of synthetic data has positive impacts on transfer learning, while the improvement will gradually saturate as the amount of data increases. Extensive experiments demonstrate that GEM achieves a new state-of-the-art on the GSD-S validation set (IoU +2.1%). Codes and datasets are available at: https://github.com/isbrycee/GEM-Glass-Segmentor.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_15282v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于玻璃区域的透明度和反射特性的模糊性，检测玻璃区域是一项具有挑战性的任务。这些透明眼镜共享透射的任意背景场景和反射物体的视觉外观，因此没有固定的图案。最近的视觉基础模型基于大量数据进行训练，在图像感知和图像生成方面表现出惊人的性能。为了更高精度地分割玻璃表面，我们充分利用了两个视觉基础模型：分割任何东西（SAM）和稳定扩散。具体来说，我们设计了一个名为GEM的简单玻璃表面分割器，它只由SAM主干、简单特征金字塔、辨别查询选择模块和掩码解码器组成。辨别查询选择可以自适应地识别玻璃表面特征，并将其分配为掩码解码器中的初始化查询。我们还提出了一个合成但逼真的大规模玻璃表面检测数据集，称为S-GSD，通过四种不同尺度的扩散模型，包含原始真实数据大小的1x、5x、10x和20x。该数据集是迁移学习的一个可行来源。合成数据的规模对迁移学习有积极影响，而随着数据量的增加，这种改善将逐渐饱和。大量实验表明，GEM在GSD-S验证集上达到了最先进的水平（IoU+2.1%）。代码和数据集可在：https://github.com/isbrycee/GEM-Glass-Segmentor.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.15282v1" target="_blank">2401.15282v1</a>
                              </td>
                              <td>GEM: Boost Simple Network for Glass Surface Segmentation via Segment Anything Model and Data Synthesis</td>
                              <td>Jing Hao</td>
                              <td>2024-01-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_15282v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.15282v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_17191v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MT-SLVR: Multi-Task Self-Supervised Learning for Transformation In(Variant) Representations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_17191v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_17191v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_17191v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Contrastive self-supervised learning has gained attention for its ability to create high-quality representations from large unlabelled data sets. A key reason that these powerful features enable data-efficient learning of downstream tasks is that they provide augmentation invariance, which is often a useful inductive bias. However, the amount and type of invariances preferred is not known apriori, and varies across different downstream tasks. We therefore propose a multi-task self-supervised framework (MT-SLVR) that learns both variant and invariant features in a parameter-efficient manner. Our multi-task representation provides a strong and flexible feature that benefits diverse downstream tasks. We evaluate our approach on few-shot classification tasks drawn from a variety of audio domains and demonstrate improved classification performance on all of them</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_17191v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对比自监督学习因其能够从大型未标记数据集中创建高质量表示而受到关注。这些强大的特征能够实现下游任务的数据高效学习的一个关键原因是，它们提供了增强不变性，这通常是一种有用的归纳偏差。然而，首选不变量的数量和类型在先验上是未知的，并且在不同的下游任务中有所不同。因此，我们提出了一种多任务自监督框架（MT-SLVR），该框架以参数有效的方式学习变异和不变特征。我们的多任务表示提供了一个强大而灵活的功能，有利于不同的下游任务。我们在从各种音频领域提取的少量镜头分类任务上评估了我们的方法，并在所有这些任务上展示了改进的分类性能</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.17191v2" target="_blank">2305.17191v2</a>
                              </td>
                              <td>MT-SLVR: Multi-Task Self-Supervised Learning for Transformation In(Variant) Representations</td>
                              <td>Calum Heggan</td>
                              <td>2023-05-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_17191v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.17191v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/CHeggan/MT-SLVR" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/cheggan/mt-slvr" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14686v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SSR: SAM is a Strong Regularizer for domain adaptive semantic segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14686v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14686v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14686v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduced SSR, which utilizes SAM (segment-anything) as a strong regularizer during training, to greatly enhance the robustness of the image encoder for handling various domains. Specifically, given the fact that SAM is pre-trained with a large number of images over the internet, which cover a diverse variety of domains, the feature encoding extracted by the SAM is obviously less dependent on specific domains when compared to the traditional ImageNet pre-trained image encoder. Meanwhile, the ImageNet pre-trained image encoder is still a mature choice of backbone for the semantic segmentation task, especially when the SAM is category-irrelevant. As a result, our SSR provides a simple yet highly effective design. It uses the ImageNet pre-trained image encoder as the backbone, and the intermediate feature of each stage (ie there are 4 stages in MiT-B5) is regularized by SAM during training. After extensive experimentation on GTA5$\rightarrow$Cityscapes, our SSR significantly improved performance over the baseline without introducing any extra inference overhead.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14686v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们引入了SSR，它在训练过程中利用SAM（分割任何东西）作为强正则化子，以大大增强图像编码器处理各种域的鲁棒性。具体而言，考虑到SAM是通过互联网上的大量图像进行预训练的，这些图像覆盖了不同的领域，与传统的ImageNet预训练图像编码器相比，SAM提取的特征编码对特定领域的依赖性明显较小。同时，ImageNet预训练的图像编码器仍然是语义分割任务的成熟骨干选择，尤其是当SAM与类别无关时。因此，我们的SSR提供了一种简单而高效的设计。它使用ImageNet预训练的图像编码器作为骨干，每个阶段（即MiT-B5中有4个阶段）的中间特征在训练期间由SAM正则化。在GTA5$\rightarrow$Cityscapes上进行了广泛的实验后，我们的SSR在没有引入任何额外推理开销的情况下显著提高了性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14686v1" target="_blank">2401.14686v1</a>
                              </td>
                              <td>SSR: SAM is a Strong Regularizer for domain adaptive semantic segmentation</td>
                              <td>Yanqi Ge</td>
                              <td>2024-01-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14686v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14686v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/geyanqi/ssr" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_01429v4_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Adapting Segment Anything Model for Change Detection in HR Remote Sensing Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_01429v4_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_01429v4_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_01429v4_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision Foundation Models (VFMs) such as the Segment Anything Model (SAM) allow zero-shot or interactive segmentation of visual contents, thus they are quickly applied in a variety of visual scenes. However, their direct use in many Remote Sensing (RS) applications is often unsatisfactory due to the special imaging characteristics of RS images. In this work, we aim to utilize the strong visual recognition capabilities of VFMs to improve the change detection of high-resolution Remote Sensing Images (RSIs). We employ the visual encoder of FastSAM, an efficient variant of the SAM, to extract visual representations in RS scenes. To adapt FastSAM to focus on some specific ground objects in the RS scenes, we propose a convolutional adaptor to aggregate the task-oriented change information. Moreover, to utilize the semantic representations that are inherent to SAM features, we introduce a task-agnostic semantic learning branch to model the semantic latent in bi-temporal RSIs. The resulting method, SAMCD, obtains superior accuracy compared to the SOTA methods and exhibits a sample-efficient learning ability that is comparable to semi-supervised CD methods. To the best of our knowledge, this is the first work that adapts VFMs for the CD of HR RSIs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_01429v4_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉基础模型（VFM）（如Segment Anything Model（SAM））允许对视觉内容进行零样本或交互式分割，因此它们可以快速应用于各种视觉场景。然而，由于遥感图像的特殊成像特性，它们在许多遥感应用中的直接使用往往不令人满意。在这项工作中，我们的目标是利用VFM强大的视觉识别能力来改进高分辨率遥感图像（RSI）的变化检测。我们使用FastSAM的视觉编码器，一种有效的SAM变体，来提取RS场景中的视觉表示。为了使FastSAM专注于RS场景中的一些特定地面对象，我们提出了一种卷积适配器来聚合面向任务的变化信息。此外，为了利用SAM特征固有的语义表示，我们引入了一个任务不可知的语义学习分支来对双时间RSI中潜在的语义进行建模。与SOTA方法相比，所得到的方法SAMCD获得了优越的精度，并表现出与半监督CD方法相当的样本有效学习能力。据我们所知，这是第一项将VFM应用于HR RSI CD的工作。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.01429v4" target="_blank">2309.01429v4</a>
                              </td>
                              <td>Adapting Segment Anything Model for Change Detection in HR Remote Sensing Images</td>
                              <td>Lei Ding</td>
                              <td>2023-09-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_01429v4_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.01429v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/ggsding/sam-cd" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_04668v4_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Deciding Equations in the Time Warp Algebra</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_04668v4_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_04668v4_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_04668v4_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Join-preserving maps on the discrete time scale $\omega^+$, referred to as time warps, have been proposed as graded modalities that can be used to quantify the growth of information in the course of program execution. The set of time warps forms a simple distributive involutive residuated lattice -- called the time warp algebra -- that is equipped with residual operations relevant to potential applications. In this paper, we show that although the time warp algebra generates a variety that lacks the finite model property, it nevertheless has a decidable equational theory. We also describe an implementation of a procedure for deciding equations in this algebra, written in the OCaml programming language, that makes use of the Z3 theorem prover.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_04668v4_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>离散时间尺度$\omega^+$上的连接保留映射，称为时间扭曲，已被提议作为分级模式，可用于量化程序执行过程中的信息增长。这组时间扭曲形成了一个简单的分配对合残差格，称为时间扭曲代数，它配备了与潜在应用相关的残差运算。在本文中，我们证明了尽管时间扭曲代数生成了一个缺乏有限模型性质的变种，但它仍然具有可判定的方程理论。我们还描述了用OCaml编程语言编写的、使用Z3定理证明器的、用于判定该代数中的方程的过程的实现。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.04668v4" target="_blank">2302.04668v4</a>
                              </td>
                              <td>Deciding Equations in the Time Warp Algebra</td>
                              <td>Sam van Gool</td>
                              <td>2023-01-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_04668v4_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.04668v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14159v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14159v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14159v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14159v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce Grounded SAM, which uses Grounding DINO as an open-set object detector to combine with the segment anything model (SAM). This integration enables the detection and segmentation of any regions based on arbitrary text inputs and opens a door to connecting various vision models. As shown in Fig.1, a wide range of vision tasks can be achieved by using the versatile Grounded SAM pipeline. For example, an automatic annotation pipeline based solely on input images can be realized by incorporating models such as BLIP and Recognize Anything. Additionally, incorporating Stable-Diffusion allows for controllable image editing, while the integration of OSX facilitates promptable 3D human motion analysis. Grounded SAM also shows superior performance on open-vocabulary benchmarks, achieving 48.7 mean AP on SegInW (Segmentation in the wild) zero-shot benchmark with the combination of Grounding DINO-Base and SAM-Huge models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14159v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了Grounded SAM，它使用Grounding DINO作为开集对象检测器与分段任意模型（SAM）相结合。这种集成能够基于任意文本输入检测和分割任何区域，并为连接各种视觉模型打开了大门。如图1所示，通过使用通用的接地SAM管道，可以实现广泛的视觉任务。例如，可以通过结合诸如BLIP和Recognize Anything之类的模型来实现仅基于输入图像的自动注释流水线。此外，结合Stable Diffusion可以进行可控的图像编辑，而OSX的集成有助于快速进行3D人体运动分析。Grounded SAM在开放词汇基准测试中也表现出优异的性能，在SegInW（野外细分）零样本基准测试中，通过Grounding DINO-Base和SAM-Huge模型的组合，达到了48.7的平均AP。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14159v1" target="_blank">2401.14159v1</a>
                              </td>
                              <td>Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks</td>
                              <td>Tianhe Ren</td>
                              <td>2024-01-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14159v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14159v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/IDEA-Research/Grounded-Segment-Anything" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_13961v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TriSAM: Tri-Plane SAM for zero-shot cortical blood vessel segmentation in VEM images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_13961v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_13961v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_13961v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we address a significant gap in the field of neuroimaging by introducing the largest-to-date public benchmark, BvEM, designed specifically for cortical blood vessel segmentation in Volume Electron Microscopy (VEM) images. The intricate relationship between cerebral blood vessels and neural function underscores the vital role of vascular analysis in understanding brain health. While imaging techniques at macro and mesoscales have garnered substantial attention and resources, the microscale VEM imaging, capable of revealing intricate vascular details, has lacked the necessary benchmarking infrastructure. As researchers delve deeper into the microscale intricacies of cerebral vasculature, our BvEM benchmark represents a critical step toward unraveling the mysteries of neurovascular coupling and its impact on brain function and pathology. The BvEM dataset is based on VEM image volumes from three mammal species: adult mouse, macaque, and human. We standardized the resolution, addressed imaging variations, and meticulously annotated blood vessels through semi-automatic, manual, and quality control processes, ensuring high-quality 3D segmentation. Furthermore, we developed a zero-shot cortical blood vessel segmentation method named TriSAM, which leverages the powerful segmentation model SAM for 3D segmentation. To lift SAM from 2D segmentation to 3D volume segmentation, TriSAM employs a multi-seed tracking framework, leveraging the reliability of certain image planes for tracking while using others to identify potential turning points. This approach, consisting of Tri-Plane selection, SAM-based tracking, and recursive redirection, effectively achieves long-term 3D blood vessel segmentation without model training or fine-tuning. Experimental results show that TriSAM achieved superior performances on the BvEM benchmark across three species.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_13961v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们通过引入迄今为止最大的公共基准BvEM来解决神经成像领域的一个重大差距，该基准专门为体积电子显微镜（VEM）图像中的皮层血管分割而设计。大脑血管和神经功能之间错综复杂的关系突显了血管分析在理解大脑健康方面的重要作用。虽然宏观和中尺度的成像技术已经获得了大量的关注和资源，但能够揭示复杂血管细节的微尺度VEM成像缺乏必要的基准基础设施。随着研究人员深入研究脑血管系统的微观复杂性，我们的BvEM基准代表着解开神经-血管耦合及其对大脑功能和病理影响的奥秘的关键一步。BvEM数据集基于三种哺乳动物的VEM图像体积：成年小鼠、猕猴和人类。我们通过半自动、手动和质量控制流程标准化了分辨率，解决了成像变化问题，并仔细注释了血管，确保了高质量的3D分割。此外，我们开发了一种名为TriSAM的零样本皮层血管分割方法，该方法利用强大的分割模型SAM进行3D分割。为了将SAM从2D分割提升到3D体积分割，TriSAM采用了多种子跟踪框架，利用某些图像平面的可靠性进行跟踪，同时使用其他图像平面来识别潜在的转折点。这种方法由三平面选择、基于SAM的跟踪和递归重定向组成，有效地实现了长期的3D血管分割，而无需模型训练或微调。实验结果表明，TriSAM在三个物种的BvEM基准上获得了优异的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.13961v1" target="_blank">2401.13961v1</a>
                              </td>
                              <td>TriSAM: Tri-Plane SAM for zero-shot cortical blood vessel segmentation in VEM images</td>
                              <td>Jia Wan</td>
                              <td>2024-01-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_13961v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.13961v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_10809v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Neglected Hessian component explains mysteries in Sharpness regularization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_10809v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_10809v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_10809v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent work has shown that methods like SAM which either explicitly or implicitly penalize second order information can improve generalization in deep learning. Seemingly similar methods like weight noise and gradient penalties often fail to provide such benefits. We show that these differences can be explained by the structure of the Hessian of the loss. First, we show that a common decomposition of the Hessian can be quantitatively interpreted as separating the feature exploitation from feature exploration. The feature exploration, which can be described by the Nonlinear Modeling Error matrix (NME), is commonly neglected in the literature since it vanishes at interpolation. Our work shows that the NME is in fact important as it can explain why gradient penalties are sensitive to the choice of activation function. Using this insight we design interventions to improve performance. We also provide evidence that challenges the long held equivalence of weight noise and gradient penalties. This equivalence relies on the assumption that the NME can be ignored, which we find does not hold for modern networks since they involve significant feature learning. We find that regularizing feature exploitation but not feature exploration yields performance similar to gradient penalties.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_10809v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近的工作表明，像SAM这样显式或隐式惩罚二阶信息的方法可以提高深度学习中的泛化能力。看似相似的方法，如权重噪声和梯度惩罚，往往无法提供这样的好处。我们表明，这些差异可以用黑森损失的结构来解释。首先，我们证明了Hessian的常见分解可以定量地解释为将特征开发和特征探索分离。特征探索可以用非线性建模误差矩阵（NME）来描述，但在文献中通常被忽视，因为它在插值时消失了。我们的工作表明，NME实际上很重要，因为它可以解释为什么梯度惩罚对激活函数的选择很敏感。利用这一洞察力，我们设计干预措施来提高绩效。我们还提供了挑战权重噪声和梯度惩罚的长期等价性的证据。这种等价性依赖于NME可以被忽略的假设，我们发现这对于现代网络来说是不成立的，因为它们涉及显著的特征学习。我们发现，规范化特征开发而不是特征探索会产生类似于梯度惩罚的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.10809v2" target="_blank">2401.10809v2</a>
                              </td>
                              <td>Neglected Hessian component explains mysteries in Sharpness regularization</td>
                              <td>Yann N. Dauphin</td>
                              <td>2024-01-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_10809v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.10809v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_13302v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Lagrange-Newton Approach to Smoothing-and-Mapping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_13302v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_13302v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_13302v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this report we explore the application of the Lagrange-Newton method to the SAM (smoothing-and-mapping) problem in mobile robotics. In Lagrange-Newton SAM, the angular component of each pose vector is expressed by orientation vectors and treated through Lagrange constraints. This is different from the typical Gauss-Newton approach where variations need to be mapped back and forth between Euclidean space and a manifold suitable for rotational components. We derive equations for five different types of measurements between robot poses: translation, distance, and rotation from odometry in the plane, as well as home-vector angle and compass angle from visual homing. We demonstrate the feasibility of the Lagrange-Newton approach for a simple example related to a cleaning robot scenario.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_13302v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本报告中，我们探讨了拉格朗日-牛顿方法在移动机器人SAM（平滑和映射）问题中的应用。在拉格朗日-牛顿SAM中，每个位姿向量的角度分量由方向向量表示，并通过拉格朗日约束进行处理。这与典型的高斯-牛顿方法不同，后者需要在欧几里得空间和适合旋转分量的流形之间来回映射变化。我们推导了机器人姿态之间五种不同类型测量的方程：平面内里程计的平移、距离和旋转，以及视觉归位的主矢量角和罗盘角。我们通过一个与清洁机器人场景相关的简单例子证明了拉格朗日-牛顿方法的可行性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.13302v1" target="_blank">2401.13302v1</a>
                              </td>
                              <td>A Lagrange-Newton Approach to Smoothing-and-Mapping</td>
                              <td>Ralf Möller</td>
                              <td>2024-01-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_13302v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.13302v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_15939v4_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Unleashing the Power of Prompt-driven Nucleus Instance Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_15939v4_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_15939v4_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_15939v4_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Nucleus instance segmentation in histology images is crucial for a broad spectrum of clinical applications. Current dominant algorithms rely on regression of nuclear proxy maps. Distinguishing nucleus instances from the estimated maps requires carefully curated post-processing, which is error-prone and parameter-sensitive. Recently, the Segment Anything Model (SAM) has earned huge attention in medical image segmentation, owing to its impressive generalization ability and promptable property. Nevertheless, its potential on nucleus instance segmentation remains largely underexplored. In this paper, we present a novel prompt-driven framework that consists of a nucleus prompter and SAM for automatic nucleus instance segmentation. Specifically, the prompter learns to generate a unique point prompt for each nucleus while the SAM is fine-tuned to output the corresponding mask for the prompted nucleus. Furthermore, we propose the inclusion of adjacent nuclei as negative prompts to enhance the model's capability to identify overlapping nuclei. Without complicated post-processing, our proposed method sets a new state-of-the-art performance on three challenging benchmarks. Code is available at \url{github.com/windygoo/PromptNucSeg}</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_15939v4_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>组织学图像中的细胞核实例分割对于广泛的临床应用至关重要。目前占主导地位的算法依赖于核代理映射的回归。区分核实例和估计的映射需要精心策划的后处理，这是容易出错和参数敏感的。近年来，分段任意模型（SAM）以其令人印象深刻的泛化能力和可提示性在医学图像分割中引起了极大的关注。尽管如此，其在核心实例分割方面的潜力在很大程度上仍未得到充分挖掘。在本文中，我们提出了一种新的提示驱动框架，该框架由核提示器和SAM组成，用于自动核实例分割。具体而言，提示器学习为每个细胞核生成唯一的点提示，同时SAM被微调为输出提示细胞核的相应掩码。此外，我们建议将相邻核作为负提示，以增强模型识别重叠核的能力。在没有复杂后处理的情况下，我们提出的方法在三个具有挑战性的基准上设置了新的最先进的性能。代码位于\url｛github.com/windygo/PromptNucSeg｝</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.15939v4" target="_blank">2311.15939v4</a>
                              </td>
                              <td>Unleashing the Power of Prompt-driven Nucleus Instance Segmentation</td>
                              <td>Zhongyi Shui</td>
                              <td>2023-11-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_15939v4_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.15939v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/windygoo/promptnucseg" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_13051v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PA-SAM: Prompt Adapter SAM for High-Quality Image Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_13051v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_13051v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_13051v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Segment Anything Model (SAM) has exhibited outstanding performance in various image segmentation tasks. Despite being trained with over a billion masks, SAM faces challenges in mask prediction quality in numerous scenarios, especially in real-world contexts. In this paper, we introduce a novel prompt-driven adapter into SAM, namely Prompt Adapter Segment Anything Model (PA-SAM), aiming to enhance the segmentation mask quality of the original SAM. By exclusively training the prompt adapter, PA-SAM extracts detailed information from images and optimizes the mask decoder feature at both sparse and dense prompt levels, improving the segmentation performance of SAM to produce high-quality masks. Experimental results demonstrate that our PA-SAM outperforms other SAM-based methods in high-quality, zero-shot, and open-set segmentation. We're making the source code and models available at https://github.com/xzz2/pa-sam.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_13051v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分段任意模型（SAM）在各种图像分割任务中表现出出色的性能。尽管SAM使用了超过10亿个口罩进行训练，但在许多场景中，尤其是在现实世界中，它在口罩预测质量方面面临挑战。在本文中，我们在SAM中引入了一种新的提示驱动适配器，即提示适配器分段任意模型（PA-SAM），旨在提高原始SAM的分割掩码质量。通过专门训练提示适配器，PA-SAM从图像中提取详细信息，并在稀疏和密集提示级别优化掩码解码器特性，提高SAM的分割性能以产生高质量的掩模。实验结果表明，我们的PA-SAM在高质量、零样本和开集分割方面优于其他基于SAM的方法。我们正在提供源代码和模型https://github.com/xzz2/pa-sam.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.13051v1" target="_blank">2401.13051v1</a>
                              </td>
                              <td>PA-SAM: Prompt Adapter SAM for High-Quality Image Segmentation</td>
                              <td>Zhaozhi Xie</td>
                              <td>2024-01-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_13051v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.13051v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/xzz2/pa-sam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_12033v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Momentum-SAM: Sharpness Aware Minimization without Computational Overhead</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_12033v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_12033v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_12033v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The recently proposed optimization algorithm for deep neural networks Sharpness Aware Minimization (SAM) suggests perturbing parameters before gradient calculation by a gradient ascent step to guide the optimization into parameter space regions of flat loss. While significant generalization improvements and thus reduction of overfitting could be demonstrated, the computational costs are doubled due to the additionally needed gradient calculation, making SAM unfeasible in case of limited computationally capacities. Motivated by Nesterov Accelerated Gradient (NAG) we propose Momentum-SAM (MSAM), which perturbs parameters in the direction of the accumulated momentum vector to achieve low sharpness without significant computational overhead or memory demands over SGD or Adam. We evaluate MSAM in detail and reveal insights on separable mechanisms of NAG, SAM and MSAM regarding training optimization and generalization. Code is available at https://github.com/MarlonBecker/MSAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_12033v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近提出的深度神经网络的优化算法Sharpness Aware Minimization（SAM）建议在梯度计算之前通过梯度上升步骤扰动参数，以将优化引导到平坦损耗的参数空间区域。虽然可以证明显著的泛化改进，从而减少过拟合，但由于额外需要的梯度计算，计算成本加倍，使得SAM在计算能力有限的情况下不可行。受Nesterov加速梯度（NAG）的启发，我们提出了动量SAM（MSAM），它在累积动量矢量的方向上扰动参数，以实现低清晰度，而不会产生显著的计算开销或对SGD或Adam的内存需求。我们详细评估了MSAM，并揭示了NAG、SAM和MSAM在训练优化和泛化方面的可分离机制。代码位于https://github.com/MarlonBecker/MSAM.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.12033v1" target="_blank">2401.12033v1</a>
                              </td>
                              <td>Momentum-SAM: Sharpness Aware Minimization without Computational Overhead</td>
                              <td>Marlon Becker</td>
                              <td>2024-01-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_12033v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.12033v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/marlonbecker/msam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_11599v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Reducing Usefulness of Stolen Credentials in SSO Contexts</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_11599v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_11599v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_11599v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Approximately 61% of cyber attacks involve adversaries in possession of valid credentials. Attackers acquire credentials through various means, including phishing, dark web data drops, password reuse, etc. Multi-factor authentication (MFA) helps to thwart attacks that use valid credentials, but attackers still commonly breach systems by tricking users into accepting MFA step up requests through techniques, such as ``MFA Bombing'', where multiple requests are sent to a user until they accept one. Currently, there are several solutions to this problem, each with varying levels of security and increasing invasiveness on user devices. This paper proposes a token-based enrollment architecture that is less invasive to user devices than mobile device management, but still offers strong protection against use of stolen credentials and MFA attacks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_11599v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大约61%的网络攻击涉及拥有有效证书的对手。攻击者通过各种方式获取凭据，包括网络钓鱼、暗网数据丢弃、密码重用等。多因素身份验证（MFA）有助于挫败使用有效凭据的攻击，但攻击者通常仍会通过“MFA轰炸”等技术诱骗用户接受MFA升级请求，从而破坏系统，其中向用户发送多个请求，直到用户接受一个为止。目前，有几种解决方案可以解决这个问题，每种方案都具有不同的安全级别，并且对用户设备的入侵性越来越强。本文提出了一种基于令牌的注册架构，与移动设备管理相比，该架构对用户设备的入侵更小，但仍能提供强大的保护，防止使用被盗凭据和MFA攻击。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.11599v1" target="_blank">2401.11599v1</a>
                              </td>
                              <td>Reducing Usefulness of Stolen Credentials in SSO Contexts</td>
                              <td>Sam Hays</td>
                              <td>2024-01-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_11599v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.11599v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_09826v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Boosting Few-Shot Semantic Segmentation Via Segment Anything Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_09826v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_09826v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_09826v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In semantic segmentation, accurate prediction masks are crucial for downstream tasks such as medical image analysis and image editing. Due to the lack of annotated data, few-shot semantic segmentation (FSS) performs poorly in predicting masks with precise contours. Recently, we have noticed that the large foundation model segment anything model (SAM) performs well in processing detailed features. Inspired by SAM, we propose FSS-SAM to boost FSS methods by addressing the issue of inaccurate contour. The FSS-SAM is training-free. It works as a post-processing tool for any FSS methods and can improve the accuracy of predicted masks. Specifically, we use predicted masks from FSS methods to generate prompts and then use SAM to predict new masks. To avoid predicting wrong masks with SAM, we propose a prediction result selection (PRS) algorithm. The algorithm can remarkably decrease wrong predictions. Experiment results on public datasets show that our method is superior to base FSS methods in both quantitative and qualitative aspects.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_09826v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在语义分割中，准确的预测掩码对于医学图像分析和图像编辑等下游任务至关重要。由于缺乏注释数据，少镜头语义分割（FSS）在预测具有精确轮廓的掩模时表现不佳。最近，我们注意到大型基础模型分段任意模型（SAM）在处理详细特征方面表现良好。受SAM的启发，我们提出了FSS-SAM，通过解决轮廓不准确的问题来增强FSS方法。FSS-SAM是免费培训的。它可以作为任何FSS方法的后处理工具，并可以提高预测掩码的准确性。具体来说，我们使用FSS方法中的预测掩码来生成提示，然后使用SAM来预测新掩码。为了避免SAM预测错误的掩码，我们提出了一种预测结果选择（PRS）算法。该算法可以显著减少错误预测。在公共数据集上的实验结果表明，我们的方法在定量和定性方面都优于基本的FSS方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.09826v2" target="_blank">2401.09826v2</a>
                              </td>
                              <td>Boosting Few-Shot Semantic Segmentation Via Segment Anything Model</td>
                              <td>Chen-Bin Feng</td>
                              <td>2024-01-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_09826v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.09826v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_02189v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Harvard FairSeg: A Large-Scale Medical Image Segmentation Dataset for Fairness Learning Using Segment Anything Model with Fair Error-Bound Scaling</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_02189v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_02189v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_02189v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Fairness in artificial intelligence models has gained significantly more attention in recent years, especially in the area of medicine, as fairness in medical models is critical to people's well-being and lives. High-quality medical fairness datasets are needed to promote fairness learning research. Existing medical fairness datasets are all for classification tasks, and no fairness datasets are available for medical segmentation, while medical segmentation is an equally important clinical task as classifications, which can provide detailed spatial information on organ abnormalities ready to be assessed by clinicians. In this paper, we propose the first fairness dataset for medical segmentation named Harvard-FairSeg with 10,000 subject samples. In addition, we propose a fair error-bound scaling approach to reweight the loss function with the upper error-bound in each identity group, using the segment anything model (SAM). We anticipate that the segmentation performance equity can be improved by explicitly tackling the hard cases with high training errors in each identity group. To facilitate fair comparisons, we utilize a novel equity-scaled segmentation performance metric to compare segmentation metrics in the context of fairness, such as the equity-scaled Dice coefficient. Through comprehensive experiments, we demonstrate that our fair error-bound scaling approach either has superior or comparable fairness performance to the state-of-the-art fairness learning models. The dataset and code are publicly accessible via https://ophai.hms.harvard.edu/harvard-fairseg10k.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_02189v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，人工智能模型的公平性得到了越来越多的关注，尤其是在医学领域，因为医学模型的公平对人们的福祉和生活至关重要。需要高质量的医疗公平数据集来促进公平学习研究。现有的医疗公平数据集都是用于分类任务的，没有公平数据集可用于医学分割，而医学分割是与分类同等重要的临床任务，可以提供关于器官异常的详细空间信息，供临床医生评估。在本文中，我们提出了第一个用于医学分割的公平数据集，名为Harvard FairSeg，包含10000个受试者样本。此外，我们提出了一种公平的误差界缩放方法，使用分段任意模型（SAM），在每个身份组中用误差上界重新加权损失函数。我们预计，可以通过明确处理每个身份组中具有高训练错误的困难情况来提高分割性能公平性。为了便于公平比较，我们利用一种新的公平比例分割性能指标来比较公平背景下的分割指标，例如公平比例的Dice系数。通过全面的实验，我们证明了我们的公平误差界缩放方法与最先进的公平学习模型相比具有优越或可比的公平性能。数据集和代码可通过https://ophai.hms.harvard.edu/harvard-fairseg10k.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.02189v2" target="_blank">2311.02189v2</a>
                              </td>
                              <td>Harvard FairSeg: A Large-Scale Medical Image Segmentation Dataset for Fairness Learning Using Segment Anything Model with Fair Error-Bound Scaling</td>
                              <td>Yu Tian</td>
                              <td>2023-11-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_02189v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.02189v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/harvard-ophthalmology-ai-lab/fairseg" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_10815v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RAD-DINO: Exploring Scalable Medical Image Encoders Beyond Text Supervision</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_10815v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_10815v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_10815v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Language-supervised pre-training has proven to be a valuable method for extracting semantically meaningful features from images, serving as a foundational element in multimodal systems within the computer vision and medical imaging domains. However, resulting features are limited by the information contained within the text. This is particularly problematic in medical imaging, where radiologists' written findings focus on specific observations; a challenge compounded by the scarcity of paired imaging-text data due to concerns over leakage of personal health information. In this work, we fundamentally challenge the prevailing reliance on language supervision for learning general purpose biomedical imaging encoders. We introduce RAD-DINO, a biomedical image encoder pre-trained solely on unimodal biomedical imaging data that obtains similar or greater performance than state-of-the-art biomedical language supervised models on a diverse range of benchmarks. Specifically, the quality of learned representations is evaluated on standard imaging tasks (classification and semantic segmentation), and a vision-language alignment task (text report generation from images). To further demonstrate the drawback of language supervision, we show that features from RAD-DINO correlate with other medical records (e.g., sex or age) better than language-supervised models, which are generally not mentioned in radiology reports. Finally, we conduct a series of ablations determining the factors in RAD-DINO's performance; notably, we observe that RAD-DINO's downstream performance scales well with the quantity and diversity of training data, demonstrating that image-only supervision is a scalable approach for training a foundational biomedical image encoder.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_10815v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语言监督预训练已被证明是从图像中提取语义上有意义的特征的一种有价值的方法，是计算机视觉和医学成像领域中多模式系统的基础元素。但是，生成的特征受到文本中包含的信息的限制。这在医学成像中尤其有问题，因为放射科医生的书面发现侧重于特定的观察结果；由于担心个人健康信息的泄露，配对成像文本数据的稀缺性加剧了这一挑战。在这项工作中，我们从根本上挑战了学习通用生物医学成像编码器时普遍依赖语言监督的现状。我们介绍了RAD-DINO，这是一种仅在单峰生物医学成像数据上预训练的生物医学图像编码器，在各种基准上获得与最先进的生物医学语言监督模型相似或更高的性能。具体而言，学习表示的质量是在标准成像任务（分类和语义分割）和视觉语言对齐任务（从图像生成文本报告）上评估的。为了进一步证明语言监督的缺点，我们发现RAD-DINO的特征与其他医疗记录（如性别或年龄）的相关性比语言监督模型更好，而语言监督模型通常在放射学报告中没有提及。最后，我们进行了一系列烧蚀，确定了影响RAD-DINO性能的因素；值得注意的是，我们观察到RAD-DINO的下游性能随着训练数据的数量和多样性而良好地扩展，这表明仅图像监督是训练基础生物医学图像编码器的一种可扩展方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.10815v1" target="_blank">2401.10815v1</a>
                              </td>
                              <td>RAD-DINO: Exploring Scalable Medical Image Encoders Beyond Text Supervision</td>
                              <td>Fernando Pérez-García</td>
                              <td>2024-01-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_10815v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.10815v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_10703v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DRAT Proofs of Unsatisfiability for SAT Modulo Monotonic Theories</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_10703v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_10703v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_10703v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Generating proofs of unsatisfiability is a valuable capability of most SAT solvers, and is an active area of research for SMT solvers. This paper introduces the first method to efficiently generate proofs of unsatisfiability specifically for an important subset of SMT: SAT Modulo Monotonic Theories (SMMT), which includes many useful finite-domain theories (e.g., bit vectors and many graph-theoretic properties) and is used in production at Amazon Web Services. Our method uses propositional definitions of the theory predicates, from which it generates compact Horn approximations of the definitions, which lead to efficient DRAT proofs, leveraging the large investment the SAT community has made in DRAT. In experiments on practical SMMT problems, our proof generation overhead is minimal (7.41% geometric mean slowdown, 28.8% worst-case), and we can generate and check proofs for many problems that were previously intractable.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_10703v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>生成不可满足性的证明是大多数SAT求解器的一项宝贵能力，也是SMT求解器研究的一个活跃领域。本文介绍了第一种有效生成不可满足性证明的方法，特别是针对SMT的一个重要子集：SAT模单调理论（SMMT），它包括许多有用的有限域理论（如位向量和许多图论性质），并在AmazonWebServices的生产中使用。我们的方法使用理论谓词的命题定义，从中生成定义的紧凑Horn近似，这导致了有效的DRAT证明，利用了SAT社区在DRAT中的巨大投资。在实际SMMT问题的实验中，我们的证明生成开销最小（7.41%的几何平均速度减慢，28.8%的最坏情况），并且我们可以生成和检查许多以前难以解决的问题的证明。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.10703v1" target="_blank">2401.10703v1</a>
                              </td>
                              <td>DRAT Proofs of Unsatisfiability for SAT Modulo Monotonic Theories</td>
                              <td>Nick Feng</td>
                              <td>2024-01-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_10703v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.10703v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_10409v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Session Abstract Machine (Extended Version)</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_10409v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_10409v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_10409v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We build on a fine-grained analysis of session-based interaction as provided by the linear logic typing disciplines to introduce the SAM, an abstract machine for mechanically executing session-typed processes. A remarkable feature of the SAM's design is its ability to naturally segregate and coordinate sequential with concurrent session behaviours. In particular, implicitly sequential parts of session programs may be efficiently executed by deterministic sequential application of SAM transitions, amenable to compilation, and without concurrent synchronisation mechanisms. We provide an intuitive discussion of the SAM structure and its underlying design, and state and prove its correctness for executing programs in a session calculus corresponding to full classical linear logic CLL. We also discuss extensions and applications of the SAM to the execution of linear and session-based programming languages.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_10409v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们建立在线性逻辑类型化学科提供的对基于会话的交互的细粒度分析的基础上，引入SAM，一种用于机械执行会话类型化过程的抽象机器。SAM设计的一个显著特点是它能够自然地分离和协调顺序与并发会话行为。特别地，会话程序的隐式顺序部分可以通过SAM转换的确定顺序应用有效地执行，易于编译，并且没有并发同步机制。我们提供了SAM结构及其底层设计的直观讨论，并陈述和证明了其在对应于完全经典线性逻辑CLL的会话演算中执行程序的正确性。我们还讨论了SAM在线性和基于会话的编程语言执行中的扩展和应用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.10409v1" target="_blank">2401.10409v1</a>
                              </td>
                              <td>The Session Abstract Machine (Extended Version)</td>
                              <td>Luís Caires</td>
                              <td>2024-01-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_10409v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.10409v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_10229v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">OMG-Seg: Is One Model Good Enough For All Segmentation?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_10229v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_10229v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_10229v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this work, we address various segmentation tasks, each traditionally tackled by distinct or partially unified models. We propose OMG-Seg, One Model that is Good enough to efficiently and effectively handle all the segmentation tasks, including image semantic, instance, and panoptic segmentation, as well as their video counterparts, open vocabulary settings, prompt-driven, interactive segmentation like SAM, and video object segmentation. To our knowledge, this is the first model to handle all these tasks in one model and achieve satisfactory performance. We show that OMG-Seg, a transformer-based encoder-decoder architecture with task-specific queries and outputs, can support over ten distinct segmentation tasks and yet significantly reduce computational and parameter overhead across various tasks and datasets. We rigorously evaluate the inter-task influences and correlations during co-training. Code and models are available at https://github.com/lxtGH/OMG-Seg.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_10229v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项工作中，我们处理了各种分割任务，每个任务传统上都由不同或部分统一的模型来处理。我们提出了OMG-Seg，One Model，它足够有效地处理所有分割任务，包括图像语义、实例和全景分割，以及它们的视频对应物、开放词汇设置、提示驱动的交互式分割（如SAM）和视频对象分割。据我们所知，这是第一个在一个模型中处理所有这些任务并获得令人满意性能的模型。我们展示了OMG-Seg，一种基于转换器的编码器-解码器架构，具有特定任务的查询和输出，可以支持十多个不同的分割任务，同时显著减少各种任务和数据集的计算和参数开销。我们严格评估联合训练期间任务间的影响和相关性。代码和型号可在https://github.com/lxtGH/OMG-Seg.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.10229v1" target="_blank">2401.10229v1</a>
                              </td>
                              <td>OMG-Seg: Is One Model Good Enough For All Segmentation?</td>
                              <td>Xiangtai Li</td>
                              <td>2024-01-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_10229v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.10229v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/lxtgh/omg-seg" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_10228v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RAP-SAM: Towards Real-Time All-Purpose Segment Anything</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_10228v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_10228v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_10228v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Advanced by transformer architecture, vision foundation models (VFMs) achieve remarkable progress in performance and generalization ability. Segment Anything Model (SAM) is one remarkable model that can achieve generalized segmentation. However, most VFMs cannot run in realtime, which makes it difficult to transfer them into several products. On the other hand, current real-time segmentation mainly has one purpose, such as semantic segmentation on the driving scene. We argue that diverse outputs are needed for real applications. Thus, this work explores a new real-time segmentation setting, named all-purpose segmentation in real-time, to transfer VFMs in real-time deployment. It contains three different tasks, including interactive segmentation, panoptic segmentation, and video segmentation. We aim to use one model to achieve the above tasks in real-time. We first benchmark several strong baselines. Then, we present Real-Time All Purpose SAM (RAP-SAM). It contains an efficient encoder and an efficient decoupled decoder to perform prompt-driven decoding. Moreover, we further explore different training strategies and tuning methods to boost co-training performance further. Our code and model are available at https://github.com/xushilin1/RAP-SAM/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_10228v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在transformer架构的推动下，视觉基础模型在性能和泛化能力方面取得了显著进步。分段任意模型（SAM）是实现广义分割的一个显著模型。然而，大多数VFM不能实时运行，这使得很难将它们转移到多个产品中。另一方面，目前的实时分割主要有一个目的，比如对驾驶场景进行语义分割。我们认为，实际应用需要多样化的输出。因此，这项工作探索了一种新的实时分割设置，称为实时通用分割，以在实时部署中传输VFM。它包含三个不同的任务，包括交互式分割、全景分割和视频分割。我们的目标是使用一个模型来实时实现上述任务。我们首先对几个强大的基线进行基准测试。然后，我们介绍了实时通用SAM（RAP-SAM）。它包含一个高效编码器和一个高效解耦解码器来执行提示驱动解码。此外，我们进一步探索了不同的训练策略和调整方法，以进一步提高联合训练的表现。我们的代码和型号可在https://github.com/xushilin1/RAP-SAM/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.10228v1" target="_blank">2401.10228v1</a>
                              </td>
                              <td>RAP-SAM: Towards Real-Time All-Purpose Segment Anything</td>
                              <td>Shilin Xu</td>
                              <td>2024-01-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_10228v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.10228v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/xushilin1/rap-sam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="CLIP"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2401_17186v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Embracing Language Inclusivity and Diversity in CLIP through Continual Language Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17186v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17186v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17186v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While vision-language pre-trained models (VL-PTMs) have advanced multimodal research in recent years, their mastery in a few languages like English restricts their applicability in broader communities. To this end, there is an increasing interest in developing multilingual VL models via a joint-learning setup, which, however, could be unrealistic due to expensive costs and data availability. In this work, we propose to extend VL-PTMs' language capacity by continual language learning (CLL), where a model needs to update its linguistic knowledge incrementally without suffering from catastrophic forgetting (CF). We begin our study by introducing a model dubbed CLL-CLIP, which builds upon CLIP, a prevailing VL-PTM that has acquired image-English text alignment. Specifically, CLL-CLIP contains an expandable token embedding layer to handle linguistic differences. It solely trains token embeddings to improve memory stability and is optimized under cross-modal and cross-lingual objectives to learn the alignment between images and multilingual texts. To alleviate CF raised by covariate shift and lexical overlap, we further propose a novel approach that ensures the identical distribution of all token embeddings during initialization and regularizes token embedding learning during training. We construct a CLL benchmark covering 36 languages based on MSCOCO and XM3600 datasets and then evaluate multilingual image-text retrieval performance. Extensive experiments verify the effectiveness of CLL-CLIP and show that our approach can boost CLL-CLIP, e.g., by 6.7% in text-to-image average Recall@1 on XM3600, and improve various state-of-the-art methods consistently. Our code and data are available at \url{https://github.com/yangbang18/CLFM}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17186v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>虽然视觉语言预训练模型（VL-PTM）近年来在多模式研究方面取得了进展，但它们对英语等少数语言的掌握限制了它们在更广泛社区中的适用性。为此，人们对通过联合学习设置开发多语言VL模型越来越感兴趣，然而，由于成本高昂和数据可用性，这可能是不现实的。在这项工作中，我们建议通过持续语言学习（CLL）来扩展VL-PTM的语言能力，其中模型需要在不遭受灾难性遗忘（CF）的情况下逐步更新其语言知识。我们首先介绍了一个名为CLL-CLIP的模型，该模型建立在CLIP的基础上，CLIP是一种流行的VL-PTM，它已经获得了图像-英语-文本对齐。具体来说，CLL-CLIP包含一个可扩展的令牌嵌入层来处理语言差异。它只训练令牌嵌入以提高记忆稳定性，并在跨模态和跨语言目标下进行优化，以学习图像和多语言文本之间的对齐。为了减轻协变移位和词汇重叠引起的CF，我们进一步提出了一种新的方法，该方法确保初始化期间所有令牌嵌入的分布相同，并在训练期间正则化令牌嵌入学习。我们基于MSCOCO和XM3600数据集构建了一个覆盖36种语言的CLL基准，然后评估了多语言图像文本检索性能。大量实验验证了CLL-CLIP的有效性，并表明我们的方法可以提高CLL-CLP，例如，在文本到图像的平均值中提高6.7%Recall@1在XM3600上，并不断改进各种最先进的方法。我们的代码和数据位于\url{https://github.com/yangbang18/CLFM}.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17186v1" target="_blank">2401.17186v1</a>
                              </td>
                              <td>Embracing Language Inclusivity and Diversity in CLIP through Continual Language Learning</td>
                              <td>Bang Yang</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17186v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17186v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_16702v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-granularity Correspondence Learning from Long-term Noisy Videos</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_16702v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_16702v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_16702v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Existing video-language studies mainly focus on learning short video clips, leaving long-term temporal dependencies rarely explored due to over-high computational cost of modeling long videos. To address this issue, one feasible solution is learning the correspondence between video clips and captions, which however inevitably encounters the multi-granularity noisy correspondence (MNC) problem. To be specific, MNC refers to the clip-caption misalignment (coarse-grained) and frame-word misalignment (fine-grained), hindering temporal learning and video understanding. In this paper, we propose NOise Robust Temporal Optimal traNsport (Norton) that addresses MNC in a unified optimal transport (OT) framework. In brief, Norton employs video-paragraph and clip-caption contrastive losses to capture long-term dependencies based on OT. To address coarse-grained misalignment in video-paragraph contrast, Norton filters out the irrelevant clips and captions through an alignable prompt bucket and realigns asynchronous clip-caption pairs based on transport distance. To address the fine-grained misalignment, Norton incorporates a soft-maximum operator to identify crucial words and key frames. Additionally, Norton exploits the potential faulty negative samples in clip-caption contrast by rectifying the alignment target with OT assignment to ensure precise temporal modeling. Extensive experiments on video retrieval, videoQA, and action segmentation verify the effectiveness of our method. Code is available at https://lin-yijie.github.io/projects/Norton.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_16702v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现有的视频语言研究主要集中在学习短视频片段上，由于长视频建模的计算成本过高，长期的时间依赖性很少被探索。为了解决这个问题，一个可行的解决方案是学习视频片段和字幕之间的对应关系，然而这不可避免地会遇到多粒度噪声对应关系（MNC）问题。具体来说，MNC指的是剪辑字幕错位（粗粒度）和帧词错位（细粒度），阻碍了时间学习和视频理解。在本文中，我们提出了在统一最优传输（OT）框架中解决MNC问题的NOise鲁棒时间最优traNsport（Norton）。简而言之，Norton采用视频段落和剪辑字幕对比损失来捕捉基于OT的长期依赖关系。为了解决视频段落对比中的粗粒度错位问题，Norton通过可对齐的提示桶过滤掉不相关的剪辑和字幕，并根据传输距离重新对齐异步剪辑和字幕对。为了解决细粒度的错位问题，Norton引入了一个软最大算子来识别关键单词和关键帧。此外，Norton通过OT分配校正对准目标，利用剪辑字幕对比度中潜在的错误负样本，以确保精确的时间建模。在视频检索、视频质量保证和动作分割方面的大量实验验证了我们方法的有效性。代码位于https://lin-yijie.github.io/projects/Norton.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.16702v1" target="_blank">2401.16702v1</a>
                              </td>
                              <td>Multi-granularity Correspondence Learning from Long-term Noisy Videos</td>
                              <td>Yijie Lin</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_16702v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.16702v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_16347v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Cross-Modal Coordination Across a Diverse Set of Input Modalities</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_16347v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_16347v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_16347v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Cross-modal retrieval is the task of retrieving samples of a given modality by using queries of a different one. Due to the wide range of practical applications, the problem has been mainly focused on the vision and language case, e.g. text to image retrieval, where models like CLIP have proven effective in solving such tasks. The dominant approach to learning such coordinated representations consists of projecting them onto a common space where matching views stay close and those from non-matching pairs are pushed away from each other. Although this cross-modal coordination has been applied also to other pairwise combinations, extending it to an arbitrary number of diverse modalities is a problem that has not been fully explored in the literature. In this paper, we propose two different approaches to the problem. The first is based on an extension of the CLIP contrastive objective to an arbitrary number of input modalities, while the second departs from the contrastive formulation and tackles the coordination problem by regressing the cross-modal similarities towards a target that reflects two simple and intuitive constraints of the cross-modal retrieval task. We run experiments on two different datasets, over different combinations of input modalities and show that the approach is not only simple and effective but also allows for tackling the retrieval problem in novel ways. Besides capturing a more diverse set of pair-wise interactions, we show that we can use the learned representations to improve retrieval performance by combining the embeddings from two or more such modalities.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_16347v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>跨模态检索是通过使用不同模态的查询来检索给定模态的样本的任务。由于实际应用范围广泛，问题主要集中在视觉和语言方面，例如文本到图像检索，其中像CLIP这样的模型已被证明在解决此类任务方面是有效的。学习这种协调表示的主要方法是将它们投影到一个公共空间上，在该空间中，匹配的视图保持接近，而来自不匹配对的视图则被推离。尽管这种跨模态协调也适用于其他成对组合，但将其扩展到任意数量的不同模态是一个文献中尚未充分探讨的问题。在本文中，我们提出了两种不同的方法来解决这个问题。第一种是基于将CLIP对比目标扩展到任意数量的输入模态，而第二种则偏离对比公式，通过将跨模态相似性回归到反映跨模态检索任务的两个简单直观约束的目标来解决协调问题。我们在两个不同的数据集上，在不同的输入模式组合上进行了实验，结果表明该方法不仅简单有效，而且可以以新颖的方式解决检索问题。除了捕获一组更多样的成对交互之外，我们还表明，我们可以使用学习到的表示，通过组合来自两个或多个这样的模态的嵌入来提高检索性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.16347v1" target="_blank">2401.16347v1</a>
                              </td>
                              <td>Cross-Modal Coordination Across a Diverse Set of Input Modalities</td>
                              <td>Jorge Sánchez</td>
                              <td>2024-01-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_16347v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.16347v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_16280v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Cutup and Detect: Human Fall Detection on Cutup Untrimmed Videos Using a Large Foundational Video Understanding Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_16280v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_16280v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_16280v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This work explores the performance of a large video understanding foundation model on the downstream task of human fall detection on untrimmed video and leverages a pretrained vision transformer for multi-class action detection, with classes: "Fall", "Lying" and "Other/Activities of daily living (ADL)". A method for temporal action localization that relies on a simple cutup of untrimmed videos is demonstrated. The methodology includes a preprocessing pipeline that converts datasets with timestamp action annotations into labeled datasets of short action clips. Simple and effective clip-sampling strategies are introduced. The effectiveness of the proposed method has been empirically evaluated on the publicly available High-Quality Fall Simulation Dataset (HQFSD). The experimental results validate the performance of the proposed pipeline. The results are promising for real-time application, and the falls are detected on video level with a state-of-the-art 0.96 F1 score on the HQFSD dataset under the given experimental settings. The source code will be made available on GitHub.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_16280v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>这项工作探索了大型视频理解基础模型在未修剪视频上人类跌倒检测下游任务中的性能，并利用预训练的视觉转换器进行多类动作检测，包括“跌倒”、“说谎”和“日常生活的其他/活动（ADL）”。演示了一种基于未修剪视频的简单剪切的时间动作定位方法。该方法包括一个预处理管道，用于将带有时间戳动作注释的数据集转换为短动作片段的标记数据集。介绍了简单有效的剪辑采样策略。已在公开的高质量秋季模拟数据集（HQFSD）上对所提出方法的有效性进行了实证评估。实验结果验证了所提出的管道的性能。该结果有希望用于实时应用，并且在给定的实验设置下，在HQFSD数据集上以最先进的0.96F1分数在视频级别上检测到跌倒。源代码将在GitHub上提供。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.16280v1" target="_blank">2401.16280v1</a>
                              </td>
                              <td>Cutup and Detect: Human Fall Detection on Cutup Untrimmed Videos Using a Large Foundational Video Understanding Model</td>
                              <td>Till Grutschus</td>
                              <td>2024-01-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_16280v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.16280v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_16265v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CO2: Efficient Distributed Training with Full Communication-Computation Overlap</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_16265v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_16265v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_16265v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_16265v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型的根本成功取决于大规模分布式训练技术的有效实现。尽管如此，构建一个具有高速通信互连性的庞大、高性能集群的成本高得令人望而却步，而且只有知名实体才能访问。在这项工作中，我们的目标是降低这一障碍，并在带宽有限的集群中实现大规模训练的民主化。我们提出了一种称为CO2的新方法，该方法将本地更新和异步通信引入分布式数据并行训练，从而促进了CO2通信与计算的完全重叠。即使在受非常有限的通信带宽限制的广泛的多节点集群上，CO2也能够获得高可扩展性。我们进一步提出了过时间隙惩罚和外部动量修剪技术以及CO2，以增强其收敛性和训练稳定性。此外，CO2与成熟的ZeRO系列优化器无缝集成，通过大型模型训练减少模型状态的内存消耗。我们还提供了收敛性的数学证明，并建立了严格的上界。此外，我们通过一系列广泛的实践实验验证了我们的发现，这些实验涵盖了计算机视觉和自然语言处理领域的广泛任务。这些实验用于证明当在包括多达128个A100 GPU的配置中部署时，CO2在收敛性、通用性和可扩展性方面的能力。无论是在具有800Gbps RDMA或80Gbps TCP/IP节点间连接的集群上，结果都强调了CO2的卓越容量，可以极大地提高可扩展性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.16265v1" target="_blank">2401.16265v1</a>
                              </td>
                              <td>CO2: Efficient Distributed Training with Full Communication-Computation Overlap</td>
                              <td>Weigao Sun</td>
                              <td>2024-01-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_16265v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.16265v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_12665v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_12665v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_12665v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_12665v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, foundational models such as CLIP and SAM have shown promising performance for the task of Zero-Shot Anomaly Segmentation (ZSAS). However, either CLIP-based or SAM-based ZSAS methods still suffer from non-negligible key drawbacks: 1) CLIP primarily focuses on global feature alignment across different inputs, leading to imprecise segmentation of local anomalous parts; 2) SAM tends to generate numerous redundant masks without proper prompt constraints, resulting in complex post-processing requirements. In this work, we innovatively propose a CLIP and SAM collaboration framework called ClipSAM for ZSAS. The insight behind ClipSAM is to employ CLIP's semantic understanding capability for anomaly localization and rough segmentation, which is further used as the prompt constraints for SAM to refine the anomaly segmentation results. In details, we introduce a crucial Unified Multi-scale Cross-modal Interaction (UMCI) module for interacting language with visual features at multiple scales of CLIP to reason anomaly positions. Then, we design a novel Multi-level Mask Refinement (MMR) module, which utilizes the positional information as multi-level prompts for SAM to acquire hierarchical levels of masks and merges them. Extensive experiments validate the effectiveness of our approach, achieving the optimal segmentation performance on the MVTec-AD and VisA datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_12665v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，CLIP和SAM等基础模型在零样本异常分割（ZSAS）任务中表现出了良好的性能。然而，基于CLIP或基于SAM的ZSAS方法仍然存在不可忽略的关键缺点：1）CLIP主要关注不同输入的全局特征对齐，导致局部异常部分的分割不精确；2） SAM倾向于在没有适当提示约束的情况下生成大量冗余掩码，从而导致复杂的后处理要求。在这项工作中，我们创新性地为ZSAS提出了一个名为ClipSAM的CLIP和SAM合作框架。ClipSAM背后的见解是利用CLIP的语义理解能力进行异常定位和粗略分割，这进一步被用作SAM细化异常分割结果的提示约束。详细地说，我们介绍了一个关键的统一多尺度跨模态交互（UMCI）模块，用于在CLIP的多个尺度上与视觉特征进行交互，以确定异常位置。然后，我们设计了一个新的多级掩码细化（MMR）模块，该模块利用位置信息作为SAM的多级提示来获取分层掩码并将其合并。大量实验验证了我们方法的有效性，在MVTec AD和VisA数据集上实现了最佳分割性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.12665v2" target="_blank">2401.12665v2</a>
                              </td>
                              <td>ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation</td>
                              <td>Shengze Li</td>
                              <td>2024-01-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_12665v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.12665v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/lszcoding/clipsam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16048v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Benchmarking Zero-Shot Recognition with Vision-Language Models: Challenges on Granularity and Specificity</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16048v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16048v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16048v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper introduces innovative benchmarks to evaluate Vision-Language Models (VLMs) in real-world zero-shot recognition tasks, focusing on the granularity and specificity of prompting text. We propose a unique evaluation protocol using adapted ImageNet and MS-COCO datasets to assess models' consistency in recognizing concepts at varying granularity levels and their sensitivity to the specificity of language inputs. Our extensive evaluation reveals that state-of-the-art VLMs, including contrastive models like CLIP, struggle with granularity and are sensitive to text specificity, impacting their effectiveness in open-world settings. This comprehensive study, a first in evaluating VLMs from these perspectives, provides valuable insights and tools for the community, highlighting the limitations and paving the way for enhanced models with better generalization in zero-shot recognition.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16048v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了在现实零样本识别任务中评估视觉语言模型（VLM）的创新基准，重点关注提示文本的粒度和特异性。我们提出了一种独特的评估协议，使用经过调整的ImageNet和MS-COCO数据集来评估模型在不同粒度水平上识别概念的一致性及其对语言输入的特异性的敏感性。我们的广泛评估表明，最先进的VLM，包括CLIP等对比模型，在粒度上很吃力，并且对文本特异性很敏感，影响了它们在开放世界环境中的有效性。这项综合研究首次从这些角度评估VLM，为社区提供了有价值的见解和工具，突出了其局限性，并为增强模型在零样本识别中更好地泛化铺平了道路。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16048v2" target="_blank">2306.16048v2</a>
                              </td>
                              <td>Benchmarking Zero-Shot Recognition with Vision-Language Models: Challenges on Granularity and Specificity</td>
                              <td>Zhenlin Xu</td>
                              <td>2023-06-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16048v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16048v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_16025v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Simple Policy Optimization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_16025v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_16025v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_16025v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>PPO (Proximal Policy Optimization) algorithm has demonstrated excellent performance in many fields, and it is considered as a simple version of TRPO (Trust Region Policy Optimization) algorithm. However, the ratio clipping operation in PPO may not always effectively enforce the trust region constraints, this can be a potential factor affecting the stability of the algorithm. In this paper, we propose SPO (Simple Policy Optimization) algorithm, which introduces a novel clipping method for KL divergence between the old and current policies. SPO can effectively enforce the trust region constraints in almost all environments, while still maintaining the simplicity of a first-order algorithm. Comparative experiments in Atari 2600 environments show that SPO sometimes provides stronger performance than PPO. Code is available at https://github.com/MyRepositories-hub/Simple-Policy-Optimization.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_16025v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>PPO（Proximal Policy Optimization）算法在许多领域都表现出了优异的性能，被认为是TRPO（Trust Region Policy Optimization）算法的一个简单版本。然而，PPO中的比率裁剪操作可能并不总是有效地执行信任区域约束，这可能是影响算法稳定性的潜在因素。在本文中，我们提出了SPO（简单策略优化）算法，该算法引入了一种新的裁剪方法来解决旧策略和当前策略之间的KL分歧。SPO可以在几乎所有环境中有效地执行信任区域约束，同时仍然保持一阶算法的简单性。在雅达利2600环境中的对比实验表明，SPO有时比PPO提供更强的性能。代码位于https://github.com/MyRepositories-hub/Simple-Policy-Optimization.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.16025v1" target="_blank">2401.16025v1</a>
                              </td>
                              <td>Simple Policy Optimization</td>
                              <td>Zhengpeng Xie</td>
                              <td>2024-01-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_16025v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.16025v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_15896v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">$\boldsymbol{M^2}$-Encoder: Advancing Bilingual Image-Text Understanding by Large-scale Efficient Pretraining</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_15896v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_15896v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_15896v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision-language foundation models like CLIP have revolutionized the field of artificial intelligence. Nevertheless, VLM models supporting multi-language, e.g., in both Chinese and English, have lagged due to the relative scarcity of large-scale pretraining datasets. Toward this end, we introduce a comprehensive bilingual (Chinese-English) dataset BM-6B with over 6 billion image-text pairs, aimed at enhancing multimodal foundation models to well understand images in both languages. To handle such a scale of dataset, we propose a novel grouped aggregation approach for image-text contrastive loss computation, which reduces the communication overhead and GPU memory demands significantly, facilitating a 60% increase in training speed. We pretrain a series of bilingual image-text foundation models with an enhanced fine-grained understanding ability on BM-6B, the resulting models, dubbed as $M^2$-Encoders (pronounced "M-Square"), set new benchmarks in both languages for multimodal retrieval and classification tasks. Notably, Our largest $M^2$-Encoder-10B model has achieved top-1 accuracies of 88.5% on ImageNet and 80.7% on ImageNet-CN under a zero-shot classification setting, surpassing previously reported SoTA methods by 2.2% and 21.1%, respectively. The $M^2$-Encoder series represents one of the most comprehensive bilingual image-text foundation models to date, so we are making it available to the research community for further exploration and development.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_15896v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>像CLIP这样的视觉语言基础模型已经彻底改变了人工智能领域。然而，由于大规模预训练数据集的相对稀缺，支持多语言（如中文和英文）的VLM模型已经落后。为此，我们引入了一个综合的双语（汉英）数据集BM-6B，该数据集包含超过60亿个图像-文本对，旨在增强多模式基础模型，以更好地理解两种语言的图像。为了处理这样规模的数据集，我们提出了一种新的用于图像-文本对比损失计算的分组聚合方法，该方法显著降低了通信开销和GPU内存需求，有助于训练速度提高60%。我们在BM-6B上预训练了一系列具有增强的细粒度理解能力的双语图像-文本基础模型，这些模型被称为$M^2$-编码器（发音为“M-Square”），为多模式检索和分类任务在两种语言中设置了新的基准。值得注意的是，在零样本分类设置下，我们最大的$M^2$-Encoder-10B模型在ImageNet和ImageNet-CN上分别获得了88.5%和80.7%的前1准确率，分别超过了之前报道的SoTA方法2.2%和21.1%。$M^2$-Encoder系列代表了迄今为止最全面的双语图像文本基础模型之一，因此我们正在将其提供给研究界进行进一步的探索和开发。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.15896v1" target="_blank">2401.15896v1</a>
                              </td>
                              <td>$\boldsymbol{M^2}$-Encoder: Advancing Bilingual Image-Text Understanding by Large-scale Efficient Pretraining</td>
                              <td>Qingpei Guo</td>
                              <td>2024-01-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_15896v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.15896v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/alipay/Ant-Multi-Modal-Framework/tree/main/prj/M2_Encoder" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_15657v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Data-Free Generalized Zero-Shot Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_15657v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_15657v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_15657v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Deep learning models have the ability to extract rich knowledge from large-scale datasets. However, the sharing of data has become increasingly challenging due to concerns regarding data copyright and privacy. Consequently, this hampers the effective transfer of knowledge from existing data to novel downstream tasks and concepts. Zero-shot learning (ZSL) approaches aim to recognize new classes by transferring semantic knowledge learned from base classes. However, traditional generative ZSL methods often require access to real images from base classes and rely on manually annotated attributes, which presents challenges in terms of data restrictions and model scalability. To this end, this paper tackles a challenging and practical problem dubbed as data-free zero-shot learning (DFZSL), where only the CLIP-based base classes data pre-trained classifier is available for zero-shot classification. Specifically, we propose a generic framework for DFZSL, which consists of three main components. Firstly, to recover the virtual features of the base data, we model the CLIP features of base class images as samples from a von Mises-Fisher (vMF) distribution based on the pre-trained classifier. Secondly, we leverage the text features of CLIP as low-cost semantic information and propose a feature-language prompt tuning (FLPT) method to further align the virtual image features and textual features. Thirdly, we train a conditional generative model using the well-aligned virtual image features and corresponding semantic text features, enabling the generation of new classes features and achieve better zero-shot generalization. Our framework has been evaluated on five commonly used benchmarks for generalized ZSL, as well as 11 benchmarks for the base-to-new ZSL. The results demonstrate the superiority and effectiveness of our approach. Our code is available in https://github.com/ylong4/DFZSL</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_15657v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>深度学习模型能够从大规模数据集中提取丰富的知识。然而，由于对数据版权和隐私的担忧，数据共享变得越来越具有挑战性。因此，这阻碍了知识从现有数据到新的下游任务和概念的有效转移。零样本学习（ZSL）方法旨在通过转移从基类学习的语义知识来识别新的类。然而，传统的生成ZSL方法通常需要从基类访问真实图像，并依赖于手动注释的属性，这在数据限制和模型可扩展性方面带来了挑战。为此，本文解决了一个具有挑战性和实用性的问题，称为无数据零样本学习（DFZSL），其中只有基于CLIP的基类数据预训练分类器可用于零样本分类。具体来说，我们为DFZSL提出了一个通用框架，它由三个主要组件组成。首先，为了恢复基本数据的虚拟特征，我们基于预训练的分类器，将基类图像的CLIP特征建模为来自von Mises Fisher（vMF）分布的样本。其次，我们利用CLIP的文本特征作为低成本的语义信息，提出了一种特征语言提示调整（FLPT）方法来进一步调整虚拟图像特征和文本特征。第三，我们使用对齐的虚拟图像特征和相应的语义文本特征来训练条件生成模型，从而能够生成新的类特征，并实现更好的零样本泛化。我们的框架已经在通用ZSL的五个常用基准以及新ZSL的基础的11个基准上进行了评估。结果证明了我们的方法的优越性和有效性。我们的代码位于https://github.com/ylong4/DFZSL</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.15657v1" target="_blank">2401.15657v1</a>
                              </td>
                              <td>Data-Free Generalized Zero-Shot Learning</td>
                              <td>Bowen Tang</td>
                              <td>2024-01-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_15657v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.15657v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_17891v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Rise of AI Language Pathologists: Exploring Two-level Prompt Learning for Few-shot Weakly-supervised Whole Slide Image Classification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_17891v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_17891v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_17891v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper introduces the novel concept of few-shot weakly supervised learning for pathology Whole Slide Image (WSI) classification, denoted as FSWC. A solution is proposed based on prompt learning and the utilization of a large language model, GPT-4. Since a WSI is too large and needs to be divided into patches for processing, WSI classification is commonly approached as a Multiple Instance Learning (MIL) problem. In this context, each WSI is considered a bag, and the obtained patches are treated as instances. The objective of FSWC is to classify both bags and instances with only a limited number of labeled bags. Unlike conventional few-shot learning problems, FSWC poses additional challenges due to its weak bag labels within the MIL framework. Drawing inspiration from the recent achievements of vision-language models (V-L models) in downstream few-shot classification tasks, we propose a two-level prompt learning MIL framework tailored for pathology, incorporating language prior knowledge. Specifically, we leverage CLIP to extract instance features for each patch, and introduce a prompt-guided pooling strategy to aggregate these instance features into a bag feature. Subsequently, we employ a small number of labeled bags to facilitate few-shot prompt learning based on the bag features. Our approach incorporates the utilization of GPT-4 in a question-and-answer mode to obtain language prior knowledge at both the instance and bag levels, which are then integrated into the instance and bag level language prompts. Additionally, a learnable component of the language prompts is trained using the available few-shot labeled data. We conduct extensive experiments on three real WSI datasets encompassing breast cancer, lung cancer, and cervical cancer, demonstrating the notable performance of the proposed method in bag and instance classification. All codes will be available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_17891v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了用于病理学全玻片图像（WSI）分类的少镜头弱监督学习的新概念，称为FSWC。提出了一种基于即时学习和利用大型语言模型GPT-4的解决方案。由于WSI太大，需要分为多个补丁进行处理，因此WSI分类通常被视为多实例学习（MIL）问题。在这种情况下，每个WSI都被视为一个包，并且所获得的补丁被视为实例。FSWC的目标是用有限数量的贴有标签的袋子对袋子和实例进行分类。与传统的少镜头学习问题不同，FSWC由于其在MIL框架内的薄弱袋标签而带来了额外的挑战。从视觉语言模型（V-L模型）在下游少镜头分类任务中的最新成就中汲取灵感，我们提出了一个针对病理学量身定制的两级即时学习MIL框架，结合了语言先验知识。具体来说，我们利用CLIP为每个补丁提取实例特征，并引入一种提示引导的池策略，将这些实例特征聚合到一个包特征中。随后，我们使用少量标记的袋子来促进基于袋子特征的少镜头提示学习。我们的方法在问答模式中结合了GPT-4的使用，以获得实例和包级别的语言先验知识，然后将其集成到实例和包级的语言提示中。此外，使用可用的少量镜头标记数据来训练语言提示的可学习成分。我们在三个真实的WSI数据集上进行了广泛的实验，包括乳腺癌症、癌症和癌症，证明了所提出的方法在袋和实例分类中的显著性能。所有代码都可用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.17891v2" target="_blank">2305.17891v2</a>
                              </td>
                              <td>The Rise of AI Language Pathologists: Exploring Two-level Prompt Learning for Few-shot Weakly-supervised Whole Slide Image Classification</td>
                              <td>Linhao Qu</td>
                              <td>2023-05-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_17891v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.17891v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_15362v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Transformer-based Clipped Contrastive Quantization Learning for Unsupervised Image Retrieval</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_15362v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_15362v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_15362v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Unsupervised image retrieval aims to learn the important visual characteristics without any given level to retrieve the similar images for a given query image. The Convolutional Neural Network (CNN)-based approaches have been extensively exploited with self-supervised contrastive learning for image hashing. However, the existing approaches suffer due to lack of effective utilization of global features by CNNs and biased-ness created by false negative pairs in the contrastive learning. In this paper, we propose a TransClippedCLR model by encoding the global context of an image using Transformer having local context through patch based processing, by generating the hash codes through product quantization and by avoiding the potential false negative pairs through clipped contrastive learning. The proposed model is tested with superior performance for unsupervised image retrieval on benchmark datasets, including CIFAR10, NUS-Wide and Flickr25K, as compared to the recent state-of-the-art deep models. The results using the proposed clipped contrastive learning are greatly improved on all datasets as compared to same backbone network with vanilla contrastive learning.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_15362v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>无监督图像检索旨在学习重要的视觉特征，而不需要任何给定的级别来检索给定查询图像的相似图像。基于卷积神经网络（CNN）的方法已被广泛用于图像哈希的自监督对比学习。然而，由于细胞神经网络缺乏对全局特征的有效利用，以及在对比学习中假阴性对造成的偏见，现有的方法受到了影响。在本文中，我们提出了一种TransClippedCLR模型，通过基于补丁的处理，使用具有局部上下文的Transformer对图像的全局上下文进行编码，通过乘积量化生成哈希码，并通过剪裁对比学习避免潜在的假负对。与最近最先进的深度模型相比，所提出的模型在包括CIFAR10、NUS-Wide和Flickr25K在内的基准数据集上以优异的无监督图像检索性能进行了测试。与使用普通对比学习的相同骨干网络相比，使用所提出的剪裁对比学习的结果在所有数据集上都得到了极大的改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.15362v1" target="_blank">2401.15362v1</a>
                              </td>
                              <td>Transformer-based Clipped Contrastive Quantization Learning for Unsupervised Image Retrieval</td>
                              <td>Ayush Dubey</td>
                              <td>2024-01-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_15362v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.15362v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14111v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Image Synthesis with Graph Conditioning: CLIP-Guided Diffusion Models for Scene Graphs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14111v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14111v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14111v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Advancements in generative models have sparked significant interest in generating images while adhering to specific structural guidelines. Scene graph to image generation is one such task of generating images which are consistent with the given scene graph. However, the complexity of visual scenes poses a challenge in accurately aligning objects based on specified relations within the scene graph. Existing methods approach this task by first predicting a scene layout and generating images from these layouts using adversarial training. In this work, we introduce a novel approach to generate images from scene graphs which eliminates the need of predicting intermediate layouts. We leverage pre-trained text-to-image diffusion models and CLIP guidance to translate graph knowledge into images. Towards this, we first pre-train our graph encoder to align graph features with CLIP features of corresponding images using a GAN based training. Further, we fuse the graph features with CLIP embedding of object labels present in the given scene graph to create a graph consistent CLIP guided conditioning signal. In the conditioning input, object embeddings provide coarse structure of the image and graph features provide structural alignment based on relationships among objects. Finally, we fine tune a pre-trained diffusion model with the graph consistent conditioning signal with reconstruction and CLIP alignment loss. Elaborate experiments reveal that our method outperforms existing methods on standard benchmarks of COCO-stuff and Visual Genome dataset.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14111v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>生成模型的进步引发了人们对在遵守特定结构准则的同时生成图像的极大兴趣。场景图到图像生成是生成与给定场景图一致的图像的一个这样的任务。然而，视觉场景的复杂性对基于场景图中的指定关系准确对齐对象提出了挑战。现有的方法通过首先预测场景布局并使用对抗性训练从这些布局生成图像来完成这项任务。在这项工作中，我们介绍了一种从场景图生成图像的新方法，该方法无需预测中间布局。我们利用预先训练的文本到图像扩散模型和CLIP指导将图形知识转化为图像。为此，我们首先预训练我们的图编码器，使用基于GAN的训练将图特征与相应图像的CLIP特征对齐。此外，我们将图特征与给定场景图中存在的对象标签的CLIP嵌入相融合，以创建图一致的CLIP引导的条件信号。在条件输入中，对象嵌入提供图像的粗略结构，而图特征提供基于对象之间关系的结构对齐。最后，我们用具有重建和CLIP对准损失的图一致性条件信号来微调预训练的扩散模型。详细的实验表明，我们的方法在COCO材料和Visual Genome数据集的标准基准上优于现有方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14111v2" target="_blank">2401.14111v2</a>
                              </td>
                              <td>Image Synthesis with Graph Conditioning: CLIP-Guided Diffusion Models for Scene Graphs</td>
                              <td>Rameshwar Mishra</td>
                              <td>2024-01-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14111v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14111v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14733v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Personality Perception in Human Videos Altered by Motion Transfer Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14733v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14733v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14733v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The successful portrayal of personality in digital characters improves communication and immersion. Current research focuses on expressing personality through modifying animations using heuristic rules or data-driven models. While studies suggest motion style highly influences the apparent personality, the role of appearance can be similarly essential. This work analyzes the influence of movement and appearance on the perceived personality of short videos altered by motion transfer networks. We label the personalities in conference video clips with a user study to determine the samples that best represent the Five-Factor model's high, neutral, and low traits. We alter these videos using the Thin-Plate Spline Motion Model, utilizing the selected samples as the source and driving inputs. We follow five different cases to study the influence of motion and appearance on personality perception. Our comparative study reveals that motion and appearance influence different factors: motion strongly affects perceived extraversion, and appearance helps convey agreeableness and neuroticism.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14733v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在数字人物中成功地刻画了个性，提高了沟通和沉浸感。目前的研究重点是通过使用启发式规则或数据驱动模型修改动画来表达个性。虽然研究表明，动作风格对外表个性有很大影响，但外表的作用也同样重要。本文分析了运动和外表对运动传递网络改变的短视频感知个性的影响。我们通过用户研究对会议视频剪辑中的人物进行标记，以确定最能代表五因素模型的高、中性和低特征的样本。我们使用薄板样条线运动模型来更改这些视频，使用选定的样本作为源和驱动输入。我们遵循五个不同的案例来研究动作和外表对人格感知的影响。我们的比较研究表明，运动和外表影响不同的因素：运动强烈影响感知的外向性，外表有助于传达宜人性和神经质。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14733v1" target="_blank">2401.14733v1</a>
                              </td>
                              <td>Personality Perception in Human Videos Altered by Motion Transfer Networks</td>
                              <td>Ayda Yurtoğlu</td>
                              <td>2024-01-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14733v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14733v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/sinansonlu/motion-transfer-personality" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14688v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Taiyi-Diffusion-XL: Advancing Bilingual Text-to-Image Generation with Large Vision-Language Model Support</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14688v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14688v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14688v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advancements in text-to-image models have significantly enhanced image generation capabilities, yet a notable gap of open-source models persists in bilingual or Chinese language support. To address this need, we present Taiyi-Diffusion-XL, a new Chinese and English bilingual text-to-image model which is developed by extending the capabilities of CLIP and Stable-Diffusion-XL through a process of bilingual continuous pre-training. This approach includes the efficient expansion of vocabulary by integrating the most frequently used Chinese characters into CLIP's tokenizer and embedding layers, coupled with an absolute position encoding expansion. Additionally, we enrich text prompts by large vision-language model, leading to better images captions and possess higher visual quality. These enhancements are subsequently applied to downstream text-to-image models. Our empirical results indicate that the developed CLIP model excels in bilingual image-text retrieval.Furthermore, the bilingual image generation capabilities of Taiyi-Diffusion-XL surpass previous models. This research leads to the development and open-sourcing of the Taiyi-Diffusion-XL model, representing a notable advancement in the field of image generation, particularly for Chinese language applications. This contribution is a step forward in addressing the need for more diverse language support in multimodal research. The model and demonstration are made publicly available at \href{https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-XL-3.5B/}{this https URL}, fostering further research and collaboration in this domain.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14688v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文本到图像模型的最新进展显著增强了图像生成能力，但开源模型在双语或中文支持方面仍存在显著差距。为了满足这一需求，我们提出了Taiyi Diffusion XL，这是一种新的中英文双语文本到图像模型，它是通过双语连续预训练过程扩展CLIP和Stable Diffusion XL。这种方法包括通过将最常用的汉字集成到CLIP的标记器和嵌入层中，再加上绝对位置编码扩展，来有效地扩展词汇。此外，我们通过大视觉语言模型丰富文本提示，从而获得更好的图像字幕，并具有更高的视觉质量。这些增强随后应用于下游的文本到图像模型。我们的实证结果表明，所开发的CLIP模型在双语图像文本检索方面表现出色。此外，泰翼Diffusion XL的双语图像生成能力超过了以前的型号。这项研究导致了Taiyi Diffusion XL模型的开发和开源，代表了图像生成领域的显著进步，尤其是在中文应用领域。这一贡献是朝着解决多模式研究中更多样化的语言支持需求迈出的一步。模型和演示可在\href上公开获取{https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-XL-3.5B/}｛此https URL｝，促进该领域的进一步研究和合作。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14688v1" target="_blank">2401.14688v1</a>
                              </td>
                              <td>Taiyi-Diffusion-XL: Advancing Bilingual Text-to-Image Generation with Large Vision-Language Model Support</td>
                              <td>Xiaojun Wu</td>
                              <td>2024-01-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14688v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14688v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14675v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-model learning by sequential reading of untrimmed videos for action recognition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14675v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14675v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14675v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a new method for learning videos by aggregating multiple models by sequentially extracting video clips from untrimmed video. The proposed method reduces the correlation between clips by feeding clips to multiple models in turn and synchronizes these models through federated learning. Experimental results show that the proposed method improves the performance compared to the no synchronization.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14675v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的视频学习方法，通过从未修剪的视频中顺序提取视频片段来聚合多个模型。所提出的方法通过依次向多个模型提供剪辑来减少剪辑之间的相关性，并通过联合学习来同步这些模型。实验结果表明，与不同步的方法相比，该方法提高了性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14675v1" target="_blank">2401.14675v1</a>
                              </td>
                              <td>Multi-model learning by sequential reading of untrimmed videos for action recognition</td>
                              <td>Kodai Kamiya</td>
                              <td>2024-01-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14675v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14675v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_09603v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rethinking FID: Towards a Better Evaluation Metric for Image Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_09603v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_09603v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_09603v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>As with many machine learning problems, the progress of image generation methods hinges on good evaluation metrics. One of the most popular is the Frechet Inception Distance (FID). FID estimates the distance between a distribution of Inception-v3 features of real images, and those of images generated by the algorithm. We highlight important drawbacks of FID: Inception's poor representation of the rich and varied content generated by modern text-to-image models, incorrect normality assumptions, and poor sample complexity. We call for a reevaluation of FID's use as the primary quality metric for generated images. We empirically demonstrate that FID contradicts human raters, it does not reflect gradual improvement of iterative text-to-image models, it does not capture distortion levels, and that it produces inconsistent results when varying the sample size. We also propose an alternative new metric, CMMD, based on richer CLIP embeddings and the maximum mean discrepancy distance with the Gaussian RBF kernel. It is an unbiased estimator that does not make any assumptions on the probability distribution of the embeddings and is sample efficient. Through extensive experiments and analysis, we demonstrate that FID-based evaluations of text-to-image models may be unreliable, and that CMMD offers a more robust and reliable assessment of image quality.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_09603v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>与许多机器学习问题一样，图像生成方法的进展取决于良好的评估指标。其中最受欢迎的是Frechet启始距离（FID）。FID估计真实图像的Inception-v3特征的分布与由该算法生成的图像的Incept-v3特征之间的距离。我们强调了FID的重要缺点：Inception对现代文本到图像模型生成的丰富多样的内容的糟糕表示、不正确的正态性假设和糟糕的样本复杂性。我们呼吁重新评估FID作为生成图像的主要质量指标的用途。我们实证证明，FID与人类评分者相矛盾，它不能反映迭代文本到图像模型的逐步改进，它不能捕捉失真水平，并且在改变样本量时会产生不一致的结果。我们还提出了一种替代的新度量，CMMD，基于更丰富的CLIP嵌入和高斯RBF核的最大均值差异距离。它是一个无偏估计器，不对嵌入的概率分布进行任何假设，并且是样本有效的。通过广泛的实验和分析，我们证明了基于FID的文本到图像模型评估可能是不可靠的，并且CMMD提供了更稳健和可靠的图像质量评估。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.09603v2" target="_blank">2401.09603v2</a>
                              </td>
                              <td>Rethinking FID: Towards a Better Evaluation Metric for Image Generation</td>
                              <td>Sadeep Jayasumana</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_09603v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.09603v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/google-research/google-research" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14542v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exploring Musical Roots: Applying Audio Embeddings to Empower Influence Attribution for a Generative Music Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14542v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14542v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14542v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Every artist has a creative process that draws inspiration from previous artists and their works. Today, "inspiration" has been automated by generative music models. The black box nature of these models obscures the identity of the works that influence their creative output. As a result, users may inadvertently appropriate, misuse, or copy existing artists' works. We establish a replicable methodology to systematically identify similar pieces of music audio in a manner that is useful for understanding training data attribution. A key aspect of our approach is to harness an effective music audio similarity measure. We compare the effect of applying CLMR and CLAP embeddings to similarity measurement in a set of 5 million audio clips used to train VampNet, a recent open source generative music model. We validate this approach with a human listening study. We also explore the effect that modifications of an audio example (e.g., pitch shifting, time stretching, background noise) have on similarity measurements. This work is foundational to incorporating automated influence attribution into generative modeling, which promises to let model creators and users move from ignorant appropriation to informed creation. Audio samples that accompany this paper are available at https://tinyurl.com/exploring-musical-roots.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14542v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>每个艺术家都有一个创作过程，从以前的艺术家及其作品中汲取灵感。今天，“灵感”已经被生成的音乐模型自动化了。这些模型的黑匣子性质掩盖了影响其创造性输出的作品的身份。因此，用户可能会无意中盗用、误用或复制现有艺术家的作品。我们建立了一种可复制的方法，以系统地识别相似的音乐音频片段，这种方法有助于理解训练数据的归属。我们方法的一个关键方面是利用有效的音乐音频相似性度量。我们比较了将CLMR和CLAP嵌入应用于一组500万个音频片段的相似性测量的效果，这些音频片段用于训练VampNet，这是一个最近的开源生成音乐模型。我们通过一项人类听力研究验证了这种方法。我们还探讨了音频示例的修改（例如，音高偏移、时间拉伸、背景噪声）对相似性测量的影响。这项工作是将自动影响归因纳入生成建模的基础，它有望让模型创建者和用户从无知的挪用转向知情的创作。本文附带的音频样本可在https://tinyurl.com/exploring-musical-roots.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14542v1" target="_blank">2401.14542v1</a>
                              </td>
                              <td>Exploring Musical Roots: Applying Audio Embeddings to Empower Influence Attribution for a Generative Music Model</td>
                              <td>Julia Barnett</td>
                              <td>2024-01-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14542v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14542v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14405v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14405v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14405v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14405v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose to improve transformers of a specific modality with irrelevant data from other modalities, e.g., improve an ImageNet model with audio or point cloud datasets. We would like to highlight that the data samples of the target modality are irrelevant to the other modalities, which distinguishes our method from other works utilizing paired (e.g., CLIP) or interleaved data of different modalities. We propose a methodology named Multimodal Pathway - given a target modality and a transformer designed for it, we use an auxiliary transformer trained with data of another modality and construct pathways to connect components of the two models so that data of the target modality can be processed by both models. In this way, we utilize the universal sequence-to-sequence modeling abilities of transformers obtained from two modalities. As a concrete implementation, we use a modality-specific tokenizer and task-specific head as usual but utilize the transformer blocks of the auxiliary model via a proposed method named Cross-Modal Re-parameterization, which exploits the auxiliary weights without any inference costs. On the image, point cloud, video, and audio recognition tasks, we observe significant and consistent performance improvements with irrelevant data from other modalities. The code and models are available at https://github.com/AILab-CVC/M2PT.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14405v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们建议使用来自其他模态的无关数据来改进特定模态的转换器，例如，使用音频或点云数据集来改进ImageNet模型。我们想强调的是，目标模态的数据样本与其他模态无关，这将我们的方法与其他使用不同模态的配对（例如CLIP）或交织数据的工作区分开来。我们提出了一种名为多模式路径的方法——给定一个目标模态和为其设计的转换器，我们使用用另一个模态的数据训练的辅助转换器，并构建连接两个模型的组件的路径，以便两个模型都能处理目标模态的数据。通过这种方式，我们利用了从两种模态获得的变换器的通用序列到序列建模能力。作为一种具体的实现方式，我们像往常一样使用特定于模态的标记器和特定于任务的头，但通过所提出的名为跨模态重新参数化的方法利用辅助模型的变换器块，该方法利用辅助权重而无需任何推理成本。在图像、点云、视频和音频识别任务中，我们观察到来自其他模态的无关数据显著且一致地提高了性能。代码和型号可在https://github.com/AILab-CVC/M2PT.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14405v1" target="_blank">2401.14405v1</a>
                              </td>
                              <td>Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities</td>
                              <td>Yiyuan Zhang</td>
                              <td>2024-01-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14405v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14405v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/ailab-cvc/m2pt" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14379v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">UrbanGenAI: Reconstructing Urban Landscapes using Panoptic Segmentation and Diffusion Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14379v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14379v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14379v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In contemporary design practices, the integration of computer vision and generative artificial intelligence (genAI) represents a transformative shift towards more interactive and inclusive processes. These technologies offer new dimensions of image analysis and generation, which are particularly relevant in the context of urban landscape reconstruction. This paper presents a novel workflow encapsulated within a prototype application, designed to leverage the synergies between advanced image segmentation and diffusion models for a comprehensive approach to urban design. Our methodology encompasses the OneFormer model for detailed image segmentation and the Stable Diffusion XL (SDXL) diffusion model, implemented through ControlNet, for generating images from textual descriptions. Validation results indicated a high degree of performance by the prototype application, showcasing significant accuracy in both object detection and text-to-image generation. This was evidenced by superior Intersection over Union (IoU) and CLIP scores across iterative evaluations for various categories of urban landscape features. Preliminary testing included utilising UrbanGenAI as an educational tool enhancing the learning experience in design pedagogy, and as a participatory instrument facilitating community-driven urban planning. Early results suggested that UrbanGenAI not only advances the technical frontiers of urban landscape reconstruction but also provides significant pedagogical and participatory planning benefits. The ongoing development of UrbanGenAI aims to further validate its effectiveness across broader contexts and integrate additional features such as real-time feedback mechanisms and 3D modelling capabilities. Keywords: generative AI; panoptic image segmentation; diffusion models; urban landscape design; design pedagogy; co-design</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14379v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在当代设计实践中，计算机视觉和生成人工智能（genAI）的融合代表着向更具互动性和包容性的过程的变革性转变。这些技术提供了图像分析和生成的新维度，这在城市景观重建的背景下尤其重要。本文提出了一种封装在原型应用程序中的新工作流程，旨在利用先进的图像分割和扩散模型之间的协同作用，实现城市设计的综合方法。我们的方法包括用于详细图像分割的OneFormer模型和通过ControlNet实现的用于从文本描述生成图像的稳定扩散XL（SDXL）扩散模型。验证结果表明，原型应用程序具有高度的性能，在对象检测和文本到图像生成方面都显示出显著的准确性。通过对各类城市景观特征的迭代评估，交叉点优于并集（IoU）和CLIP得分证明了这一点。初步测试包括将UrbanGenAI作为一种教育工具，增强设计教学法中的学习体验，并作为一种促进社区驱动的城市规划的参与性工具。早期研究结果表明，UrbanGenAI不仅推进了城市景观重建的技术前沿，还提供了显著的教学和参与式规划效益。UrbanGenAI正在进行的开发旨在进一步验证其在更广泛背景下的有效性，并集成实时反馈机制和3D建模能力等附加功能。关键词：生成人工智能；全景图像分割；扩散模型；城市景观设计；设计教育学；共同设计</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14379v1" target="_blank">2401.14379v1</a>
                              </td>
                              <td>UrbanGenAI: Reconstructing Urban Landscapes using Panoptic Segmentation and Diffusion Models</td>
                              <td>Timo Kapsalis</td>
                              <td>2024-01-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14379v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14379v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14159v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14159v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14159v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14159v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce Grounded SAM, which uses Grounding DINO as an open-set object detector to combine with the segment anything model (SAM). This integration enables the detection and segmentation of any regions based on arbitrary text inputs and opens a door to connecting various vision models. As shown in Fig.1, a wide range of vision tasks can be achieved by using the versatile Grounded SAM pipeline. For example, an automatic annotation pipeline based solely on input images can be realized by incorporating models such as BLIP and Recognize Anything. Additionally, incorporating Stable-Diffusion allows for controllable image editing, while the integration of OSX facilitates promptable 3D human motion analysis. Grounded SAM also shows superior performance on open-vocabulary benchmarks, achieving 48.7 mean AP on SegInW (Segmentation in the wild) zero-shot benchmark with the combination of Grounding DINO-Base and SAM-Huge models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14159v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了Grounded SAM，它使用Grounding DINO作为开集对象检测器与分段任意模型（SAM）相结合。这种集成能够基于任意文本输入检测和分割任何区域，并为连接各种视觉模型打开了大门。如图1所示，通过使用通用的接地SAM管道，可以实现广泛的视觉任务。例如，可以通过结合诸如BLIP和Recognize Anything之类的模型来实现仅基于输入图像的自动注释流水线。此外，结合Stable Diffusion可以进行可控的图像编辑，而OSX的集成有助于快速进行3D人体运动分析。Grounded SAM在开放词汇基准测试中也表现出优异的性能，在SegInW（野外细分）零样本基准测试中，通过Grounding DINO-Base和SAM-Huge模型的组合，达到了48.7的平均AP。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14159v1" target="_blank">2401.14159v1</a>
                              </td>
                              <td>Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks</td>
                              <td>Tianhe Ren</td>
                              <td>2024-01-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14159v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14159v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/IDEA-Research/Grounded-Segment-Anything" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_17249v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_17249v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_17249v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_17249v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Prompting and in-context learning (ICL) have become efficient learning paradigms for large language models (LLMs). However, LLMs suffer from prompt brittleness and various bias factors in the prompt, including but not limited to the formatting, the choice verbalizers, and the ICL examples. To address this problem that results in unexpected performance degradation, calibration methods have been developed to mitigate the effects of these biases while recovering LLM performance. In this work, we first conduct a systematic analysis of the existing calibration methods, where we both provide a unified view and reveal the failure cases. Inspired by these analyses, we propose Batch Calibration (BC), a simple yet intuitive method that controls the contextual bias from the batched input, unifies various prior approaches, and effectively addresses the aforementioned issues. BC is zero-shot, inference-only, and incurs negligible additional costs. In the few-shot setup, we further extend BC to allow it to learn the contextual bias from labeled data. We validate the effectiveness of BC with PaLM 2-(S, M, L) and CLIP models and demonstrate state-of-the-art performance over previous calibration baselines across more than 10 natural language understanding and image classification tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_17249v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>提示和上下文学习（ICL）已成为大型语言模型（LLM）的有效学习范式。然而，LLM存在提示脆性和提示中的各种偏见因素，包括但不限于格式、选择动词和ICL示例。为了解决这一导致意外性能下降的问题，已经开发了校准方法来减轻这些偏差的影响，同时恢复LLM性能。在这项工作中，我们首先对现有的校准方法进行了系统的分析，在这些方法中，我们既提供了统一的观点，又揭示了失败的案例。受这些分析的启发，我们提出了批量校准（BC），这是一种简单而直观的方法，可以控制批量输入的上下文偏差，统一各种先前的方法，并有效地解决上述问题。BC是零样本，只进行推理，产生的额外成本可以忽略不计。在少数镜头设置中，我们进一步扩展了BC，使其能够从标记数据中学习上下文偏差。我们用PaLM 2-（S，M，L）和CLIP模型验证了BC的有效性，并在10多项自然语言理解和图像分类任务中展示了与先前校准基线相比的最先进性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.17249v2" target="_blank">2309.17249v2</a>
                              </td>
                              <td>Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering</td>
                              <td>Han Zhou</td>
                              <td>2023-09-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_17249v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.17249v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_01403v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_01403v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_01403v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_01403v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Open-vocabulary dense prediction tasks including object detection and image segmentation have been advanced by the success of Contrastive Language-Image Pre-training (CLIP). CLIP models, particularly those incorporating vision transformers (ViTs), have exhibited remarkable generalization ability in zero-shot image classification. However, when transferring the vision-language alignment of CLIP from global image representation to local region representation for the open-vocabulary dense prediction tasks, CLIP ViTs suffer from the domain shift from full images to local image regions. In this paper, we embark on an in-depth analysis of the region-language alignment in CLIP models, which is essential for downstream open-vocabulary dense prediction tasks. Subsequently, we propose an approach named CLIPSelf, which adapts the image-level recognition ability of CLIP ViT to local image regions without needing any region-text pairs. CLIPSelf empowers ViTs to distill itself by aligning a region representation extracted from its dense feature map with the image-level representation of the corresponding image crop. With the enhanced CLIP ViTs, we achieve new state-of-the-art performance on open-vocabulary object detection, semantic segmentation, and panoptic segmentation across various benchmarks. Models and code are released at https://github.com/wusize/CLIPSelf.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_01403v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对比语言图像预训练（CLIP）的成功推进了包括对象检测和图像分割在内的开放式词汇密集预测任务。CLIP模型，特别是包含视觉变换器（ViTs）的CLIP模型在零样本图像分类中表现出显著的泛化能力。然而，当为开放词汇密集预测任务将CLIP的视觉语言对齐从全局图像表示转移到局部区域表示时，CLIP-VT遭受从全图像到局部图像区域的域转移。在本文中，我们对CLIP模型中的区域语言对齐进行了深入分析，这对下游开放词汇密集预测任务至关重要。随后，我们提出了一种名为CLIPSelf的方法，该方法将CLIP-ViT的图像级识别能力适应于局部图像区域，而不需要任何区域-文本对。CLIPSelf使ViTs能够通过将从其密集特征图中提取的区域表示与相应图像裁剪的图像级表示对齐来提取自身。通过增强的CLIP-VT，我们在开放词汇对象检测、语义分割和全景分割方面实现了跨各种基准的最先进的性能。型号和代码发布于https://github.com/wusize/CLIPSelf.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.01403v2" target="_blank">2310.01403v2</a>
                              </td>
                              <td>CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction</td>
                              <td>Size Wu</td>
                              <td>2023-10-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_01403v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.01403v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/wusize/clipself" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_13613v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Enhancing Image Retrieval : A Comprehensive Study on Photo Search using the CLIP Mode</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_13613v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_13613v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_13613v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Photo search, the task of retrieving images based on textual queries, has witnessed significant advancements with the introduction of CLIP (Contrastive Language-Image Pretraining) model. CLIP leverages a vision-language pre training approach, wherein it learns a shared representation space for images and text, enabling cross-modal understanding. This model demonstrates the capability to understand the semantic relationships between diverse image and text pairs, allowing for efficient and accurate retrieval of images based on natural language queries. By training on a large-scale dataset containing images and their associated textual descriptions, CLIP achieves remarkable generalization, providing a powerful tool for tasks such as zero-shot learning and few-shot classification. This abstract summarizes the foundational principles of CLIP and highlights its potential impact on advancing the field of photo search, fostering a seamless integration of natural language understanding and computer vision for improved information retrieval in multimedia applications</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_13613v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>照片搜索是基于文本查询检索图像的任务，随着CLIP（对比语言图像预训练）模型的引入，照片搜索取得了重大进展。CLIP利用视觉语言预训练方法，学习图像和文本的共享表示空间，实现跨模态理解。该模型展示了理解不同图像和文本对之间语义关系的能力，允许基于自然语言查询高效准确地检索图像。通过在包含图像及其相关文本描述的大规模数据集上进行训练，CLIP实现了显著的泛化，为零样本学习和少热点分类等任务提供了强大的工具。本摘要总结了CLIP的基本原理，并强调了其对推进照片搜索领域的潜在影响，促进自然语言理解和计算机视觉的无缝集成，以改进多媒体应用中的信息检索</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.13613v1" target="_blank">2401.13613v1</a>
                              </td>
                              <td>Enhancing Image Retrieval : A Comprehensive Study on Photo Search using the CLIP Mode</td>
                              <td>Naresh Kumar Lahajal</td>
                              <td>2024-01-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_13613v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.13613v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_06082v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_06082v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_06082v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_06082v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Incremental decision making in real-world environments is one of the most challenging tasks in embodied artificial intelligence. One particularly demanding scenario is Vision and Language Navigation~(VLN) which requires visual and natural language understanding as well as spatial and temporal reasoning capabilities. The embodied agent needs to ground its understanding of navigation instructions in observations of a real-world environment like Street View. Despite the impressive results of LLMs in other research areas, it is an ongoing problem of how to best connect them with an interactive visual environment. In this work, we propose VELMA, an embodied LLM agent that uses a verbalization of the trajectory and of visual environment observations as contextual prompt for the next action. Visual information is verbalized by a pipeline that extracts landmarks from the human written navigation instructions and uses CLIP to determine their visibility in the current panorama view. We show that VELMA is able to successfully follow navigation instructions in Street View with only two in-context examples. We further finetune the LLM agent on a few thousand examples and achieve 25%-30% relative improvement in task completion over the previous state-of-the-art for two datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_06082v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现实世界环境中的增量决策是嵌入式人工智能中最具挑战性的任务之一。一个要求特别高的场景是视觉和语言导航（VLN），它需要视觉和自然语言理解以及空间和时间推理能力。具体化的代理需要将其对导航指令的理解建立在对街景等真实世界环境的观察中。尽管LLM在其他研究领域取得了令人印象深刻的成果，但如何将其与交互式视觉环境最佳地联系起来，这是一个持续存在的问题。在这项工作中，我们提出了VELMA，这是一种具体的LLM代理，它使用轨迹和视觉环境观察的语言化作为下一个动作的上下文提示。视觉信息由管道表达，该管道从人工编写的导航指令中提取地标，并使用CLIP来确定其在当前全景视图中的可见性。我们仅通过两个上下文示例展示了VELMA能够成功地遵循街景中的导航指令。我们在几千个例子中进一步微调了LLM代理，并在两个数据集的任务完成率上实现了25%-30%的相对改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.06082v2" target="_blank">2307.06082v2</a>
                              </td>
                              <td>VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View</td>
                              <td>Raphael Schumann</td>
                              <td>2023-07-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_06082v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.06082v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/raphael-sch/velma" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_13478v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_13478v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_13478v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_13478v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multi-modal information retrieval (MMIR) is a rapidly evolving field, where significant progress, particularly in image-text pairing, has been made through advanced representation learning and cross-modality alignment research. However, current benchmarks for evaluating MMIR performance in image-text pairing within the scientific domain show a notable gap, where chart and table images described in scholarly language usually do not play a significant role. To bridge this gap, we develop a specialised scientific MMIR (SciMMIR) benchmark by leveraging open-access paper collections to extract data relevant to the scientific domain. This benchmark comprises 530K meticulously curated image-text pairs, extracted from figures and tables with detailed captions in scientific documents. We further annotate the image-text pairs with two-level subset-subcategory hierarchy annotations to facilitate a more comprehensive evaluation of the baselines. We conducted zero-shot and fine-tuning evaluations on prominent multi-modal image-captioning and visual language models, such as CLIP and BLIP. Our analysis offers critical insights for MMIR in the scientific domain, including the impact of pre-training and fine-tuning settings and the influence of the visual and textual encoders. All our data and checkpoints are publicly available at https://github.com/Wusiwei0410/SciMMIR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_13478v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多模态信息检索（MMIR）是一个快速发展的领域，通过高级表示学习和跨模态对齐研究，在该领域取得了重大进展，特别是在图像-文本配对方面。然而，目前在科学领域内评估MMIR在图像-文本配对中的性能的基准显示出显著的差距，其中用学术语言描述的图表和表格图像通常不会发挥重要作用。为了弥补这一差距，我们利用开放获取的论文收集来提取与科学领域相关的数据，从而开发了一个专门的科学MMIR（SciMMIR）基准。该基准包括530K个精心策划的图像文本对，这些图像文本对是从科学文件中带有详细说明的图表中提取的。我们进一步用两级子集子类别层次注释对图像-文本对进行注释，以便于对基线进行更全面的评估。我们对突出的多模态图像自适应和视觉语言模型（如CLIP和BLIP）进行了零样本和微调评估。我们的分析为MMIR在科学领域提供了重要的见解，包括预训练和微调设置的影响以及视觉和文本编码器的影响。我们所有的数据和检查点都可以在https://github.com/Wusiwei0410/SciMMIR.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.13478v1" target="_blank">2401.13478v1</a>
                              </td>
                              <td>SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval</td>
                              <td>Siwei Wu</td>
                              <td>2024-01-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_13478v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.13478v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/wusiwei0410/scimmir" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_13296v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Visual Objectification in Films: Towards a New AI Task for Video Interpretation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_13296v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_13296v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_13296v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In film gender studies, the concept of 'male gaze' refers to the way the characters are portrayed on-screen as objects of desire rather than subjects. In this article, we introduce a novel video-interpretation task, to detect character objectification in films. The purpose is to reveal and quantify the usage of complex temporal patterns operated in cinema to produce the cognitive perception of objectification. We introduce the ObyGaze12 dataset, made of 1914 movie clips densely annotated by experts for objectification concepts identified in film studies and psychology. We evaluate recent vision models, show the feasibility of the task and where the challenges remain with concept bottleneck models. Our new dataset and code are made available to the community.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_13296v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在电影性别研究中，“男性凝视”的概念是指角色在屏幕上被描绘成欲望的对象而不是主体的方式。在这篇文章中，我们介绍了一种新颖的视频解读任务，来检测电影中的角色对象化。目的是揭示和量化电影中复杂时间模式的使用，以产生客观化的认知感知。我们介绍了ObyGaze12数据集，该数据集由1914个电影片段组成，由电影研究和心理学中确定的对象化概念的专家密集注释。我们评估了最近的愿景模型，展示了任务的可行性以及概念瓶颈模型的挑战所在。我们的新数据集和代码可供社区使用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.13296v1" target="_blank">2401.13296v1</a>
                              </td>
                              <td>Visual Objectification in Films: Towards a New AI Task for Video Interpretation</td>
                              <td>Julie Tores</td>
                              <td>2024-01-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_13296v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.13296v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_12972v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On the Efficacy of Text-Based Input Modalities for Action Anticipation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_12972v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_12972v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_12972v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Although the task of anticipating future actions is highly uncertain, information from additional modalities help to narrow down plausible action choices. Each modality provides different environmental context for the model to learn from. While previous multi-modal methods leverage information from modalities such as video and audio, we primarily explore how text inputs for actions and objects can also enable more accurate action anticipation. Therefore, we propose a Multi-modal Anticipative Transformer (MAT), an attention-based video transformer architecture that jointly learns from multi-modal features and text captions. We train our model in two-stages, where the model first learns to predict actions in the video clip by aligning with captions, and during the second stage, we fine-tune the model to predict future actions. Compared to existing methods, MAT has the advantage of learning additional environmental context from two kinds of text inputs: action descriptions during the pre-training stage, and the text inputs for detected objects and actions during modality feature fusion. Through extensive experiments, we evaluate the effectiveness of the pre-training stage, and show that our model outperforms previous methods on all datasets. In addition, we examine the impact of object and action information obtained via text and perform extensive ablations. We evaluate the performance on on three datasets: EpicKitchens-100, EpicKitchens-55 and EGTEA GAZE+; and show that text descriptions do indeed aid in more effective action anticipation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_12972v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管预测未来行动的任务非常不确定，但来自其他模式的信息有助于缩小合理的行动选择范围。每种模态都为模型提供了不同的环境上下文供其学习。虽然以前的多模态方法利用了视频和音频等模态的信息，但我们主要探讨了动作和对象的文本输入如何实现更准确的动作预期。因此，我们提出了一种多模式预期转换器（MAT），这是一种基于注意力的视频转换器架构，可以联合学习多模式特征和文本字幕。我们分两个阶段训练我们的模型，其中模型首先通过与字幕对齐来学习预测视频剪辑中的动作，在第二阶段，我们对模型进行微调以预测未来的动作。与现有方法相比，MAT具有从两种文本输入中学习额外环境上下文的优势：预训练阶段的动作描述，以及模态特征融合过程中检测到的物体和动作的文本输入。通过广泛的实验，我们评估了预训练阶段的有效性，并表明我们的模型在所有数据集上都优于以前的方法。此外，我们还检查了通过文本获得的物体和动作信息的影响，并进行了广泛的消融。我们在三个数据集上评估了性能：EpicKitchens-100、EpicKit切尼-55和EGTEA GAZE+；并表明文本描述确实有助于更有效的行动预期。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.12972v1" target="_blank">2401.12972v1</a>
                              </td>
                              <td>On the Efficacy of Text-Based Input Modalities for Action Anticipation</td>
                              <td>Apoorva Beedu</td>
                              <td>2024-01-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_12972v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.12972v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_00367v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with TikZ</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_00367v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_00367v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_00367v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Generating bitmap graphics from text has gained considerable attention, yet for scientific figures, vector graphics are often preferred. Given that vector graphics are typically encoded using low-level graphics primitives, generating them directly is difficult. To address this, we propose the use of TikZ, a well-known abstract graphics language that can be compiled to vector graphics, as an intermediate representation of scientific figures. TikZ offers human-oriented, high-level commands, thereby facilitating conditional language modeling with any large language model. To this end, we introduce DaTikZ, the first large-scale TikZ dataset consisting of 120k TikZ drawings aligned with captions. We fine-tune LLaMA on DaTikZ, as well as our new model CLiMA, which augments LLaMA with multimodal CLIP embeddings. In both human and automatic evaluation, CLiMA and LLaMA outperform commercial GPT-4 and Claude 2 in terms of similarity to human-created figures, with CLiMA additionally improving text-image alignment. Our detailed analysis shows that all models generalize well and are not susceptible to memorization. GPT-4 and Claude 2, however, tend to generate more simplistic figures compared to both humans and our models. We make our framework, AutomaTikZ, along with model weights and datasets, publicly available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_00367v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从文本生成位图图形已经获得了相当大的关注，但对于科学图形，矢量图形通常是首选。鉴于矢量图形通常使用低级图形基元进行编码，因此很难直接生成它们。为了解决这一问题，我们建议使用TikZ，这是一种众所周知的抽象图形语言，可以编译为矢量图形，作为科学图形的中间表示。TikZ提供了面向人类的高级命令，从而促进了任何大型语言模型的条件语言建模。为此，我们介绍了DaTikZ，这是第一个大型TikZ数据集，由120k张与字幕对齐的TikZ图片组成。我们在DaTikZ上微调LLaMA，以及我们的新模型CLiMA，它通过多模式CLIP嵌入增强了LLaMA。在人工和自动评估中，CLiMA和LLaMA在与人工创建的图形的相似性方面优于商业GPT-4和Claude 2，CLiMA还改进了文本图像对齐。我们的详细分析表明，所有模型都能很好地概括，不易记忆。然而，与人类和我们的模型相比，GPT-4和Claude 2往往会生成更简单的数据。我们公开了我们的框架AutomaTikZ，以及模型权重和数据集。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.00367v2" target="_blank">2310.00367v2</a>
                              </td>
                              <td>AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with TikZ</td>
                              <td>Jonas Belouadi</td>
                              <td>2023-09-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_00367v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.00367v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/potamides/automatikz" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_12649v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Space-time unfitted finite elements on moving explicit geometry representations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_12649v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_12649v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_12649v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This work proposes a novel variational approximation of partial differential equations on moving geometries determined by explicit boundary representations. The benefits of the proposed formulation are the ability to handle large displacements of explicitly represented domain boundaries without generating body-fitted meshes and remeshing techniques. For the space discretization, we use a background mesh and an unfitted method that relies on integration on cut cells only. We perform this intersection by using clipping algorithms. To deal with the mesh movement, we pullback the equations to a reference configuration (the spatial mesh at the initial time slab times the time interval) that is constant in time. This way, the geometrical intersection algorithm is only required in 3D, another key property of the proposed scheme. At the end of the time slab, we compute the deformed mesh, intersect the deformed boundary with the background mesh, and consider an exact transfer operator between meshes to compute jump terms in the time discontinuous Galerkin integration. The transfer is also computed using geometrical intersection algorithms. We demonstrate the applicability of the method to fluid problems around rotating (2D and 3D) geometries described by oriented boundary meshes. We also provide a set of numerical experiments that show the optimal convergence of the method.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_12649v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>这项工作提出了由显式边界表示确定的运动几何上的偏微分方程的一种新的变分近似。所提出的公式的优点是能够处理显式表示的域边界的大位移，而无需生成贴体网格和重网格技术。对于空间离散化，我们使用背景网格和仅依赖于切割单元的积分的不适合的方法。我们通过使用剪裁算法来执行此交集。为了处理网格移动，我们将方程拉回到时间不变的参考配置（初始时间板处的空间网格乘以时间间隔）。这样，几何相交算法只需要在3D中使用，这是所提出方案的另一个关键特性。在时间板的末尾，我们计算变形网格，将变形边界与背景网格相交，并考虑网格之间的精确传递算子来计算时间不连续Galerkin积分中的跳跃项。传递也使用几何相交算法进行计算。我们证明了该方法对由定向边界网格描述的旋转（2D和3D）几何体周围的流体问题的适用性。我们还提供了一组数值实验，证明了该方法的最优收敛性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.12649v1" target="_blank">2401.12649v1</a>
                              </td>
                              <td>Space-time unfitted finite elements on moving explicit geometry representations</td>
                              <td>Santiago Badia</td>
                              <td>2024-01-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_12649v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.12649v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_12596v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">UniHDA: Towards Universal Hybrid Domain Adaptation of Image Generators</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_12596v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_12596v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_12596v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Generative domain adaptation has achieved remarkable progress, enabling us to adapt a pre-trained generator to a new target domain. However, existing methods simply adapt the generator to a single target domain and are limited to a single modality, either text-driven or image-driven. Moreover, they are prone to overfitting domain-specific attributes, which inevitably compromises cross-domain consistency. In this paper, we propose UniHDA, a unified and versatile framework for generative hybrid domain adaptation with multi-modal references from multiple domains. We use CLIP encoder to project multi-modal references into a unified embedding space and then linear interpolate the direction vectors from multiple target domains to achieve hybrid domain adaptation. To ensure the cross-domain consistency, we propose a novel cross-domain spatial structure (CSS) loss that maintains detailed spatial structure information between source and target generator. Experiments show that the adapted generator can synthesise realistic images with various attribute compositions. Additionally, our framework is versatile to multiple generators, \eg, StyleGAN2 and Diffusion Models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_12596v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>生成域自适应已经取得了显著的进展，使我们能够将预先训练的生成器自适应到新的目标域。然而，现有的方法只是将生成器适应于单个目标域，并且仅限于文本驱动或图像驱动的单个模态。此外，它们容易过度拟合特定领域的属性，这不可避免地会损害跨领域的一致性。在本文中，我们提出了UniHDA，这是一个统一而通用的生成混合域自适应框架，具有来自多个域的多模态引用。我们使用CLIP编码器将多模态参考投影到统一的嵌入空间中，然后对来自多个目标域的方向向量进行线性插值，以实现混合域自适应。为了确保跨域一致性，我们提出了一种新的跨域空间结构（CSS）损失，该损失保持源生成器和目标生成器之间的详细空间结构信息。实验表明，自适应生成器可以合成具有各种属性组成的逼真图像。此外，我们的框架适用于多个生成器，例如StyleGAN2和Diffusion Models。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.12596v1" target="_blank">2401.12596v1</a>
                              </td>
                              <td>UniHDA: Towards Universal Hybrid Domain Adaptation of Image Generators</td>
                              <td>Hengjia Li</td>
                              <td>2024-01-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_12596v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.12596v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_06827v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">APLe: Token-Wise Adaptive for Multi-Modal Prompt Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_06827v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_06827v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_06827v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Pre-trained Vision-Language (V-L) models set the benchmark for generalization to downstream tasks among the noteworthy contenders. Many characteristics of the V-L model have been explored in existing research including the challenge of the sensitivity to text input and the tuning process across multi-modal prompts. With the advanced utilization of the V-L model like CLIP, recent approaches deploy learnable prompts instead of hand-craft prompts to boost the generalization performance and address the aforementioned challenges. Inspired by layer-wise training, which is wildly used in image fusion, we note that using a sequential training process to adapt different modalities branches of CLIP efficiently facilitates the improvement of generalization. In the context of addressing the multi-modal prompting challenge, we propose Token-wise Adaptive for Multi-modal Prompt Learning (APLe) for tuning both modalities prompts, vision and language, as tokens in a sequential manner. APLe addresses the challenges in V-L models to promote prompt learning across both modalities, which indicates a competitive generalization performance in line with the state-of-the-art. Preeminently, APLe shows robustness and favourable performance in prompt-length experiments with an absolute advantage in adopting the V-L models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_06827v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>预先训练的视觉语言（V-L）模型在值得注意的竞争者中为下游任务的泛化设置了基准。在现有的研究中，已经探索了V-L模型的许多特征，包括对文本输入的敏感性和跨多模式提示的调整过程的挑战。随着像CLIP这样的V-L模型的高级利用，最近的方法部署了可学习的提示，而不是手工提示，以提高泛化性能并解决上述挑战。受图像融合中广泛使用的逐层训练的启发，我们注意到使用顺序训练过程来适应CLIP的不同模态分支可以有效地提高泛化能力。在应对多模式提示挑战的背景下，我们提出了多模式提示学习的令牌自适应（APLe），用于以顺序的方式将模式提示、视觉和语言作为令牌进行调整。APLe解决了V-L模型中的挑战，以促进跨两种模式的快速学习，这表明具有与最先进技术相一致的竞争力的泛化性能。首先，APLe在即时长度实验中表现出鲁棒性和良好的性能，在采用V-L模型方面具有绝对优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.06827v2" target="_blank">2401.06827v2</a>
                              </td>
                              <td>APLe: Token-Wise Adaptive for Multi-Modal Prompt Learning</td>
                              <td>Guiming Cao</td>
                              <td>2024-01-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_06827v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.06827v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_12425v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Neglected Tails of Vision-Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_12425v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_12425v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_12425v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision-language models (VLMs) excel in zero-shot recognition but exhibit drastically imbalanced performance across visual concepts. For example, CLIP, despite an impressive mean zero-shot accuracy on ImageNet (72.7%), yields $<$10% on ten concepts (e.g., gyromitra and night snake), presumably, because these concepts are under-represented in VLMs' imbalanced pretraining data. Yet, assessing this imbalance is challenging as it is non-trivial to calculate the frequency of specific concepts within VLMs' large-scale pretraining data. Our work makes the first attempt to measure the concept frequency by analyzing pretraining texts. We use off-the-shelf language models to help count relevant texts that contain synonyms of the given concepts and resolve linguistic ambiguity. We confirm that popular VLM datasets like LAION indeed exhibit long-tailed concept distributions, which strongly correlate with per-class accuracies. Further, contemporary multimodal systems, e.g., visual chatbots and text-to-image generators, also struggle with the rare concepts identified by our method. To mitigate VLMs' imbalanced performance in zero-shot recognition, we propose REtrieval-Augmented Learning REAL. First, instead of prompting VLMs using the original class names, REAL uses their most frequent synonyms found in VLMs' pretraining texts. This already outperforms human-engineered and LLM-generated prompts over nine benchmark datasets, likely because VLMs have seen more images associated with the frequently used synonyms. Second, REAL uses all the concept synonyms to retrieve a small, class-balanced set of pretraining data to train a robust classifier. REAL surpasses the recent retrieval-augmented solution REACT, using 400x less storage and 10,000x less training time!</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_12425v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉语言模型（VLM）在零样本识别方面表现出色，但在视觉概念方面表现出严重的不平衡性能。例如，尽管在ImageNet上有令人印象深刻的平均零样本准确率（72.7%），但CLIP在十个概念（例如陀螺和夜蛇）上产生了$<$10%的收益，这可能是因为这些概念在VLM的不平衡预训练数据中未得到充分体现。然而，评估这种不平衡是具有挑战性的，因为计算VLM大规模预训练数据中特定概念的频率并非易事。我们的工作首次尝试通过分析预训练文本来测量概念频率。我们使用现成的语言模型来帮助统计包含给定概念的同义词的相关文本，并解决语言歧义。我们证实，像LAION这样的流行VLM数据集确实表现出长尾概念分布，这与每类精度密切相关。此外，当代的多模式系统，例如视觉聊天机器人和文本到图像生成器，也在与我们的方法所识别的罕见概念作斗争。为了缓解VLM在零样本识别中的不平衡性能，我们提出了REtrieval-AugmentedLearningREAL。首先，REAL不使用原始类名提示VLM，而是使用VLM预训练文本中最常见的同义词。在九个基准数据集中，这已经优于人工设计和LLM生成的提示，这可能是因为VLM看到了更多与常用同义词相关的图像。其次，REAL使用所有概念同义词来检索一组小的、类平衡的预训练数据，以训练一个健壮的分类器。REAL超越了最近的检索增强解决方案REACT，使用的存储空间减少了400倍，训练时间减少了10000倍！</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.12425v1" target="_blank">2401.12425v1</a>
                              </td>
                              <td>The Neglected Tails of Vision-Language Models</td>
                              <td>Shubham Parashar</td>
                              <td>2024-01-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_12425v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.12425v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_12217v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exploring Simple Open-Vocabulary Semantic Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_12217v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_12217v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_12217v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Open-vocabulary semantic segmentation models aim to accurately assign a semantic label to each pixel in an image from a set of arbitrary open-vocabulary texts. In order to learn such pixel-level alignment, current approaches typically rely on a combination of (i) image-level VL model (e.g. CLIP), (ii) ground truth masks, and (iii) custom grouping encoders. In this paper, we introduce S-Seg, a novel model that can achieve surprisingly strong performance without depending on any of the above elements. S-Seg leverages pseudo-mask and language to train a MaskFormer, and can be easily trained from publicly available image-text datasets. Contrary to prior works, our model directly trains for pixel-level features and language alignment. Once trained, S-Seg generalizes well to multiple testing datasets without requiring fine-tuning. In addition, S-Seg has the extra benefits of scalability with data and consistently improvement when augmented with self-training. We believe that our simple yet effective approach will serve as a solid baseline for future research.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_12217v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>开放式词汇语义分割模型旨在从一组任意的开放式词汇文本中准确地为图像中的每个像素分配语义标签。为了学习这种像素级对准，当前的方法通常依赖于（i）图像级VL模型（例如CLIP）、（ii）地面实况掩码和（iii）自定义分组编码器的组合。在本文中，我们介绍了S-Seg，这是一种新颖的模型，它可以在不依赖上述任何元素的情况下实现令人惊讶的强大性能。S-Seg利用伪掩码和语言来训练MaskFormer，并且可以从公开的图像文本数据集中轻松地进行训练。与之前的工作相反，我们的模型直接训练像素级特征和语言对齐。一旦经过训练，S-Seg就可以很好地推广到多个测试数据集，而无需进行微调。此外，S-Seg还具有数据可扩展性和自我训练增强时持续改进的额外优势。我们相信，我们简单而有效的方法将成为未来研究的坚实基础。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.12217v1" target="_blank">2401.12217v1</a>
                              </td>
                              <td>Exploring Simple Open-Vocabulary Semantic Segmentation</td>
                              <td>Zihang Lai</td>
                              <td>2024-01-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_12217v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.12217v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zlai0/s-seg" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_05916v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Interpreting CLIP's Image Representation via Text-Based Decomposition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_05916v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_05916v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_05916v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We investigate the CLIP image encoder by analyzing how individual model components affect the final representation. We decompose the image representation as a sum across individual image patches, model layers, and attention heads, and use CLIP's text representation to interpret the summands. Interpreting the attention heads, we characterize each head's role by automatically finding text representations that span its output space, which reveals property-specific roles for many heads (e.g. location or shape). Next, interpreting the image patches, we uncover an emergent spatial localization within CLIP. Finally, we use this understanding to remove spurious features from CLIP and to create a strong zero-shot image segmenter. Our results indicate that a scalable understanding of transformer models is attainable and can be used to repair and improve models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_05916v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们通过分析单个模型组件如何影响最终表示来研究CLIP图像编码器。我们将图像表示分解为单个图像块、模型层和注意力头的总和，并使用CLIP的文本表示来解释总和。在解释注意力头部时，我们通过自动找到跨越其输出空间的文本表示来表征每个头部的角色，这揭示了许多头部的特定属性角色（例如位置或形状）。接下来，解释图像补丁，我们揭示了CLIP中出现的空间定位。最后，我们利用这种理解来消除CLIP中的虚假特征，并创建一个强大的零样本图像分割器。我们的研究结果表明，可以对变压器模型进行可扩展的理解，并可用于修复和改进模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.05916v3" target="_blank">2310.05916v3</a>
                              </td>
                              <td>Interpreting CLIP's Image Representation via Text-Based Decomposition</td>
                              <td>Yossi Gandelsman</td>
                              <td>2023-10-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_05916v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.05916v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_11791v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SemPLeS: Semantic Prompt Learning for Weakly-Supervised Semantic Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_11791v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_11791v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_11791v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Weakly-Supervised Semantic Segmentation (WSSS) aims to train segmentation models using training image data with only image-level supervision. Since precise pixel-level annotations are not accessible, existing methods typically focus on producing pseudo masks for training segmentation models by refining CAM-like heatmaps. However, the produced heatmaps may only capture discriminative image regions of target object categories or the associated co-occurring backgrounds. To address the issues, we propose a Semantic Prompt Learning for WSSS (SemPLeS) framework, which learns to effectively prompt the CLIP space to enhance the semantic alignment between the segmented regions and the target object categories. More specifically, we propose Contrastive Prompt Learning and Class-associated Semantic Refinement to learn the prompts that adequately describe and suppress the image backgrounds associated with each target object category. In this way, our proposed framework is able to perform better semantic matching between object regions and the associated text labels, resulting in desired pseudo masks for training the segmentation model. The proposed SemPLeS framework achieves SOTA performance on the standard WSSS benchmarks, PASCAL VOC and MS COCO, and demonstrated interpretability with the semantic visualization of our learned prompts. The codes will be released.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_11791v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>弱监督语义分割（WSSS）旨在使用仅具有图像级监督的训练图像数据来训练分割模型。由于无法获得精确的像素级注释，现有方法通常侧重于通过细化类似CAM的热图来生成用于训练分割模型的伪掩模。然而，所产生的热图可以仅捕获目标对象类别或相关联的共同出现的背景的有区别的图像区域。为了解决这些问题，我们提出了一种WSSS的语义提示学习（SemPLeS）框架，该框架学习有效地提示CLIP空间，以增强分割区域和目标对象类别之间的语义对齐。更具体地说，我们提出了对比提示学习和类相关语义精炼来学习充分描述和抑制与每个目标对象类别相关的图像背景的提示。通过这种方式，我们提出的框架能够在对象区域和相关联的文本标签之间执行更好的语义匹配，从而产生用于训练分割模型的所需伪掩码。所提出的SemPLeS框架在标准WSSS基准、PASCAL VOC和MS COCO上实现了SOTA性能，并通过我们学习的提示的语义可视化展示了可解释性。代码将被发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.11791v1" target="_blank">2401.11791v1</a>
                              </td>
                              <td>SemPLeS: Semantic Prompt Learning for Weakly-Supervised Semantic Segmentation</td>
                              <td>Ci-Siang Lin</td>
                              <td>2024-01-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_11791v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.11791v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_02773v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Diffusion Model is Secretly a Training-free Open Vocabulary Semantic Segmenter</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_02773v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_02773v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_02773v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The pre-trained text-image discriminative models, such as CLIP, has been explored for open-vocabulary semantic segmentation with unsatisfactory results due to the loss of crucial localization information and awareness of object shapes. Recently, there has been a growing interest in expanding the application of generative models from generation tasks to semantic segmentation. These approaches utilize generative models either for generating annotated data or extracting features to facilitate semantic segmentation. This typically involves generating a considerable amount of synthetic data or requiring additional mask annotations. To this end, we uncover the potential of generative text-to-image diffusion models (e.g., Stable Diffusion) as highly efficient open-vocabulary semantic segmenters, and introduce a novel training-free approach named DiffSegmenter. The insight is that to generate realistic objects that are semantically faithful to the input text, both the complete object shapes and the corresponding semantics are implicitly learned by diffusion models. We discover that the object shapes are characterized by the self-attention maps while the semantics are indicated through the cross-attention maps produced by the denoising U-Net, forming the basis of our segmentation results.Additionally, we carefully design effective textual prompts and a category filtering mechanism to further enhance the segmentation results. Extensive experiments on three benchmark datasets show that the proposed DiffSegmenter achieves impressive results for open-vocabulary semantic segmentation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_02773v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>已经探索了预先训练的文本图像判别模型，如CLIP，用于开放词汇语义分割，但由于丢失了关键的定位信息和对物体形状的感知，结果不令人满意。最近，人们对将生成模型的应用从生成任务扩展到语义分割越来越感兴趣。这些方法利用生成模型来生成注释数据或提取特征以促进语义分割。这通常涉及生成大量的合成数据或需要额外的掩码注释。为此，我们揭示了生成文本到图像扩散模型（例如，稳定扩散）作为高效的开放词汇语义分割器的潜力，并引入了一种新的无训练方法DiffSegmenter。我们的见解是，为了生成语义上忠实于输入文本的真实对象，完整的对象形状和相应的语义都是通过扩散模型隐式学习的。我们发现，对象的形状是由自注意图表征的，而语义是通过去噪U-Net产生的交叉注意图来表示的，这构成了我们分割结果的基础。此外，我们精心设计了有效的文本提示和类别过滤机制，以进一步增强分割结果。在三个基准数据集上的大量实验表明，所提出的DiffSegmenter在开放词汇语义分割方面取得了令人印象深刻的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.02773v3" target="_blank">2309.02773v3</a>
                              </td>
                              <td>Diffusion Model is Secretly a Training-free Open Vocabulary Semantic Segmenter</td>
                              <td>Jinglong Wang</td>
                              <td>2023-09-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_02773v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.02773v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/VCG-team/DiffSegmenter" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_11704v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">EK-Net:Real-time Scene Text Detection with Expand Kernel Distance</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_11704v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_11704v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_11704v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, scene text detection has received significant attention due to its wide application. However, accurate detection in complex scenes of multiple scales, orientations, and curvature remains a challenge. Numerous detection methods adopt the Vatti clipping (VC) algorithm for multiple-instance training to address the issue of arbitrary-shaped text. Yet we identify several bias results from these approaches called the "shrinked kernel". Specifically, it refers to a decrease in accuracy resulting from an output that overly favors the text kernel. In this paper, we propose a new approach named Expand Kernel Network (EK-Net) with expand kernel distance to compensate for the previous deficiency, which includes three-stages regression to complete instance detection. Moreover, EK-Net not only realize the precise positioning of arbitrary-shaped text, but also achieve a trade-off between performance and speed. Evaluation results demonstrate that EK-Net achieves state-of-the-art or competitive performance compared to other advanced methods, e.g., F-measure of 85.72% at 35.42 FPS on ICDAR 2015, F-measure of 85.75% at 40.13 FPS on CTW1500.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_11704v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，场景文本检测由于其广泛的应用而受到极大的关注。然而，在多个尺度、方向和曲率的复杂场景中进行准确检测仍然是一个挑战。许多检测方法采用Vatti clipping（VC）算法进行多实例训练，以解决任意形状文本的问题。然而，我们从这些被称为“收缩内核”的方法中发现了一些偏差结果。具体来说，它指的是由于输出过于偏向文本内核而导致的准确性下降。在本文中，我们提出了一种新的方法，称为具有扩展核距离的扩展核网络（EK-Net）来弥补以前的不足，该方法包括三阶段回归来完成实例检测。此外，EK-Net不仅实现了对任意形状文本的精确定位，而且实现了性能和速度之间的权衡。评估结果表明，与其他先进方法相比，EK-Net实现了最先进或有竞争力的性能，例如，2015年ICDAR在35.42 FPS时的F-测量为85.72%，CTW1500在40.13 FPS时的F-测量为85.75%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.11704v1" target="_blank">2401.11704v1</a>
                              </td>
                              <td>EK-Net:Real-time Scene Text Detection with Expand Kernel Distance</td>
                              <td>Boyuan Zhu</td>
                              <td>2024-01-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_11704v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.11704v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_11649v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">M2-CLIP: A Multimodal, Multi-task Adapting Framework for Video Action Recognition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_11649v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_11649v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_11649v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, the rise of large-scale vision-language pretrained models like CLIP, coupled with the technology of Parameter-Efficient FineTuning (PEFT), has captured substantial attraction in video action recognition. Nevertheless, prevailing approaches tend to prioritize strong supervised performance at the expense of compromising the models' generalization capabilities during transfer. In this paper, we introduce a novel Multimodal, Multi-task CLIP adapting framework named \name to address these challenges, preserving both high supervised performance and robust transferability. Firstly, to enhance the individual modality architectures, we introduce multimodal adapters to both the visual and text branches. Specifically, we design a novel visual TED-Adapter, that performs global Temporal Enhancement and local temporal Difference modeling to improve the temporal representation capabilities of the visual encoder. Moreover, we adopt text encoder adapters to strengthen the learning of semantic label information. Secondly, we design a multi-task decoder with a rich set of supervisory signals to adeptly satisfy the need for strong supervised performance and generalization within a multimodal framework. Experimental results validate the efficacy of our approach, demonstrating exceptional performance in supervised learning while maintaining strong generalization in zero-shot scenarios.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_11649v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，像CLIP这样的大规模视觉语言预训练模型的兴起，加上参数有效微调（PEFT）技术，在视频动作识别中引起了巨大的吸引力。然而，主流方法倾向于优先考虑强大的监督性能，而牺牲了模型在传输过程中的泛化能力。在本文中，我们介绍了一种新的多模式、多任务CLIP适配框架，名为\name，以应对这些挑战，同时保持高监督性能和强大的可转移性。首先，为了增强单个模态架构，我们为视觉和文本分支引入了多模态适配器。具体来说，我们设计了一种新的视觉TED适配器，它执行全局时间增强和局部时间差分建模，以提高视觉编码器的时间表示能力。此外，我们还采用了文本编码器适配器来加强语义标签信息的学习。其次，我们设计了一个具有丰富监督信号集的多任务解码器，以熟练地满足在多模式框架内对强监督性能和泛化的需求。实验结果验证了我们的方法的有效性，证明了监督学习的卓越性能，同时在零样本场景中保持了强大的泛化能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.11649v1" target="_blank">2401.11649v1</a>
                              </td>
                              <td>M2-CLIP: A Multimodal, Multi-task Adapting Framework for Video Action Recognition</td>
                              <td>Mengmeng Wang</td>
                              <td>2024-01-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_11649v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.11649v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_11633v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Zoom-shot: Fast and Efficient Unsupervised Zero-Shot Transfer of CLIP to Vision Encoders with Multimodal Loss</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_11633v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_11633v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_11633v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The fusion of vision and language has brought about a transformative shift in computer vision through the emergence of Vision-Language Models (VLMs). However, the resource-intensive nature of existing VLMs poses a significant challenge. We need an accessible method for developing the next generation of VLMs. To address this issue, we propose Zoom-shot, a novel method for transferring the zero-shot capabilities of CLIP to any pre-trained vision encoder. We do this by exploiting the multimodal information (i.e. text and image) present in the CLIP latent space through the use of specifically designed multimodal loss functions. These loss functions are (1) cycle-consistency loss and (2) our novel prompt-guided knowledge distillation loss (PG-KD). PG-KD combines the concept of knowledge distillation with CLIP's zero-shot classification, to capture the interactions between text and image features. With our multimodal losses, we train a $\textbf{linear mapping}$ between the CLIP latent space and the latent space of a pre-trained vision encoder, for only a $\textbf{single epoch}$. Furthermore, Zoom-shot is entirely unsupervised and is trained using $\textbf{unpaired}$ data. We test the zero-shot capabilities of a range of vision encoders augmented as new VLMs, on coarse and fine-grained classification datasets, outperforming the previous state-of-the-art in this problem domain. In our ablations, we find Zoom-shot allows for a trade-off between data and compute during training; and our state-of-the-art results can be obtained by reducing training from 20% to 1% of the ImageNet training data with 20 epochs. All code and models are available on GitHub.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_11633v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉和语言的融合通过视觉语言模型（VLM）的出现带来了计算机视觉的变革。然而，现有VLM的资源密集型性质带来了重大挑战。我们需要一种可访问的方法来开发下一代VLM。为了解决这个问题，我们提出了Zoom-shot，这是一种将CLIP的零样本功能转移到任何预先训练的视觉编码器的新方法。我们通过使用专门设计的多模式损失函数，利用CLIP潜在空间中存在的多模式信息（即文本和图像）来实现这一点。这些损失函数是（1）循环一致性损失和（2）我们新的即时引导知识蒸馏损失（PG-KD）。PG-KD将知识提取的概念与CLIP的零样本分类相结合，以捕捉文本和图像特征之间的相互作用。利用我们的多模态损失，我们在CLIP潜在空间和预先训练的视觉编码器的潜在空间之间训练$\textbf｛线性映射｝$，仅针对$\textbf{单个epoch}$。此外，缩放镜头是完全无监督的，并且使用$\textbf｛unpair｝$数据进行训练。我们在粗分类数据集和细粒度分类数据集上测试了作为新VLM增强的一系列视觉编码器的零样本能力，在该问题领域优于以前的先进技术。在我们的消融中，我们发现Zoom shot允许在训练期间在数据和计算之间进行权衡；并且我们最先进的结果可以通过将具有20个时期的ImageNet训练数据的训练从20%减少到1%来获得。所有代码和模型都可以在GitHub上获得。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.11633v1" target="_blank">2401.11633v1</a>
                              </td>
                              <td>Zoom-shot: Fast and Efficient Unsupervised Zero-Shot Transfer of CLIP to Vision Encoders with Multimodal Loss</td>
                              <td>Jordan Shipard</td>
                              <td>2024-01-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_11633v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.11633v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_07525v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SingFake: Singing Voice Deepfake Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_07525v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_07525v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_07525v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The rise of singing voice synthesis presents critical challenges to artists and industry stakeholders over unauthorized voice usage. Unlike synthesized speech, synthesized singing voices are typically released in songs containing strong background music that may hide synthesis artifacts. Additionally, singing voices present different acoustic and linguistic characteristics from speech utterances. These unique properties make singing voice deepfake detection a relevant but significantly different problem from synthetic speech detection. In this work, we propose the singing voice deepfake detection task. We first present SingFake, the first curated in-the-wild dataset consisting of 28.93 hours of bonafide and 29.40 hours of deepfake song clips in five languages from 40 singers. We provide a train/validation/test split where the test sets include various scenarios. We then use SingFake to evaluate four state-of-the-art speech countermeasure systems trained on speech utterances. We find these systems lag significantly behind their performance on speech test data. When trained on SingFake, either using separated vocal tracks or song mixtures, these systems show substantial improvement. However, our evaluations also identify challenges associated with unseen singers, communication codecs, languages, and musical contexts, calling for dedicated research into singing voice deepfake detection. The SingFake dataset and related resources are available at https://www.singfake.org/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_07525v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>歌声合成的兴起给艺术家和行业利益相关者带来了未经授权使用声音的严峻挑战。与合成语音不同，合成歌声通常在包含可能隐藏合成伪像的强背景音乐的歌曲中发布。此外，歌声呈现出与言语不同的声学和语言特征。这些独特的特性使歌声深度伪造检测成为一个与合成语音检测相关但明显不同的问题。在这项工作中，我们提出了歌声深度伪造检测任务。我们首先展示了SingFake，这是第一个在野外策划的数据集，由来自40位歌手的五种语言的28.93小时的真实和29.40小时的深度伪造歌曲片段组成。我们提供了一个训练/验证/测试划分，其中测试集包括各种场景。然后，我们使用SingFake来评估四个基于语音训练的最先进的语音对抗系统。我们发现这些系统在语音测试数据上明显落后于它们的性能。当在SingFake上训练时，无论是使用分离的音轨还是歌曲混合，这些系统都显示出显著的改进。然而，我们的评估也发现了与看不见的歌手、通信编解码器、语言和音乐背景相关的挑战，呼吁对歌声深度伪造检测进行专门研究。SingFake数据集和相关资源可在https://www.singfake.org/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.07525v2" target="_blank">2309.07525v2</a>
                              </td>
                              <td>SingFake: Singing Voice Deepfake Detection</td>
                              <td>Yongyi Zang</td>
                              <td>2023-09-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_07525v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.07525v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/yongyizang/SingFake" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_11337v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Prompting Large Vision-Language Models for Compositional Reasoning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_11337v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_11337v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_11337v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision-language models such as CLIP have shown impressive capabilities in encoding texts and images into aligned embeddings, enabling the retrieval of multimodal data in a shared embedding space. However, these embedding-based models still face challenges in effectively matching images and texts with similar visio-linguistic compositionality, as evidenced by their performance on the recent Winoground dataset. In this paper, we argue that this limitation stems from two factors: the use of single vector representations for complex multimodal data, and the absence of step-by-step reasoning in these embedding-based methods. To address this issue, we make an exploratory step using a novel generative method that prompts large vision-language models (e.g., GPT-4) to depict images and perform compositional reasoning. Our method outperforms other embedding-based methods on the Winoground dataset, and obtains further improvement of up to 10% accuracy when enhanced with the optimal description.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_11337v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>CLIP等视觉语言模型在将文本和图像编码为对齐嵌入方面表现出了令人印象深刻的能力，从而能够在共享嵌入空间中检索多模式数据。然而，这些基于嵌入的模型在有效匹配具有相似视觉-语言合成性的图像和文本方面仍然面临挑战，正如它们在最近的Winoground数据集上的表现所证明的那样。在本文中，我们认为这种限制源于两个因素：对复杂的多模式数据使用单向量表示，以及在这些基于嵌入的方法中缺乏逐步推理。为了解决这个问题，我们使用一种新的生成方法进行了探索，该方法提示大型视觉语言模型（例如，GPT-4）来描述图像并执行合成推理。在Winoground数据集上，我们的方法优于其他基于嵌入的方法，并在优化描述的基础上进一步提高了高达10%的准确性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.11337v1" target="_blank">2401.11337v1</a>
                              </td>
                              <td>Prompting Large Vision-Language Models for Compositional Reasoning</td>
                              <td>Timothy Ossowski</td>
                              <td>2024-01-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_11337v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.11337v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/tossowski/keycomp" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_11311v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Novel Benchmark for Few-Shot Semantic Segmentation in the Era of Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_11311v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_11311v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_11311v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, the rapid evolution of computer vision has seen the emergence of various vision foundation models, each tailored to specific data types and tasks. While large language models often share a common pretext task, the diversity in vision foundation models arises from their varying training objectives. In this study, we delve into the quest for identifying the most effective vision foundation models for few-shot semantic segmentation, a critical task in computer vision. Specifically, we conduct a comprehensive comparative analysis of four prominent foundation models: DINO V2, Segment Anything, CLIP, Masked AutoEncoders, and a straightforward ResNet50 pre-trained on the COCO dataset. Our investigation focuses on their adaptability to new semantic segmentation tasks, leveraging only a limited number of segmented images. Our experimental findings reveal that DINO V2 consistently outperforms the other considered foundation models across a diverse range of datasets and adaptation methods. This outcome underscores DINO V2's superior capability to adapt to semantic segmentation tasks compared to its counterparts. Furthermore, our observations indicate that various adapter methods exhibit similar performance, emphasizing the paramount importance of selecting a robust feature extractor over the intricacies of the adaptation technique itself. This insight sheds light on the critical role of feature extraction in the context of few-shot semantic segmentation. This research not only contributes valuable insights into the comparative performance of vision foundation models in the realm of few-shot semantic segmentation but also highlights the significance of a robust feature extractor in this domain.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_11311v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，随着计算机视觉的快速发展，出现了各种视觉基础模型，每种模型都适合特定的数据类型和任务。虽然大型语言模型通常有一个共同的借口任务，但视觉基础模型的多样性源于它们不同的训练目标。在这项研究中，我们深入探讨了为少镜头语义分割（计算机视觉中的一项关键任务）确定最有效的视觉基础模型的探索。具体而言，我们对四个突出的基础模型进行了全面的比较分析：DINO V2、Segment Anything、CLIP、Masked AutoEncoders和在COCO数据集上预训练的直接ResNet50。我们的研究重点是它们对新的语义分割任务的适应性，仅利用有限数量的分割图像。我们的实验结果表明，在各种数据集和适应方法中，DINO V2始终优于其他考虑的基础模型。这一结果突显了与同类产品相比，DINO V2在适应语义分割任务方面的卓越能力。此外，我们的观察结果表明，各种适配器方法表现出相似的性能，强调了选择鲁棒特征提取器的至关重要性，而不是自适应技术本身的复杂性。这一见解揭示了特征提取在少镜头语义分割中的关键作用。这项研究不仅为视觉基础模型在少镜头语义分割领域的比较性能提供了有价值的见解，而且突出了鲁棒特征提取器在该领域的重要性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.11311v1" target="_blank">2401.11311v1</a>
                              </td>
                              <td>A Novel Benchmark for Few-Shot Semantic Segmentation in the Era of Foundation Models</td>
                              <td>Reda Bensaid</td>
                              <td>2024-01-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_11311v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.11311v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_11243v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LRP-QViT: Mixed-Precision Vision Transformer Quantization via Layer-wise Relevance Propagation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_11243v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_11243v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_11243v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision transformers (ViTs) have demonstrated remarkable performance across various visual tasks. However, ViT models suffer from substantial computational and memory requirements, making it challenging to deploy them on resource-constrained platforms. Quantization is a popular approach for reducing model size, but most studies mainly focus on equal bit-width quantization for the entire network, resulting in sub-optimal solutions. While there are few works on mixed precision quantization (MPQ) for ViTs, they typically rely on search space-based methods or employ mixed precision arbitrarily. In this paper, we introduce LRP-QViT, an explainability-based method for assigning mixed-precision bit allocations to different layers based on their importance during classification. Specifically, to measure the contribution score of each layer in predicting the target class, we employ the Layer-wise Relevance Propagation (LRP) method. LRP assigns local relevance at the output layer and propagates it through all layers, distributing the relevance until it reaches the input layers. These relevance scores serve as indicators for computing the layer contribution score. Additionally, we have introduced a clipped channel-wise quantization aimed at eliminating outliers from post-LayerNorm activations to alleviate severe inter-channel variations. To validate and assess our approach, we employ LRP-QViT across ViT, DeiT, and Swin transformer models on various datasets. Our experimental findings demonstrate that both our fixed-bit and mixed-bit post-training quantization methods surpass existing models in the context of 4-bit and 6-bit quantization.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_11243v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉转换器（ViTs）在各种视觉任务中都表现出了非凡的性能。然而，ViT模型存在大量的计算和内存需求，因此在资源受限的平台上部署它们具有挑战性。量化是减少模型大小的一种流行方法，但大多数研究主要集中在整个网络的等位宽量化上，从而产生次优解。虽然很少有关于ViTs的混合精度量化（MPQ）的工作，但它们通常依赖于基于搜索空间的方法或任意使用混合精度。在本文中，我们介绍了LRP-QViT，这是一种基于可解释性的方法，用于根据分类过程中的重要性将混合精度比特分配分配给不同的层。具体来说，为了测量每一层在预测目标类别中的贡献分数，我们采用了逐层相关传播（LRP）方法。LRP在输出层分配局部相关性，并将其传播到所有层，分发相关性，直到它到达输入层。这些相关性得分用作计算层贡献得分的指标。此外，我们还引入了一种削波信道量化，旨在消除后LayerNorm激活中的异常值，以缓解严重的信道间变化。为了验证和评估我们的方法，我们在各种数据集上的ViT、DeiT和Swin转换器模型中使用LRP-QViT。我们的实验结果表明，在4比特和6比特量化的背景下，我们的固定比特和混合比特后训练量化方法都超过了现有的模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.11243v1" target="_blank">2401.11243v1</a>
                              </td>
                              <td>LRP-QViT: Mixed-Precision Vision Transformer Quantization via Layer-wise Relevance Propagation</td>
                              <td>Navin Ranjan</td>
                              <td>2024-01-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_11243v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.11243v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_00106v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FashionFlow: Leveraging Diffusion Models for Dynamic Fashion Video Synthesis from Static Imagery</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_00106v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_00106v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_00106v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Our study introduces a new image-to-video generator called FashionFlow to generate fashion videos. By utilising a diffusion model, we are able to create short videos from still fashion images. Our approach involves developing and connecting relevant components with the diffusion model, which results in the creation of high-fidelity videos that are aligned with the conditional image. The components include the use of pseudo-3D convolutional layers to generate videos efficiently. VAE and CLIP encoders capture vital characteristics from still images to condition the diffusion model at a global level. Our research demonstrates a successful synthesis of fashion videos featuring models posing from various angles, showcasing the fit and appearance of the garment. Our findings hold great promise for improving and enhancing the shopping experience for the online fashion industry.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_00106v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们的研究将一种新的图像引入到视频生成器FashionFlow中，以生成时尚视频。通过使用扩散模型，我们能够从静态时尚图像中创建短视频。我们的方法包括开发相关组件并将其与扩散模型连接起来，从而创建与条件图像对齐的高保真视频。这些组件包括使用伪3D卷积层来高效地生成视频。VAE和CLIP编码器从静止图像中捕捉重要特征，以在全局水平上调节扩散模型。我们的研究成功地合成了时尚视频，模特们从不同角度摆出姿势，展示了服装的合身度和外观。我们的研究结果有望改善和增强在线时尚行业的购物体验。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.00106v2" target="_blank">2310.00106v2</a>
                              </td>
                              <td>FashionFlow: Leveraging Diffusion Models for Dynamic Fashion Video Synthesis from Static Imagery</td>
                              <td>Tasin Islam</td>
                              <td>2023-09-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_00106v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.00106v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/1702609/fashionflow" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="DINO"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2401_05925v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians with Dual Feature Fusion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05925v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05925v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05925v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a method for compact 3D-consistent scene segmentation at fast rendering speed with only RGB images input. Previous NeRF-based segmentation methods have relied on time-consuming neural scene optimization. While recent 3D Gaussian Splatting has notably improved speed, existing Gaussian-based segmentation methods struggle to produce compact masks, especially in zero-shot segmentation. This issue probably stems from their straightforward assignment of learnable parameters to each Gaussian, resulting in a lack of robustness against cross-view inconsistent 2D machine-generated labels. Our method aims to address this problem by employing Dual Feature Fusion Network as Gaussians' segmentation field. Specifically, we first optimize 3D Gaussians under RGB supervision. After Gaussian Locating, DINO features extracted from images are applied through explicit unprojection, which are further incorporated with spatial features from the efficient point cloud processing network. Feature aggregation is utilized to fuse them in a global-to-local strategy for compact segmentation features. Experimental results show that our model outperforms baselines on both semantic and panoptic zero-shot segmentation task, meanwhile consumes less than 10% inference time compared to NeRF-based methods. Code and more results will be available at https://David-Dou.github.io/CoSSegGaussians</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05925v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了紧凑和快速分割3D高斯（CoSSegGaussians），这是一种在仅输入RGB图像的情况下以快速渲染速度进行紧凑的3D一致场景分割的方法。以前基于NeRF的分割方法依赖于耗时的神经场景优化。虽然最近的3D高斯Splatting显著提高了速度，但现有的基于高斯的分割方法很难产生紧凑的掩模，尤其是在零样本分割中。这个问题可能源于他们将可学习参数直接分配给每个高斯，导致对交叉视图不一致的2D机器生成标签缺乏鲁棒性。我们的方法旨在通过使用双特征融合网络作为高斯分割域来解决这个问题。具体来说，我们首先在RGB监督下优化3D高斯。高斯定位后，通过显式反投影应用从图像中提取的DINO特征，并将其与高效点云处理网络的空间特征进一步融合。利用特征聚合将它们融合在全局到局部的策略中，以实现紧凑的分割特征。实验结果表明，与基于NeRF的方法相比，我们的模型在语义和全景零样本分割任务上都优于基线，同时消耗不到10%的推理时间。代码和更多结果将在https://David-Dou.github.io/CoSSegGaussians</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05925v3" target="_blank">2401.05925v3</a>
                              </td>
                              <td>CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians with Dual Feature Fusion</td>
                              <td>Bin Dou</td>
                              <td>2024-01-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05925v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05925v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14555v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Revisiting Active Learning in the Era of Vision Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14555v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14555v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14555v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Foundation vision or vision-language models are trained on large unlabeled or noisy data and learn robust representations that can achieve impressive zero- or few-shot performance on diverse tasks. Given these properties, they are a natural fit for active learning (AL), which aims to maximize labeling efficiency, but the full potential of foundation models has not been explored in the context of AL, specifically in the low-budget regime. In this work, we evaluate how foundation models influence three critical components of effective AL, namely, 1) initial labeled pool selection, 2) ensuring diverse sampling, and 3) the trade-off between representative and uncertainty sampling. We systematically study how the robust representations of foundation models (DINOv2, OpenCLIP) challenge existing findings in active learning. Our observations inform the principled construction of a new simple and elegant AL strategy that balances uncertainty estimated via dropout with sample diversity. We extensively test our strategy on many challenging image classification benchmarks, including natural images as well as out-of-domain biomedical images that are relatively understudied in the AL literature. Source code will be made available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14555v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基础视觉或视觉语言模型在大的未标记或有噪声的数据上进行训练，并学习稳健的表示，这些表示可以在不同的任务上实现令人印象深刻的零或少镜头性能。考虑到这些特性，它们自然适合主动学习（AL），其目的是最大限度地提高标签效率，但尚未在主动学习的背景下，特别是在低预算制度下，探索基础模型的全部潜力。在这项工作中，我们评估了基础模型如何影响有效AL的三个关键组成部分，即1）初始标记池选择，2）确保多样化采样，以及3）代表性采样和不确定性采样之间的权衡。我们系统地研究了基础模型（DINOv2，OpenCLIP）的鲁棒表示如何挑战主动学习中的现有发现。我们的观察结果为一种新的简单而优雅的AL策略的原则性构建提供了信息，该策略平衡了通过丢弃估计的不确定性和样本多样性。我们在许多具有挑战性的图像分类基准上广泛测试了我们的策略，包括自然图像以及AL文献中研究相对不足的领域外生物医学图像。将提供源代码。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14555v1" target="_blank">2401.14555v1</a>
                              </td>
                              <td>Revisiting Active Learning in the Era of Vision Foundation Models</td>
                              <td>Sanket Rajan Gupte</td>
                              <td>2024-01-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14555v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14555v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/tempconfx/al-foundation-models" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14159v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14159v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14159v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14159v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce Grounded SAM, which uses Grounding DINO as an open-set object detector to combine with the segment anything model (SAM). This integration enables the detection and segmentation of any regions based on arbitrary text inputs and opens a door to connecting various vision models. As shown in Fig.1, a wide range of vision tasks can be achieved by using the versatile Grounded SAM pipeline. For example, an automatic annotation pipeline based solely on input images can be realized by incorporating models such as BLIP and Recognize Anything. Additionally, incorporating Stable-Diffusion allows for controllable image editing, while the integration of OSX facilitates promptable 3D human motion analysis. Grounded SAM also shows superior performance on open-vocabulary benchmarks, achieving 48.7 mean AP on SegInW (Segmentation in the wild) zero-shot benchmark with the combination of Grounding DINO-Base and SAM-Huge models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14159v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了Grounded SAM，它使用Grounding DINO作为开集对象检测器与分段任意模型（SAM）相结合。这种集成能够基于任意文本输入检测和分割任何区域，并为连接各种视觉模型打开了大门。如图1所示，通过使用通用的接地SAM管道，可以实现广泛的视觉任务。例如，可以通过结合诸如BLIP和Recognize Anything之类的模型来实现仅基于输入图像的自动注释流水线。此外，结合Stable Diffusion可以进行可控的图像编辑，而OSX的集成有助于快速进行3D人体运动分析。Grounded SAM在开放词汇基准测试中也表现出优异的性能，在SegInW（野外细分）零样本基准测试中，通过Grounding DINO-Base和SAM-Huge模型的组合，达到了48.7的平均AP。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14159v1" target="_blank">2401.14159v1</a>
                              </td>
                              <td>Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks</td>
                              <td>Tianhe Ren</td>
                              <td>2024-01-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14159v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14159v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/IDEA-Research/Grounded-Segment-Anything" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_13987v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Cross-Domain Few-Shot Learning via Adaptive Transformer Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_13987v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_13987v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_13987v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Most few-shot learning works rely on the same domain assumption between the base and the target tasks, hindering their practical applications. This paper proposes an adaptive transformer network (ADAPTER), a simple but effective solution for cross-domain few-shot learning where there exist large domain shifts between the base task and the target task. ADAPTER is built upon the idea of bidirectional cross-attention to learn transferable features between the two domains. The proposed architecture is trained with DINO to produce diverse, and less biased features to avoid the supervision collapse problem. Furthermore, the label smoothing approach is proposed to improve the consistency and reliability of the predictions by also considering the predicted labels of the close samples in the embedding space. The performance of ADAPTER is rigorously evaluated in the BSCD-FSL benchmarks in which it outperforms prior arts with significant margins.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_13987v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大多数少数镜头学习工作依赖于基础任务和目标任务之间的相同领域假设，阻碍了它们的实际应用。本文提出了一种自适应变换网络（ADAPTER），这是一种简单但有效的跨域少镜头学习解决方案，其中基本任务和目标任务之间存在较大的域偏移。ADAPTER建立在双向交叉注意力的思想之上，以学习两个领域之间的可转移特征。所提出的体系结构是用DINO训练的，以产生多样化的、较少偏见的特征，从而避免监督崩溃的问题。此外，还提出了标签平滑方法，通过考虑嵌入空间中紧密样本的预测标签来提高预测的一致性和可靠性。ADAPTER的性能在BSCD-FSL基准中得到了严格评估，在这些基准中，它以显著的优势优于现有技术。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.13987v1" target="_blank">2401.13987v1</a>
                              </td>
                              <td>Cross-Domain Few-Shot Learning via Adaptive Transformer Networks</td>
                              <td>Naeem Paeedeh</td>
                              <td>2024-01-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_13987v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.13987v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/naeem-paeedeh/adapter" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_11673v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MVSFormer++: Revealing the Devil in Transformer's Details for Multi-View Stereo</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_11673v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_11673v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_11673v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advancements in learning-based Multi-View Stereo (MVS) methods have prominently featured transformer-based models with attention mechanisms. However, existing approaches have not thoroughly investigated the profound influence of transformers on different MVS modules, resulting in limited depth estimation capabilities. In this paper, we introduce MVSFormer++, a method that prudently maximizes the inherent characteristics of attention to enhance various components of the MVS pipeline. Formally, our approach involves infusing cross-view information into the pre-trained DINOv2 model to facilitate MVS learning. Furthermore, we employ different attention mechanisms for the feature encoder and cost volume regularization, focusing on feature and spatial aggregations respectively. Additionally, we uncover that some design details would substantially impact the performance of transformer modules in MVS, including normalized 3D positional encoding, adaptive attention scaling, and the position of layer normalization. Comprehensive experiments on DTU, Tanks-and-Temples, BlendedMVS, and ETH3D validate the effectiveness of the proposed method. Notably, MVSFormer++ achieves state-of-the-art performance on the challenging DTU and Tanks-and-Temples benchmarks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_11673v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于学习的多视图立体（MVS）方法的最新进展突出了具有注意力机制的基于变换器的模型。然而，现有的方法没有彻底研究变压器对不同MVS模块的深刻影响，导致深度估计能力有限。在本文中，我们介绍了MVSFormer++，这是一种谨慎地最大化注意力的固有特性以增强MVS管道的各个组件的方法。从形式上讲，我们的方法包括将跨视图信息注入预先训练的DINOv2模型中，以促进MVS学习。此外，我们对特征编码器和代价体积正则化采用了不同的注意力机制，分别关注特征和空间聚合。此外，我们发现一些设计细节会显著影响MVS中转换器模块的性能，包括归一化的3D位置编码、自适应注意力缩放和层归一化的位置。在DTU、Tanks and Temples、BlendedMVS和ETH3D上进行的综合实验验证了该方法的有效性。值得注意的是，MVSFormer++在具有挑战性的DTU和坦克与神庙基准上实现了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.11673v1" target="_blank">2401.11673v1</a>
                              </td>
                              <td>MVSFormer++: Revealing the Devil in Transformer's Details for Multi-View Stereo</td>
                              <td>Chenjie Cao</td>
                              <td>2024-01-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_11673v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.11673v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_11311v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Novel Benchmark for Few-Shot Semantic Segmentation in the Era of Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_11311v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_11311v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_11311v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, the rapid evolution of computer vision has seen the emergence of various vision foundation models, each tailored to specific data types and tasks. While large language models often share a common pretext task, the diversity in vision foundation models arises from their varying training objectives. In this study, we delve into the quest for identifying the most effective vision foundation models for few-shot semantic segmentation, a critical task in computer vision. Specifically, we conduct a comprehensive comparative analysis of four prominent foundation models: DINO V2, Segment Anything, CLIP, Masked AutoEncoders, and a straightforward ResNet50 pre-trained on the COCO dataset. Our investigation focuses on their adaptability to new semantic segmentation tasks, leveraging only a limited number of segmented images. Our experimental findings reveal that DINO V2 consistently outperforms the other considered foundation models across a diverse range of datasets and adaptation methods. This outcome underscores DINO V2's superior capability to adapt to semantic segmentation tasks compared to its counterparts. Furthermore, our observations indicate that various adapter methods exhibit similar performance, emphasizing the paramount importance of selecting a robust feature extractor over the intricacies of the adaptation technique itself. This insight sheds light on the critical role of feature extraction in the context of few-shot semantic segmentation. This research not only contributes valuable insights into the comparative performance of vision foundation models in the realm of few-shot semantic segmentation but also highlights the significance of a robust feature extractor in this domain.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_11311v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，随着计算机视觉的快速发展，出现了各种视觉基础模型，每种模型都适合特定的数据类型和任务。虽然大型语言模型通常有一个共同的借口任务，但视觉基础模型的多样性源于它们不同的训练目标。在这项研究中，我们深入探讨了为少镜头语义分割（计算机视觉中的一项关键任务）确定最有效的视觉基础模型的探索。具体而言，我们对四个突出的基础模型进行了全面的比较分析：DINO V2、Segment Anything、CLIP、Masked AutoEncoders和在COCO数据集上预训练的直接ResNet50。我们的研究重点是它们对新的语义分割任务的适应性，仅利用有限数量的分割图像。我们的实验结果表明，在各种数据集和适应方法中，DINO V2始终优于其他考虑的基础模型。这一结果突显了与同类产品相比，DINO V2在适应语义分割任务方面的卓越能力。此外，我们的观察结果表明，各种适配器方法表现出相似的性能，强调了选择鲁棒特征提取器的至关重要性，而不是自适应技术本身的复杂性。这一见解揭示了特征提取在少镜头语义分割中的关键作用。这项研究不仅为视觉基础模型在少镜头语义分割领域的比较性能提供了有价值的见解，而且突出了鲁棒特征提取器在该领域的重要性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.11311v1" target="_blank">2401.11311v1</a>
                              </td>
                              <td>A Novel Benchmark for Few-Shot Semantic Segmentation in the Era of Foundation Models</td>
                              <td>Reda Bensaid</td>
                              <td>2024-01-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_11311v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.11311v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_10815v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RAD-DINO: Exploring Scalable Medical Image Encoders Beyond Text Supervision</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_10815v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_10815v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_10815v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Language-supervised pre-training has proven to be a valuable method for extracting semantically meaningful features from images, serving as a foundational element in multimodal systems within the computer vision and medical imaging domains. However, resulting features are limited by the information contained within the text. This is particularly problematic in medical imaging, where radiologists' written findings focus on specific observations; a challenge compounded by the scarcity of paired imaging-text data due to concerns over leakage of personal health information. In this work, we fundamentally challenge the prevailing reliance on language supervision for learning general purpose biomedical imaging encoders. We introduce RAD-DINO, a biomedical image encoder pre-trained solely on unimodal biomedical imaging data that obtains similar or greater performance than state-of-the-art biomedical language supervised models on a diverse range of benchmarks. Specifically, the quality of learned representations is evaluated on standard imaging tasks (classification and semantic segmentation), and a vision-language alignment task (text report generation from images). To further demonstrate the drawback of language supervision, we show that features from RAD-DINO correlate with other medical records (e.g., sex or age) better than language-supervised models, which are generally not mentioned in radiology reports. Finally, we conduct a series of ablations determining the factors in RAD-DINO's performance; notably, we observe that RAD-DINO's downstream performance scales well with the quantity and diversity of training data, demonstrating that image-only supervision is a scalable approach for training a foundational biomedical image encoder.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_10815v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语言监督预训练已被证明是从图像中提取语义上有意义的特征的一种有价值的方法，是计算机视觉和医学成像领域中多模式系统的基础元素。但是，生成的特征受到文本中包含的信息的限制。这在医学成像中尤其有问题，因为放射科医生的书面发现侧重于特定的观察结果；由于担心个人健康信息的泄露，配对成像文本数据的稀缺性加剧了这一挑战。在这项工作中，我们从根本上挑战了学习通用生物医学成像编码器时普遍依赖语言监督的现状。我们介绍了RAD-DINO，这是一种仅在单峰生物医学成像数据上预训练的生物医学图像编码器，在各种基准上获得与最先进的生物医学语言监督模型相似或更高的性能。具体而言，学习表示的质量是在标准成像任务（分类和语义分割）和视觉语言对齐任务（从图像生成文本报告）上评估的。为了进一步证明语言监督的缺点，我们发现RAD-DINO的特征与其他医疗记录（如性别或年龄）的相关性比语言监督模型更好，而语言监督模型通常在放射学报告中没有提及。最后，我们进行了一系列烧蚀，确定了影响RAD-DINO性能的因素；值得注意的是，我们观察到RAD-DINO的下游性能随着训练数据的数量和多样性而良好地扩展，这表明仅图像监督是训练基础生物医学图像编码器的一种可扩展方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.10815v1" target="_blank">2401.10815v1</a>
                              </td>
                              <td>RAD-DINO: Exploring Scalable Medical Image Encoders Beyond Text Supervision</td>
                              <td>Fernando Pérez-García</td>
                              <td>2024-01-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_10815v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.10815v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_07951v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Image Similarity using An Ensemble of Context-Sensitive Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_07951v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_07951v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_07951v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Image similarity has been extensively studied in computer vision. In recently years, machine-learned models have shown their ability to encode more semantics than traditional multivariate metrics. However, in labelling similarity, assigning a numerical score to a pair of images is less intuitive than determining if an image A is closer to a reference image R than another image B. In this work, we present a novel approach for building an image similarity model based on labelled data in the form of A:R vs B:R. We address the challenges of sparse sampling in the image space (R, A, B) and biases in the models trained with context-based data by using an ensemble model. In particular, we employed two ML techniques to construct such an ensemble model, namely dimensionality reduction and MLP regressors. Our testing results show that the ensemble model constructed performs ~5% better than the best individual context-sensitive models. They also performed better than the model trained with mixed imagery data as well as existing similarity models, e.g., CLIP and DINO. This work demonstrate that context-based labelling and model training can be effective when an appropriate ensemble approach is used to alleviate the limitation due to sparse sampling.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_07951v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>图像相似性在计算机视觉中得到了广泛的研究。近年来，机器学习模型已经显示出其比传统的多元度量编码更多语义的能力。然而，在标记相似性时，将数字分数分配给一对图像不如确定一个图像a是否比另一个图像B更接近参考图像R直观。在这项工作中，我们提出了一种基于a:R与B:R形式的标记数据构建图像相似性模型的新方法。我们通过使用集成模型来解决图像空间（R，A，B）中的稀疏采样和使用基于上下文的数据训练的模型中的偏差的挑战。特别地，我们使用了两种ML技术来构建这样的集成模型，即降维和MLP回归。我们的测试结果表明，所构建的集成模型的性能比最好的单个上下文敏感模型好约5%。它们的性能也优于用混合图像数据训练的模型以及现有的相似性模型，例如CLIP和DINO。这项工作表明，当使用适当的集成方法来缓解稀疏采样带来的限制时，基于上下文的标记和模型训练是有效的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.07951v1" target="_blank">2401.07951v1</a>
                              </td>
                              <td>Image Similarity using An Ensemble of Context-Sensitive Models</td>
                              <td>Zukang Liao</td>
                              <td>2024-01-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_07951v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.07951v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_06013v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Surgical-DINO: Adapter Learning of Foundation Models for Depth Estimation in Endoscopic Surgery</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_06013v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_06013v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_06013v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Purpose: Depth estimation in robotic surgery is vital in 3D reconstruction, surgical navigation and augmented reality visualization. Although the foundation model exhibits outstanding performance in many vision tasks, including depth estimation (e.g., DINOv2), recent works observed its limitations in medical and surgical domain-specific applications. This work presents a low-ranked adaptation (LoRA) of the foundation model for surgical depth estimation. Methods: We design a foundation model-based depth estimation method, referred to as Surgical-DINO, a low-rank adaptation of the DINOv2 for depth estimation in endoscopic surgery. We build LoRA layers and integrate them into DINO to adapt with surgery-specific domain knowledge instead of conventional fine-tuning. During training, we freeze the DINO image encoder, which shows excellent visual representation capacity, and only optimize the LoRA layers and depth decoder to integrate features from the surgical scene. Results: Our model is extensively validated on a MICCAI challenge dataset of SCARED, which is collected from da Vinci Xi endoscope surgery. We empirically show that Surgical-DINO significantly outperforms all the state-of-the-art models in endoscopic depth estimation tasks. The analysis with ablation studies has shown evidence of the remarkable effect of our LoRA layers and adaptation. Conclusion: Surgical-DINO shed some light on the successful adaptation of the foundation models into the surgical domain for depth estimation. There is clear evidence in the results that zero-shot prediction on pre-trained weights in computer vision datasets or naive fine-tuning is not sufficient to use the foundation model in the surgical domain directly. Code is available at https://github.com/BeileiCui/SurgicalDINO.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_06013v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>目的：机器人手术中的深度估计在三维重建、手术导航和增强现实可视化中至关重要。尽管基础模型在许多视觉任务中表现出出色的性能，包括深度估计（例如，DINOv2），但最近的工作观察到其在医学和外科领域特定应用中的局限性。这项工作提出了一种用于手术深度估计的基础模型的低阶自适应（LoRA）。方法：我们设计了一种基于基础模型的深度估计方法，称为Surgical DINO，这是对DINOv2的低阶自适应，用于内窥镜手术中的深度估计。我们构建了LoRA层，并将其集成到DINO中，以适应手术特定领域的知识，而不是传统的微调。在训练过程中，我们冻结了显示出出色视觉表示能力的DINO图像编码器，并仅优化了LoRA层和深度解码器，以集成来自手术场景的特征。结果：我们的模型在SCARED的MICCAI挑战数据集上得到了广泛验证，该数据集是从达芬奇Xi内窥镜手术中收集的。我们的经验表明，外科DINO在内窥镜深度估计任务中显著优于所有最先进的模型。消融研究的分析表明，我们的LoRA层和适应具有显著效果。结论：外科DINO为基础模型成功适应外科领域进行深度估计提供了一些启示。结果中有明确证据表明，对计算机视觉数据集中预先训练的权重进行零样本预测或简单微调不足以直接在外科领域使用基础模型。代码位于https://github.com/BeileiCui/SurgicalDINO.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.06013v2" target="_blank">2401.06013v2</a>
                              </td>
                              <td>Surgical-DINO: Adapter Learning of Foundation Models for Depth Estimation in Endoscopic Surgery</td>
                              <td>Beilei Cui</td>
                              <td>2024-01-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_06013v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.06013v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/beileicui/surgicaldino" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_14093v4_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Weakly Supervised 3D Open-vocabulary Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_14093v4_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_14093v4_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_14093v4_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting pre-trained foundation models CLIP and DINO in a weakly supervised manner. Specifically, given only the open-vocabulary text descriptions of the objects in a scene, we distill the open-vocabulary multimodal knowledge and object reasoning capability of CLIP and DINO into a neural radiance field (NeRF), which effectively lifts 2D features into view-consistent 3D segmentation. A notable aspect of our approach is that it does not require any manual segmentation annotations for either the foundation models or the distillation process. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations in certain scenes, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs. Code is available at \url{https://github.com/Kunhao-Liu/3D-OVS}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_14093v4_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三维场景的开放式词汇分割是人类感知的基本功能，因此也是计算机视觉研究的重要目标。然而，由于缺乏用于训练健壮和可推广模型的大规模和多样化的3D开放词汇分割数据集，这项任务受到了严重阻碍。从预先训练的2D开放词汇分割模型中提取知识有帮助，但它损害了开放词汇的特征，因为2D模型大多是用紧密的词汇数据集进行微调的。我们通过以弱监督的方式利用预先训练的基础模型CLIP和DINO来解决3D开放词汇分割中的挑战。具体而言，仅给定场景中对象的开放词汇文本描述，我们将CLIP和DINO的开放词汇多模态知识和对象推理能力提取到神经辐射场（NeRF）中，这有效地将2D特征提升到视图一致的3D分割中。我们的方法的一个值得注意的方面是，它不需要对基础模型或蒸馏过程进行任何手动分割注释。大量实验表明，在某些场景中，我们的方法甚至优于用分割注释训练的完全监督模型，这表明可以从2D图像和文本图像对中有效地学习3D开放词汇分割。代码位于\url{https://github.com/Kunhao-Liu/3D-OVS}.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.14093v4" target="_blank">2305.14093v4</a>
                              </td>
                              <td>Weakly Supervised 3D Open-vocabulary Segmentation</td>
                              <td>Kunhao Liu</td>
                              <td>2023-05-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_14093v4_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.14093v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/kunhao-liu/3d-ovs" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02957v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Denoising Vision Transformers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02957v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02957v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02957v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We delve into a nuanced but significant challenge inherent to Vision Transformers (ViTs): feature maps of these models exhibit grid-like artifacts, which detrimentally hurt the performance of ViTs in downstream tasks. Our investigations trace this fundamental issue down to the positional embeddings at the input stage. To address this, we propose a novel noise model, which is universally applicable to all ViTs. Specifically, the noise model dissects ViT outputs into three components: a semantics term free from noise artifacts and two artifact-related terms that are conditioned on pixel locations. Such a decomposition is achieved by enforcing cross-view feature consistency with neural fields in a per-image basis. This per-image optimization process extracts artifact-free features from raw ViT outputs, providing clean features for offline applications. Expanding the scope of our solution to support online functionality, we introduce a learnable denoiser to predict artifact-free features directly from unprocessed ViT outputs, which shows remarkable generalization capabilities to novel data without the need for per-image optimization. Our two-stage approach, termed Denoising Vision Transformers (DVT), does not require re-training existing pre-trained ViTs and is immediately applicable to any Transformer-based architecture. We evaluate our method on a variety of representative ViTs (DINO, MAE, DeiT-III, EVA02, CLIP, DINOv2, DINOv2-reg). Extensive evaluations demonstrate that our DVT consistently and significantly improves existing state-of-the-art general-purpose models in semantic and geometric tasks across multiple datasets (e.g., +3.84 mIoU). We hope our study will encourage a re-evaluation of ViT design, especially regarding the naive use of positional embeddings.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02957v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们深入研究了视觉转换器（ViTs）固有的一个细微但重大的挑战：这些模型的特征图显示出网格状的伪影，这对ViTs在下游任务中的性能造成了不利影响。我们的研究将这个基本问题追溯到输入阶段的位置嵌入。为了解决这一问题，我们提出了一种新的噪声模型，该模型普遍适用于所有的ViT。具体来说，噪声模型将ViT输出分解为三个部分：一个没有噪声伪影的语义术语和两个以像素位置为条件的伪影相关术语。这种分解是通过在每幅图像的基础上加强与神经场的交叉视图特征一致性来实现的。这种逐图像优化过程从原始ViT输出中提取无伪影特征，为离线应用程序提供干净的特征。扩大了我们的解决方案的范围，以支持在线功能，我们引入了一种可学习的去噪器，直接从未处理的ViT输出中预测无伪影特征，这显示出对新数据的显著泛化能力，而无需对每张图像进行优化。我们的两阶段方法，称为去噪视觉转换器（DVT），不需要重新训练现有的预先训练的ViT，并且立即适用于任何基于转换器的架构。我们在各种具有代表性的ViT（DINO、MAE、DeiT III、EVA02、CLIP、DINOv2、DINOv2-reg）上评估了我们的方法。广泛的评估表明，我们的DVT在多个数据集（例如+3.84mIoU）的语义和几何任务中持续显著地改进了现有的最先进的通用模型。我们希望我们的研究将鼓励对ViT设计进行重新评估，特别是关于位置嵌入的天真使用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02957v1" target="_blank">2401.02957v1</a>
                              </td>
                              <td>Denoising Vision Transformers</td>
                              <td>Jiawei Yang</td>
                              <td>2024-01-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02957v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02957v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02361v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">An Open and Comprehensive Pipeline for Unified Object Grounding and Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02361v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02361v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02361v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Grounding-DINO is a state-of-the-art open-set detection model that tackles multiple vision tasks including Open-Vocabulary Detection (OVD), Phrase Grounding (PG), and Referring Expression Comprehension (REC). Its effectiveness has led to its widespread adoption as a mainstream architecture for various downstream applications. However, despite its significance, the original Grounding-DINO model lacks comprehensive public technical details due to the unavailability of its training code. To bridge this gap, we present MM-Grounding-DINO, an open-source, comprehensive, and user-friendly baseline, which is built with the MMDetection toolbox. It adopts abundant vision datasets for pre-training and various detection and grounding datasets for fine-tuning. We give a comprehensive analysis of each reported result and detailed settings for reproduction. The extensive experiments on the benchmarks mentioned demonstrate that our MM-Grounding-DINO-Tiny outperforms the Grounding-DINO-Tiny baseline. We release all our models to the research community. Codes and trained models are released at https://github.com/open-mmlab/mmdetection/tree/main/configs/mm_grounding_dino.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02361v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Grounding DINO是一种最先进的开放集检测模型，可处理多种视觉任务，包括开放词汇检测（OVD）、短语基础（PG）和参考表达理解（REC）。它的有效性导致它被广泛采用为各种下游应用程序的主流架构。然而，尽管其意义重大，但由于其培训代码的不可用，最初的Grounding DINO模型缺乏全面的公共技术细节。为了弥补这一差距，我们推出了MM Grounding DINO，这是一个开源、全面、用户友好的基线，它是用MMDetection工具箱构建的。它采用丰富的视觉数据集进行预训练，并采用各种检测和基础数据集进行微调。我们对每一个报告的结果进行了全面的分析，并对复制进行了详细的设置。对上述基准的广泛实验表明，我们的MM Grounding DINO Tiny优于Grounding DINO Tiny基线。我们向研究界发布所有模型。代码和经过训练的模型发布于https://github.com/open-mmlab/mmdetection/tree/main/configs/mm_grounding_dino.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02361v2" target="_blank">2401.02361v2</a>
                              </td>
                              <td>An Open and Comprehensive Pipeline for Unified Object Grounding and Detection</td>
                              <td>Xiangyu Zhao</td>
                              <td>2024-01-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02361v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02361v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/open-mmlab/mmdetection" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_12735v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fast-iTPN: Integrally Pre-Trained Transformer Pyramid Network with Token Migration</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_12735v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_12735v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_12735v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose integrally pre-trained transformer pyramid network (iTPN), towards jointly optimizing the network backbone and the neck, so that transfer gap between representation models and downstream tasks is minimal. iTPN is born with two elaborated designs: 1) The first pre-trained feature pyramid upon vision transformer (ViT). 2) Multi-stage supervision to the feature pyramid using masked feature modeling (MFM). iTPN is updated to Fast-iTPN, reducing computational memory overhead and accelerating inference through two flexible designs. 1) Token migration: dropping redundant tokens of the backbone while replenishing them in the feature pyramid without attention operations. 2) Token gathering: reducing computation cost caused by global attention by introducing few gathering tokens. The base/large-level Fast-iTPN achieve 88.75%/89.5% top-1 accuracy on ImageNet-1K. With 1x training schedule using DINO, the base/large-level Fast-iTPN achieves 58.4%/58.8% box AP on COCO object detection, and a 57.5%/58.7% mIoU on ADE20K semantic segmentation using MaskDINO. Fast-iTPN can accelerate the inference procedure by up to 70%, with negligible performance loss, demonstrating the potential to be a powerful backbone for downstream vision tasks. The code is available at: github.com/sunsmarterjie/iTPN.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_12735v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了整体预训练的变换金字塔网络（iTPN），以联合优化网络主干和瓶颈，从而使表示模型和下游任务之间的传输间隙最小。iTPN诞生于两个精心设计：1）第一个预先训练的视觉转换器上的特征金字塔（ViT）。2） 使用掩蔽特征建模（MFM）对特征金字塔进行多阶段监督。iTPN更新为Fast iTPN，通过两种灵活的设计减少了计算内存开销并加速了推理。1） 令牌迁移：丢弃主干的冗余令牌，同时在功能金字塔中补充它们，而无需注意操作。2） 代币采集：通过引入少量采集代币，降低全球关注带来的计算成本。基本/大级别Fast iTPN在ImageNet-1K上实现了88.75%/89.5%的前1级精度。在使用DINO的1x训练计划的情况下，基本/大级别Fast iTPN在COCO对象检测上实现了58.4%/58.8%的box AP，在使用MaskDINO的ADE20K语义分割上实现了57.5%/58.7%的mIoU。快速iTPN可以将推理过程加速70%，而性能损失可以忽略不计，这表明它有潜力成为下游视觉任务的强大支柱。该代码位于：github.com/sunsmarterjie/iTPN。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.12735v2" target="_blank">2211.12735v2</a>
                              </td>
                              <td>Fast-iTPN: Integrally Pre-Trained Transformer Pyramid Network with Token Migration</td>
                              <td>Yunjie Tian</td>
                              <td>2022-11-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_12735v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.12735v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/sunsmarterjie/itpn" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_01013v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Boosting Transformer's Robustness and Efficacy in PPG Signal Artifact Detection with Self-Supervised Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_01013v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_01013v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_01013v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent research at CHU Sainte Justine's Pediatric Critical Care Unit (PICU) has revealed that traditional machine learning methods, such as semi-supervised label propagation and K-nearest neighbors, outperform Transformer-based models in artifact detection from PPG signals, mainly when data is limited. This study addresses the underutilization of abundant unlabeled data by employing self-supervised learning (SSL) to extract latent features from these data, followed by fine-tuning on labeled data. Our experiments demonstrate that SSL significantly enhances the Transformer model's ability to learn representations, improving its robustness in artifact classification tasks. Among various SSL techniques, including masking, contrastive learning, and DINO (self-distillation with no labels)-contrastive learning exhibited the most stable and superior performance in small PPG datasets. Further, we delve into optimizing contrastive loss functions, which are crucial for contrastive SSL. Inspired by InfoNCE, we introduce a novel contrastive loss function that facilitates smoother training and better convergence, thereby enhancing performance in artifact classification. In summary, this study establishes the efficacy of SSL in leveraging unlabeled data, particularly in enhancing the capabilities of the Transformer model. This approach holds promise for broader applications in PICU environments, where annotated data is often limited.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_01013v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>CHU Sainte Justine儿科重症监护室（PICU）最近的研究表明，传统的机器学习方法，如半监督标签传播和K近邻，在PPG信号的伪影检测方面，主要是在数据有限的情况下，优于基于Transformer的模型。这项研究通过使用自监督学习（SSL）从这些数据中提取潜在特征，然后对标记数据进行微调，解决了大量未标记数据利用不足的问题。我们的实验表明，SSL显著增强了Transformer模型学习表示的能力，提高了其在工件分类任务中的稳健性。在各种SSL技术中，包括掩蔽、对比学习和DINO（无标签的自蒸馏），对比学习在小型PPG数据集中表现出最稳定和优越的性能。此外，我们深入研究了优化对比损失函数，这对对比SSL至关重要。受InfoNCE的启发，我们引入了一种新的对比损失函数，该函数有助于更平滑的训练和更好的收敛，从而提高伪像分类的性能。总之，本研究确定了SSL在利用未标记数据方面的有效性，特别是在增强Transformer模型的能力方面。这种方法有望在PICU环境中获得更广泛的应用，在PICU中，注释数据通常是有限的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.01013v1" target="_blank">2401.01013v1</a>
                              </td>
                              <td>Boosting Transformer's Robustness and Efficacy in PPG Signal Artifact Detection with Self-Supervised Learning</td>
                              <td>Thanh-Dung Le</td>
                              <td>2024-01-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_01013v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.01013v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_00463v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Analyzing Local Representations of Self-supervised Vision Transformers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_00463v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_00463v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_00463v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we present a comparative analysis of various self-supervised Vision Transformers (ViTs), focusing on their local representative power. Inspired by large language models, we examine the abilities of ViTs to perform various computer vision tasks with little to no fine-tuning. We design an evaluation framework to analyze the quality of local, i.e. patch-level, representations in the context of few-shot semantic segmentation, instance identification, object retrieval, and tracking. We discover that contrastive learning based methods like DINO produce more universal patch representations that can be immediately applied for downstream tasks with no parameter tuning, compared to masked image modeling. The embeddings learned using the latter approach, e.g. in masked autoencoders, have high variance features that harm distance-based algorithms, such as k-NN, and do not contain useful information for most downstream tasks. Furthermore, we demonstrate that removing these high-variance features enhances k-NN by providing an analysis of the benchmarks for this work and for Scale-MAE, a recent extension of masked autoencoders. Finally, we find an object instance retrieval setting where DINOv2, a model pretrained on two orders of magnitude more data, performs worse than its less compute-intensive counterpart DINO.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_00463v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们对各种自监督视觉变压器（ViT）进行了比较分析，重点分析了它们的局部代表性。受大型语言模型的启发，我们研究了ViT在几乎没有微调的情况下执行各种计算机视觉任务的能力。我们设计了一个评估框架来分析局部（即补丁级别）表示在少镜头语义分割、实例识别、对象检索和跟踪背景下的质量。我们发现，与掩蔽图像建模相比，基于对比学习的方法（如DINO）产生了更通用的补丁表示，可以立即应用于下游任务，而无需参数调整。使用后一种方法学习的嵌入，例如在掩码自动编码器中，具有高方差特征，这会损害基于距离的算法，例如k-NN，并且不包含用于大多数下游任务的有用信息。此外，我们证明，通过为这项工作和Scale MAE（掩蔽自动编码器的最近扩展）提供基准分析，去除这些高方差特征可以增强k-NN。最后，我们找到了一个对象实例检索设置，其中DINOv2，一个在两个数量级以上的数据上预训练的模型，其性能比计算密集度较低的对应DINO差。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.00463v1" target="_blank">2401.00463v1</a>
                              </td>
                              <td>Analyzing Local Representations of Self-supervised Vision Transformers</td>
                              <td>Ani Vanyan</td>
                              <td>2023-12-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_00463v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.00463v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_03940v4_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Hard View Selection for Self-Supervised Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_03940v4_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_03940v4_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_03940v4_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Many Self-Supervised Learning (SSL) methods train their models to be invariant to different "views" of an image input for which a good data augmentation pipeline is crucial. While considerable efforts were directed towards improving pre-text tasks, architectures, or robustness (e.g., Siamese networks or teacher-softmax centering), the majority of these methods remain strongly reliant on the random sampling of operations within the image augmentation pipeline, such as the random resized crop or color distortion operation. In this paper, we argue that the role of the view generation and its effect on performance has so far received insufficient attention. To address this, we propose an easy, learning-free, yet powerful Hard View Selection (HVS) strategy designed to extend the random view generation to expose the pretrained model to harder samples during SSL training. It encompasses the following iterative steps: 1) randomly sample multiple views and create pairs of two views, 2) run forward passes for each view pair on the currently trained model, 3) adversarially select the pair yielding the worst loss, and 4) run the backward pass with the selected pair. In our empirical analysis we show that under the hood, HVS increases task difficulty by controlling the Intersection over Union of views during pretraining. With only 300-epoch pretraining, HVS is able to closely rival the 800-epoch DINO baseline which remains very favorable even when factoring in the slowdown induced by the additional forwards of HVS. Additionally, HVS consistently achieves accuracy improvements on ImageNet between 0.4% and 1.9% on linear evaluation and similar improvements on transfer tasks across multiple SSL methods, such as DINO, SimSiam, iBOT, and SimCLR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_03940v4_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>许多自监督学习（SSL）方法将其模型训练为对图像输入的不同“视图”保持不变，而良好的数据增强管道对图像输入至关重要。虽然在改进文本前任务、架构或稳健性（例如，连体网络或教师softmax居中）方面做出了相当大的努力，但这些方法中的大多数仍然强烈依赖于图像增强管道内的操作的随机采样，例如随机调整大小的裁剪或颜色失真操作。在本文中，我们认为到目前为止，视图生成的作用及其对性能的影响还没有得到足够的关注。为了解决这一问题，我们提出了一种简单、无需学习但功能强大的硬视图选择（HVS）策略，旨在扩展随机视图生成，以便在SSL训练期间将预训练的模型暴露给更硬的样本。它包括以下迭代步骤：1）随机采样多个视图并创建两个视图对，2）在当前训练的模型上为每个视图对运行前向通道，3）对抗性地选择产生最差损失的一对，以及4）使用所选的一对运行后向通道。在我们的实证分析中，我们发现在引擎盖下，HVS通过在预训练过程中控制视图并集上的交集来增加任务难度。只有300个历元的预训练，HVS能够与800个历元DINO基线相媲美，即使考虑到HVS额外前锋导致的速度减慢，这一基线仍然非常有利。此外，HVS在ImageNet上的线性评估准确率持续提高0.4%至1.9%，在多种SSL方法（如DINO、SimSiam、iBOT和SimCLR）的传输任务上也实现了类似的提高。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.03940v4" target="_blank">2310.03940v4</a>
                              </td>
                              <td>Hard View Selection for Self-Supervised Learning</td>
                              <td>Fabio Ferreira</td>
                              <td>2023-10-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_03940v4_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.03940v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_18628v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Lightweight Clustering Framework for Unsupervised Semantic Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_18628v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_18628v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_18628v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Unsupervised semantic segmentation aims to categorize each pixel in an image into a corresponding class without the use of annotated data. It is a widely researched area as obtaining labeled datasets is expensive. While previous works in the field have demonstrated a gradual improvement in model accuracy, most required neural network training. This made segmentation equally expensive, especially when dealing with large-scale datasets. We thus propose a lightweight clustering framework for unsupervised semantic segmentation. We discovered that attention features of the self-supervised Vision Transformer exhibit strong foreground-background differentiability. Therefore, clustering can be employed to effectively separate foreground and background image patches. In our framework, we first perform multilevel clustering across the Dataset-level, Category-level, and Image-level, and maintain consistency throughout. Then, the binary patch-level pseudo-masks extracted are upsampled, refined and finally labeled. Furthermore, we provide a comprehensive analysis of the self-supervised Vision Transformer features and a detailed comparison between DINO and DINOv2 to justify our claims. Our framework demonstrates great promise in unsupervised semantic segmentation and achieves state-of-the-art results on PASCAL VOC and MS COCO datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_18628v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>无监督语义分割旨在将图像中的每个像素分类到相应的类别中，而不使用注释数据。这是一个广泛研究的领域，因为获得标记的数据集是昂贵的。虽然该领域先前的工作已经证明模型精度逐渐提高，但大多数都需要神经网络训练。这使得分割同样昂贵，尤其是在处理大规模数据集时。因此，我们提出了一种用于无监督语义分割的轻量级聚类框架。我们发现，自监督视觉转换器的注意力特征表现出很强的前景-背景可微性。因此，可以采用聚类来有效地分离前景和背景图像块。在我们的框架中，我们首先在数据集级别、类别级别和图像级别执行多级聚类，并始终保持一致性。然后，对提取的二进制补丁级伪掩码进行上采样、细化和最终标记。此外，我们对自监督视觉转换器的功能进行了全面分析，并对DINO和DINOv2进行了详细比较，以证明我们的说法是正确的。我们的框架在无监督语义分割方面表现出了巨大的前景，并在PASCAL VOC和MS COCO数据集上取得了最先进的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.18628v2" target="_blank">2311.18628v2</a>
                              </td>
                              <td>A Lightweight Clustering Framework for Unsupervised Semantic Segmentation</td>
                              <td>Yau Shing Jonathan Cheung</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_18628v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.18628v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_17742v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Vision from Models Rivals Learning Vision from Data</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_17742v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_17742v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_17742v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce SynCLR, a novel approach for learning visual representations exclusively from synthetic images and synthetic captions, without any real data. We synthesize a large dataset of image captions using LLMs, then use an off-the-shelf text-to-image model to generate multiple images corresponding to each synthetic caption. We perform visual representation learning on these synthetic images via contrastive learning, treating images sharing the same caption as positive pairs. The resulting representations transfer well to many downstream tasks, competing favorably with other general-purpose visual representation learners such as CLIP and DINO v2 in image classification tasks. Furthermore, in dense prediction tasks such as semantic segmentation, SynCLR outperforms previous self-supervised methods by a significant margin, e.g., improving over MAE and iBOT by 6.2 and 4.3 mIoU on ADE20k for ViT-B/16.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_17742v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了SynCLR，这是一种专门从合成图像和合成字幕中学习视觉表示的新方法，无需任何真实数据。我们使用LLM合成了一个大型的图像字幕数据集，然后使用现成的文本到图像模型来生成与每个合成字幕相对应的多个图像。我们通过对比学习对这些合成图像进行视觉表征学习，将共享同一字幕的图像视为正对。由此产生的表示很好地转移到许多下游任务，在图像分类任务中与其他通用视觉表示学习器（如CLIP和DINO v2）竞争。此外，在语义分割等密集预测任务中，SynCLR显著优于以前的自监督方法，例如，在ViT-B/16的ADE20k上比MAE和iBOT提高了6.2和4.3mIoU。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.17742v1" target="_blank">2312.17742v1</a>
                              </td>
                              <td>Learning Vision from Models Rivals Learning Vision from Data</td>
                              <td>Yonglong Tian</td>
                              <td>2023-12-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_17742v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.17742v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/google-research/syn-rep-learn" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02366v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards General Purpose Vision Foundation Models for Medical Image Analysis: An Experimental Study of DINOv2 on Radiology Benchmarks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02366v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02366v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02366v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The integration of deep learning systems into healthcare has been hindered by the resource-intensive process of data annotation and the inability of these systems to generalize to different data distributions. Foundation models, which are models pre-trained on large datasets, have emerged as a solution to reduce reliance on annotated data and enhance model generalizability and robustness. DINOv2 is an open-source foundation model pre-trained with self-supervised learning on 142 million curated natural images that exhibits promising capabilities across various vision tasks. Nevertheless, a critical question remains unanswered regarding DINOv2's adaptability to radiological imaging, and whether its features are sufficiently general to benefit radiology image analysis. Therefore, this study comprehensively evaluates DINOv2 for radiology, conducting over 100 experiments across diverse modalities (X-ray, CT, and MRI). To measure the effectiveness and generalizability of DINOv2's feature representations, we analyze the model across medical image analysis tasks including disease classification and organ segmentation on both 2D and 3D images, and under different settings like kNN, few-shot learning, linear-probing, end-to-end fine-tuning, and parameter-efficient fine-tuning. Comparative analyses with established supervised, self-supervised, and weakly-supervised models reveal DINOv2's superior performance and cross-task generalizability. The findings contribute insights to potential avenues for optimizing pre-training strategies for medical imaging and enhancing the broader understanding of DINOv2's role in bridging the gap between natural and radiological image analysis. Our code is available at https://github.com/MohammedSB/DINOv2ForRadiology</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02366v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>数据注释的资源密集型过程以及这些系统无法推广到不同的数据分布，阻碍了深度学习系统与医疗保健的集成。基础模型是在大型数据集上预先训练的模型，已成为减少对注释数据的依赖并增强模型可推广性和稳健性的解决方案。DINOv2是一个开源的基础模型，通过对1.42亿张策划的自然图像进行自我监督学习进行预训练，在各种视觉任务中表现出有希望的能力。然而，关于DINOv2对放射学成像的适应性，以及其特征是否足够通用以有利于放射学图像分析，一个关键问题仍未得到解答。因此，本研究全面评估了DINOv2的放射学，在不同的模式（X射线、CT和MRI）下进行了100多项实验。为了衡量DINOv2特征表示的有效性和可推广性，我们分析了医学图像分析任务中的模型，包括2D和3D图像上的疾病分类和器官分割，以及在不同的设置下，如kNN、少镜头学习、线性探测、端到端微调和参数有效微调。与已建立的监督、自监督和弱监督模型的比较分析揭示了DINOv2的优越性能和跨任务可推广性。这些发现有助于深入了解优化医学成像预训练策略的潜在途径，并增进对DINOv2在弥合自然图像分析和放射学图像分析之间差距方面的作用的更广泛理解。我们的代码可在https://github.com/MohammedSB/DINOv2ForRadiology</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02366v3" target="_blank">2312.02366v3</a>
                              </td>
                              <td>Towards General Purpose Vision Foundation Models for Medical Image Analysis: An Experimental Study of DINOv2 on Radiology Benchmarks</td>
                              <td>Mohammed Baharoon</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02366v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02366v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/mohammedsb/dinov2forradiology" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_17116v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Generalizable Visual Reinforcement Learning with Segment Anything Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_17116v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_17116v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_17116v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Learning policies that can generalize to unseen environments is a fundamental challenge in visual reinforcement learning (RL). While most current methods focus on acquiring robust visual representations through auxiliary supervision, pre-training, or data augmentation, the potential of modern vision foundation models remains underleveraged. In this work, we introduce Segment Anything Model for Generalizable visual RL (SAM-G), a novel framework that leverages the promptable segmentation ability of Segment Anything Model (SAM) to enhance the generalization capabilities of visual RL agents. We utilize image features from DINOv2 and SAM to find correspondence as point prompts to SAM, and then SAM produces high-quality masked images for agents directly. Evaluated across 8 DMControl tasks and 3 Adroit tasks, SAM-G significantly improves the visual generalization ability without altering the RL agents' architecture but merely their observations. Notably, SAM-G achieves 44% and 29% relative improvements on the challenging video hard setting on DMControl and Adroit respectively, compared to state-of-the-art methods. Video and code: https://yanjieze.com/SAM-G/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_17116v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>能够推广到看不见的环境中的学习策略是视觉强化学习（RL）中的一个基本挑战。虽然目前的大多数方法都侧重于通过辅助监督、预训练或数据增强来获取稳健的视觉表示，但现代视觉基础模型的潜力仍然不足。在这项工作中，我们介绍了可泛化视觉RL的分段任意模型（SAM-G），这是一个新的框架，利用分段任意模型的可提示分割能力来增强视觉RL代理的泛化能力。我们利用DINOv2和SAM的图像特征来寻找与SAM的点提示对应关系，然后SAM直接为代理生成高质量的掩码图像。在8个DMControl任务和3个Adroit任务中进行评估后，SAM-G显著提高了视觉泛化能力，而不会改变RL代理的架构，而只是改变他们的观察结果。值得注意的是，与最先进的方法相比，SAM-G在DMControl和Adroit上具有挑战性的视频硬设置上分别实现了44%和29%的相对改进。视频和代码：https://yanjieze.com/SAM-G/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.17116v1" target="_blank">2312.17116v1</a>
                              </td>
                              <td>Generalizable Visual Reinforcement Learning with Segment Anything Model</td>
                              <td>Ziyu Wang</td>
                              <td>2023-12-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_17116v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.17116v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/wadiuvatzy/sam-g" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_16084v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LangSplat: 3D Language Gaussian Splatting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_16084v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_16084v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_16084v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Human lives in a 3D world and commonly uses natural language to interact with a 3D scene. Modeling a 3D language field to support open-ended language queries in 3D has gained increasing attention recently. This paper introduces LangSplat, which constructs a 3D language field that enables precise and efficient open-vocabulary querying within 3D spaces. Unlike existing methods that ground CLIP language embeddings in a NeRF model, LangSplat advances the field by utilizing a collection of 3D Gaussians, each encoding language features distilled from CLIP, to represent the language field. By employing a tile-based splatting technique for rendering language features, we circumvent the costly rendering process inherent in NeRF. Instead of directly learning CLIP embeddings, LangSplat first trains a scene-wise language autoencoder and then learns language features on the scene-specific latent space, thereby alleviating substantial memory demands imposed by explicit modeling. Existing methods struggle with imprecise and vague 3D language fields, which fail to discern clear boundaries between objects. We delve into this issue and propose to learn hierarchical semantics using SAM, thereby eliminating the need for extensively querying the language field across various scales and the regularization of DINO features. Extensive experiments on open-vocabulary 3D object localization and semantic segmentation demonstrate that LangSplat significantly outperforms the previous state-of-the-art method LERF by a large margin. Notably, LangSplat is extremely efficient, achieving a {\speed} $\times$ speedup compared to LERF at the resolution of 1440 $\times$ 1080. We strongly recommend readers to check out our video results at https://langsplat.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_16084v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人类生活在3D世界中，通常使用自然语言与3D场景进行交互。最近，对3D语言字段进行建模以支持3D中的开放式语言查询越来越受到关注。本文介绍了LangSplat，它构建了一个三维语言字段，可以在三维空间中进行精确高效的开放式词汇查询。与现有的将CLIP语言嵌入NeRF模型的方法不同，LangSplat通过利用3D高斯集合来表示语言领域，从而推进了这一领域的发展，每个高斯集合都对从CLIP中提取的语言特征进行编码。通过使用基于瓦片的飞溅技术来渲染语言特征，我们避免了NeRF中固有的昂贵的渲染过程。LangSplat不是直接学习CLIP嵌入，而是首先训练场景式语言自动编码器，然后在特定场景的潜在空间上学习语言特征，从而减轻显式建模带来的大量内存需求。现有的方法难以处理不精确和模糊的3D语言字段，这些字段无法辨别对象之间的清晰边界。我们深入研究了这个问题，并建议使用SAM学习分层语义，从而消除了在各种规模上广泛查询语言字段和DINO特征正则化的需要。在开放词汇三维对象定位和语义分割方面的大量实验表明，LangSplat显著优于先前最先进的方法LERF。值得注意的是，LangSplat非常高效，与分辨率为1440$\times$1080的LERF相比，它实现了｛\speed｝$\times$的加速。我们强烈建议读者在上查看我们的视频结果https://langsplat.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.16084v1" target="_blank">2312.16084v1</a>
                              </td>
                              <td>LangSplat: 3D Language Gaussian Splatting</td>
                              <td>Minghan Qin</td>
                              <td>2023-12-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_16084v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.16084v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/minghanqin/LangSplat" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_06709v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AM-RADIO: Agglomerative Model -- Reduce All Domains Into One</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_06709v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_06709v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_06709v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A handful of visual foundation models (VFMs) have recently emerged as the backbones for numerous downstream tasks. VFMs like CLIP, DINOv2, SAM are trained with distinct objectives, exhibiting unique characteristics for various downstream tasks. We find that despite their conceptual differences, these models can be effectively merged into a unified model through multi-teacher distillation. We name this approach AM-RADIO (Agglomerative Model -- Reduce All Domains Into One). This integrative approach not only surpasses the performance of individual teacher models but also amalgamates their distinctive features, such as zero-shot vision-language comprehension, detailed pixel-level understanding, and open vocabulary segmentation capabilities. In pursuit of the most hardware-efficient backbone, we evaluated numerous architectures in our multi-teacher distillation pipeline using the same training recipe. This led to the development of a novel architecture (E-RADIO) that exceeds the performance of its predecessors and is at least 7x faster than the teacher models. Our comprehensive benchmarking process covers downstream tasks including ImageNet classification, ADE20k semantic segmentation, COCO object detection and LLaVa-1.5 framework.   Code: https://github.com/NVlabs/RADIO</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_06709v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近出现了一些视觉基础模型（VFM）作为许多下游任务的骨干。像CLIP、DINOv2、SAM这样的VFM都是以不同的目标进行训练的，在各种下游任务中表现出独特的特征。我们发现，尽管这些模型在概念上存在差异，但通过多教师提炼，它们可以有效地合并为一个统一的模型。我们将这种方法命名为AM-RADIO（聚集模型——将所有域归一）。这种综合方法不仅超越了个别教师模型的性能，而且融合了它们的独特特征，如零样本视觉语言理解、详细的像素级理解和开放的词汇分割能力。为了追求硬件效率最高的主干，我们使用相同的培训方法评估了多教师蒸馏管道中的许多架构。这导致了一种新型架构（E-RADIO）的开发，其性能超过了其前身，并且至少比教师模型快7倍。我们的全面基准测试过程涵盖下游任务，包括ImageNet分类、ADE20k语义分割、COCO对象检测和LLaVa-1.5框架。密码https://github.com/NVlabs/RADIO</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.06709v3" target="_blank">2312.06709v3</a>
                              </td>
                              <td>AM-RADIO: Agglomerative Model -- Reduce All Domains Into One</td>
                              <td>Mike Ranzinger</td>
                              <td>2023-12-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_06709v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.06709v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/nvlabs/radio" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_16211v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">An Explainable AI Approach to Large Language Model Assisted Causal Model Auditing and Development</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_16211v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_16211v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_16211v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Causal networks are widely used in many fields, including epidemiology, social science, medicine, and engineering, to model the complex relationships between variables. While it can be convenient to algorithmically infer these models directly from observational data, the resulting networks are often plagued with erroneous edges. Auditing and correcting these networks may require domain expertise frequently unavailable to the analyst. We propose the use of large language models such as ChatGPT as an auditor for causal networks. Our method presents ChatGPT with a causal network, one edge at a time, to produce insights about edge directionality, possible confounders, and mediating variables. We ask ChatGPT to reflect on various aspects of each causal link and we then produce visualizations that summarize these viewpoints for the human analyst to direct the edge, gather more data, or test further hypotheses. We envision a system where large language models, automated causal inference, and the human analyst and domain expert work hand in hand as a team to derive holistic and comprehensive causal models for any given case scenario. This paper presents first results obtained with an emerging prototype.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_16211v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>因果网络被广泛应用于许多领域，包括流行病学、社会科学、医学和工程，以对变量之间的复杂关系进行建模。虽然直接从观测数据中用算法推断这些模型很方便，但由此产生的网络往往存在错误边缘。审核和纠正这些网络可能需要分析员经常无法获得的领域专业知识。我们建议使用大型语言模型（如ChatGPT）作为因果网络的审计员。我们的方法为ChatGPT提供了一个因果网络，一次一个边缘，以产生关于边缘方向性、可能的混杂因素和中介变量的见解。我们要求ChatGPT反思每个因果关系的各个方面，然后我们生成可视化结果，总结这些观点，供人类分析师指导边缘、收集更多数据或测试进一步的假设。我们设想一个系统，其中大型语言模型、自动因果推理以及人类分析师和领域专家作为一个团队携手合作，为任何给定的案例场景推导出整体和全面的因果模型。本文介绍了一个新兴原型获得的第一个结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.16211v1" target="_blank">2312.16211v1</a>
                              </td>
                              <td>An Explainable AI Approach to Large Language Model Assisted Causal Model Auditing and Development</td>
                              <td>Yanming Zhang</td>
                              <td>2023-12-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_16211v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.16211v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_14810v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Accelerating Bayesian Optimal Experimental Design with Derivative-Informed Neural Operators</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_14810v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_14810v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_14810v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We consider optimal experimental design (OED) for nonlinear Bayesian inverse problems governed by large-scale partial differential equations (PDEs). For the optimality criteria of Bayesian OED, we consider both expected information gain and summary statistics including the trace and determinant of the information matrix that involves the evaluation of the parameter-to-observable (PtO) map and its derivatives. However, it is prohibitive to compute and optimize these criteria when the PDEs are very expensive to solve, the parameters to estimate are high-dimensional, and the optimization problem is combinatorial, high-dimensional, and non-convex. To address these challenges, we develop an accurate, scalable, and efficient computational framework to accelerate the solution of Bayesian OED. In particular, the framework is developed based on derivative-informed neural operator (DINO) surrogates with proper dimension reduction techniques and a modified swapping greedy algorithm. We demonstrate the high accuracy of the DINO surrogates in the computation of the PtO map and the optimality criteria compared to high-fidelity finite element approximations. We also show that the proposed method is scalable with increasing parameter dimensions. Moreover, we demonstrate that it achieves high efficiency with over 1000X speedup compared to a high-fidelity Bayesian OED solution for a three-dimensional PDE example with tens of thousands of parameters, including both online evaluation and offline construction costs of the surrogates.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_14810v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们考虑由大规模偏微分方程（PDE）控制的非线性贝叶斯反问题的最优实验设计（OED）。对于贝叶斯OED的最优性标准，我们考虑了预期信息增益和汇总统计，包括信息矩阵的迹和行列式，该信息矩阵涉及对参数-可观测（PtO）图及其导数的评估。然而，当偏微分方程的求解非常昂贵，要估计的参数是高维的，并且优化问题是组合的、高维的和非凸的时，计算和优化这些准则是禁止的。为了应对这些挑战，我们开发了一个准确、可扩展和高效的计算框架来加速贝叶斯OED的解决方案。特别是，该框架是基于导数知情神经算子（DINO）代理，采用适当的降维技术和改进的交换贪婪算法开发的。与高保真有限元近似相比，我们证明了DINO替代物在PtO映射计算中的高精度和最优性标准。我们还证明了所提出的方法随着参数维数的增加是可扩展的。此外，我们证明，对于具有数万个参数的三维PDE示例，与高保真度贝叶斯OED解决方案相比，它实现了超过1000倍的加速，包括代理的在线评估和离线构建成本。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.14810v1" target="_blank">2312.14810v1</a>
                              </td>
                              <td>Accelerating Bayesian Optimal Experimental Design with Derivative-Informed Neural Operators</td>
                              <td>Jinwoo Go</td>
                              <td>2023-12-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_14810v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.14810v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_12359v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLIP-DINOiser: Teaching CLIP a few DINO tricks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_12359v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_12359v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_12359v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The popular CLIP model displays impressive zero-shot capabilities thanks to its seamless interaction with arbitrary text prompts. However, its lack of spatial awareness makes it unsuitable for dense computer vision tasks, e.g., semantic segmentation, without an additional fine-tuning step that often uses annotations and can potentially suppress its original open-vocabulary properties. Meanwhile, self-supervised representation methods have demonstrated good localization properties without human-made annotations nor explicit supervision. In this work, we take the best of both worlds and propose a zero-shot open-vocabulary semantic segmentation method, which does not require any annotations. We propose to locally improve dense MaskCLIP features, computed with a simple modification of CLIP's last pooling layer, by integrating localization priors extracted from self-supervised features. By doing so, we greatly improve the performance of MaskCLIP and produce smooth outputs. Moreover, we show that the used self-supervised feature properties can directly be learnt from CLIP features therefore allowing us to obtain the best results with a single pass through CLIP model. Our method CLIP-DINOiser needs only a single forward pass of CLIP and two light convolutional layers at inference, no extra supervision nor extra memory and reaches state-of-the-art results on challenging and fine-grained benchmarks such as COCO, Pascal Context, Cityscapes and ADE20k. The code to reproduce our results is available at https://github.com/wysoczanska/clip_dinoiser.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_12359v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于其与任意文本提示的无缝交互，流行的CLIP模型显示了令人印象深刻的零样本功能。然而，它缺乏空间意识，不适合于密集的计算机视觉任务，例如语义分割，而不需要额外的微调步骤，该步骤通常使用注释，并可能抑制其原始的开放词汇特性。同时，自监督表示方法在没有人为注释和明确监督的情况下表现出良好的定位特性。在这项工作中，我们两全其美，提出了一种零样本开放词汇语义分割方法，该方法不需要任何注释。我们建议通过集成从自监督特征中提取的定位先验，局部改进密集的MaskCLIP特征，该特征通过对CLIP的最后一个池化层进行简单修改来计算。通过这样做，我们大大提高了MaskCLIP的性能，并产生了平滑的输出。此外，我们证明了所使用的自监督特征属性可以直接从CLIP特征中学习，因此允许我们使用单次通过的CLIP模型获得最佳结果。我们的方法CLIP DINOiser在推理时只需要一次CLIP的前向传递和两个轻卷积层，无需额外的监督和额外的内存，并且在具有挑战性的细粒度基准（如COCO、Pascal Context、Cityscapes和ADE20k）上达到了最先进的结果。重现我们结果的代码可在https://github.com/wysoczanska/clip_dinoiser.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.12359v1" target="_blank">2312.12359v1</a>
                              </td>
                              <td>CLIP-DINOiser: Teaching CLIP a few DINO tricks</td>
                              <td>Monika Wysoczańska</td>
                              <td>2023-12-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_12359v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.12359v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/wysoczanska/clip_dinoiser" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_10912v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Training-free Open-world Segmentation via Image Prompt Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_10912v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_10912v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_10912v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The realm of computer vision has witnessed a paradigm shift with the advent of foundational models, mirroring the transformative influence of large language models in the domain of natural language processing. This paper delves into the exploration of open-world segmentation, presenting a novel approach called Image Prompt Segmentation (IPSeg) that harnesses the power of vision foundational models. IPSeg lies the principle of a training-free paradigm, which capitalizes on image prompt techniques. Specifically, IPSeg utilizes a single image containing a subjective visual concept as a flexible prompt to query vision foundation models like DINOv2 and Stable Diffusion. Our approach extracts robust features for the prompt image and input image, then matches the input representations to the prompt representations via a novel feature interaction module to generate point prompts highlighting target objects in the input image. The generated point prompts are further utilized to guide the Segment Anything Model to segment the target object in the input image. The proposed method stands out by eliminating the need for exhaustive training sessions, thereby offering a more efficient and scalable solution. Experiments on COCO, PASCAL VOC, and other datasets demonstrate IPSeg's efficacy for flexible open-world segmentation using intuitive image prompts. This work pioneers tapping foundation models for open-world understanding through visual concepts conveyed in images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_10912v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着基础模型的出现，计算机视觉领域发生了范式转变，反映了大型语言模型在自然语言处理领域的变革性影响。本文深入探讨了开放世界分割的探索，提出了一种称为图像提示分割（IPSeg）的新方法，该方法利用了视觉基础模型的力量。IPSeg是无训练范式的原则，它利用了图像提示技术。具体来说，IPSeg利用包含主观视觉概念的单个图像作为灵活的提示来查询视觉基础模型，如DINOv2和Stable Diffusion。我们的方法提取提示图像和输入图像的鲁棒特征，然后通过一个新颖的特征交互模块将输入表示与提示表示进行匹配，以生成突出显示输入图像中目标对象的点提示。生成的点提示进一步用于引导Segment Anything Model对输入图像中的目标对象进行分割。所提出的方法通过消除对详尽培训课程的需求而脱颖而出，从而提供了更高效和可扩展的解决方案。在COCO、PASCAL VOC和其他数据集上的实验证明了IPSeg使用直观的图像提示进行灵活的开放世界分割的有效性。这项工作开创了通过图像中传达的视觉概念来挖掘开放世界理解的基础模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.10912v2" target="_blank">2310.10912v2</a>
                              </td>
                              <td>Towards Training-free Open-world Segmentation via Image Prompt Foundation Models</td>
                              <td>Lv Tang</td>
                              <td>2023-10-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_10912v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.10912v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11125v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SecondPose: SE(3)-Consistent Dual-Stream Feature Fusion for Category-Level Pose Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11125v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11125v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11125v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Category-level object pose estimation, aiming to predict the 6D pose and 3D size of objects from known categories, typically struggles with large intra-class shape variation. Existing works utilizing mean shapes often fall short of capturing this variation. To address this issue, we present SecondPose, a novel approach integrating object-specific geometric features with semantic category priors from DINOv2. Leveraging the advantage of DINOv2 in providing SE(3)-consistent semantic features, we hierarchically extract two types of SE(3)-invariant geometric features to further encapsulate local-to-global object-specific information. These geometric features are then point-aligned with DINOv2 features to establish a consistent object representation under SE(3) transformations, facilitating the mapping from camera space to the pre-defined canonical space, thus further enhancing pose estimation. Extensive experiments on NOCS-REAL275 demonstrate that SecondPose achieves a 12.4% leap forward over the state-of-the-art. Moreover, on a more complex dataset HouseCat6D which provides photometrically challenging objects, SecondPose still surpasses other competitors by a large margin. The code will be released soon.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11125v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>类别级物体姿态估计旨在从已知类别中预测物体的6D姿态和3D尺寸，通常难以应对大的类内形状变化。利用平均形状的现有作品往往无法捕捉到这种变化。为了解决这个问题，我们提出了SecondPose，这是一种将DINOv2中特定于对象的几何特征与语义类别先验相结合的新方法。利用DINOv2在提供SE（3）一致语义特征方面的优势，我们分层提取了两种类型的SE（3（3）不变几何特征，以进一步封装局部到全局的特定对象信息。然后，这些几何特征与DINOv2特征点对齐，以在SE（3）变换下建立一致的对象表示，促进从相机空间到预定义规范空间的映射，从而进一步增强姿态估计。在NOCS-REAL275上进行的大量实验表明，SecondPose比最先进的技术进步了12.4%。此外，在更复杂的数据集HouseCat6D上，SecondPose仍然以很大的优势超过了其他竞争对手，该数据集提供了具有光度挑战性的物体。代码将很快发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11125v2" target="_blank">2311.11125v2</a>
                              </td>
                              <td>SecondPose: SE(3)-Consistent Dual-Stream Feature Fusion for Category-Level Pose Estimation</td>
                              <td>Yamei Chen</td>
                              <td>2023-11-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11125v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11125v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/NOrangeeroli/SecondPose" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_08825v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Guided Diffusion from Self-Supervised Diffusion Features</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_08825v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_08825v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_08825v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Guidance serves as a key concept in diffusion models, yet its effectiveness is often limited by the need for extra data annotation or classifier pretraining. That is why guidance was harnessed from self-supervised learning backbones, like DINO. However, recent studies have revealed that the feature representation derived from diffusion model itself is discriminative for numerous downstream tasks as well, which prompts us to propose a framework to extract guidance from, and specifically for, diffusion models. Our research has yielded several significant contributions. Firstly, the guidance signals from diffusion models are on par with those from class-conditioned diffusion models. Secondly, feature regularization, when based on the Sinkhorn-Knopp algorithm, can further enhance feature discriminability in comparison to unconditional diffusion models. Thirdly, we have constructed an online training approach that can concurrently derive guidance from diffusion models for diffusion models. Lastly, we have extended the application of diffusion models along the constant velocity path of ODE to achieve a more favorable balance between sampling steps and fidelity. The performance of our methods has been outstanding, outperforming related baseline comparisons in large-resolution datasets, such as ImageNet256, ImageNet256-100 and LSUN-Churches. Our code will be released.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_08825v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>指导是扩散模型中的一个关键概念，但其有效性往往受到额外数据注释或分类器预训练需求的限制。这就是为什么指导是由自我监督的学习骨干，如DINO来利用的。然而，最近的研究表明，从扩散模型本身导出的特征表示对许多下游任务也是有区别的，这促使我们提出一个框架来从扩散模型中提取指导，特别是针对扩散模型。我们的研究取得了一些重大贡献。首先，来自扩散模型的引导信号与来自类条件扩散模型的指导信号是一致的。其次，与无条件扩散模型相比，基于Sinkhorn-Knopp算法的特征正则化可以进一步提高特征的可分辨性。第三，我们构建了一种在线培训方法，可以同时从扩散模型中获得对扩散模型的指导。最后，我们扩展了扩散模型在ODE等速路径上的应用，以在采样步骤和保真度之间实现更有利的平衡。我们的方法的性能非常出色，在大分辨率数据集（如ImageNet256、ImageNet256-100和LSUN Churches）中优于相关的基线比较。我们的代码将会发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.08825v1" target="_blank">2312.08825v1</a>
                              </td>
                              <td>Guided Diffusion from Self-Supervised Diffusion Features</td>
                              <td>Vincent Tao Hu</td>
                              <td>2023-12-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_08825v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.08825v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_09118v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">WildlifeDatasets: An open-source toolkit for animal re-identification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_09118v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_09118v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_09118v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we present WildlifeDatasets (https://github.com/WildlifeDatasets/wildlife-datasets) - an open-source toolkit intended primarily for ecologists and computer-vision / machine-learning researchers. The WildlifeDatasets is written in Python, allows straightforward access to publicly available wildlife datasets, and provides a wide variety of methods for dataset pre-processing, performance analysis, and model fine-tuning. We showcase the toolkit in various scenarios and baseline experiments, including, to the best of our knowledge, the most comprehensive experimental comparison of datasets and methods for wildlife re-identification, including both local descriptors and deep learning approaches. Furthermore, we provide the first-ever foundation model for individual re-identification within a wide range of species - MegaDescriptor - that provides state-of-the-art performance on animal re-identification datasets and outperforms other pre-trained models such as CLIP and DINOv2 by a significant margin. To make the model available to the general public and to allow easy integration with any existing wildlife monitoring applications, we provide multiple MegaDescriptor flavors (i.e., Small, Medium, and Large) through the HuggingFace hub (https://huggingface.co/BVRA).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_09118v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了WildlifeDataset(https://github.com/WildlifeDatasets/wildlife-datasets)-主要面向生态学家和计算机视觉/机器学习研究人员的开源工具包。WildlifeDataset是用Python编写的，允许直接访问公开可用的野生动物数据集，并为数据集预处理、性能分析和模型微调提供了多种方法。我们在各种场景和基线实验中展示了该工具包，据我们所知，包括对野生动物重新识别的数据集和方法进行最全面的实验比较，包括局部描述符和深度学习方法。此外，我们提供了第一个用于广泛物种内个体重新识别的基础模型MegaDescriptor，该模型在动物重新识别数据集上提供了最先进的性能，并显著优于其他预先训练的模型，如CLIP和DINOv2。为了向公众提供该模型，并允许与任何现有的野生动物监测应用程序轻松集成，我们通过HuggingFace中心提供多种MegaDescriptor风格（即小型、中型和大型）(https://huggingface.co/BVRA).</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.09118v2" target="_blank">2311.09118v2</a>
                              </td>
                              <td>WildlifeDatasets: An open-source toolkit for animal re-identification</td>
                              <td>Vojtěch Čermák</td>
                              <td>2023-11-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_09118v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.09118v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/wildlifedatasets/wildlife-tools" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_03999v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Adapting Self-Supervised Representations to Multi-Domain Setups</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_03999v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_03999v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_03999v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Current state-of-the-art self-supervised approaches, are effective when trained on individual domains but show limited generalization on unseen domains. We observe that these models poorly generalize even when trained on a mixture of domains, making them unsuitable to be deployed under diverse real-world setups. We therefore propose a general-purpose, lightweight Domain Disentanglement Module (DDM) that can be plugged into any self-supervised encoder to effectively perform representation learning on multiple, diverse domains with or without shared classes. During pre-training according to a self-supervised loss, DDM enforces a disentanglement in the representation space by splitting it into a domain-variant and a domain-invariant portion. When domain labels are not available, DDM uses a robust clustering approach to discover pseudo-domains. We show that pre-training with DDM can show up to 3.5% improvement in linear probing accuracy on state-of-the-art self-supervised models including SimCLR, MoCo, BYOL, DINO, SimSiam and Barlow Twins on multi-domain benchmarks including PACS, DomainNet and WILDS. Models trained with DDM show significantly improved generalization (7.4%) to unseen domains compared to baselines. Therefore, DDM can efficiently adapt self-supervised encoders to provide high-quality, generalizable representations for diverse multi-domain data.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_03999v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当前最先进的自监督方法在单个领域上训练时是有效的，但在看不见的领域上表现出有限的泛化能力。我们观察到，即使在混合域上训练，这些模型的泛化能力也很差，这使得它们不适合在不同的现实世界设置下部署。因此，我们提出了一种通用的、轻量级的域解纠缠模块（DDM），该模块可以插入任何自监督编码器，以在具有或不具有共享类的多个不同域上有效地执行表示学习。在根据自监督损失进行预训练期间，DDM通过将表示空间拆分为域变体和域不变部分，在表示空间中强制解纠缠。当域标签不可用时，DDM使用稳健的集群方法来发现伪域。我们发现，在包括PACS、DomainNet和WILDS在内的多领域基准测试上，在包括SimCLR、MoCo、BYOL、DINO、SimSiam和Barlow Twins在内的最先进的自监督模型上，使用DDM的预训练可以显示高达3.5%的线性探测精度提高。与基线相比，用DDM训练的模型对看不见的领域的泛化能力显著提高（7.4%）。因此，DDM可以有效地调整自监督编码器，为不同的多域数据提供高质量、可推广的表示。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.03999v2" target="_blank">2309.03999v2</a>
                              </td>
                              <td>Adapting Self-Supervised Representations to Multi-Domain Setups</td>
                              <td>Neha Kalibhat</td>
                              <td>2023-09-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_03999v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.03999v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2203_01881v6_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2203_01881v6_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2203_01881v6_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2203_01881v6_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised learning (SSL) has shown impressive results in downstream classification tasks. However, there is limited work in understanding their failure modes and interpreting their learned representations. In this paper, we study the representation space of state-of-the-art self-supervised models including SimCLR, SwaV, MoCo, BYOL, DINO, SimSiam, VICReg and Barlow Twins. Without the use of class label information, we discover discriminative features that correspond to unique physical attributes in images, present mostly in correctly-classified representations. Using these features, we can compress the representation space by up to 40% without significantly affecting linear classification performance. We then propose Self-Supervised Representation Quality Score (or Q-Score), an unsupervised score that can reliably predict if a given sample is likely to be mis-classified during linear evaluation, achieving AUPRC of 91.45 on ImageNet-100 and 78.78 on ImageNet-1K. Q-Score can also be used as a regularization term on pre-trained encoders to remedy low-quality representations. Fine-tuning with Q-Score regularization can boost the linear probing accuracy of SSL models by up to 5.8% on ImageNet-100 and 3.7% on ImageNet-1K compared to their baselines. Finally, using gradient heatmaps and Salient ImageNet masks, we define a metric to quantify the interpretability of each representation. We show that discriminative features are strongly correlated to core attributes and, enhancing these features through Q-score regularization makes SSL representations more interpretable.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2203_01881v6_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督学习（SSL）在下游分类任务中显示出令人印象深刻的结果。然而，在理解他们的失败模式和解释他们的习得表征方面的工作有限。在本文中，我们研究了最先进的自监督模型的表示空间，包括SimCLR、SwaV、MoCo、BYOL、DINO、SimSiam、VICReg和Barlow Twins。在不使用类标签信息的情况下，我们发现了与图像中的独特物理属性相对应的判别特征，这些特征主要存在于正确分类的表示中。使用这些特征，我们可以将表示空间压缩40%，而不会显著影响线性分类性能。然后，我们提出了自监督表示质量分数（或Q-Score），这是一种无监督的分数，可以可靠地预测给定样本在线性评估过程中是否可能被错误分类，在ImageNet-100上实现了91.45的AUPRC，在ImageNet-1K上实现了78.78的AUPRC。Q-Score也可以用作预训练编码器上的正则化术语，以补救低质量表示。与基线相比，使用Q-Score正则化进行微调可以在ImageNet-100上将SSL模型的线性探测精度提高5.8%，在ImageNet-1K上提高3.7%。最后，使用梯度热图和突出的ImageNet掩码，我们定义了一个度量来量化每个表示的可解释性。我们表明，判别特征与核心属性密切相关，通过Q分数正则化增强这些特征使SSL表示更具可解释性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2203.01881v6" target="_blank">2203.01881v6</a>
                              </td>
                              <td>Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features</td>
                              <td>Neha Kalibhat</td>
                              <td>2022-03-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2203_01881v6_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2203.01881v6" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_07006v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Mixed Pseudo Labels for Semi-Supervised Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_07006v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_07006v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_07006v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While the pseudo-label method has demonstrated considerable success in semi-supervised object detection tasks, this paper uncovers notable limitations within this approach. Specifically, the pseudo-label method tends to amplify the inherent strengths of the detector while accentuating its weaknesses, which is manifested in the missed detection of pseudo-labels, particularly for small and tail category objects. To overcome these challenges, this paper proposes Mixed Pseudo Labels (MixPL), consisting of Mixup and Mosaic for pseudo-labeled data, to mitigate the negative impact of missed detections and balance the model's learning across different object scales. Additionally, the model's detection performance on tail categories is improved by resampling labeled data with relevant instances. Notably, MixPL consistently improves the performance of various detectors and obtains new state-of-the-art results with Faster R-CNN, FCOS, and DINO on COCO-Standard and COCO-Full benchmarks. Furthermore, MixPL also exhibits good scalability on large models, improving DINO Swin-L by 2.5% mAP and achieving nontrivial new records (60.2% mAP) on the COCO val2017 benchmark without extra annotations.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_07006v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>虽然伪标签方法在半监督对象检测任务中取得了相当大的成功，但本文揭示了该方法的显著局限性。具体而言，伪标签方法倾向于放大检测器的固有优势，同时强调其弱点，这表现在伪标签的遗漏检测，特别是对于小型和尾部类别的对象。为了克服这些挑战，本文提出了混合伪标签（MixPL），由伪标签数据的Mixup和Mosaic组成，以减轻遗漏检测的负面影响，并平衡模型在不同对象尺度上的学习。此外，通过对带有相关实例的标记数据进行重新采样，提高了模型在尾部类别上的检测性能。值得注意的是，MixPL持续改进了各种探测器的性能，并在COCO标准和COCO完整基准上使用Faster R-CNN、FCOS和DINO获得了最先进的新结果。此外，MixPL在大型模型上也表现出良好的可扩展性，在没有额外注释的情况下，将DINO Swin-L提高了2.5%的mAP，并在COCO val2017基准上实现了非平凡的新记录（60.2%的mAP）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.07006v1" target="_blank">2312.07006v1</a>
                              </td>
                              <td>Mixed Pseudo Labels for Semi-Supervised Object Detection</td>
                              <td>Zeming Chen</td>
                              <td>2023-12-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_07006v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.07006v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/czm369/mixpl" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_15404v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RoMa: Robust Dense Feature Matching</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_15404v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_15404v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_15404v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Feature matching is an important computer vision task that involves estimating correspondences between two images of a 3D scene, and dense methods estimate all such correspondences. The aim is to learn a robust model, i.e., a model able to match under challenging real-world changes. In this work, we propose such a model, leveraging frozen pretrained features from the foundation model DINOv2. Although these features are significantly more robust than local features trained from scratch, they are inherently coarse. We therefore combine them with specialized ConvNet fine features, creating a precisely localizable feature pyramid. To further improve robustness, we propose a tailored transformer match decoder that predicts anchor probabilities, which enables it to express multimodality. Finally, we propose an improved loss formulation through regression-by-classification with subsequent robust regression. We conduct a comprehensive set of experiments that show that our method, RoMa, achieves significant gains, setting a new state-of-the-art. In particular, we achieve a 36% improvement on the extremely challenging WxBS benchmark. Code is provided at https://github.com/Parskatt/RoMa</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_15404v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>特征匹配是一项重要的计算机视觉任务，涉及估计3D场景的两个图像之间的对应关系，密集方法估计所有这些对应关系。其目的是学习一个稳健的模型，即能够在具有挑战性的现实世界变化下进行匹配的模型。在这项工作中，我们提出了这样一个模型，利用来自基础模型DINOv2的冻结预训练特征。尽管这些特征明显比从头开始训练的局部特征更健壮，但它们本质上是粗糙的。因此，我们将它们与专门的ConvNet精细特征相结合，创建了一个可精确定位的特征金字塔。为了进一步提高鲁棒性，我们提出了一种定制的变换器匹配解码器，该解码器预测锚概率，使其能够表达多模态。最后，我们通过分类回归和随后的稳健回归，提出了一个改进的损失公式。我们进行了一系列全面的实验，结果表明我们的RoMa方法取得了显著的成果，创造了新的最先进水平。特别是，我们在极具挑战性的WxBS基准上实现了36%的改进。代码提供于https://github.com/Parskatt/RoMa</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.15404v2" target="_blank">2305.15404v2</a>
                              </td>
                              <td>RoMa: Robust Dense Feature Matching</td>
                              <td>Johan Edstedt</td>
                              <td>2023-05-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_15404v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.15404v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/parskatt/roma" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10907v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Role of Entropy and Reconstruction in Multi-View Self-Supervised Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10907v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10907v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10907v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The mechanisms behind the success of multi-view self-supervised learning (MVSSL) are not yet fully understood. Contrastive MVSSL methods have been studied through the lens of InfoNCE, a lower bound of the Mutual Information (MI). However, the relation between other MVSSL methods and MI remains unclear. We consider a different lower bound on the MI consisting of an entropy and a reconstruction term (ER), and analyze the main MVSSL families through its lens. Through this ER bound, we show that clustering-based methods such as DeepCluster and SwAV maximize the MI. We also re-interpret the mechanisms of distillation-based approaches such as BYOL and DINO, showing that they explicitly maximize the reconstruction term and implicitly encourage a stable entropy, and we confirm this empirically. We show that replacing the objectives of common MVSSL methods with this ER bound achieves competitive performance, while making them stable when training with smaller batch sizes or smaller exponential moving average (EMA) coefficients.   Github repo: https://github.com/apple/ml-entropy-reconstruction.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10907v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多视角自我监督学习（MVSSL）成功背后的机制尚不完全清楚。通过互信息（MI）的下界InfoNCE的视角研究了MVSSL的对比方法。然而，其他MVSSL方法与MI之间的关系仍不清楚。我们考虑由熵和重建项（ER）组成的MI的不同下界，并通过其透镜分析主要的MVSSL族。通过这个ER界，我们证明了基于聚类的方法，如DeepCluster和SwAV，最大化了MI。我们还重新解释了基于蒸馏的方法（如BYOL和DINO）的机制，表明它们明确地最大化了重建项，隐含地鼓励了稳定的熵，我们从经验上证实了这一点。我们表明，用该ER界取代常见MVSSL方法的目标可以获得有竞争力的性能，同时在使用较小的批量或较小的指数移动平均（EMA）系数进行训练时使其稳定。Github回购：https://github.com/apple/ml-entropy-reconstruction.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10907v2" target="_blank">2307.10907v2</a>
                              </td>
                              <td>The Role of Entropy and Reconstruction in Multi-View Self-Supervised Learning</td>
                              <td>Borja Rodríguez-Gálvez</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10907v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10907v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/apple/ml-entropy-reconstruction" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_05464v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Identifying and Mitigating Model Failures through Few-shot CLIP-aided Diffusion Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_05464v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_05464v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_05464v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Deep learning models can encounter unexpected failures, especially when dealing with challenging sub-populations. One common reason for these failures is the occurrence of objects in backgrounds that are rarely seen during training. To gain a better understanding of these failure modes, human-interpretable descriptions are crucial for further analysis and improvement which is expensive. In this study, we propose an end-to-end framework that utilizes the capabilities of large language models (ChatGPT) and vision-language deep models (CLIP) to generate text descriptions of failure modes associated with spurious correlations (e.g. rarely seen backgrounds) without human-in-the-loop intervention. These descriptions can be used to generate synthetic data using generative models, such as diffusion models. The model can now use this generated data to learn from its weaknesses and enhance its performance on backgrounds that are uncommon for each class of data. Our approach serves as a broad solution, promising progress in comprehending model failure modes and strengthening deep learning models across a wide range of failure scenarios (e.g. bacckgrounds, colors) automatically in a few-shot manner. Our experiments have shown remarkable \textbf{improvements in accuracy ($\sim \textbf{21%}$)} on hard sub-populations (particularly for wrong background association) across $40$ different models, such as ResNets, EfficientNets, DenseNets, Vision Transformer (ViT), SwAVs, MoCos, DINOs, and CLIPs on various datasets such as ImageNet-1000, CIFAR-10, and CIFAR-100.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_05464v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>深度学习模型可能会遇到意想不到的失败，尤其是在处理具有挑战性的子群体时。这些失败的一个常见原因是在训练过程中很少看到背景中的物体。为了更好地理解这些故障模式，人类可解释的描述对于进一步的分析和改进至关重要，这是昂贵的。在这项研究中，我们提出了一个端到端的框架，该框架利用大型语言模型（ChatGPT）和视觉语言深度模型（CLIP）的能力，在没有人工干预的情况下，生成与虚假相关性（如罕见背景）相关的故障模式的文本描述。这些描述可以用于使用生成模型（例如扩散模型）生成合成数据。该模型现在可以使用这些生成的数据来学习其弱点，并提高其在每类数据中都不常见的背景下的性能。我们的方法是一个广泛的解决方案，有望在理解模型故障模式和以少量方式自动加强各种故障场景（如百家乐、颜色）的深度学习模型方面取得进展。我们的实验表明，在ImageNet-1000、CIFAR-10和CIFAR-100等各种数据集上，在40美元的不同模型（如ResNets、EfficientNets、DenseNets、Vision Transformer（ViT）、SwAVs、MoCos、DINO和CLIP）中，硬子种群（特别是错误的背景关联）的准确度显著提高（$\sim\textbf｛21%｝$）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.05464v1" target="_blank">2312.05464v1</a>
                              </td>
                              <td>Identifying and Mitigating Model Failures through Few-shot CLIP-aided Diffusion Generation</td>
                              <td>Atoosa Chegini</td>
                              <td>2023-12-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_05464v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.05464v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_05189v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Distributed Autonomous Organizations as Public Services Supplying Platform</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_05189v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_05189v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_05189v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Servizi Elaborazioni Dati SpA is a public company owned by Municipality of L Aquila, it supplies the institution with network services and software applications for distributing services to citizens. The future policy of the company is to enlarge the offer of its services to nearby communities that are unable to set up and maintain their own network and software structures. This paper presents thus a possible architecture model to support small municipalities in supplying public services to citizens, with the aid of SED Spa. Through second level platforms based on Blockchain networks and Multi-agents Systems running on smart contracts, the system will focus on Waste Tax (Ta.Ri) management system in the Fascicolo del Cittadino environment.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_05189v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Servizi Elaborazioni Dati SpA是拉奎拉市拥有的一家上市公司，为该机构提供网络服务和软件应用程序，用于向公民分发服务。该公司未来的政策是扩大向附近社区提供的服务，这些社区无法建立和维护自己的网络和软件结构。因此，本文提出了一种可能的建筑模型，以支持小城市在SED Spa的帮助下向公民提供公共服务。通过基于区块链网络的二级平台和运行在智能合约上的多代理系统，该系统将专注于Cittadino Fascolo环境中的废物税（Ta.Ri）管理系统。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.05189v1" target="_blank">2312.05189v1</a>
                              </td>
                              <td>Distributed Autonomous Organizations as Public Services Supplying Platform</td>
                              <td>Giovanni De Gasperis</td>
                              <td>2023-12-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_05189v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.05189v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04337v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-View Unsupervised Image Generation with Cross Attention Guidance</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04337v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04337v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04337v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The growing interest in novel view synthesis, driven by Neural Radiance Field (NeRF) models, is hindered by scalability issues due to their reliance on precisely annotated multi-view images. Recent models address this by fine-tuning large text2image diffusion models on synthetic multi-view data. Despite robust zero-shot generalization, they may need post-processing and can face quality issues due to the synthetic-real domain gap. This paper introduces a novel pipeline for unsupervised training of a pose-conditioned diffusion model on single-category datasets. With the help of pretrained self-supervised Vision Transformers (DINOv2), we identify object poses by clustering the dataset through comparing visibility and locations of specific object parts. The pose-conditioned diffusion model, trained on pose labels, and equipped with cross-frame attention at inference time ensures cross-view consistency, that is further aided by our novel hard-attention guidance. Our model, MIRAGE, surpasses prior work in novel view synthesis on real images. Furthermore, MIRAGE is robust to diverse textures and geometries, as demonstrated with our experiments on synthetic images generated with pretrained Stable Diffusion.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04337v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在神经辐射场（NeRF）模型的驱动下，人们对新视图合成越来越感兴趣，但由于它们依赖于精确注释的多视图图像，因此受到可扩展性问题的阻碍。最近的模型通过在合成多视图数据上微调大文本2图像扩散模型来解决这一问题。尽管有强大的零样本泛化，但它们可能需要后处理，并可能由于合成域间隙而面临质量问题。本文介绍了一种新的流水线，用于在单类别数据集上对姿态条件扩散模型进行无监督训练。在预训练的自监督视觉变换器（DINOv2）的帮助下，我们通过比较特定对象部分的可见性和位置来对数据集进行聚类，从而识别对象姿态。在姿势标签上训练并在推理时配备跨帧注意力的姿势条件扩散模型确保了跨视图的一致性，这进一步得益于我们新颖的硬注意力引导。我们的模型，MIRAGE，在真实图像的新颖视图合成方面超越了以往的工作。此外，正如我们在使用预训练的稳定扩散生成的合成图像上的实验所证明的那样，MIRAGE对不同的纹理和几何形状是鲁棒的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04337v1" target="_blank">2312.04337v1</a>
                              </td>
                              <td>Multi-View Unsupervised Image Generation with Cross Attention Guidance</td>
                              <td>Llukman Cerkezi</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04337v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04337v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03881v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Emergent Correspondence from Image Diffusion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03881v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03881v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03881v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Finding correspondences between images is a fundamental problem in computer vision. In this paper, we show that correspondence emerges in image diffusion models without any explicit supervision. We propose a simple strategy to extract this implicit knowledge out of diffusion networks as image features, namely DIffusion FeaTures (DIFT), and use them to establish correspondences between real images. Without any additional fine-tuning or supervision on the task-specific data or annotations, DIFT is able to outperform both weakly-supervised methods and competitive off-the-shelf features in identifying semantic, geometric, and temporal correspondences. Particularly for semantic correspondence, DIFT from Stable Diffusion is able to outperform DINO and OpenCLIP by 19 and 14 accuracy points respectively on the challenging SPair-71k benchmark. It even outperforms the state-of-the-art supervised methods on 9 out of 18 categories while remaining on par for the overall performance. Project page: https://diffusionfeatures.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03881v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>寻找图像之间的对应关系是计算机视觉中的一个基本问题。在本文中，我们证明了在没有任何明确监督的情况下，图像扩散模型中会出现对应关系。我们提出了一种简单的策略来从扩散网络中提取这种隐含的知识作为图像特征，即diffusion features（DIFT），并使用它们来建立真实图像之间的对应关系。在没有对特定任务的数据或注释进行任何额外的微调或监督的情况下，DIFT能够在识别语义、几何和时间对应性方面优于弱监督方法和有竞争力的现成特征。特别是在语义对应方面，来自Stable Diffusion的DIFT能够在具有挑战性的SPair 71k基准上分别优于DINO和OpenCLIP 19和14个准确度点。它甚至在18个类别中的9个类别上优于最先进的监督方法，同时在总体性能上保持不变。项目页面：https://diffusionfeatures.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03881v2" target="_blank">2306.03881v2</a>
                              </td>
                              <td>Emergent Correspondence from Image Diffusion</td>
                              <td>Luming Tang</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03881v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03881v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01677v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-task Image Restoration Guided By Robust DINO Features</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01677v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01677v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01677v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multi-task image restoration has gained significant interest due to its inherent versatility and efficiency compared to its single-task counterpart. Despite its potential, performance degradation is observed with an increase in the number of tasks, primarily attributed to the distinct nature of each restoration task. Addressing this challenge, we introduce \mbox{\textbf{DINO-IR}}, a novel multi-task image restoration approach leveraging robust features extracted from DINOv2. Our empirical analysis shows that while shallow features of DINOv2 capture rich low-level image characteristics, the deep features ensure a robust semantic representation insensitive to degradations while preserving high-frequency contour details. Building on these features, we devise specialized components, including multi-layer semantic fusion module, DINO-Restore adaption and fusion module, and DINO perception contrastive loss, to integrate DINOv2 features into the restoration paradigm. Equipped with the aforementioned components, our DINO-IR performs favorably against existing multi-task image restoration approaches in various tasks by a large margin, indicating the superiority and necessity of reinforcing the robust features for multi-task image restoration.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01677v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>与单任务图像恢复相比，多任务图像恢复由于其固有的多功能性和效率而引起了人们的极大兴趣。尽管有其潜力，但随着任务数量的增加，性能会下降，这主要归因于每个恢复任务的不同性质。为了应对这一挑战，我们引入了\box｛\textbf｛DINO-IR｝｝，这是一种利用从DINOv2中提取的鲁棒特征的新的多任务图像恢复方法。我们的实证分析表明，虽然DINOv2的浅层特征捕捉到了丰富的低层图像特征，但深层特征确保了对退化不敏感的鲁棒语义表示，同时保留了高频轮廓细节。基于这些特征，我们设计了专门的组件，包括多层语义融合模块、DINO恢复适应和融合模块以及DINO感知对比损失，以将DINOv2特征整合到恢复范式中。配备了上述组件，我们的DINO-IR在各种任务中与现有的多任务图像恢复方法相比表现良好，这表明了增强多任务图像修复的鲁棒性特征的优越性和必要性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01677v2" target="_blank">2312.01677v2</a>
                              </td>
                              <td>Multi-task Image Restoration Guided By Robust DINO Features</td>
                              <td>Xin Lin</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01677v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01677v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_00230v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DINO-Mix: Enhancing Visual Place Recognition with Foundational Vision Model and Feature Mixing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_00230v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_00230v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_00230v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Utilizing visual place recognition (VPR) technology to ascertain the geographical location of publicly available images is a pressing issue for real-world VPR applications. Although most current VPR methods achieve favorable results under ideal conditions, their performance in complex environments, characterized by lighting variations, seasonal changes, and occlusions caused by moving objects, is generally unsatisfactory. In this study, we utilize the DINOv2 model as the backbone network for trimming and fine-tuning to extract robust image features. We propose a novel VPR architecture called DINO-Mix, which combines a foundational vision model with feature aggregation. This architecture relies on the powerful image feature extraction capabilities of foundational vision models. We employ an MLP-Mixer-based mix module to aggregate image features, resulting in globally robust and generalizable descriptors that enable high-precision VPR. We experimentally demonstrate that the proposed DINO-Mix architecture significantly outperforms current state-of-the-art (SOTA) methods. In test sets having lighting variations, seasonal changes, and occlusions (Tokyo24/7, Nordland, SF-XL-Testv1), our proposed DINO-Mix architecture achieved Top-1 accuracy rates of 91.75%, 80.18%, and 82%, respectively. Compared with SOTA methods, our architecture exhibited an average accuracy improvement of 5.14%.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_00230v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>利用视觉位置识别（VPR）技术来确定公开可用图像的地理位置是现实世界VPR应用的一个紧迫问题。尽管目前大多数VPR方法在理想条件下都能获得良好的结果，但它们在复杂环境中的性能通常不令人满意，这些环境的特点是光照变化、季节变化和移动物体引起的遮挡。在这项研究中，我们利用DINOv2模型作为骨干网络进行修剪和微调，以提取稳健的图像特征。我们提出了一种新的VPR架构，称为DINO Mix，它将基础视觉模型与特征聚合相结合。该架构依赖于基础视觉模型强大的图像特征提取能力。我们使用基于MLP Mixer的混合模块来聚合图像特征，从而产生全局鲁棒和可推广的描述符，从而实现高精度的VPR。我们通过实验证明，所提出的DINO-Mix架构显著优于当前最先进的（SOTA）方法。在具有光照变化、季节变化和遮挡的测试集（Tokyo24/7，Nordland，SF-XL-Testv1）中，我们提出的DINO Mix架构分别实现了91.75%、80.18%和82%的Top-1准确率。与SOTA方法相比，我们的体系结构的平均准确度提高了5.14%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.00230v2" target="_blank">2311.00230v2</a>
                              </td>
                              <td>DINO-Mix: Enhancing Visual Place Recognition with Foundational Vision Model and Feature Mixing</td>
                              <td>Gaoshuang Huang</td>
                              <td>2023-11-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_00230v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.00230v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/GaoShuang98/DINO-Mix" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01576v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Efficient Unsupervised Satellite Image-based Building Damage Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01576v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01576v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01576v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Existing Building Damage Detection (BDD) methods always require labour-intensive pixel-level annotations of buildings and their conditions, hence largely limiting their applications. In this paper, we investigate a challenging yet practical scenario of BDD, Unsupervised Building Damage Detection (U-BDD), where only unlabelled pre- and post-disaster satellite image pairs are provided. As a pilot study, we have first proposed an advanced U-BDD baseline that leverages pre-trained vision-language foundation models (i.e., Grounding DINO, SAM and CLIP) to address the U-BDD task. However, the apparent domain gap between satellite and generic images causes low confidence in the foundation models used to identify buildings and their damages. In response, we further present a novel self-supervised framework, U-BDD++, which improves upon the U-BDD baseline by addressing domain-specific issues associated with satellite imagery. Furthermore, the new Building Proposal Generation (BPG) module and the CLIP-enabled noisy Building Proposal Selection (CLIP-BPS) module in U-BDD++ ensure high-quality self-training. Extensive experiments on the widely used building damage assessment benchmark demonstrate the effectiveness of the proposed method for unsupervised building damage detection. The presented annotation-free and foundation model-based paradigm ensures an efficient learning phase. This study opens a new direction for real-world BDD and sets a strong baseline for future research.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01576v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现有的建筑物损坏检测（BDD）方法总是需要对建筑物及其条件进行劳动密集型像素级注释，因此在很大程度上限制了其应用。在本文中，我们研究了BDD的一个具有挑战性但实用的场景，即无监督的建筑物损坏检测（U-BDD），其中只提供未标记的灾前和灾后卫星图像对。作为一项试点研究，我们首先提出了一种先进的U-BDD基线，该基线利用预先训练的视觉语言基础模型（即基础DINO、SAM和CLIP）来解决U-BDD任务。然而，卫星图像和普通图像之间明显的领域差距导致用于识别建筑物及其损坏的基础模型的置信度较低。作为回应，我们进一步提出了一个新的自我监督框架U-BDD++，该框架通过解决与卫星图像相关的特定领域问题，改进了U-BDD基线。此外，U-BDD++中新的建筑方案生成（BPG）模块和启用CLIP的嘈杂建筑方案选择（CLIP-BPS）模块确保了高质量的自我培训。在广泛使用的建筑损伤评估基准上进行的大量实验证明了所提出的无监督建筑损伤检测方法的有效性。所提出的无注释和基于基础的范式确保了有效的学习阶段。这项研究为现实世界的BDD开辟了一个新的方向，并为未来的研究奠定了坚实的基础。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01576v1" target="_blank">2312.01576v1</a>
                              </td>
                              <td>Learning Efficient Unsupervised Satellite Image-based Building Damage Detection</td>
                              <td>Yiyun Zhang</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01576v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01576v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/fzmi/ubdd" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_03513v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exploring DINO: Emergent Properties and Limitations for Synthetic Aperture Radar Imagery</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_03513v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_03513v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_03513v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised learning (SSL) models have recently demonstrated remarkable performance across various tasks, including image segmentation. This study delves into the emergent characteristics of the Self-Distillation with No Labels (DINO) algorithm and its application to Synthetic Aperture Radar (SAR) imagery. We pre-train a vision transformer (ViT)-based DINO model using unlabeled SAR data, and later fine-tune the model to predict high-resolution land cover maps. We rigorously evaluate the utility of attention maps generated by the ViT backbone and compare them with the model's token embedding space. We observe a small improvement in model performance with pre-training compared to training from scratch and discuss the limitations and opportunities of SSL for remote sensing and land cover segmentation. Beyond small performance increases, we show that ViT attention maps hold great intrinsic value for remote sensing, and could provide useful inputs to other algorithms. With this, our work lays the groundwork for bigger and better SSL models for Earth Observation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_03513v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督学习（SSL）模型最近在包括图像分割在内的各种任务中表现出了显著的性能。本研究探讨了无标签自蒸馏（DINO）算法的涌现特性及其在合成孔径雷达（SAR）图像中的应用。我们使用未标记的SAR数据预训练基于视觉变换器（ViT）的DINO模型，然后对模型进行微调，以预测高分辨率的土地覆盖图。我们严格评估了ViT主干生成的注意力图的效用，并将其与模型的令牌嵌入空间进行了比较。我们观察到，与从头开始的训练相比，预训练的模型性能略有提高，并讨论了SSL在遥感和土地覆盖分割方面的局限性和机会。除了小幅的性能提高外，我们还表明，ViT注意力图对遥感具有巨大的内在价值，并可以为其他算法提供有用的输入。有了这一点，我们的工作为更大更好的地球观测SSL模型奠定了基础。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.03513v2" target="_blank">2310.03513v2</a>
                              </td>
                              <td>Exploring DINO: Emergent Properties and Limitations for Synthetic Aperture Radar Imagery</td>
                              <td>Joseph A. Gallego-Mejia</td>
                              <td>2023-10-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_03513v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.03513v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_02048v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exploring Generalisability of Self-Distillation with No Labels for SAR-Based Vegetation Prediction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_02048v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_02048v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_02048v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this work we pre-train a DINO-ViT based model using two Synthetic Aperture Radar datasets (S1GRD or GSSIC) across three regions (China, Conus, Europe). We fine-tune the models on smaller labeled datasets to predict vegetation percentage, and empirically study the connection between the embedding space of the models and their ability to generalize across diverse geographic regions and to unseen data. For S1GRD, embedding spaces of different regions are clearly separated, while GSSIC's overlaps. Positional patterns remain during fine-tuning, and greater distances in embeddings often result in higher errors for unfamiliar regions. With this, our work increases our understanding of generalizability for self-supervised models applied to remote sensing.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_02048v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项工作中，我们使用三个地区（中国、美国和欧洲）的两个合成孔径雷达数据集（S1GRD或GSSIC）预训练了一个基于DINO-ViT的模型。我们在较小的标记数据集上微调模型，以预测植被百分比，并实证研究模型的嵌入空间与其在不同地理区域和看不见的数据之间的联系。对于S1GRD，不同区域的嵌入空间明显分离，而GSSIC的嵌入空间重叠。在微调过程中，位置模式仍然存在，嵌入中距离越大，通常会导致不熟悉区域的误差越大。通过这一点，我们的工作增加了我们对应用于遥感的自监督模型的可推广性的理解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.02048v2" target="_blank">2310.02048v2</a>
                              </td>
                              <td>Exploring Generalisability of Self-Distillation with No Labels for SAR-Based Vegetation Prediction</td>
                              <td>Laura Martínez-Ferrer</td>
                              <td>2023-10-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_02048v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.02048v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </div>
  <div class="text-muted text-center py-2">
    <div class="container">
      <div class="row">
        <div class="col-md-12 my-4">
          <p class="mb-1">© 2023 Li Yingping - Powered by Arxiv API.</p>
        </div>
      </div>
    </div>
  </div>
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
    integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
    crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
    integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
    crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js"
    integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy"
    crossorigin="anonymous"></script>
  <pingendo onclick="window.open('https://pingendo.com/', '_blank')"
    style="cursor:pointer;position: fixed;bottom: 20px;right:20px;padding:4px;background-color: #00b0eb;border-radius: 8px; width:220px;display:flex;flex-direction:row;align-items:center;justify-content:center;font-size:14px;color:white">
    Made with Pingendo Free&nbsp;&nbsp;<img src="https://pingendo.com/site-assets/Pingendo_logo_big.png" class="d-block"
      alt="Pingendo logo" height="16"></pingendo>
</body>

</html>