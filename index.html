<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- PAGE settings -->
  <link rel="icon" href="https://templates.pingendo.com/assets/Pingendo_favicon.ico">
  <title>Arxiv Daily</title>
  <!-- CSS dependencies -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"
    type="text/css">
  <link rel="stylesheet" href="assets/wireframe.css">
</head>

<body class="bg-light">
  <div class="py-5">
    <div class="container">
      <div class="row">
        <div class="text-center col-md-7 mx-auto"> <i class="fa d-block fa-bullseye fa-5x mb-4 text-info"></i>
          <h2><b>Arxiv Daily - 2023-08-05</b></h2>
        </div>
      </div>
    </div>
  </div>
  <div class="py-2">
    <div class="container">
      <div class="row">
        <div class="col-md-12">
          <ul class="nav nav-tabs">
            
            <li class="nav-item"> <a href="" class="nav-link active"
                data-toggle="pill" data-target="#SLAM"><i class="fa fa-home"></i> SLAM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#SFM"><i class="fa fa-home"></i> SFM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#LLM"><i class="fa fa-home"></i> LLM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#SAM"><i class="fa fa-home"></i> SAM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#CLIP"><i class="fa fa-home"></i> CLIP</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#DINO"><i class="fa fa-home"></i> DINO</a>
            </li>
            
          </ul>
          <div class="tab-content mt-2">
            
            <div class="tab-pane fade active show" id="SLAM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2303_16500v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AirLine: Efficient Learnable Line Detection with Local Edge Voting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_16500v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_16500v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_16500v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Line detection is widely used in many robotic tasks such as scene recognition, 3D reconstruction, and simultaneous localization and mapping (SLAM). Compared to points, lines can provide both low-level and high-level geometrical information for downstream tasks. In this paper, we propose a novel learnable edge-based line detection algorithm, AirLine, which can be applied to various tasks. In contrast to existing learnable endpoint-based methods, which are sensitive to the geometrical condition of environments, AirLine can extract line segments directly from edges, resulting in a better generalization ability for unseen environments. To balance efficiency and accuracy, we introduce a region-grow algorithm and a local edge voting scheme for line parameterization. To the best of our knowledge, AirLine is one of the first learnable edge-based line detection methods. Our extensive experiments have shown that it retains state-of-the-art-level precision, yet with a 3 to 80 times runtime acceleration compared to other learning-based methods, which is critical for low-power robots.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_16500v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>直线检测被广泛应用于许多机器人任务中，如场景识别、三维重建和同时定位与映射（SLAM）。与点相比，线可以为下游任务提供低级和高级几何信息。在本文中，我们提出了一种新的可学习的基于边缘的直线检测算法AirLine，该算法可以应用于各种任务。与现有的对环境几何条件敏感的基于端点的可学习方法相比，AirLine可以直接从边缘提取线段，从而对看不见的环境具有更好的泛化能力。为了平衡效率和准确性，我们引入了一种区域增长算法和一种用于线参数化的局部边缘投票方案。据我们所知，AirLine是首批可学习的基于边缘的线路检测方法之一。我们的大量实验表明，它保持了最先进的精度，但与其他基于学习的方法相比，运行时加速是其他方法的3到80倍，这对低功耗机器人至关重要。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.16500v2" target="_blank">2303.16500v2</a>
                              </td>
                              <td>AirLine: Efficient Learnable Line Detection with Local Edge Voting</td>
                              <td>Xiao Lin</td>
                              <td>2023-03-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_16500v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.16500v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01553v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Uncertainty analysis for accurate ground truth trajectories with robotic total stations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01553v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01553v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01553v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In the context of robotics, accurate ground truth positioning is essential for the development of Simultaneous Localization and Mapping (SLAM) and control algorithms. Robotic Total Stations (RTSs) provide accurate and precise reference positions in different types of outdoor environments, especially when compared to the limited accuracy of Global Navigation Satellite System (GNSS) in cluttered areas. Three RTSs give the possibility to obtain the six-Degrees Of Freedom (DOF) reference pose of a robotic platform. However, the uncertainty of every pose is rarely computed for trajectory evaluation. As evaluation algorithms are getting increasingly precise, it becomes crucial to take into account this uncertainty. We propose a method to compute this six-DOF uncertainty from the fusion of three RTSs based on Monte Carlo (MC) methods. This solution relies on point-to-point minimization to propagate the noise of RTSs on the pose of the robotic platform. Five main noise sources are identified to model this uncertainty: noise inherent to the instrument, tilt noise, atmospheric factors, time synchronization noise, and extrinsic calibration noise. Based on extensive experimental work, we compare the impact of each noise source on the prism uncertainty and the final estimated pose. Tested on more than 50 km of trajectories, our comparison highlighted the importance of the calibration noise and the measurement distance, which should be ideally under 75 m. Moreover, it has been noted that the uncertainty on the pose of the robot is not prominently affected by one particular noise source, compared to the others.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01553v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在机器人技术的背景下，准确的地面实况定位对于同步定位和映射（SLAM）和控制算法的开发至关重要。机器人全站仪（RTS）在不同类型的户外环境中提供精确的参考位置，尤其是与全球导航卫星系统（GNSS）在杂乱区域的有限精度相比。三个RTS提供了获得机器人平台的六自由度（DOF）参考姿态的可能性。然而，很少为轨迹评估计算每个姿势的不确定性。随着评估算法越来越精确，考虑这种不确定性变得至关重要。我们提出了一种基于蒙特卡罗（MC）方法计算三个RTS融合的六自由度不确定性的方法。该解决方案依赖于点对点最小化来在机器人平台的姿态上传播RTS的噪声。确定了五个主要的噪声源来模拟这种不确定性：仪器固有的噪声、倾斜噪声、大气因素、时间同步噪声和外部校准噪声。基于大量的实验工作，我们比较了每个噪声源对棱镜不确定性和最终估计姿态的影响。在50多公里的轨迹上进行了测试，我们的比较强调了校准噪声和测量距离的重要性，理想情况下测量距离应在75米以下。此外，值得注意的是，与其他噪声源相比，机器人姿态的不确定性不受一个特定噪声源的显著影响。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01553v1" target="_blank">2308.01553v1</a>
                              </td>
                              <td>Uncertainty analysis for accurate ground truth trajectories with robotic total stations</td>
                              <td>Maxime Vaidis</td>
                              <td>2023-08-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01553v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01553v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01398v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Small Form Factor Aerial Research Vehicle for Pick-and-Place Tasks with Onboard Real-Time Object Detection and Visual Odometry</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01398v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01398v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01398v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper introduces a novel, small form-factor, aerial vehicle research platform for agile object detection, classification, tracking, and interaction tasks. General-purpose hardware components were designed to augment a given aerial vehicle and enable it to perform safe and reliable grasping. These components include a custom collision tolerant cage and low-cost Gripper Extension Package, which we call GREP, for object grasping. Small vehicles enable applications in highly constrained environments, but are often limited by computational resources. This work evaluates the challenges of pick-and-place tasks, with entirely onboard computation of object pose and visual odometry based state estimation on a small platform, and demonstrates experiments with enough accuracy to reliably grasp objects. In a total of 70 trials across challenging cases such as cluttered environments, obstructed targets, and multiple instances of the same target, we demonstrated successfully grasping the target in 93% of trials. Both the hardware component designs and software framework are released as open-source, since our intention is to enable easy reproduction and application on a wide range of small vehicles.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01398v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了一种新颖的小型飞行器研究平台，用于敏捷物体的检测、分类、跟踪和交互任务。通用硬件组件的设计是为了增强给定的飞行器，使其能够进行安全可靠的抓取。这些组件包括一个自定义的防撞笼和低成本的Gripper扩展包，我们称之为GREP，用于抓取物体。小型车辆能够在高度受限的环境中应用，但通常受到计算资源的限制。这项工作评估了拾取和放置任务的挑战，在小型平台上对物体姿态进行了完全车载计算，并基于视觉里程计进行了状态估计，并展示了具有足够精度的实验，以可靠地抓取物体。在总共70项具有挑战性的试验中，如杂乱的环境、障碍目标和同一目标的多个实例，我们在93%的试验中成功地抓住了目标。硬件组件设计和软件框架都是开源的，因为我们的目的是在各种小型车辆上轻松复制和应用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01398v1" target="_blank">2308.01398v1</a>
                              </td>
                              <td>A Small Form Factor Aerial Research Vehicle for Pick-and-Place Tasks with Onboard Real-Time Object Detection and Visual Odometry</td>
                              <td>Cora A. Dimmig</td>
                              <td>2023-08-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01398v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01398v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07607v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SubT-MRS: A Subterranean, Multi-Robot, Multi-Spectral and Multi-Degraded Dataset for Robust SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07607v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07607v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07607v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, significant progress has been made in the field of simultaneous localization and mapping (SLAM) research. However, current state-of-the-art solutions still struggle with limited accuracy and robustness in real-world applications. One major reason is the lack of datasets that fully capture the conditions faced by robots in the wild. To address this problem, we present SubT-MRS, an extremely challenging real-world dataset designed to push the limits of SLAM and perception algorithms.   SubT-MRS is a multi-modal, multi-robot dataset collected mainly from subterranean environments having multi-degraded conditions including structureless corridors, varying lighting conditions, and perceptual obscurants such as smoke and dust. Furthermore, the dataset packages information from a diverse range of time-synchronized sensors, including LiDAR, visual cameras, thermal cameras, and IMUs captured using varied vehicular motions like aerial, legged, and wheeled, to support research in sensor fusion, which is essential for achieving accurate and robust robotic perception in complex environments. To evaluate the accuracy of SLAM systems, we also provide a dense 3D model with sub-centimeter-level accuracy, as well as accurate 6DoF ground truth. Our benchmarking approach includes several state-of-the-art methods to demonstrate the challenges our datasets introduce, particularly in the case of multi-degraded environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07607v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，同步定位与映射（SLAM）研究领域取得了重大进展。然而，当前最先进的解决方案在现实应用中仍然难以达到有限的准确性和稳健性。一个主要原因是缺乏能够完全捕捉机器人在野外所面临状况的数据集。为了解决这个问题，我们提出了SubT-MRS，这是一个极具挑战性的真实世界数据集，旨在突破SLAM和感知算法的极限。SubT-MRS是一个多模态、多机器人数据集，主要从具有多重退化条件的地下环境中收集，包括无结构走廊、不同的照明条件和烟雾和灰尘等感知遮蔽物。此外，该数据集封装了来自各种时间同步传感器的信息，包括激光雷达、视觉相机、热像仪和使用各种车辆运动（如空中、腿部和轮式）捕获的IMU，以支持传感器融合研究，这对于在复杂环境中实现准确和稳健的机器人感知至关重要。为了评估SLAM系统的精度，我们还提供了一个亚厘米级精度的密集3D模型，以及精确的6DoF地面实况。我们的基准测试方法包括几种最先进的方法，以证明我们的数据集带来的挑战，特别是在多重退化环境的情况下。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07607v2" target="_blank">2307.07607v2</a>
                              </td>
                              <td>SubT-MRS: A Subterranean, Multi-Robot, Multi-Spectral and Multi-Degraded Dataset for Robust SLAM</td>
                              <td>Shibo Zhao</td>
                              <td>2023-07-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07607v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07607v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01125v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Stereo Visual Odometry with Deep Learning-Based Point and Line Feature Matching using an Attention Graph Neural Network</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01125v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01125v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01125v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Robust feature matching forms the backbone for most Visual Simultaneous Localization and Mapping (vSLAM), visual odometry, 3D reconstruction, and Structure from Motion (SfM) algorithms. However, recovering feature matches from texture-poor scenes is a major challenge and still remains an open area of research. In this paper, we present a Stereo Visual Odometry (StereoVO) technique based on point and line features which uses a novel feature-matching mechanism based on an Attention Graph Neural Network that is designed to perform well even under adverse weather conditions such as fog, haze, rain, and snow, and dynamic lighting conditions such as nighttime illumination and glare scenarios. We perform experiments on multiple real and synthetic datasets to validate the ability of our method to perform StereoVO under low visibility weather and lighting conditions through robust point and line matches. The results demonstrate that our method achieves more line feature matches than state-of-the-art line matching algorithms, which when complemented with point feature matches perform consistently well in adverse weather and dynamic lighting conditions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01125v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>稳健的特征匹配构成了大多数视觉同步定位和映射（vSLAM）、视觉里程计、3D重建和运动结构（SfM）算法的支柱。然而，从纹理差的场景中恢复特征匹配是一个重大挑战，并且仍然是一个开放的研究领域。在本文中，我们提出了一种基于点和线特征的立体视觉Odometry（StereoVO）技术，该技术使用了一种新的基于注意力图神经网络的特征匹配机制，即使在雾、霾、雨和雪等恶劣天气条件以及夜间照明和眩光等动态照明条件下也能表现良好。我们在多个真实和合成数据集上进行了实验，以验证我们的方法通过稳健的点和线匹配在低能见度天气和照明条件下执行StereoVO的能力。结果表明，与最先进的线匹配算法相比，我们的方法实现了更多的线特征匹配，当与点特征匹配相补充时，线匹配算法在恶劣天气和动态照明条件下始终表现良好。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01125v1" target="_blank">2308.01125v1</a>
                              </td>
                              <td>Stereo Visual Odometry with Deep Learning-Based Point and Line Feature Matching using an Attention Graph Neural Network</td>
                              <td>Shenbagaraj Kannapiran</td>
                              <td>2023-08-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01125v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01125v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_13513v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Preliminary Design of the Dragonfly Navigation Filter</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_13513v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_13513v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_13513v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Dragonfly is scheduled to begin exploring Titan by 2034 using a series of multi-kilometer surface flights. This paper outlines the preliminary design of the navigation filter for the Dragonfly Mobility subsystem. The software architecture and filter formulation for lidar, visual odometry, pressure sensors, and redundant IMUs are described in detail. Special discussion is given to developments to achieve multi-kilometer surface flights, including optimizing sequential image baselines, modeling correlating image processing errors, and an efficient approximation to the Simultaneous Localization and Mapping (SLAM) problem.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_13513v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Dragonfly计划在2034年开始探索泰坦，进行一系列长达数公里的地面飞行。本文概述了Dragonfly Mobility子系统导航滤波器的初步设计。详细描述了激光雷达、视觉里程计、压力传感器和冗余IMU的软件架构和滤波器公式。特别讨论了实现多公里地面飞行的发展，包括优化顺序图像基线、建模相关图像处理误差，以及同时定位和映射（SLAM）问题的有效近似。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.13513v2" target="_blank">2307.13513v2</a>
                              </td>
                              <td>Preliminary Design of the Dragonfly Navigation Filter</td>
                              <td>Ben Schilling</td>
                              <td>2023-07-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_13513v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.13513v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_00235v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Demonstrating Autonomous 3D Path Planning on a Novel Scalable UGV-UAV Morphing Robot</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_00235v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_00235v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_00235v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Some animals exhibit multi-modal locomotion capability to traverse a wide range of terrains and environments, such as amphibians that can swim and walk or birds that can fly and walk. This capability is extremely beneficial for expanding the animal's habitat range and they can choose the most energy efficient mode of locomotion in a given environment. The robotic biomimicry of this multi-modal locomotion capability can be very challenging but offer the same advantages. However, the expanded range of locomotion also increases the complexity of performing localization and path planning. In this work, we present our morphing multi-modal robot, which is capable of ground and aerial locomotion, and the implementation of readily available SLAM and path planning solutions to navigate a complex indoor environment.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_00235v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>一些动物表现出多模式运动能力，可以穿越各种地形和环境，例如会游泳和行走的两栖动物或会飞行和行走的鸟类。这种能力对扩大动物的栖息地范围非常有益，它们可以在特定环境中选择最节能的运动模式。这种多模式运动能力的机器人仿生可能非常具有挑战性，但也具有相同的优势。然而，移动范围的扩大也增加了执行定位和路径规划的复杂性。在这项工作中，我们介绍了我们的变形多模态机器人，它能够进行地面和空中运动，并实现了现成的SLAM和路径规划解决方案，以在复杂的室内环境中导航。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.00235v1" target="_blank">2308.00235v1</a>
                              </td>
                              <td>Demonstrating Autonomous 3D Path Planning on a Novel Scalable UGV-UAV Morphing Robot</td>
                              <td>Eric Sihite</td>
                              <td>2023-08-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_00235v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.00235v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_16710v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning whom to trust in navigation: dynamically switching between classical and neural planning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_16710v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_16710v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_16710v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Navigation of terrestrial robots is typically addressed either with localization and mapping (SLAM) followed by classical planning on the dynamically created maps, or by machine learning (ML), often through end-to-end training with reinforcement learning (RL) or imitation learning (IL). Recently, modular designs have achieved promising results, and hybrid algorithms that combine ML with classical planning have been proposed. Existing methods implement these combinations with hand-crafted functions, which cannot fully exploit the complementary nature of the policies and the complex regularities between scene structure and planning performance. Our work builds on the hypothesis that the strengths and weaknesses of neural planners and classical planners follow some regularities, which can be learned from training data, in particular from interactions. This is grounded on the assumption that, both, trained planners and the mapping algorithms underlying classical planning are subject to failure cases depending on the semantics of the scene and that this dependence is learnable: for instance, certain areas, objects or scene structures can be reconstructed easier than others. We propose a hierarchical method composed of a high-level planner dynamically switching between a classical and a neural planner. We fully train all neural policies in simulation and evaluate the method in both simulation and real experiments with a LoCoBot robot, showing significant gains in performance, in particular in the real environment. We also qualitatively conjecture on the nature of data regularities exploited by the high-level planner.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_16710v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>陆地机器人的导航通常通过定位和映射（SLAM），然后在动态创建的地图上进行经典规划，或者通过机器学习（ML），通常通过强化学习（RL）或模仿学习（IL）的端到端训练来解决。最近，模块化设计取得了有希望的结果，并提出了将ML与经典规划相结合的混合算法。现有的方法通过手工制作的功能来实现这些组合，无法充分利用策略的互补性以及场景结构和规划性能之间的复杂规律。我们的工作建立在这样一个假设的基础上，即神经规划者和经典规划者的优势和劣势遵循一些规律，这些规律可以从训练数据中学习，特别是从互动中学习。这是基于这样一种假设，即受过训练的规划者和经典规划背后的映射算法都会根据场景的语义发生故障，并且这种依赖性是可学习的：例如，某些区域、对象或场景结构可以比其他区域、对象和场景结构更容易重建。我们提出了一种由高级规划器在经典规划器和神经规划器之间动态切换组成的分层方法。我们在模拟中充分训练了所有神经策略，并在模拟和实际实验中使用LoCoBot机器人对该方法进行了评估，显示出显著的性能提升，特别是在实际环境中。我们还定性地推测了高级规划师所利用的数据规律的性质。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.16710v1" target="_blank">2307.16710v1</a>
                              </td>
                              <td>Learning whom to trust in navigation: dynamically switching between classical and neural planning</td>
                              <td>Sombit Dey</td>
                              <td>2023-07-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_16710v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.16710v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_05162v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">EVOLIN Benchmark: Evaluation of Line Detection and Association</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_05162v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_05162v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_05162v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Lines are interesting geometrical features commonly seen in indoor and urban environments. There is missing a complete benchmark where one can evaluate lines from a sequential stream of images in all its stages: Line detection, Line Association and Pose error. To do so, we present a complete and exhaustive benchmark for visual lines in a SLAM front-end, both for RGB and RGBD, by providing a plethora of complementary metrics. We have also labelled data from well-known SLAM datasets in order to have all in one poses and accurately annotated lines. In particular, we have evaluated 17 line detection algorithms, 5 line associations methods and the resultant pose error for aligning a pair of frames with several combinations of detector-association. We have packaged all methods and evaluations metrics and made them publicly available on web-page https://prime-slam.github.io/evolin/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_05162v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>线条是室内和城市环境中常见的有趣的几何特征。缺少一个完整的基准，在该基准中，可以评估所有阶段的连续图像流中的线条：线条检测、线条关联和姿势错误。为此，我们通过提供大量补充指标，为SLAM前端的RGB和RGBD视觉线条提供了一个完整而详尽的基准。我们还标记了来自知名SLAM数据集的数据，以便具有一体式姿势和精确注释的线条。特别地，我们评估了17种线检测算法、5种线关联方法以及用于将一对帧与检测器关联的几种组合对准的结果姿态误差。我们已经将所有方法和评估指标打包，并在网页上公开https://prime-slam.github.io/evolin/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.05162v2" target="_blank">2303.05162v2</a>
                              </td>
                              <td>EVOLIN Benchmark: Evaluation of Line Detection and Association</td>
                              <td>Kirill Ivanov</td>
                              <td>2023-03-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_05162v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.05162v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_10561v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LiDAR-Based Place Recognition For Autonomous Driving: A Survey</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_10561v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_10561v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_10561v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>LiDAR-based place recognition (LPR) plays a pivotal role in autonomous driving, which assists Simultaneous Localization and Mapping (SLAM) systems in reducing accumulated errors and achieving reliable localization. However, existing reviews predominantly concentrate on visual place recognition (VPR) methods. Despite the recent remarkable progress in LPR, to the best of our knowledge, there is no dedicated systematic review in this area. This paper bridges the gap by providing a comprehensive review of place recognition methods employing LiDAR sensors, thus facilitating and encouraging further research. We commence by delving into the problem formulation of place recognition, exploring existing challenges, and describing relations to previous surveys. Subsequently, we conduct an in-depth review of related research, which offers detailed classifications, strengths and weaknesses, and architectures. Finally, we summarize existing datasets, commonly used evaluation metrics, and comprehensive evaluation results from various methods on public datasets. This paper can serve as a valuable tutorial for newcomers entering the field of place recognition and for researchers interested in long-term robot localization. We pledge to maintain an up-to-date project on our website https://github.com/ShiPC-AI/LPR-Survey.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_10561v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于激光雷达的位置识别（LPR）在自动驾驶中发挥着关键作用，它有助于同时定位和映射（SLAM）系统减少累积误差，实现可靠的定位。然而，现有的综述主要集中在视觉位置识别（VPR）方法上。尽管LPR最近取得了显著进展，但据我们所知，在这一领域还没有专门的系统审查。本文通过对使用激光雷达传感器的位置识别方法进行全面综述来弥补这一差距，从而促进和鼓励进一步的研究。我们首先深入研究地点识别的问题公式，探索现有的挑战，并描述与以前调查的关系。随后，我们对相关研究进行了深入回顾，提供了详细的分类、优势和劣势以及架构。最后，我们总结了现有的数据集、常用的评估指标，以及在公共数据集上各种方法的综合评估结果。本文可以为进入位置识别领域的新手和对长期机器人定位感兴趣的研究人员提供宝贵的指导。我们保证在我们的网站上保持最新的项目https://github.com/ShiPC-AI/LPR-Survey.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.10561v2" target="_blank">2306.10561v2</a>
                              </td>
                              <td>LiDAR-Based Place Recognition For Autonomous Driving: A Survey</td>
                              <td>Yongjun Zhang</td>
                              <td>2023-06-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_10561v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.10561v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_04747v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Graph-based Optimization Framework for Hand-Eye Calibration for Multi-Camera Setups</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_04747v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_04747v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_04747v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Hand-eye calibration is the problem of estimating the spatial transformation between a reference frame, usually the base of a robot arm or its gripper, and the reference frame of one or multiple cameras. Generally, this calibration is solved as a non-linear optimization problem, what instead is rarely done is to exploit the underlying graph structure of the problem itself. Actually, the problem of hand-eye calibration can be seen as an instance of the Simultaneous Localization and Mapping (SLAM) problem. Inspired by this fact, in this work we present a pose-graph approach to the hand-eye calibration problem that extends a recent state-of-the-art solution in two different ways: i) by formulating the solution to eye-on-base setups with one camera; ii) by covering multi-camera robotic setups. The proposed approach has been validated in simulation against standard hand-eye calibration methods. Moreover, a real application is shown. In both scenarios, the proposed approach overcomes all alternative methods. We release with this paper an open-source implementation of our graph-based optimization framework for multi-camera setups.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_04747v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>手眼校准是估计参考系（通常是机械臂或其夹具的底座）与一个或多个相机的参考系之间的空间变换的问题。通常，这种校准是作为一个非线性优化问题来解决的，相反，很少做的是利用问题本身的底层图结构。实际上，手眼校准问题可以看作是同时定位和映射（SLAM）问题的一个例子。受此启发，在这项工作中，我们提出了一种解决手眼校准问题的姿势图方法，该方法以两种不同的方式扩展了最近最先进的解决方案：i）通过用一台相机将解决方案公式化为基于眼睛的设置；ii）通过覆盖多摄像机机器人设置。所提出的方法已经在模拟中与标准手眼校准方法进行了验证。此外，还展示了一个实际应用。在这两种情况下，所提出的方法都克服了所有备选方法。我们在本文中发布了一个基于图形的多摄像头优化框架的开源实现。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.04747v2" target="_blank">2303.04747v2</a>
                              </td>
                              <td>A Graph-based Optimization Framework for Hand-Eye Calibration for Multi-Camera Setups</td>
                              <td>Daniele Evangelista</td>
                              <td>2023-03-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_04747v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.04747v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_15005v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FLiCR: A Fast and Lightweight LiDAR Point Cloud Compression Based on Lossy RI</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_15005v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_15005v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_15005v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Light detection and ranging (LiDAR) sensors are becoming available on modern mobile devices and provide a 3D sensing capability. This new capability is beneficial for perceptions in various use cases, but it is challenging for resource-constrained mobile devices to use the perceptions in real-time because of their high computational complexity. In this context, edge computing can be used to enable LiDAR online perceptions, but offloading the perceptions on the edge server requires a low-latency, lightweight, and efficient compression due to the large volume of LiDAR point clouds data.   This paper presents FLiCR, a fast and lightweight LiDAR point cloud compression method for enabling edge-assisted online perceptions. FLiCR is based on range images (RI) as an intermediate representation (IR), and dictionary coding for compressing RIs. FLiCR achieves its benefits by leveraging lossy RIs, and we show the efficiency of bytestream compression is largely improved with quantization and subsampling. In addition, we identify the limitation of current quality metrics for presenting the entropy of a point cloud, and introduce a new metric that reflects both point-wise and entropy-wise qualities for lossy IRs. The evaluation results show FLiCR is more suitable for edge-assisted real-time perceptions than the existing LiDAR compressions, and we demonstrate the effectiveness of our compression and metric with the evaluations on 3D object detection and LiDAR SLAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_15005v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>光探测和测距（LiDAR）传感器在现代移动设备上变得可用，并提供3D传感能力。这种新功能有利于各种使用情况下的感知，但由于资源受限的移动设备计算复杂度高，因此实时使用感知具有挑战性。在这种情况下，边缘计算可以用于实现激光雷达在线感知，但由于激光雷达点云数据量大，在边缘服务器上卸载感知需要低延迟、轻量和高效的压缩。本文介绍了FLiCR，这是一种快速、轻量级的激光雷达点云压缩方法，用于实现边缘辅助在线感知。FLiCR基于作为中间表示（IR）的距离图像（RI）和用于压缩RI的字典编码。FLiCR通过利用有损RI来实现其优势，我们表明，通过量化和子采样，字节流压缩的效率大大提高。此外，我们确定了当前质量度量用于表示点云熵的局限性，并引入了一种新的度量，该度量反映了有损IR的逐点和逐熵质量。评估结果表明，与现有的激光雷达压缩相比，FLiCR更适合于边缘辅助实时感知，我们通过对3D目标检测和激光雷达SLAM的评估证明了我们的压缩和度量的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.15005v1" target="_blank">2307.15005v1</a>
                              </td>
                              <td>FLiCR: A Fast and Lightweight LiDAR Point Cloud Compression Based on Lossy RI</td>
                              <td>Jin Heo</td>
                              <td>2023-07-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_15005v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.15005v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01188v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Event-based Stereo Visual Odometry with Native Temporal Resolution via Continuous-time Gaussian Process Regression</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01188v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01188v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01188v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Event-based cameras asynchronously capture individual visual changes in a scene. This makes them more robust than traditional frame-based cameras to highly dynamic motions and poor illumination. It also means that every measurement in a scene can occur at a unique time.   Handling these different measurement times is a major challenge of using event-based cameras. It is often addressed in visual odometry (VO) pipelines by approximating temporally close measurements as occurring at one common time. This grouping simplifies the estimation problem but, absent additional sensors, sacrifices the inherent temporal resolution of event-based cameras.   This paper instead presents a complete stereo VO pipeline that estimates directly with individual event-measurement times without requiring any grouping or approximation in the estimation state. It uses continuous-time trajectory estimation to maintain the temporal fidelity and asynchronous nature of event-based cameras through Gaussian process regression with a physically motivated prior. Its performance is evaluated on the MVSEC dataset, where it achieves 7.9e-3 and 5.9e-3 RMS relative error on two independent sequences, outperforming the existing publicly available event-based stereo VO pipeline by two and four times, respectively.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01188v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于事件的摄影机异步捕捉场景中的各个视觉变化。这使得它们比传统的基于帧的相机对高动态运动和较差的照明更具鲁棒性。这也意味着场景中的每个测量都可以在唯一的时间发生。处理这些不同的测量时间是使用基于事件的相机的主要挑战。它通常在视觉里程计（VO）管道中通过将时间上的近距离测量近似为在一个公共时间发生来解决。这种分组简化了估计问题，但在没有额外传感器的情况下，牺牲了基于事件的相机固有的时间分辨率。相反，本文提出了一个完整的立体VO管道，该管道直接用单个事件测量时间进行估计，而不需要在估计状态下进行任何分组或近似。它使用连续时间轨迹估计，通过具有物理动机先验的高斯过程回归来保持基于事件的相机的时间保真度和异步性质。它的性能在MVSEC数据集上进行了评估，在两个独立序列上实现了7.9e-3和5.9e-3的RMS相对误差，分别比现有的公开可用的基于事件的立体声VO管道高出两倍和四倍。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01188v2" target="_blank">2306.01188v2</a>
                              </td>
                              <td>Event-based Stereo Visual Odometry with Native Temporal Resolution via Continuous-time Gaussian Process Regression</td>
                              <td>Jianeng Wang</td>
                              <td>2023-06-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01188v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01188v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_05086v4_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Stereo Event-based Visual-Inertial Odometry</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_05086v4_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_05086v4_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_05086v4_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Event-based cameras are new type vision sensors whose pixels work independently and respond asynchronously to brightness change with microsecond resolution, instead of providing standard intensity frames. Compared with traditional cameras, event-based cameras have low latency, no motion blur, and high dynamic range (HDR), which provide possibilities for robots to deal with some challenging scenes. We propose a visual-inertial odometry for stereo event-based cameras based on Error-State Kalman Filter (ESKF). The visual module updates the pose relies on the edge alignment of a semi-dense 3D map to a 2D image, and the IMU module updates pose by median integral. We evaluate our method on public datasets with general 6-DoF motion and compare the results against ground truth. We show that our proposed pipeline provides improved accuracy over the result of the state-of-the-art visual odometry for stereo event-based cameras, while running in real-time on a standard CPU (low-resolution cameras). To the best of our knowledge, this is the first published visual-inertial odometry for stereo event-based cameras.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_05086v4_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于事件的相机是一种新型的视觉传感器，其像素独立工作，并以微秒的分辨率异步响应亮度变化，而不是提供标准的强度帧。与传统相机相比，基于事件的相机具有低延迟、无运动模糊和高动态范围（HDR），这为机器人处理一些具有挑战性的场景提供了可能性。我们提出了一种基于误差状态卡尔曼滤波器（ESKF）的立体事件相机视觉惯性里程计。视觉模块根据半密集3D地图到2D图像的边缘对齐来更新姿态，IMU模块通过中值积分来更新姿态。我们在具有一般6-DoF运动的公共数据集上评估了我们的方法，并将结果与地面实况进行了比较。我们表明，我们提出的管道在标准CPU（低分辨率相机）上实时运行的同时，为基于立体事件的相机提供了比最先进的视觉里程计结果更高的精度。据我们所知，这是第一个发表的用于基于立体事件的相机的视觉惯性里程计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.05086v4" target="_blank">2303.05086v4</a>
                              </td>
                              <td>Stereo Event-based Visual-Inertial Odometry</td>
                              <td>Kunfeng Wang</td>
                              <td>2023-03-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_05086v4_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.05086v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_06524v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SST: Real-time End-to-end Monocular 3D Reconstruction via Sparse Spatial-Temporal Guidance</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_06524v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_06524v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_06524v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Real-time monocular 3D reconstruction is a challenging problem that remains unsolved. Although recent end-to-end methods have demonstrated promising results, tiny structures and geometric boundaries are hardly captured due to their insufficient supervision neglecting spatial details and oversimplified feature fusion ignoring temporal cues. To address the problems, we propose an end-to-end 3D reconstruction network SST, which utilizes Sparse estimated points from visual SLAM system as additional Spatial guidance and fuses Temporal features via a novel cross-modal attention mechanism, achieving more detailed reconstruction results. We propose a Local Spatial-Temporal Fusion module to exploit more informative spatial-temporal cues from multi-view color information and sparse priors, as well a Global Spatial-Temporal Fusion module to refine the local TSDF volumes with the world-frame model from coarse to fine. Extensive experiments on ScanNet and 7-Scenes demonstrate that SST outperforms all state-of-the-art competitors, whilst keeping a high inference speed at 59 FPS, enabling real-world applications with real-time requirements.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_06524v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>实时单目三维重建是一个尚未解决的具有挑战性的问题。尽管最近的端到端方法已经证明了有希望的结果，但由于忽略空间细节的监督不足和忽略时间线索的特征融合过于简单，很难捕捉到微小的结构和几何边界。为了解决这些问题，我们提出了一种端到端的3D重建网络SST，该网络利用视觉SLAM系统的稀疏估计点作为额外的空间引导，并通过一种新的跨模态注意力机制融合时间特征，获得更详细的重建结果。我们提出了一个局部时空融合模块来利用来自多视图颜色信息和稀疏先验的更具信息性的时空线索，以及一个全局时空融合模块，用世界帧模型从粗到细细化局部TSDF体积。在ScanNet和7-Scenes上进行的大量实验表明，SST优于所有最先进的竞争对手，同时保持59 FPS的高推理速度，使现实世界的应用程序具有实时要求。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.06524v2" target="_blank">2212.06524v2</a>
                              </td>
                              <td>SST: Real-time End-to-end Monocular 3D Reconstruction via Sparse Spatial-Temporal Guidance</td>
                              <td>Chenyangguang Zhang</td>
                              <td>2022-12-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_06524v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.06524v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_12836v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GNSS-stereo-inertial SLAM for arable farming</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_12836v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_12836v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_12836v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The accelerating pace in the automation of agricultural tasks demands highly accurate and robust localization systems for field robots. Simultaneous Localization and Mapping (SLAM) methods inevitably accumulate drift on exploratory trajectories and primarily rely on place revisiting and loop closing to keep a bounded global localization error. Loop closure techniques are significantly challenging in agricultural fields, as the local visual appearance of different views is very similar and might change easily due to weather effects. A suitable alternative in practice is to employ global sensor positioning systems jointly with the rest of the robot sensors. In this paper we propose and implement the fusion of global navigation satellite system (GNSS), stereo views, and inertial measurements for localization purposes. Specifically, we incorporate, in a tightly coupled manner, GNSS measurements into the stereo-inertial ORB-SLAM3 pipeline. We thoroughly evaluate our implementation in the sequences of the Rosario data set, recorded by an autonomous robot in soybean fields, and our own in-house data. Our data includes measurements from a conventional GNSS, rarely included in evaluations of state-of-the-art approaches. We characterize the performance of GNSS-stereo-inertial SLAM in this application case, reporting pose error reductions between 10% and 30% compared to visual-inertial and loosely coupled GNSS-stereo-inertial baselines. In addition to such analysis, we also release the code of our implementation as open source.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_12836v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>农业任务自动化的加速要求现场机器人具有高度准确和稳健的定位系统。同时定位和映射（SLAM）方法不可避免地会在探索轨迹上积累漂移，并且主要依靠位置重访和闭环来保持有界的全局定位误差。闭环技术在农业领域具有重大挑战性，因为不同视图的局部视觉外观非常相似，并且可能很容易因天气影响而发生变化。实践中合适的替代方案是将全局传感器定位系统与机器人传感器的其余部分联合使用。在本文中，我们提出并实现了全球导航卫星系统（GNSS）、立体视图和惯性测量的融合，用于定位目的。具体而言，我们以紧密耦合的方式将全球导航卫星系统的测量纳入立体惯性ORB-SLAM3管道。我们彻底评估了我们在罗萨里奥数据集序列中的实现，该数据集由大豆田中的自主机器人记录，以及我们自己的内部数据。我们的数据包括传统全球导航卫星系统的测量结果，很少包括在对最先进方法的评估中。我们在这个应用案例中描述了GNSS立体惯性SLAM的性能，报告称与视觉惯性和松耦合GNSS立体惯导基线相比，姿态误差降低了10%至30%。除了这样的分析，我们还将实现的代码作为开源发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.12836v1" target="_blank">2307.12836v1</a>
                              </td>
                              <td>GNSS-stereo-inertial SLAM for arable farming</td>
                              <td>Javier Cremona</td>
                              <td>2023-07-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_12836v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.12836v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_12326v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Scale jump-aware pose graph relaxation for monocular SLAM with re-initializations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_12326v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_12326v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_12326v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Pose graph relaxation has become an indispensable addition to SLAM enabling efficient global registration of sensor reference frames under the objective of satisfying pair-wise relative transformation constraints. The latter may be given by incremental motion estimation or global place recognition. While the latter case enables loop closures and drift compensation, care has to be taken in the monocular case in which local estimates of structure and displacements can differ from reality not just in terms of noise, but also in terms of a scale factor. Owing to the accumulation of scale propagation errors, this scale factor is drifting over time, hence scale-drift aware pose graph relaxation has been introduced. We extend this idea to cases in which the relative scale between subsequent sensor frames is unknown, a situation that can easily occur if monocular SLAM enters re-initialization and no reliable overlap between successive local maps can be identified. The approach is realized by a hybrid pose graph formulation that combines the regular similarity consistency terms with novel, scale-blind constraints. We apply the technique to the practically relevant case of small indoor service robots capable of effectuating purely rotational displacements, a condition that can easily cause tracking failures. We demonstrate that globally consistent trajectories can be recovered even if multiple re-initializations occur along the loop, and present an in-depth study of success and failure cases.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_12326v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>姿态图松弛已成为SLAM不可或缺的补充，能够在满足成对相对变换约束的目标下实现传感器参考帧的有效全局配准。后者可以通过增量运动估计或全局位置识别来给出。虽然后一种情况能够实现闭环和漂移补偿，但在单目情况下必须小心，在这种情况下，结构和位移的局部估计不仅在噪声方面，而且在比例因子方面都可能与现实不同。由于尺度传播误差的累积，该尺度因子随着时间的推移而漂移，因此引入了尺度漂移感知的姿态图松弛。我们将这一想法扩展到后续传感器帧之间的相对比例未知的情况，如果单目SLAM进入重新初始化，并且无法识别连续局部映射之间的可靠重叠，这种情况很容易发生。该方法通过一种混合姿态图公式来实现，该公式将规则相似性一致性项与新颖的尺度盲约束相结合。我们将该技术应用于能够实现纯旋转位移的小型室内服务机器人的实际相关案例，这种情况很容易导致跟踪故障。我们证明，即使在循环中发生多次重新初始化，也可以恢复全局一致的轨迹，并对成功和失败案例进行了深入研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.12326v1" target="_blank">2307.12326v1</a>
                              </td>
                              <td>Scale jump-aware pose graph relaxation for monocular SLAM with re-initializations</td>
                              <td>Runze Yuan</td>
                              <td>2023-07-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_12326v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.12326v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10984v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Metric3D: Towards Zero-shot Metric 3D Prediction from A Single Image</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10984v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10984v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10984v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Reconstructing accurate 3D scenes from images is a long-standing vision task. Due to the ill-posedness of the single-image reconstruction problem, most well-established methods are built upon multi-view geometry. State-of-the-art (SOTA) monocular metric depth estimation methods can only handle a single camera model and are unable to perform mixed-data training due to the metric ambiguity. Meanwhile, SOTA monocular methods trained on large mixed datasets achieve zero-shot generalization by learning affine-invariant depths, which cannot recover real-world metrics. In this work, we show that the key to a zero-shot single-view metric depth model lies in the combination of large-scale data training and resolving the metric ambiguity from various camera models. We propose a canonical camera space transformation module, which explicitly addresses the ambiguity problems and can be effortlessly plugged into existing monocular models. Equipped with our module, monocular models can be stably trained with over 8 million images with thousands of camera models, resulting in zero-shot generalization to in-the-wild images with unseen camera settings. Experiments demonstrate SOTA performance of our method on 7 zero-shot benchmarks. Notably, our method won the championship in the 2nd Monocular Depth Estimation Challenge. Our method enables the accurate recovery of metric 3D structures on randomly collected internet images, paving the way for plausible single-image metrology. The potential benefits extend to downstream tasks, which can be significantly improved by simply plugging in our model. For example, our model relieves the scale drift issues of monocular-SLAM (Fig. 1), leading to high-quality metric scale dense mapping. The code is available at https://github.com/YvanYin/Metric3D.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10984v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从图像中重建精确的3D场景是一项长期的视觉任务。由于单图像重建问题的不适定性，大多数公认的方法都建立在多视图几何的基础上。现有技术（SOTA）单目度量深度估计方法只能处理单个相机模型，并且由于度量模糊性而无法执行混合数据训练。同时，在大型混合数据集上训练的SOTA单目方法通过学习仿射不变深度来实现零样本泛化，而仿射不变深度不能恢复真实世界的度量。在这项工作中，我们表明零样本单视图测量深度模型的关键在于结合大规模数据训练和解决各种相机模型的测量模糊性。我们提出了一个规范的相机空间转换模块，它明确地解决了模糊问题，并且可以毫不费力地插入到现有的单目模型中。配备我们的模块，单目模型可以通过数千个相机模型稳定地训练超过800万张图像，从而实现对具有看不见相机设置的现场图像的零样本泛化。实验证明了我们的方法在7个零样本基准上的SOTA性能。值得注意的是，我们的方法在第二届单目深度估计挑战赛中获得了冠军。我们的方法能够在随机收集的互联网图像上准确恢复度量三维结构，为合理的单图像度量铺平了道路。潜在的好处延伸到下游任务，只需插入我们的模型就可以显著改善这些任务。例如，我们的模型缓解了单目SLAM的尺度漂移问题（图1），导致了高质量的度量尺度密集映射。代码可在https://github.com/YvanYin/Metric3D.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10984v1" target="_blank">2307.10984v1</a>
                              </td>
                              <td>Metric3D: Towards Zero-shot Metric 3D Prediction from A Single Image</td>
                              <td>Wei Yin</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10984v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10984v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_07894v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">iSLAM: Imperative SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_07894v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_07894v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_07894v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localization and mapping (SLAM) stands as one of the critical challenges in robot navigation. Recent advancements suggest that methods based on supervised learning deliver impressive performance in front-end odometry, while traditional optimization-based methods still play a vital role in the back-end for minimizing estimation drift. In this paper, we found that such decoupled paradigm can lead to only sub-optimal performance, consequently curtailing system capabilities and generalization potential. To solve this problem, we proposed a novel self-supervised learning framework, imperative SLAM (iSLAM), which fosters reciprocal correction between the front-end and back-end, thus enhancing performance without necessitating any external supervision. Specifically, we formulate a SLAM system as a bi-level optimization problem so that the two components are bidirectionally connected. As a result, the front-end model is able to learn global geometric knowledge obtained through pose graph optimization by back-propagating the residuals from the back-end. This significantly improves the generalization ability of the entire system and thus achieves the accuracy improvement up to 45%. To the best of our knowledge, iSLAM is the first SLAM system showing that the front-end and back-end can learn jointly and mutually contribute to each other in a self-supervised manner.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_07894v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和映射（SLAM）是机器人导航中的关键挑战之一。最近的进展表明，基于监督学习的方法在前端里程计中提供了令人印象深刻的性能，而传统的基于优化的方法在后端仍然发挥着至关重要的作用，以最大限度地减少估计漂移。在本文中，我们发现这种解耦范式只能导致次优性能，从而削弱系统能力和泛化潜力。为了解决这个问题，我们提出了一种新的自我监督学习框架，即命令式SLAM（iSLAM），它促进了前端和后端之间的相互校正，从而在不需要任何外部监督的情况下提高了性能。具体来说，我们将SLAM系统公式化为双层优化问题，使两个组件双向连接。因此，前端模型能够通过从后端反向传播残差来学习通过位姿图优化获得的全局几何知识。这显著提高了整个系统的泛化能力，从而实现了高达45%的精度提高。据我们所知，iSLAM是第一个SLAM系统，表明前端和后端可以以自我监督的方式共同学习并相互贡献。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.07894v3" target="_blank">2306.07894v3</a>
                              </td>
                              <td>iSLAM: Imperative SLAM</td>
                              <td>Taimeng Fu</td>
                              <td>2023-06-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_07894v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.07894v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10015v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Optimizing the extended Fourier Mellin Transformation Algorithm</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10015v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10015v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10015v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With the increasing application of robots, stable and efficient Visual Odometry (VO) algorithms are becoming more and more important. Based on the Fourier Mellin Transformation (FMT) algorithm, the extended Fourier Mellin Transformation (eFMT) is an image registration approach that can be applied to downward-looking cameras, for example on aerial and underwater vehicles. eFMT extends FMT to multi-depth scenes and thus more application scenarios. It is a visual odometry method which estimates the pose transformation between three overlapping images. On this basis, we develop an optimized eFMT algorithm that improves certain aspects of the method and combines it with back-end optimization for the small loop of three consecutive frames. For this we investigate the extraction of uncertainty information from the eFMT registration, the related objective function and the graph-based optimization. Finally, we design a series of experiments to investigate the properties of this approach and compare it with other VO and SLAM (Simultaneous Localization and Mapping) algorithms. The results show the superior accuracy and speed of our o-eFMT approach, which is published as open source.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10015v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着机器人应用的日益广泛，稳定高效的视觉测距算法变得越来越重要。基于傅立叶-梅林变换（FMT）算法，扩展傅立叶-梅林转换（eFMT）是一种图像配准方法，可应用于向下看的相机，例如航空和水下航行器。eFMT将FMT扩展到多深度场景，从而扩展到更多的应用场景。这是一种视觉里程计方法，用于估计三个重叠图像之间的姿态变换。在此基础上，我们开发了一种优化的eFMT算法，该算法改进了该方法的某些方面，并将其与三个连续帧的小循环的后端优化相结合。为此，我们研究了从eFMT配准、相关目标函数和基于图的优化中提取不确定性信息。最后，我们设计了一系列实验来研究该方法的性能，并将其与其他VO和SLAM（同步定位和映射）算法进行了比较。结果表明，我们的o-eFMT方法具有卓越的准确性和速度，该方法以开源形式发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10015v1" target="_blank">2307.10015v1</a>
                              </td>
                              <td>Optimizing the extended Fourier Mellin Transformation Algorithm</td>
                              <td>Wenqing Jiang</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10015v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10015v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09531v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LOG-LIO: A LiDAR-Inertial Odometry with Efficient Local Geometric Information Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09531v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09531v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09531v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Local geometric information, i.e. normal and point distribution, is crucial for LiDAR-based simultaneous localization and mapping (SLAM) because it provides constrains for data association, which further determines the direction of optimization and ultimately affects the accuracy of poses. However, estimating normal and point distribution are time-consuming tasks even with the assistance of the KDtree or volumetic maps. To achieve fast normal estimation, we look into the structural information of LiDAR scan and propose a novel fast approximate least squares (FALS) method. With the pre-computed bearing information, estimating the normal requires only the range information of the points when a new scan arrives. To efficiently estimate the distribution of points, we extend the ikd-tree to manage the map in voxels and update its point cloud distribution incrementally while maintaining its consistency with the normals. For scan points that satisfy visibility and consistency checks based on normal, we devise a robust and accurate hierarchical data association schema considering the distribution where point-to-surfel is prioritized over point-to-plane. We further fix voxels after the distribution convergences to balance the time consumption and the correctness of representation. Extensive experiments on diverse public datasets demonstrate the advantages of our system compared to other state-of-the-art methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09531v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>局部几何信息，即法线和点分布，对于基于激光雷达的同时定位和映射（SLAM）至关重要，因为它为数据关联提供了约束，从而进一步决定了优化的方向，并最终影响姿态的准确性。然而，即使在KDtree或体积图的帮助下，估计正态分布和点分布也是耗时的任务。为了实现快速正态估计，我们研究了激光雷达扫描的结构信息，并提出了一种新的快速近似最小二乘法。利用预先计算的方位信息，当新的扫描到达时，估计法线只需要点的距离信息。为了有效地估计点的分布，我们扩展了ikd树来管理体素中的贴图，并在保持其与法线一致性的同时逐步更新其点云分布。对于满足基于正态的可见性和一致性检查的扫描点，我们设计了一个稳健而准确的分层数据关联模式，考虑到点到表面优先于点到平面的分布。我们在分布收敛后进一步固定体素，以平衡时间消耗和表示的正确性。在不同的公共数据集上进行的大量实验表明，与其他最先进的方法相比，我们的系统具有优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09531v1" target="_blank">2307.09531v1</a>
                              </td>
                              <td>LOG-LIO: A LiDAR-Inertial Odometry with Efficient Local Geometric Information Estimation</td>
                              <td>Kai Huang</td>
                              <td>2023-07-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09531v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09531v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09044v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D-SeqMOS: A Novel Sequential 3D Moving Object Segmentation in Autonomous Driving</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09044v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09044v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09044v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>For the SLAM system in robotics and autonomous driving, the accuracy of front-end odometry and back-end loop-closure detection determine the whole intelligent system performance. But the LiDAR-SLAM could be disturbed by current scene moving objects, resulting in drift errors and even loop-closure failure. Thus, the ability to detect and segment moving objects is essential for high-precision positioning and building a consistent map. In this paper, we address the problem of moving object segmentation from 3D LiDAR scans to improve the odometry and loop-closure accuracy of SLAM. We propose a novel 3D Sequential Moving-Object-Segmentation (3D-SeqMOS) method that can accurately segment the scene into moving and static objects, such as moving and static cars. Different from the existing projected-image method, we process the raw 3D point cloud and build a 3D convolution neural network for MOS task. In addition, to make full use of the spatio-temporal information of point cloud, we propose a point cloud residual mechanism using the spatial features of current scan and the temporal features of previous residual scans. Besides, we build a complete SLAM framework to verify the effectiveness and accuracy of 3D-SeqMOS. Experiments on SemanticKITTI dataset show that our proposed 3D-SeqMOS method can effectively detect moving objects and improve the accuracy of LiDAR odometry and loop-closure detection. The test results show our 3D-SeqMOS outperforms the state-of-the-art method by 12.4%. We extend the proposed method to the SemanticKITTI: Moving Object Segmentation competition and achieve the 2nd in the leaderboard, showing its effectiveness.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09044v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对于机器人和自动驾驶领域的SLAM系统来说，前端里程计和后端闭环检测的准确性决定了整个智能系统的性能。但激光雷达SLAM可能会受到当前场景移动物体的干扰，导致漂移误差，甚至环路闭合失败。因此，检测和分割移动物体的能力对于高精度定位和构建一致的地图至关重要。在本文中，我们解决了从三维激光雷达扫描中分割运动对象的问题，以提高SLAM的里程计和闭环精度。我们提出了一种新的3D顺序运动对象分割（3D-SeqMOS）方法，该方法可以准确地将场景分割为运动和静态对象，如运动和静态汽车。与现有的投影图像方法不同，我们对原始的三维点云进行了处理，并构建了用于MOS任务的三维卷积神经网络。此外，为了充分利用点云的时空信息，我们利用当前扫描的空间特征和先前残差扫描的时间特征，提出了一种点云残差机制。此外，我们建立了一个完整的SLAM框架来验证3D SeqMOS的有效性和准确性。在SemanticKITTI数据集上的实验表明，我们提出的3D SeqMOS方法可以有效地检测运动物体，并提高激光雷达里程计和闭环检测的准确性。测试结果表明，我们的3D SeqMOS比最先进的方法高12.4%。我们将所提出的方法扩展到SemanticKITTI:运动对象分割竞赛中，并在排行榜上排名第二，显示了它的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09044v1" target="_blank">2307.09044v1</a>
                              </td>
                              <td>3D-SeqMOS: A Novel Sequential 3D Moving Object Segmentation in Autonomous Driving</td>
                              <td>Qipeng Li</td>
                              <td>2023-07-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09044v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09044v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_08221v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NDT-Map-Code: A 3D global descriptor for real-time loop closure detection in lidar SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_08221v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_08221v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_08221v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Loop-closure detection, also known as place recognition, aiming to identify previously visited locations, is an essential component of a SLAM system. Existing research on lidar-based loop closure heavily relies on dense point cloud and 360 FOV lidars. This paper proposes an out-of-the-box NDT (Normal Distribution Transform) based global descriptor, NDT-Map-Code, designed for both on-road driving and underground valet parking scenarios. NDT-Map-Code can be directly extracted from the NDT map without the need for a dense point cloud, resulting in excellent scalability and low maintenance cost. The NDT representation is leveraged to identify representative patterns, which are further encoded according to their spatial location (bearing, range, and height). Experimental results on the NIO underground parking lot dataset and the KITTI dataset demonstrate that our method achieves significantly better performance compared to the state-of-the-art.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_08221v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>闭环检测，也称为位置识别，旨在识别以前访问过的位置，是SLAM系统的重要组成部分。现有的基于激光雷达的闭环研究在很大程度上依赖于密集点云和360视场激光雷达。本文提出了一种开箱即用的基于无损检测（正态分布变换）的全局描述符无损检测地图代码，该描述符专为道路驾驶和地下代客泊车场景设计。无损检测图代码可以直接从无损检测图中提取，无需密集的点云，具有良好的可扩展性和较低的维护成本。无损检测表示用于识别代表性图案，并根据其空间位置（方位、范围和高度）对其进行进一步编码。在NIO地下停车场数据集和KITTI数据集上的实验结果表明，与最先进的方法相比，我们的方法实现了显著更好的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.08221v1" target="_blank">2307.08221v1</a>
                              </td>
                              <td>NDT-Map-Code: A 3D global descriptor for real-time loop closure detection in lidar SLAM</td>
                              <td>Lizhou Liao</td>
                              <td>2023-07-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_08221v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.08221v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07936v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Joint Beam Management and SLAM for mmWave Communication Systems</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07936v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07936v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07936v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The millimeter-wave (mmWave) communication technology, which employs large-scale antenna arrays, enables inherent sensing capabilities. Simultaneous localization and mapping (SLAM) can utilize channel multipath angle estimates to realize integrated sensing and communication design in 6G communication systems. However, existing works have ignored the significant overhead required by the mmWave beam management when implementing SLAM with angle estimates. This study proposes a joint beam management and SLAM design that utilizes the strong coupling between the radio map and channel multipath for simultaneous beam management, localization, and mapping. In this approach, we first propose a hierarchical sweeping and sensing service design. The path angles are estimated in the hierarchical sweeping, enabling angle-based SLAM with the aid of an inertial measurement unit (IMU) to realize sensing service. Then, feature-aided tracking is proposed that utilizes prior angle information generated from the radio map and IMU. Finally, a switching module is introduced to enable flexible switching between hierarchical sweeping and feature-aided tracking. Simulations show that the proposed joint design can achieve sub-meter level localization and mapping accuracy (with an error < 0.5 m). Moreover, the beam management overhead can be reduced by approximately 40% in different wireless environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07936v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>毫米波（mmWave）通信技术采用了大规模天线阵列，实现了固有的传感能力。同时定位和映射（SLAM）可以利用信道多径角度估计来实现6G通信系统中的集成传感和通信设计。然而，现有工作忽略了毫米波波束管理在使用角度估计实现SLAM时所需的大量开销。本研究提出了一种联合波束管理和SLAM设计，该设计利用无线电映射和信道多径之间的强耦合来同时进行波束管理、定位和映射。在这种方法中，我们首先提出了一种分层的扫描和感知服务设计。在分层扫描中估计路径角度，使基于角度的SLAM能够在惯性测量单元（IMU）的帮助下实现传感服务。然后，提出了利用无线电地图和IMU生成的先验角度信息的特征辅助跟踪。最后，引入了一个切换模块，以实现分层扫描和特征辅助跟踪之间的灵活切换。仿真表明，所提出的联合设计可以实现亚米级的定位和映射精度（误差＜0.5m）。此外，在不同的无线环境中，波束管理开销可以减少大约40%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07936v1" target="_blank">2307.07936v1</a>
                              </td>
                              <td>Joint Beam Management and SLAM for mmWave Communication Systems</td>
                              <td>Hang Que</td>
                              <td>2023-07-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07936v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07936v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07763v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Tightly-Coupled LiDAR-Visual SLAM Based on Geometric Features for Mobile Agents</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07763v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07763v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07763v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The mobile robot relies on SLAM (Simultaneous Localization and Mapping) to provide autonomous navigation and task execution in complex and unknown environments. However, it is hard to develop a dedicated algorithm for mobile robots due to dynamic and challenging situations, such as poor lighting conditions and motion blur. To tackle this issue, we propose a tightly-coupled LiDAR-visual SLAM based on geometric features, which includes two sub-systems (LiDAR and monocular visual SLAM) and a fusion framework. The fusion framework associates the depth and semantics of the multi-modal geometric features to complement the visual line landmarks and to add direction optimization in Bundle Adjustment (BA). This further constrains visual odometry. On the other hand, the entire line segment detected by the visual subsystem overcomes the limitation of the LiDAR subsystem, which can only perform the local calculation for geometric features. It adjusts the direction of linear feature points and filters out outliers, leading to a higher accurate odometry system. Finally, we employ a module to detect the subsystem's operation, providing the LiDAR subsystem's output as a complementary trajectory to our system while visual subsystem tracking fails. The evaluation results on the public dataset M2DGR, gathered from ground robots across various indoor and outdoor scenarios, show that our system achieves more accurate and robust pose estimation compared to current state-of-the-art multi-modal methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07763v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>移动机器人依靠SLAM（同步定位和映射）在复杂和未知的环境中提供自主导航和任务执行。然而，由于动态和具有挑战性的情况，例如较差的照明条件和运动模糊，很难为移动机器人开发专用算法。为了解决这个问题，我们提出了一种基于几何特征的紧密耦合激光雷达视觉SLAM，它包括两个子系统（激光雷达和单目视觉SLAM）和一个融合框架。融合框架将多模态几何特征的深度和语义相关联，以补充视觉线地标，并在束调整（BA）中添加方向优化。这进一步限制了视觉里程计。另一方面，视觉子系统检测到的整个线段克服了激光雷达子系统只能对几何特征进行局部计算的局限性。它调整线性特征点的方向并过滤掉异常值，从而实现更高精度的里程计系统。最后，我们使用一个模块来检测子系统的操作，在视觉子系统跟踪失败时，将激光雷达子系统的输出作为我们系统的补充轨迹。从各种室内和室外场景中的地面机器人收集的公共数据集M2DGR的评估结果表明，与当前最先进的多模态方法相比，我们的系统实现了更准确、更稳健的姿态估计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07763v1" target="_blank">2307.07763v1</a>
                              </td>
                              <td>Tightly-Coupled LiDAR-Visual SLAM Based on Geometric Features for Mobile Agents</td>
                              <td>Ke Cao</td>
                              <td>2023-07-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07763v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07763v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07296v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Reinforcement Learning with Frontier-Based Exploration via Autonomous Environment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07296v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07296v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07296v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Active Simultaneous Localisation and Mapping (SLAM) is a critical problem in autonomous robotics, enabling robots to navigate to new regions while building an accurate model of their surroundings. Visual SLAM is a popular technique that uses virtual elements to enhance the experience. However, existing frontier-based exploration strategies can lead to a non-optimal path in scenarios where there are multiple frontiers with similar distance. This issue can impact the efficiency and accuracy of Visual SLAM, which is crucial for a wide range of robotic applications, such as search and rescue, exploration, and mapping. To address this issue, this research combines both an existing Visual-Graph SLAM known as ExploreORB with reinforcement learning. The proposed algorithm allows the robot to learn and optimize exploration routes through a reward-based system to create an accurate map of the environment with proper frontier selection. Frontier-based exploration is used to detect unexplored areas, while reinforcement learning optimizes the robot's movement by assigning rewards for optimal frontier points. Graph SLAM is then used to integrate the robot's sensory data and build an accurate map of the environment. The proposed algorithm aims to improve the efficiency and accuracy of ExploreORB by optimizing the exploration process of frontiers to build a more accurate map. To evaluate the effectiveness of the proposed approach, experiments will be conducted in various virtual environments using Gazebo, a robot simulation software. Results of these experiments will be compared with existing methods to demonstrate the potential of the proposed approach as an optimal solution for SLAM in autonomous robotics.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07296v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>主动同步定位和映射（SLAM）是自主机器人的一个关键问题，它使机器人能够导航到新的区域，同时建立其周围环境的准确模型。视觉SLAM是一种流行的技术，它使用虚拟元素来增强体验。然而，在存在距离相似的多个边界的情况下，现有的基于边界的勘探策略可能会导致非最优路径。这个问题可能会影响视觉SLAM的效率和准确性，而视觉SLAM对于搜索和救援、勘探和测绘等广泛的机器人应用至关重要。为了解决这个问题，本研究将现有的可视化图形SLAM ExploreORB与强化学习相结合。所提出的算法允许机器人通过基于奖励的系统学习和优化探索路线，以创建具有适当边界选择的准确环境地图。基于边界的探索用于检测未探索的区域，而强化学习通过为最佳边界点分配奖励来优化机器人的运动。然后使用图形SLAM来整合机器人的感官数据，并构建准确的环境地图。所提出的算法旨在通过优化边界探索过程来构建更准确的地图，从而提高ExploreORB的效率和准确性。为了评估所提出方法的有效性，将使用机器人模拟软件Gazebo在各种虚拟环境中进行实验。将这些实验的结果与现有方法进行比较，以证明所提出的方法作为自主机器人SLAM的最佳解决方案的潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07296v1" target="_blank">2307.07296v1</a>
                              </td>
                              <td>Reinforcement Learning with Frontier-Based Exploration via Autonomous Environment</td>
                              <td>Kenji Leong</td>
                              <td>2023-07-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07296v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07296v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_07308v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NeuSE: Neural SE(3)-Equivariant Embedding for Consistent Spatial Understanding with Objects</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_07308v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_07308v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_07308v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present NeuSE, a novel Neural SE(3)-Equivariant Embedding for objects, and illustrate how it supports object SLAM for consistent spatial understanding with long-term scene changes. NeuSE is a set of latent object embeddings created from partial object observations. It serves as a compact point cloud surrogate for complete object models, encoding full shape information while transforming SE(3)-equivariantly in tandem with the object in the physical world. With NeuSE, relative frame transforms can be directly derived from inferred latent codes. Our proposed SLAM paradigm, using NeuSE for object shape and pose characterization, can operate independently or in conjunction with typical SLAM systems. It directly infers SE(3) camera pose constraints that are compatible with general SLAM pose graph optimization, while also maintaining a lightweight object-centric map that adapts to real-world changes. Our approach is evaluated on synthetic and real-world sequences featuring changed objects and shows improved localization accuracy and change-aware mapping capability, when working either standalone or jointly with a common SLAM pipeline.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_07308v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了NeuSE，一种新的用于对象的Neural SE（3）-等变量嵌入，并说明了它如何支持对象SLAM在长期场景变化的情况下实现一致的空间理解。NeuSE是根据部分对象观测创建的一组潜在对象嵌入。它充当完整对象模型的紧凑点云代理，对完整形状信息进行编码，同时与物理世界中的对象等变地变换SE（3）。使用NeuSE，可以直接从推断的潜在代码中导出相对帧变换。我们提出的SLAM范式，使用NeuSE进行物体形状和姿态表征，可以独立操作，也可以与典型的SLAM系统结合操作。它直接推断SE（3）相机姿势约束，这些约束与通用SLAM姿势图优化兼容，同时还保持了一个适应现实世界变化的轻量级以对象为中心的贴图。我们的方法在以变化对象为特征的合成序列和真实世界序列上进行了评估，并在独立或与通用SLAM管道联合工作时显示出改进的定位精度和变化感知映射能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.07308v2" target="_blank">2303.07308v2</a>
                              </td>
                              <td>NeuSE: Neural SE(3)-Equivariant Embedding for Consistent Spatial Understanding with Objects</td>
                              <td>Jiahui Fu</td>
                              <td>2023-03-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_07308v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.07308v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_08978v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Visual-LiDAR Odometry and Mapping with Monocular Scale Correction and Visual Bootstrapping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_08978v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_08978v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_08978v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a novel visual-LiDAR odometry and mapping method with low-drift characteristics. The proposed method is based on two popular approaches, ORB-SLAM and A-LOAM, with monocular scale correction and visual-bootstrapped LiDAR poses initialization modifications. The scale corrector calculates the proportion between the depth of image keypoints recovered by triangulation and that provided by LiDAR, using an outlier rejection process for accuracy improvement. Concerning LiDAR poses initialization, the visual odometry approach gives the initial guesses of LiDAR motions for better performance. This methodology is not only applicable to high-resolution LiDAR but can also adapt to low-resolution LiDAR. To evaluate the proposed SLAM system's robustness and accuracy, we conducted experiments on the KITTI Odometry and S3E datasets. Experimental results illustrate that our method significantly outperforms standalone ORB-SLAM2 and A-LOAM. Furthermore, regarding the accuracy of visual odometry with scale correction, our method performs similarly to the stereo-mode ORB-SLAM2.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_08978v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种新的具有低漂移特性的视觉激光雷达里程计和测绘方法。该方法基于两种流行的方法，ORB-SLAM和A-LOAM，并对单目尺度校正和视觉自举激光雷达姿态初始化进行了修改。比例校正器使用异常值抑制过程来计算通过三角测量恢复的图像关键点的深度与由激光雷达提供的图像关键点将的深度之间的比例，以提高精度。关于激光雷达姿态初始化，视觉里程计方法对激光雷达的运动进行了初步猜测，以获得更好的性能。该方法不仅适用于高分辨率激光雷达，也适用于低分辨率激光雷达。为了评估所提出的SLAM系统的鲁棒性和准确性，我们在KITTI Odometry和S3E数据集上进行了实验。实验结果表明，我们的方法显著优于独立的ORB-SLAM2和A-LOAM。此外，关于带刻度校正的视觉里程计的准确性，我们的方法与立体模式ORB-SLAM2类似。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.08978v2" target="_blank">2304.08978v2</a>
                              </td>
                              <td>Visual-LiDAR Odometry and Mapping with Monocular Scale Correction and Visual Bootstrapping</td>
                              <td>Hanyu Cai</td>
                              <td>2023-04-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_08978v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.08978v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03890v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Ground-Challenge: A Multi-sensor SLAM Dataset Focusing on Corner Cases for Ground Robots</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03890v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03890v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03890v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>High-quality datasets can speed up breakthroughs and reveal potential developing directions in SLAM research. To support the research on corner cases of visual SLAM systems, this paper presents Ground-Challenge: a challenging dataset comprising 36 trajectories with diverse corner cases such as aggressive motion, severe occlusion, changing illumination, few textures, pure rotation, motion blur, wheel suspension, etc. The dataset was collected by a ground robot with multiple sensors including an RGB-D camera, an inertial measurement unit (IMU), a wheel odometer and a 3D LiDAR. All of these sensors were well-calibrated and synchronized, and their data were recorded simultaneously. To evaluate the performance of cutting-edge SLAM systems, we tested them on our dataset and demonstrated that these systems are prone to drift and fail on specific sequences. We will release the full dataset and relevant materials upon paper publication to benefit the research community. For more information, visit our project website at https://github.com/sjtuyinjie/Ground-Challenge.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03890v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>高质量的数据集可以加速SLAM研究的突破并揭示潜在的发展方向。为了支持对视觉SLAM系统拐角情况的研究，本文提出了Ground Challenge：一个具有挑战性的数据集，包括36条具有不同拐角情况的轨迹，如剧烈运动、严重遮挡、变化的照明、少量纹理、纯旋转、运动模糊、车轮悬架等。数据集由地面机器人收集，该机器人具有多个传感器，包括RGB-D相机、惯性测量单元（IMU）、车轮里程表和3D激光雷达。所有这些传感器都经过了良好的校准和同步，并且同时记录了它们的数据。为了评估尖端SLAM系统的性能，我们在数据集上对其进行了测试，并证明这些系统在特定序列上容易漂移和失败。我们将在论文发表后发布完整的数据集和相关材料，以造福研究界。欲了解更多信息，请访问我们的项目网站https://github.com/sjtuyinjie/Ground-Challenge.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03890v1" target="_blank">2307.03890v1</a>
                              </td>
                              <td>Ground-Challenge: A Multi-sensor SLAM Dataset Focusing on Corner Cases for Ground Robots</td>
                              <td>Jie Yin</td>
                              <td>2023-07-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03890v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03890v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_17673v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">OSPC: Online Sequential Photometric Calibration</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_17673v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_17673v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_17673v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Photometric calibration is essential to many computer vision applications. One of its key benefits is enhancing the performance of Visual SLAM, especially when it depends on a direct method for tracking, such as the standard KLT algorithm. Another advantage could be in retrieving the sensor irradiance values from measured intensities, as a pre-processing step for some vision algorithms, such as shape-from-shading. Current photometric calibration systems rely on a joint optimization problem and encounter an ambiguity in the estimates, which can only be resolved using ground truth information. We propose a novel method that solves for photometric parameters using a sequential estimation approach. Our proposed method achieves high accuracy in estimating all parameters; furthermore, the formulations are linear and convex, which makes the solution fast and suitable for online applications. Experiments on a Visual Odometry system validate the proposed method and demonstrate its advantages.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_17673v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>光度校准对许多计算机视觉应用至关重要。它的一个关键好处是提高了Visual SLAM的性能，尤其是当它依赖于直接的跟踪方法时，例如标准的KLT算法。另一个优点可以是从测量的强度中检索传感器辐照度值，作为一些视觉算法的预处理步骤，例如来自阴影的形状。当前的光度校准系统依赖于联合优化问题，并且在估计中遇到模糊性，这只能使用地面实况信息来解决。我们提出了一种新的方法，使用顺序估计方法来求解光度参数。我们提出的方法在估计所有参数方面实现了高精度；此外，该公式具有线性和凸性，使求解速度快，适用于在线应用。在视觉里程计系统上的实验验证了所提出的方法并证明了其优点。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.17673v2" target="_blank">2305.17673v2</a>
                              </td>
                              <td>OSPC: Online Sequential Photometric Calibration</td>
                              <td>Jawad Haidar</td>
                              <td>2023-05-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_17673v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.17673v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_01121v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Artifacts Mapping: Multi-Modal Semantic Mapping for Object Detection and 3D Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_01121v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_01121v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_01121v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Geometric navigation is nowadays a well-established field of robotics and the research focus is shifting towards higher-level scene understanding, such as Semantic Mapping. When a robot needs to interact with its environment, it must be able to comprehend the contextual information of its surroundings. This work focuses on classifying and localising objects within a map, which is under construction (SLAM) or already built. To further explore this direction, we propose a framework that can autonomously detect and localize predefined objects in a known environment using a multi-modal sensor fusion approach (combining RGB and depth data from an RGB-D camera and a lidar). The framework consists of three key elements: understanding the environment through RGB data, estimating depth through multi-modal sensor fusion, and managing artifacts (i.e., filtering and stabilizing measurements). The experiments show that the proposed framework can accurately detect 98% of the objects in the real sample environment, without post-processing, while 85% and 80% of the objects were mapped using the single RGBD camera or RGB + lidar setup respectively. The comparison with single-sensor (camera or lidar) experiments is performed to show that sensor fusion allows the robot to accurately detect near and far obstacles, which would have been noisy or imprecise in a purely visual or laser-based approach.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_01121v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>几何导航是当今机器人学的一个成熟领域，研究重点正转向更高层次的场景理解，如语义映射。当机器人需要与环境互动时，它必须能够理解周围环境的上下文信息。这项工作的重点是对正在建造或已经建造的地图中的对象进行分类和定位。为了进一步探索这一方向，我们提出了一个框架，该框架可以使用多模式传感器融合方法（结合RGB-D相机和激光雷达的RGB和深度数据）在已知环境中自主检测和定位预定义对象。该框架由三个关键元素组成：通过RGB数据了解环境，通过多模态传感器融合估计深度，以及管理伪影（即过滤和稳定测量）。实验表明，该框架可以在不进行后处理的情况下准确检测真实样本环境中98%的物体，而85%和80%的物体分别使用单个RGBD相机或RGB+激光雷达装置进行了映射。与单传感器（相机或激光雷达）实验的比较表明，传感器融合使机器人能够准确检测远近障碍物，而在纯视觉或基于激光的方法中，这些障碍物可能会有噪声或不精确。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.01121v1" target="_blank">2307.01121v1</a>
                              </td>
                              <td>Artifacts Mapping: Multi-Modal Semantic Mapping for Object Detection and 3D Localization</td>
                              <td>Federico Rollo</td>
                              <td>2023-07-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_01121v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.01121v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_10029v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TextSLAM: Visual SLAM with Semantic Planar Text Features</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_10029v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_10029v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_10029v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a novel visual SLAM method that integrates text objects tightly by treating them as semantic features via fully exploring their geometric and semantic prior. The text object is modeled as a texture-rich planar patch whose semantic meaning is extracted and updated on the fly for better data association. With the full exploration of locally planar characteristics and semantic meaning of text objects, the SLAM system becomes more accurate and robust even under challenging conditions such as image blurring, large viewpoint changes, and significant illumination variations (day and night). We tested our method in various scenes with the ground truth data. The results show that integrating texture features leads to a more superior SLAM system that can match images across day and night. The reconstructed semantic 3D text map could be useful for navigation and scene understanding in robotic and mixed reality applications. Our project page: https://github.com/SJTU-ViSYS/TextSLAM .</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_10029v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的视觉SLAM方法，该方法通过充分探索文本对象的几何和语义先验，将文本对象视为语义特征，从而紧密地集成文本对象。文本对象被建模为纹理丰富的平面补丁，其语义被实时提取和更新以获得更好的数据关联。随着对文本对象局部平面特征和语义的充分探索，即使在图像模糊、大的视点变化和显著的光照变化（白天和晚上）等具有挑战性的条件下，SLAM系统也变得更加准确和稳健。我们用地面实况数据在各种场景中测试了我们的方法。结果表明，集成纹理特征可以获得更优越的SLAM系统，该系统可以匹配昼夜图像。重建的语义3D文本地图可用于机器人和混合现实应用中的导航和场景理解。我们的项目页面：https://github.com/SJTU-ViSYS/TextSLAM。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.10029v2" target="_blank">2305.10029v2</a>
                              </td>
                              <td>TextSLAM: Visual SLAM with Semantic Planar Text Features</td>
                              <td>Boying Li</td>
                              <td>2023-05-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_10029v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.10029v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_00488v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">POV-SLAM: Probabilistic Object-Aware Variational SLAM in Semi-Static Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_00488v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_00488v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_00488v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localization and mapping (SLAM) in slowly varying scenes is important for long-term robot task completion. Failing to detect scene changes may lead to inaccurate maps and, ultimately, lost robots. Classical SLAM algorithms assume static scenes, and recent works take dynamics into account, but require scene changes to be observed in consecutive frames. Semi-static scenes, wherein objects appear, disappear, or move slowly over time, are often overlooked, yet are critical for long-term operation. We propose an object-aware, factor-graph SLAM framework that tracks and reconstructs semi-static object-level changes. Our novel variational expectation-maximization strategy is used to optimize factor graphs involving a Gaussian-Uniform bimodal measurement likelihood for potentially-changing objects. We evaluate our approach alongside the state-of-the-art SLAM solutions in simulation and on our novel real-world SLAM dataset captured in a warehouse over four months. Our method improves the robustness of localization in the presence of semi-static changes, providing object-level reasoning about the scene.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_00488v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在缓慢变化的场景中同时定位和映射（SLAM）对于长期完成机器人任务很重要。未能检测到场景变化可能会导致地图不准确，最终导致机器人丢失。经典的SLAM算法假设静态场景，最近的工作考虑了动态，但要求在连续帧中观察场景变化。半静态场景中，物体随着时间的推移出现、消失或缓慢移动，通常被忽视，但对长期操作至关重要。我们提出了一个对象感知的因子图SLAM框架，用于跟踪和重建半静态对象级别的变化。我们新的变分期望最大化策略用于优化因子图，该因子图涉及潜在变化对象的高斯均匀双峰测量似然。我们在模拟中评估了我们的方法以及最先进的SLAM解决方案，并在四个月内在仓库中捕获了我们新颖的真实世界SLAM数据集。我们的方法在半静态变化的情况下提高了定位的鲁棒性，提供了关于场景的对象级推理。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.00488v1" target="_blank">2307.00488v1</a>
                              </td>
                              <td>POV-SLAM: Probabilistic Object-Aware Variational SLAM in Semi-Static Environments</td>
                              <td>Jingxing Qian</td>
                              <td>2023-07-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_00488v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.00488v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_17529v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Locking On: Leveraging Dynamic Vehicle-Imposed Motion Constraints to Improve Visual Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_17529v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_17529v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_17529v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Most 6-DoF localization and SLAM systems use static landmarks but ignore dynamic objects because they cannot be usefully incorporated into a typical pipeline. Where dynamic objects have been incorporated, typical approaches have attempted relatively sophisticated identification and localization of these objects, limiting their robustness or general utility. In this research, we propose a middle ground, demonstrated in the context of autonomous vehicles, using dynamic vehicles to provide limited pose constraint information in a 6-DoF frame-by-frame PnP-RANSAC localization pipeline. We refine initial pose estimates with a motion model and propose a method for calculating the predicted quality of future pose estimates, triggered based on whether or not the autonomous vehicle's motion is constrained by the relative frame-to-frame location of dynamic vehicles in the environment. Our approach detects and identifies suitable dynamic vehicles to define these pose constraints to modify a pose filter, resulting in improved recall across a range of localization tolerances from $0.25m$ to $5m$, compared to a state-of-the-art baseline single image PnP method and its vanilla pose filtering. Our constraint detection system is active for approximately $35\%$ of the time on the Ford AV dataset and localization is particularly improved when the constraint detection is active.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_17529v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大多数6-DoF定位和SLAM系统使用静态地标，但忽略动态对象，因为它们不能有效地合并到典型的管道中。在包含动态对象的情况下，典型的方法尝试对这些对象进行相对复杂的识别和定位，限制了它们的鲁棒性或通用性。在这项研究中，我们提出了一种中间立场，在自动驾驶汽车的背景下进行了演示，使用动态车辆在6-DoF逐帧PnP-RANSAC定位管道中提供有限的姿态约束信息。我们用运动模型改进了初始姿态估计，并提出了一种计算未来姿态估计预测质量的方法，该方法基于自动驾驶车辆的运动是否受到环境中动态车辆的相对帧间位置的约束而触发。我们的方法检测并识别合适的动态车辆，以定义这些姿势约束，从而修改姿势过滤器，与最先进的基线单图像PnP方法及其普通姿势过滤相比，在25万美元至500万美元的定位公差范围内提高了召回率。在Ford AV数据集上，我们的约束检测系统在大约$35\%$的时间内是活动的，并且当约束检测是活动的时，定位特别改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.17529v1" target="_blank">2306.17529v1</a>
                              </td>
                              <td>Locking On: Leveraging Dynamic Vehicle-Imposed Motion Constraints to Improve Visual Localization</td>
                              <td>Stephen Hausler</td>
                              <td>2023-06-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_17529v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.17529v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16917v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Drunkard's Odometry: Estimating Camera Motion in Deforming Scenes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16917v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16917v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16917v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Estimating camera motion in deformable scenes poses a complex and open research challenge. Most existing non-rigid structure from motion techniques assume to observe also static scene parts besides deforming scene parts in order to establish an anchoring reference. However, this assumption does not hold true in certain relevant application cases such as endoscopies. Deformable odometry and SLAM pipelines, which tackle the most challenging scenario of exploratory trajectories, suffer from a lack of robustness and proper quantitative evaluation methodologies. To tackle this issue with a common benchmark, we introduce the Drunkard's Dataset, a challenging collection of synthetic data targeting visual navigation and reconstruction in deformable environments. This dataset is the first large set of exploratory camera trajectories with ground truth inside 3D scenes where every surface exhibits non-rigid deformations over time. Simulations in realistic 3D buildings lets us obtain a vast amount of data and ground truth labels, including camera poses, RGB images and depth, optical flow and normal maps at high resolution and quality. We further present a novel deformable odometry method, dubbed the Drunkard's Odometry, which decomposes optical flow estimates into rigid-body camera motion and non-rigid scene deformations. In order to validate our data, our work contains an evaluation of several baselines as well as a novel tracking error metric which does not require ground truth data. Dataset and code: https://davidrecasens.github.io/TheDrunkard'sOdometry/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16917v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在可变形场景中估计相机运动是一个复杂而开放的研究挑战。大多数现有的非刚性结构运动技术假设除了变形场景部分之外，还观察静态场景部分，以建立锚定参考。然而，这一假设在某些相关应用案例中并不成立，例如内镜。可变形里程计和SLAM管道解决了最具挑战性的探索轨迹场景，但缺乏稳健性和适当的定量评估方法。为了用一个通用的基准来解决这个问题，我们引入了Drunkard的数据集，这是一个具有挑战性的合成数据集，旨在在可变形环境中进行视觉导航和重建。该数据集是第一个在3D场景中具有地面实况的大型探索相机轨迹集，其中每个表面随着时间的推移都表现出非刚性变形。在逼真的3D建筑中进行模拟可以让我们获得大量的数据和地面实况标签，包括相机姿态、RGB图像和深度、光流和高分辨率和高质量的法线图。我们进一步提出了一种新的可变形里程计方法，称为Drunkard里程计，该方法将光流估计分解为刚体相机运动和非刚体场景变形。为了验证我们的数据，我们的工作包括对几个基线的评估，以及一种新的跟踪误差度量，该度量不需要地面实况数据。数据集和代码：https://davidrecasens.github.io/TheDrunkard'国内/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16917v1" target="_blank">2306.16917v1</a>
                              </td>
                              <td>The Drunkard's Odometry: Estimating Camera Motion in Deforming Scenes</td>
                              <td>David Recasens</td>
                              <td>2023-06-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16917v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16917v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16585v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SeMLaPS: Real-time Semantic Mapping with Latent Prior Networks and Quasi-Planar Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16585v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16585v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16585v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The availability of real-time semantics greatly improves the core geometric functionality of SLAM systems, enabling numerous robotic and AR/VR applications. We present a new methodology for real-time semantic mapping from RGB-D sequences that combines a 2D neural network and a 3D network based on a SLAM system with 3D occupancy mapping. When segmenting a new frame we perform latent feature re-projection from previous frames based on differentiable rendering. Fusing re-projected feature maps from previous frames with current-frame features greatly improves image segmentation quality, compared to a baseline that processes images independently. For 3D map processing, we propose a novel geometric quasi-planar over-segmentation method that groups 3D map elements likely to belong to the same semantic classes, relying on surface normals. We also describe a novel neural network design for lightweight semantic map post-processing. Our system achieves state-of-the-art semantic mapping quality within 2D-3D networks-based systems and matches the performance of 3D convolutional networks on three real indoor datasets, while working in real-time. Moreover, it shows better cross-sensor generalization abilities compared to 3D CNNs, enabling training and inference with different depth sensors. Code and data will be released on project page: http://jingwenwang95.github.io/SeMLaPS</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16585v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>实时语义的可用性极大地提高了SLAM系统的核心几何功能，实现了许多机器人和AR/VR应用。我们提出了一种从RGB-D序列进行实时语义映射的新方法，该方法将2D神经网络和基于SLAM系统的3D网络与3D占用映射相结合。在分割新帧时，我们基于可微分渲染从先前帧执行潜在特征重新投影。与独立处理图像的基线相比，将来自先前帧的重新投影的特征图与当前帧特征融合可以大大提高图像分割质量。对于3D地图处理，我们提出了一种新的几何准平面过分割方法，该方法根据曲面法线对可能属于相同语义类的3D地图元素进行分组。我们还描述了一种用于轻量级语义图后处理的新型神经网络设计。我们的系统在基于2D-3D网络的系统中实现了最先进的语义映射质量，并在三个真实的室内数据集上匹配3D卷积网络的性能，同时实时工作。此外，与3D细胞神经网络相比，它显示出更好的跨传感器泛化能力，能够使用不同的深度传感器进行训练和推理。代码和数据将在项目页面上发布：http://jingwenwang95.github.io/SeMLaPS</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16585v1" target="_blank">2306.16585v1</a>
                              </td>
                              <td>SeMLaPS: Real-time Semantic Mapping with Latent Prior Networks and Quasi-Planar Segmentation</td>
                              <td>Jingwen Wang</td>
                              <td>2023-06-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16585v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16585v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2205_05916v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Dynamic Dense RGB-D SLAM using Learning-based Visual Odometry</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2205_05916v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2205_05916v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2205_05916v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a dense dynamic RGB-D SLAM pipeline based on a learning-based visual odometry, TartanVO. TartanVO, like other direct methods rather than feature-based, estimates camera pose through dense optical flow, which only applies to static scenes and disregards dynamic objects. Due to the color constancy assumption, optical flow is not able to differentiate between dynamic and static pixels. Therefore, to reconstruct a static map through such direct methods, our pipeline resolves dynamic/static segmentation by leveraging the optical flow output, and only fuse static points into the map. Moreover, we rerender the input frames such that the dynamic pixels are removed and iteratively pass them back into the visual odometry to refine the pose estimate.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2205_05916v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种基于学习的视觉里程计TartanVO的密集动态RGB-D SLAM管道。TartanVO与其他直接方法一样，而不是基于特征的方法，通过密集的光流来估计相机姿态，这只适用于静态场景，而忽略了动态对象。由于颜色恒定性假设，光流无法区分动态像素和静态像素。因此，为了通过这种直接的方法重建静态地图，我们的管道通过利用光流输出来解决动态/静态分割，并且只将静态点融合到地图中。此外，我们重新绘制输入帧，以便去除动态像素，并迭代地将它们传递回视觉里程计，以细化姿态估计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2205.05916v2" target="_blank">2205.05916v2</a>
                              </td>
                              <td>Dynamic Dense RGB-D SLAM using Learning-based Visual Odometry</td>
                              <td>Shihao Shen</td>
                              <td>2022-05-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2205_05916v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2205.05916v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16530v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Image-based Visual Servo Control for Aerial Manipulation Using a Fully-Actuated UAV</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16530v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16530v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16530v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Using Unmanned Aerial Vehicles (UAVs) to perform high-altitude manipulation tasks beyond just passive visual application can reduce the time, cost, and risk of human workers. Prior research on aerial manipulation has relied on either ground truth state estimate or GPS/total station with some Simultaneous Localization and Mapping (SLAM) algorithms, which may not be practical for many applications close to infrastructure with degraded GPS signal or featureless environments. Visual servo can avoid the need to estimate robot pose. Existing works on visual servo for aerial manipulation either address solely end-effector position control or rely on precise velocity measurement and pre-defined visual visual marker with known pattern. Furthermore, most of previous work used under-actuated UAVs, resulting in complicated mechanical and hence control design for the end-effector. This paper develops an image-based visual servo control strategy for bridge maintenance using a fully-actuated UAV. The main components are (1) a visual line detection and tracking system, (2) a hybrid impedance force and motion control system. Our approach does not rely on either robot pose/velocity estimation from an external localization system or pre-defined visual markers. The complexity of the mechanical system and controller architecture is also minimized due to the fully-actuated nature. Experiments show that the system can effectively execute motion tracking and force holding using only the visual guidance for the bridge painting. To the best of our knowledge, this is one of the first studies on aerial manipulation using visual servo that is capable of achieving both motion and force control without the need of external pose/velocity information or pre-defined visual guidance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16530v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用无人机执行高空操纵任务，而不仅仅是被动视觉应用，可以减少人类工人的时间、成本和风险。先前对空中操纵的研究依赖于地面实况状态估计或GPS/全站仪以及一些同步定位和测绘（SLAM）算法，这对于许多接近GPS信号退化或无特征环境的基础设施的应用来说可能不实用。视觉伺服可以避免估计机器人姿态的需要。用于空中操纵的视觉伺服的现有工作要么仅涉及末端执行器位置控制，要么依赖于精确的速度测量和具有已知模式的预定义视觉标记。此外，以前的大多数工作都使用欠驱动无人机，导致末端执行器的机械设计和控制设计复杂。本文利用全驱动无人机开发了一种基于图像的桥梁维护视觉伺服控制策略。主要部件是（1）视觉线检测和跟踪系统，（2）混合阻抗力和运动控制系统。我们的方法既不依赖于来自外部定位系统的机器人姿态/速度估计，也不依赖于预定义的视觉标记。由于完全致动的性质，机械系统和控制器结构的复杂性也被最小化。实验表明，该系统只需对桥梁涂装进行视觉引导，就能有效地进行运动跟踪和力保持。据我们所知，这是第一批使用视觉伺服进行空中操纵的研究之一，该伺服能够在不需要外部姿态/速度信息或预定义视觉引导的情况下实现运动和力控制。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16530v1" target="_blank">2306.16530v1</a>
                              </td>
                              <td>Image-based Visual Servo Control for Aerial Manipulation Using a Fully-Actuated UAV</td>
                              <td>Guanqi He</td>
                              <td>2023-06-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16530v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16530v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_09553v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Mu$^{2}$SLAM: Multitask, Multilingual Speech and Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_09553v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_09553v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_09553v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present Mu$^{2}$SLAM, a multilingual sequence-to-sequence model pre-trained jointly on unlabeled speech, unlabeled text and supervised data spanning Automatic Speech Recognition (ASR), Automatic Speech Translation (AST) and Machine Translation (MT), in over 100 languages. By leveraging a quantized representation of speech as a target, Mu$^{2}$SLAM trains the speech-text models with a sequence-to-sequence masked denoising objective similar to T5 on the decoder and a masked language modeling (MLM) objective on the encoder, for both unlabeled speech and text, while utilizing the supervised tasks to improve cross-lingual and cross-modal representation alignment within the model. On CoVoST AST, Mu$^{2}$SLAM establishes a new state-of-the-art for models trained on public datasets, improving on xx-en translation over the previous best by 1.9 BLEU points and on en-xx translation by 1.1 BLEU points. On Voxpopuli ASR, our model matches the performance of an mSLAM model fine-tuned with an RNN-T decoder, despite using a relatively weaker sequence-to-sequence architecture. On text understanding tasks, our model improves by more than 6\% over mSLAM on XNLI, getting closer to the performance of mT5 models of comparable capacity on XNLI and TydiQA, paving the way towards a single model for all speech and text understanding tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_09553v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了Mu$^｛2｝$SLAM，这是一种多语言序列到序列模型，在100多种语言中，针对未标记语音、未标记文本和监督数据，联合预训练，涵盖自动语音识别（ASR）、自动语音翻译（AST）和机器翻译（MT）。通过利用语音的量化表示作为目标，Mu$^{2}$SLAM用解码器上类似于T5的序列到序列掩蔽去噪目标和编码器上的掩蔽语言建模（MLM）目标来训练语音文本模型，用于未标记的语音和文本，同时利用监督任务来改进模型内的跨语言和跨模态表示对齐。在CoVoST AST上，Mu$^｛2｝$SLAM为在公共数据集上训练的模型建立了一个新的最先进的技术，在xx-en翻译上比以前的最佳翻译提高了1.9 BLEU点，在en-xx翻译上提高了1.1 BLEU点。在Voxpopuli ASR上，尽管使用了相对较弱的序列到序列架构，但我们的模型与使用RNN-T解码器微调的mSLAM模型的性能相匹配。在文本理解任务上，我们的模型在XNLI上比mSLAM改进了6\%以上，更接近于具有可比容量的mT5模型在XNLI和TydiQA上的性能，为所有语音和文本理解任务的单一模型铺平了道路。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.09553v2" target="_blank">2212.09553v2</a>
                              </td>
                              <td>Mu$^{2}$SLAM: Multitask, Multilingual Speech and Language Models</td>
                              <td>Yong Cheng</td>
                              <td>2022-12-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_09553v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.09553v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_14812v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MOVESe: MOVablE and Moving LiDAR Scene Segmentation with Improved Navigation in Seg-label free settings</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_14812v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_14812v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_14812v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Accurate detection of movable and moving objects in LiDAR is of vital importance for navigation. Most existing works focus on extracting and removing moving objects during navigation. Movable objects like pedestrians, parked vehicles, etc. although static may move in the future. This leads to erroneous navigation and accidents. In such cases, it becomes necessary to detect potentially movable objects. To this end, we present a learning-based approach that segments movable and moving objects by generating static parts of scenes that are otherwise occluded. Our model performs superior to existing baselines on static LiDAR reconstructions using 3 datasets including a challenging sparse industrial dataset. We achieve this without the assistance of any segmentation labels because such labels might not always be available for less popular yet important settings like industrial environments. The non-movable static parts of the scene generated by our model are of vital importance for downstream navigation for SLAM. The movable objects detected by our model can be fed to a downstream 3D detector for aiding navigation. Though we do not use segmentation, we evaluate our method against navigation baselines that use it to remove dynamic objects for SLAM. Through extensive experiments on several datasets, we showcase that our model surpasses these baselines on navigation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_14812v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>激光雷达中可移动和移动物体的精确检测对导航至关重要。现有的大多数工作都集中在导航过程中提取和移除移动对象。可移动物体，如行人、停放的车辆等，尽管静止，但未来可能会移动。这会导致错误导航和事故。在这种情况下，有必要检测潜在的可移动物体。为此，我们提出了一种基于学习的方法，通过生成场景中被遮挡的静态部分来分割可移动和移动对象。在使用3个数据集（包括一个具有挑战性的稀疏工业数据集）进行静态激光雷达重建时，我们的模型表现优于现有基线。我们在没有任何细分标签的帮助下实现了这一点，因为这样的标签可能并不总是适用于不太流行但重要的环境，如工业环境。我们的模型生成的场景中不可移动的静态部分对于SLAM的下游导航至关重要。我们的模型检测到的可移动物体可以被馈送到下游的3D检测器，用于辅助导航。虽然我们不使用分割，但我们根据导航基线来评估我们的方法，导航基线使用它来移除SLAM的动态对象。通过在几个数据集上进行广泛的实验，我们展示了我们的模型在导航方面超过了这些基线。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.14812v1" target="_blank">2306.14812v1</a>
                              </td>
                              <td>MOVESe: MOVablE and Moving LiDAR Scene Segmentation with Improved Navigation in Seg-label free settings</td>
                              <td>Prashant Kumar</td>
                              <td>2023-06-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_14812v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.14812v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_14137v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BotanicGarden: A high-quality and large-scale robot navigation dataset in challenging natural environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_14137v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_14137v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_14137v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The rapid developments of mobile robotics and autonomous navigation over the years are largely empowered by public datasets for testing and upgrading, such as SLAM and localization tasks. Impressive demos and benchmark results have arisen, indicating the establishment of a mature technical framework. However, from the view point of real-world deployments, there are still critical defects of robustness in challenging environments, especially in large-scale, GNSS-denied, textural-monotonous, and unstructured scenarios. To meet the pressing validation demands in such scope, we build a novel challenging robot navigation dataset in a large botanic garden of more than 48000m2. Comprehensive sensors are employed, including high-res/rate stereo Gray&RGB cameras, rotational and forward 3D LiDARs, and low-cost and industrial-grade IMUs, all of which are well calibrated and accurately hardware-synchronized. An all-terrain wheeled robot is configured to mount the sensor suite and provide odometry data. A total of 32 long and short sequences of 2.3 million images are collected, covering scenes of thick woods, riversides, narrow paths, bridges, and grasslands that rarely appeared in previous resources. Excitedly, both highly-accurate ego-motions and 3D map ground truth are provided, along with fine-annotated vision semantics. Our goal is to contribute a high-quality dataset to advance robot navigation and sensor fusion research to a higher level.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_14137v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多年来，移动机器人和自主导航的快速发展在很大程度上得益于用于测试和升级的公共数据集，如SLAM和定位任务。令人印象深刻的演示和基准测试结果已经出现，表明建立了一个成熟的技术框架。然而，从现实世界部署的角度来看，在具有挑战性的环境中，尤其是在大规模、拒绝全球导航卫星系统、纹理单调和非结构化的场景中，仍然存在稳健性的关键缺陷。为了满足这种范围内紧迫的验证需求，我们在一个超过48000平方米的大型植物园中构建了一个具有挑战性的机器人导航数据集。采用了全面的传感器，包括高分辨率/速率立体声Gray和RGB相机、旋转和正向3D激光雷达，以及低成本和工业级IMU，所有这些都经过了良好的校准和精确的硬件同步。全地形轮式机器人被配置为安装传感器套件并提供里程计数据。共收集了32个长短序列，230万幅图像，涵盖了以前资源中很少出现的茂密的树林、河岸、狭窄的小路、桥梁和草原的场景。令人兴奋的是，提供了高度准确的自我运动和3D地图地面实况，以及精细的注释视觉语义。我们的目标是贡献一个高质量的数据集，将机器人导航和传感器融合研究推向更高的水平。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.14137v1" target="_blank">2306.14137v1</a>
                              </td>
                              <td>BotanicGarden: A high-quality and large-scale robot navigation dataset in challenging natural environments</td>
                              <td>Yuanzhi Liu</td>
                              <td>2023-06-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_14137v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.14137v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_03872v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LSGDDN-LCD: An Appearance-based Loop Closure Detection using Local Superpixel Grid Descriptors and Incremental Dynamic Nodes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_03872v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_03872v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_03872v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Loop Closure Detection (LCD) is an essential component of visual simultaneous localization and mapping (SLAM) systems. It enables the recognition of previously visited scenes to eliminate pose and map estimate drifts arising from long-term exploration. However, current appearance-based LCD methods face significant challenges, including high computational costs, viewpoint variance, and dynamic objects in scenes. This paper introduced an online appearance based LCD using local superpixel grids descriptor and dynamic node, i.e, LSGDDN-LCD, to find similarities between scenes via hand-crafted features extracted from LSGD. Unlike traditional Bag-of-Words (BoW) based LCD, which requires pre-training, we proposed an adaptive mechanism to group similar images called $\textbf{\textit{dynamic}}$ $\textbf{\textit{node}}$, which incrementally adjusted the database in an online manner, allowing for efficient and online retrieval of previously viewed images without need of the pre-training. Experimental results confirmed that the LSGDDN-LCD significantly improved LCD precision-recall and efficiency, and outperformed several state-of-the-art (SOTA) approaches on multiple typical datasets, indicating its great potential as a generic LCD framework.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_03872v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>闭环检测（LCD）是视觉同步定位和映射（SLAM）系统的重要组成部分。它能够识别以前访问过的场景，以消除长期勘探产生的姿态和地图估计漂移。然而，当前基于外观的LCD方法面临着重大挑战，包括高计算成本、视点变化和场景中的动态对象。本文介绍了一种基于在线外观的LCD，该LCD使用局部超像素网格描述符和动态节点，即LSGDDN-LCD，通过从LSGD中提取的手工特征来查找场景之间的相似性。与传统的基于单词袋（BoW）的LCD需要预训练不同，我们提出了一种自适应机制来对类似图像进行分组，称为$\textbf｛\textit｛dynamic｝｝$\textbf｛\text｛node｝}$，该机制以在线方式增量调整数据库，允许在不需要预训练的情况下高效在线检索先前查看的图像。实验结果证实，LSGDDN-LCD显著提高了LCD的查全率和效率，并在多个典型数据集上优于几种最先进的（SOTA）方法，表明其作为通用LCD框架的巨大潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.03872v2" target="_blank">2304.03872v2</a>
                              </td>
                              <td>LSGDDN-LCD: An Appearance-based Loop Closure Detection using Local Superpixel Grid Descriptors and Incremental Dynamic Nodes</td>
                              <td>Baosheng Zhang</td>
                              <td>2023-04-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_03872v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.03872v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2205_08207v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DynPL-SVO: A Robust Stereo Visual Odometry for Dynamic Scenes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2205_08207v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2205_08207v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2205_08207v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Most feature-based stereo visual odometry (SVO) approaches estimate the motion of mobile robots by matching and tracking point features along a sequence of stereo images. However, in dynamic scenes mainly comprising moving pedestrians, vehicles, etc., there are insufficient robust static point features to enable accurate motion estimation, causing failures when reconstructing robotic motion. In this paper, we proposed DynPL-SVO, a complete dynamic SVO method that integrated united cost functions containing information between matched point features and re-projection errors perpendicular and parallel to the direction of the line features. Additionally, we introduced a \textit{dynamic} \textit{grid} algorithm to enhance its performance in dynamic scenes. The stereo camera motion was estimated through Levenberg-Marquard minimization of the re-projection errors of both point and line features. Comprehensive experimental results on KITTI and EuRoC MAV datasets showed that accuracy of the DynPL-SVO was improved by over 20\% on average compared to other state-of-the-art SVO systems, especially in dynamic scenes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2205_08207v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大多数基于特征的立体视觉里程计（SVO）方法通过沿着立体图像序列匹配和跟踪点特征来估计移动机器人的运动。然而，在主要包括移动的行人、车辆等的动态场景中，没有足够的鲁棒静态点特征来实现精确的运动估计，导致在重建机器人运动时失败。在本文中，我们提出了DynPL SVO，这是一种完整的动态SVO方法，它集成了包含匹配点特征之间信息的联合代价函数和垂直和平行于线特征方向的重投影误差。此外，我们引入了\textit｛dynamic｝\textit{grid｝算法，以提高其在动态场景中的性能。立体相机的运动是通过Levenberg-Marquard最小化点和线特征的重投影误差来估计的。在KITTI和EuRoC MAV数据集上的综合实验结果表明，与其他最先进的SVO系统相比，DynPL SVO的精度平均提高了20%以上，尤其是在动态场景中。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2205.08207v3" target="_blank">2205.08207v3</a>
                              </td>
                              <td>DynPL-SVO: A Robust Stereo Visual Odometry for Dynamic Scenes</td>
                              <td>Baosheng Zhang</td>
                              <td>2022-05-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2205_08207v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2205.08207v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2109_12910v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Biologically-Inspired Simultaneous Localization and Mapping System Based on LiDAR Sensor</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2109_12910v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2109_12910v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2109_12910v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localization and mapping (SLAM) is one of the essential techniques and functionalities used by robots to perform autonomous navigation tasks. Inspired by the rodent hippocampus, this paper presents a biologically inspired SLAM system based on a LiDAR sensor using a hippocampal model to build a cognitive map and estimate the robot pose in indoor environments. Based on the biologically inspired models mimicking boundary cells, place cells, and head direction cells, the SLAM system using LiDAR point cloud data is capable of leveraging the self-motion cues from the LiDAR odometry and the boundary cues from the LiDAR boundary cells to build a cognitive map and estimate the robot pose. Experiment results show that with the LiDAR boundary cells the proposed SLAM system greatly outperforms the camera-based brain-inspired method in both simulation and indoor environments, and is competitive with the conventional LiDAR-based SLAM methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2109_12910v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和映射（SLAM）是机器人执行自主导航任务的基本技术和功能之一。受啮齿动物海马体的启发，本文提出了一种基于激光雷达传感器的生物启发SLAM系统，该系统使用海马模型构建认知地图并估计机器人在室内环境中的姿势。基于模仿边界细胞、位置细胞和头部方向细胞的生物学启发模型，使用激光雷达点云数据的SLAM系统能够利用来自激光雷达里程计的自运动线索和来自激光雷达边界细胞的边界线索来构建认知图并估计机器人姿态。实验结果表明，使用激光雷达边界单元，所提出的SLAM系统在模拟和室内环境中都大大优于基于相机的大脑启发方法，并且与传统的基于激光雷达的SLAM方法具有竞争力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2109.12910v2" target="_blank">2109.12910v2</a>
                              </td>
                              <td>A Biologically-Inspired Simultaneous Localization and Mapping System Based on LiDAR Sensor</td>
                              <td>Genghang Zhuang</td>
                              <td>2021-09-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2109_12910v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2109.12910v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_11823v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">HDPV-SLAM: Hybrid Depth-augmented Panoramic Visual SLAM for Mobile Mapping System with Tilted LiDAR and Panoramic Visual Camera</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_11823v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_11823v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_11823v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper proposes a novel visual simultaneous localization and mapping (SLAM) system called Hybrid Depth-augmented Panoramic Visual SLAM (HDPV-SLAM), that employs a panoramic camera and a tilted multi-beam LiDAR scanner to generate accurate and metrically-scaled trajectories. RGB-D SLAM was the design basis for HDPV-SLAM, which added depth information to visual features. It aims to solve the two major issues hindering the performance of similar SLAM systems. The first obstacle is the sparseness of LiDAR depth, which makes it difficult to correlate it with the extracted visual features of the RGB image. A deep learning-based depth estimation module for iteratively densifying sparse LiDAR depth was suggested to address this issue. The second issue pertains to the difficulties in depth association caused by a lack of horizontal overlap between the panoramic camera and the tilted LiDAR sensor. To surmount this difficulty, we present a hybrid depth association module that optimally combines depth information estimated by two independent procedures, feature-based triangulation and depth estimation. During a phase of feature tracking, this hybrid depth association module aims to maximize the use of more accurate depth information between the triangulated depth with visual features tracked and the deep learning-based corrected depth. We evaluated the efficacy of HDPV-SLAM using the 18.95 km-long York University and Teledyne Optech (YUTO) MMS dataset. The experimental results demonstrate that the two proposed modules contribute substantially to the performance of HDPV-SLAM, which surpasses that of the state-of-the-art (SOTA) SLAM systems.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_11823v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种新的视觉同时定位和映射（SLAM）系统，称为混合深度增强全景视觉SLAM（HDPV-SLAM），该系统使用全景相机和倾斜的多波束激光雷达扫描仪来生成精确且按度量缩放的轨迹。RGB-D SLAM是HDPV-SLAM的设计基础，它为视觉特征添加了深度信息。它旨在解决阻碍类似SLAM系统性能的两个主要问题。第一个障碍是激光雷达深度的稀疏性，这使得它很难与RGB图像的提取视觉特征相关联。为了解决这个问题，提出了一种基于深度学习的深度估计模块，用于迭代加密稀疏激光雷达深度。第二个问题涉及由全景相机和倾斜的激光雷达传感器之间缺乏水平重叠引起的深度关联的困难。为了克服这一困难，我们提出了一种混合深度关联模块，该模块将通过两个独立过程（基于特征的三角测量和深度估计）估计的深度信息进行最佳组合。在特征跟踪阶段，该混合深度关联模块旨在最大限度地使用具有跟踪的视觉特征的三角化深度和基于深度学习的校正深度之间的更准确的深度信息。我们使用长18.95km的约克大学和Teledyne Optech（YUTO）MMS数据集评估了HDPV-SLAM的疗效。实验结果表明，所提出的两个模块对HDPV-SLAM的性能有很大贡献，超过了最先进的（SOTA）SLAM系统。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.11823v3" target="_blank">2301.11823v3</a>
                              </td>
                              <td>HDPV-SLAM: Hybrid Depth-augmented Panoramic Visual SLAM for Mobile Mapping System with Tilted LiDAR and Panoramic Visual Camera</td>
                              <td>Mostafa Ahmadi</td>
                              <td>2023-01-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_11823v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.11823v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_12901v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Map Point Selection for Visual SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_12901v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_12901v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_12901v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localisation and mapping (SLAM) play a vital role in autonomous robotics. Robotic platforms are often resource-constrained, and this limitation motivates resource-efficient SLAM implementations. While sparse visual SLAM algorithms offer good accuracy for modest hardware requirements, even these more scalable sparse approaches face limitations when applied to large-scale and long-term scenarios. A contributing factor is that the point clouds resulting from SLAM are inefficient to use and contain significant redundancy.   This paper proposes the use of subset selection algorithms to reduce the map produced by sparse visual SLAM algorithms. Information-theoretic techniques have been applied to simpler related problems before, but they do not scale if applied to the full visual SLAM problem. This paper proposes a number of novel information\hyp{}theoretic utility functions for map point selection and optimises these functions using greedy algorithms. The reduced maps are evaluated using practical data alongside an existing visual SLAM implementation (ORB-SLAM 2). Approximate selection techniques proposed in this paper achieve trajectory accuracy comparable to an offline baseline while being suitable for online use. These techniques enable the practical reduction of maps for visual SLAM with competitive trajectory accuracy.   Results also demonstrate that SLAM front-end performance can significantly impact the performance of map point selection. This shows the importance of testing map point selection with a front-end implementation. To exploit this, this paper proposes an approach that includes a model of the front-end in the utility function when additional information is available. This approach outperforms alternatives on applicable datasets and highlights future research directions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_12901v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和映射（SLAM）在自主机器人中发挥着至关重要的作用。机器人平台通常受到资源限制，这种限制促使实现资源高效的SLAM。虽然稀疏视觉SLAM算法为适度的硬件需求提供了良好的准确性，但即使是这些更具可扩展性的稀疏方法在应用于大规模和长期场景时也面临限制。一个促成因素是SLAM产生的点云使用效率低，并且包含显著的冗余。本文提出使用子集选择算法来减少稀疏视觉SLAM算法产生的映射。信息论技术以前曾被应用于更简单的相关问题，但如果应用于全视觉SLAM问题，它们就无法扩展。本文提出了一些新的用于地图点选择的信息论效用函数，并使用贪婪算法对这些函数进行了优化。使用实际数据以及现有的可视化SLAM实现（ORB-SLAM 2）来评估缩减后的映射。本文提出的近似选择技术实现了与离线基线相当的轨迹精度，同时适合在线使用。这些技术能够以具有竞争力的轨迹精度对视觉SLAM的地图进行实际缩减。结果还表明，SLAM前端性能会显著影响地图点选择的性能。这表明了使用前端实现测试地图点选择的重要性。为了利用这一点，本文提出了一种方法，当有额外信息可用时，在效用函数中包括前端的模型。这种方法在适用的数据集上优于其他方法，并突出了未来的研究方向。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.12901v1" target="_blank">2306.12901v1</a>
                              </td>
                              <td>Map Point Selection for Visual SLAM</td>
                              <td>Christiaan J. Müller</td>
                              <td>2023-06-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_12901v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.12901v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="SFM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2308_01246v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Tirtha -- An Automated Platform to Crowdsource Images and Create 3D Models of Heritage Sites</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01246v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01246v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01246v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Digital preservation of Cultural Heritage (CH) sites is crucial to protect them against damage from natural disasters or human activities. Creating 3D models of CH sites has become a popular method of digital preservation thanks to advancements in computer vision and photogrammetry. However, the process is time-consuming, expensive, and typically requires specialized equipment and expertise, posing challenges in resource-limited developing countries. Additionally, the lack of an open repository for 3D models hinders research and public engagement with their heritage. To address these issues, we propose Tirtha, a web platform for crowdsourcing images of CH sites and creating their 3D models. Tirtha utilizes state-of-the-art Structure from Motion (SfM) and Multi-View Stereo (MVS) techniques. It is modular, extensible and cost-effective, allowing for the incorporation of new techniques as photogrammetry advances. Tirtha is accessible through a web interface at https://tirtha.niser.ac.in and can be deployed on-premise or in a cloud environment. In our case studies, we demonstrate the pipeline's effectiveness by creating 3D models of temples in Odisha, India, using crowdsourced images. These models are available for viewing, interaction, and download on the Tirtha website. Our work aims to provide a dataset of crowdsourced images and 3D reconstructions for research in computer vision, heritage conservation, and related domains. Overall, Tirtha is a step towards democratizing digital preservation, primarily in resource-limited developing countries.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01246v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文化遗产（CH）遗址的数字保护对于保护它们免受自然灾害或人类活动的破坏至关重要。由于计算机视觉和摄影测量的进步，创建CH遗址的3D模型已成为一种流行的数字保存方法。然而，这一过程耗时、昂贵，通常需要专门的设备和专业知识，这对资源有限的发展中国家构成了挑战。此外，缺乏开放的3D模型存储库阻碍了研究和公众对其遗产的参与。为了解决这些问题，我们提出了Tirtha，一个用于众包CH网站图像并创建其3D模型的网络平台。Tirtha采用了最先进的运动结构（SfM）和多视图立体（MVS）技术。它是模块化的、可扩展的和具有成本效益的，允许随着摄影测量的发展而结合新技术。Tirtha可以通过以下网站的web界面访问：https://tirtha.niser.ac.in并且可以在内部部署或在云环境中部署。在我们的案例研究中，我们通过使用众包图像创建印度奥迪沙寺庙的3D模型来证明该管道的有效性。这些模型可在Tirtha网站上查看、交互和下载。我们的工作旨在为计算机视觉、遗产保护和相关领域的研究提供众包图像和3D重建的数据集。总的来说，Tirtha是朝着数字保护民主化迈出的一步，主要是在资源有限的发展中国家。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01246v1" target="_blank">2308.01246v1</a>
                              </td>
                              <td>Tirtha -- An Automated Platform to Crowdsource Images and Create 3D Models of Heritage Sites</td>
                              <td>Jyotirmaya Shivottam</td>
                              <td>2023-08-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01246v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01246v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01125v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Stereo Visual Odometry with Deep Learning-Based Point and Line Feature Matching using an Attention Graph Neural Network</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01125v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01125v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01125v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Robust feature matching forms the backbone for most Visual Simultaneous Localization and Mapping (vSLAM), visual odometry, 3D reconstruction, and Structure from Motion (SfM) algorithms. However, recovering feature matches from texture-poor scenes is a major challenge and still remains an open area of research. In this paper, we present a Stereo Visual Odometry (StereoVO) technique based on point and line features which uses a novel feature-matching mechanism based on an Attention Graph Neural Network that is designed to perform well even under adverse weather conditions such as fog, haze, rain, and snow, and dynamic lighting conditions such as nighttime illumination and glare scenarios. We perform experiments on multiple real and synthetic datasets to validate the ability of our method to perform StereoVO under low visibility weather and lighting conditions through robust point and line matches. The results demonstrate that our method achieves more line feature matches than state-of-the-art line matching algorithms, which when complemented with point feature matches perform consistently well in adverse weather and dynamic lighting conditions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01125v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>稳健的特征匹配构成了大多数视觉同步定位和映射（vSLAM）、视觉里程计、3D重建和运动结构（SfM）算法的支柱。然而，从纹理差的场景中恢复特征匹配是一个重大挑战，并且仍然是一个开放的研究领域。在本文中，我们提出了一种基于点和线特征的立体视觉Odometry（StereoVO）技术，该技术使用了一种新的基于注意力图神经网络的特征匹配机制，即使在雾、霾、雨和雪等恶劣天气条件以及夜间照明和眩光等动态照明条件下也能表现良好。我们在多个真实和合成数据集上进行了实验，以验证我们的方法通过稳健的点和线匹配在低能见度天气和照明条件下执行StereoVO的能力。结果表明，与最先进的线匹配算法相比，我们的方法实现了更多的线特征匹配，当与点特征匹配相补充时，线匹配算法在恶劣天气和动态照明条件下始终表现良好。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01125v1" target="_blank">2308.01125v1</a>
                              </td>
                              <td>Stereo Visual Odometry with Deep Learning-Based Point and Line Feature Matching using an Attention Graph Neural Network</td>
                              <td>Shenbagaraj Kannapiran</td>
                              <td>2023-08-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01125v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01125v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_11702v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SACReg: Scene-Agnostic Coordinate Regression for Visual Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_11702v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_11702v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_11702v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Scene coordinates regression (SCR), i.e., predicting 3D coordinates for every pixel of a given image, has recently shown promising potential. However, existing methods remain mostly scene-specific or limited to small scenes and thus hardly scale to realistic datasets. In this paper, we propose a new paradigm where a single generic SCR model is trained once to be then deployed to new test scenes, regardless of their scale and without further finetuning. For a given query image, it collects inputs from off-the-shelf image retrieval techniques and Structure-from-Motion databases: a list of relevant database images with sparse pointwise 2D-3D annotations. The model is based on the transformer architecture and can take a variable number of images and sparse 2D-3D annotations as input. It is trained on a few diverse datasets and significantly outperforms other scene regression approaches on several benchmarks, including scene-specific models, for visual localization. In particular, we set a new state of the art on the Cambridge localization benchmark, even outperforming feature-matching-based approaches.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_11702v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>场景坐标回归（SCR），即预测给定图像的每个像素的3D坐标，最近显示出了很有前途的潜力。然而，现有的方法大多是针对场景的，或者仅限于小场景，因此很难扩展到真实的数据集。在本文中，我们提出了一种新的范式，其中单个通用SCR模型被训练一次，然后部署到新的测试场景中，而不考虑其规模，也不需要进一步的微调。对于给定的查询图像，它从现成的图像检索技术和运动数据库的结构中收集输入：具有稀疏逐点2D-3D注释的相关数据库图像列表。该模型基于转换器架构，并且可以采用可变数量的图像和稀疏的2D-3D注释作为输入。它在几个不同的数据集上进行了训练，在视觉定位方面，它在几个基准测试（包括特定场景的模型）上显著优于其他场景回归方法。特别是，我们在剑桥本地化基准上设定了一个新的技术水平，甚至优于基于特征匹配的方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.11702v2" target="_blank">2307.11702v2</a>
                              </td>
                              <td>SACReg: Scene-Agnostic Coordinate Regression for Visual Localization</td>
                              <td>Jerome Revaud</td>
                              <td>2023-07-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_11702v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.11702v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_15055v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_15055v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_15055v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_15055v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce PointOdyssey, a large-scale synthetic dataset, and data generation framework, for the training and evaluation of long-term fine-grained tracking algorithms. Our goal is to advance the state-of-the-art by placing emphasis on long videos with naturalistic motion. Toward the goal of naturalism, we animate deformable characters using real-world motion capture data, we build 3D scenes to match the motion capture environments, and we render camera viewpoints using trajectories mined via structure-from-motion on real videos. We create combinatorial diversity by randomizing character appearance, motion profiles, materials, lighting, 3D assets, and atmospheric effects. Our dataset currently includes 104 videos, averaging 2,000 frames long, with orders of magnitude more correspondence annotations than prior work. We show that existing methods can be trained from scratch in our dataset and outperform the published variants. Finally, we introduce modifications to the PIPs point tracking method, greatly widening its temporal receptive field, which improves its performance on PointOdyssey as well as on two real-world benchmarks. Our data and code are publicly available at: https://pointodyssey.com</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_15055v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了PointOdyssey，一个大规模的合成数据集和数据生成框架，用于长期细粒度跟踪算法的训练和评估。我们的目标是通过强调自然运动的长视频来推进最先进的技术。为了实现自然主义的目标，我们使用真实世界的运动捕捉数据制作可变形角色的动画，我们构建3D场景以匹配运动捕捉环境，我们使用通过真实视频上的运动结构挖掘的轨迹来渲染相机视点。我们通过随机化角色外观、运动剖面、材质、照明、3D资产和大气效果来创造组合多样性。我们的数据集目前包括104个视频，平均2000帧长，与之前的工作相比，对应注释多了几个数量级。我们表明，现有的方法可以在我们的数据集中从头开始训练，并且优于已发布的变体。最后，我们介绍了对PIP点跟踪方法的修改，极大地拓宽了其时间感受野，这提高了其在PointOdyssey和两个真实世界基准上的性能。我们的数据和代码可在以下网址公开获取：https://pointodyssey.com</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.15055v1" target="_blank">2307.15055v1</a>
                              </td>
                              <td>PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking</td>
                              <td>Yang Zheng</td>
                              <td>2023-07-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_15055v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.15055v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_07250v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fusing Structure from Motion and Simulation-Augmented Pose Regression from Optical Flow for Challenging Indoor Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_07250v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_07250v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_07250v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The localization of objects is a crucial task in various applications such as robotics, virtual and augmented reality, and the transportation of goods in warehouses. Recent advances in deep learning have enabled the localization using monocular visual cameras. While structure from motion (SfM) predicts the absolute pose from a point cloud, absolute pose regression (APR) methods learn a semantic understanding of the environment through neural networks. However, both fields face challenges caused by the environment such as motion blur, lighting changes, repetitive patterns, and feature-less structures. This study aims to address these challenges by incorporating additional information and regularizing the absolute pose using relative pose regression (RPR) methods. RPR methods suffer under different challenges, i.e., motion blur. The optical flow between consecutive images is computed using the Lucas-Kanade algorithm, and the relative pose is predicted using an auxiliary small recurrent convolutional network. The fusion of absolute and relative poses is a complex task due to the mismatch between the global and local coordinate systems. State-of-the-art methods fusing absolute and relative poses use pose graph optimization (PGO) to regularize the absolute pose predictions using relative poses. In this work, we propose recurrent fusion networks to optimally align absolute and relative pose predictions to improve the absolute pose prediction. We evaluate eight different recurrent units and construct a simulation environment to pre-train the APR and RPR networks for better generalized training. Additionally, we record a large database of different scenarios in a challenging large-scale indoor environment that mimics a warehouse with transportation robots. We conduct hyperparameter searches and experiments to show the effectiveness of our recurrent fusion method compared to PGO.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_07250v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>物体的定位在机器人、虚拟和增强现实以及仓库中的货物运输等各种应用中是一项至关重要的任务。深度学习的最新进展使得能够使用单目视觉相机进行定位。虽然运动结构（SfM）从点云预测绝对姿态，但绝对姿态回归（APR）方法通过神经网络学习对环境的语义理解。然而，这两个领域都面临着环境带来的挑战，如运动模糊、照明变化、重复模式和无特征结构。本研究旨在通过结合额外信息和使用相对姿态回归（RPR）方法规范绝对姿态来应对这些挑战。RPR方法面临不同的挑战，即运动模糊。使用Lucas Kanade算法计算连续图像之间的光流，并使用辅助的小递归卷积网络预测相对姿态。由于全局坐标系和局部坐标系之间的不匹配，绝对姿态和相对姿态的融合是一项复杂的任务。融合绝对姿态和相对姿态的现有技术方法使用姿态图优化（PGO）来使用相对姿态正则化绝对姿态预测。在这项工作中，我们提出了递归融合网络来优化绝对和相对姿态预测，以改进绝对姿态预测。我们评估了八个不同的递归单元，并构建了一个模拟环境来预训练APR和RPR网络，以便更好地进行广义训练。此外，我们在一个具有挑战性的大型室内环境中记录了不同场景的大型数据库，该环境模拟了带有运输机器人的仓库。我们进行了超参数搜索和实验，以显示与PGO相比，我们的递归融合方法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.07250v2" target="_blank">2304.07250v2</a>
                              </td>
                              <td>Fusing Structure from Motion and Simulation-Augmented Pose Regression from Optical Flow for Challenging Indoor Environments</td>
                              <td>Felix Ott</td>
                              <td>2023-04-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_07250v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.07250v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09981v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Lazy Visual Localization via Motion Averaging</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09981v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09981v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09981v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual (re)localization is critical for various applications in computer vision and robotics. Its goal is to estimate the 6 degrees of freedom (DoF) camera pose for each query image, based on a set of posed database images. Currently, all leading solutions are structure-based that either explicitly construct 3D metric maps from the database with structure-from-motion, or implicitly encode the 3D information with scene coordinate regression models. On the contrary, visual localization without reconstructing the scene in 3D offers clear benefits. It makes deployment more convenient by reducing database pre-processing time, releasing storage requirements, and remaining unaffected by imperfect reconstruction, etc. In this technical report, we demonstrate that it is possible to achieve high localization accuracy without reconstructing the scene from the database. The key to achieving this owes to a tailored motion averaging over database-query pairs. Experiments show that our visual localization proposal, LazyLoc, achieves comparable performance against state-of-the-art structure-based methods. Furthermore, we showcase the versatility of LazyLoc, which can be easily extended to handle complex configurations such as multi-query co-localization and camera rigs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09981v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉（再）定位对于计算机视觉和机器人的各种应用至关重要。其目标是基于一组摆姿势的数据库图像，估计每个查询图像的6个自由度（DoF）相机姿势。目前，所有领先的解决方案都是基于结构的，它们要么从数据库中显式地构建具有运动结构的3D度量图，要么用场景坐标回归模型隐式地编码3D信息。相反，在不重建3D场景的情况下进行视觉定位提供了明显的好处。它通过减少数据库预处理时间、释放存储需求、不受不完美重建的影响等方式使部署更加方便。在本技术报告中，我们证明了在不从数据库重建场景的情况下实现高定位精度是可能的。实现这一点的关键在于对数据库查询对进行定制的运动平均。实验表明，我们的视觉定位方案LazyLoc与最先进的基于结构的方法相比，具有相当的性能。此外，我们还展示了LazyLoc的多功能性，它可以很容易地扩展到处理复杂的配置，如多查询协同定位和相机钻机。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09981v1" target="_blank">2307.09981v1</a>
                              </td>
                              <td>Lazy Visual Localization via Motion Averaging</td>
                              <td>Siyan Dong</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09981v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09981v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07524v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Reducing Causality to Functions with Structural Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07524v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07524v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07524v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The precise definition of causality is currently an open problem in philosophy and statistics. We believe causality should be defined as functions (in mathematics) that map causes to effects. We propose a reductive definition of causality based on Structural Functional Model (SFM). Using delta compression and contrastive forward inference, SFM can produce causal utterances like "X causes Y" and "X is the cause of Y" that match our intuitions. We compile a dataset of causal scenarios and use SFM in all of them. SFM is compatible with but not reducible to probability theory. We also compare SFM with other theories of causation and apply SFM to downstream problems like free will, causal explanation, and mental causation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07524v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>因果关系的精确定义目前是哲学和统计学中一个悬而未决的问题。我们认为因果关系应该被定义为（在数学中）将原因映射到效果的函数。基于结构函数模型，我们提出了因果关系的简化定义。使用delta压缩和对比前向推理，SFM可以产生与我们的直觉相匹配的因果话语，如“X导致Y”和“X是Y的原因”。我们编译了一个因果场景的数据集，并在所有场景中使用SFM。SFM与概率论是相容的，但不可简化为概率论。我们还将SFM与其他因果关系理论进行了比较，并将SFM应用于自由意志、因果解释和精神因果关系等下游问题。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07524v1" target="_blank">2307.07524v1</a>
                              </td>
                              <td>Reducing Causality to Functions with Structural Models</td>
                              <td>Tianyi Miao</td>
                              <td>2023-07-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07524v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07524v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04520v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Efficient Match Pair Retrieval for Large-scale UAV Images via Graph Indexed Global Descriptor</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04520v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04520v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04520v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>SfM (Structure from Motion) has been extensively used for UAV (Unmanned Aerial Vehicle) image orientation. Its efficiency is directly influenced by feature matching. Although image retrieval has been extensively used for match pair selection, high computational costs are consumed due to a large number of local features and the large size of the used codebook. Thus, this paper proposes an efficient match pair retrieval method and implements an integrated workflow for parallel SfM reconstruction. First, an individual codebook is trained online by considering the redundancy of UAV images and local features, which avoids the ambiguity of training codebooks from other datasets. Second, local features of each image are aggregated into a single high-dimension global descriptor through the VLAD (Vector of Locally Aggregated Descriptors) aggregation by using the trained codebook, which remarkably reduces the number of features and the burden of nearest neighbor searching in image indexing. Third, the global descriptors are indexed via the HNSW (Hierarchical Navigable Small World) based graph structure for the nearest neighbor searching. Match pairs are then retrieved by using an adaptive threshold selection strategy and utilized to create a view graph for divide-and-conquer based parallel SfM reconstruction. Finally, the performance of the proposed solution has been verified using three large-scale UAV datasets. The test results demonstrate that the proposed solution accelerates match pair retrieval with a speedup ratio ranging from 36 to 108 and improves the efficiency of SfM reconstruction with competitive accuracy in both relative and absolute orientation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04520v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>SfM（Structure from Motion）已被广泛用于无人机（UAV）的图像定向。其效率直接受到特征匹配的影响。尽管图像检索已被广泛用于匹配对选择，但由于大量的局部特征和所使用的码本的大尺寸，消耗了高计算成本。因此，本文提出了一种高效的匹配对检索方法，并实现了一个用于并行SfM重建的集成工作流。首先，考虑无人机图像和局部特征的冗余性，在线训练单个码本，避免了其他数据集训练码本的模糊性。其次，通过使用训练后的码本进行VLAD（Vector of Locally aggregated Descriptors）聚合，将每个图像的局部特征聚合为单个高维全局描述符，显著减少了图像索引中特征的数量和最近邻搜索的负担。第三，通过基于HNSW（分层导航小世界）的图结构对全局描述符进行索引，用于最近邻居搜索。然后通过使用自适应阈值选择策略来检索匹配对，并用于创建用于基于分治的并行SfM重建的视图图。最后，使用三个大型无人机数据集验证了所提出的解决方案的性能。测试结果表明，所提出的解决方案以36到108的加速比加速了匹配对检索，并在相对和绝对方向上以具有竞争力的精度提高了SfM重建的效率。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04520v1" target="_blank">2307.04520v1</a>
                              </td>
                              <td>Efficient Match Pair Retrieval for Large-scale UAV Images via Graph Indexed Global Descriptor</td>
                              <td>San Jiang</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04520v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04520v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03404v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RGB-D Mapping and Tracking in a Plenoxel Radiance Field</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03404v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03404v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03404v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Building on the success of Neural Radiance Fields (NeRFs), recent years have seen significant advances in the domain of novel view synthesis. These models capture the scene's volumetric radiance field, creating highly convincing dense photorealistic models through the use of simple, differentiable rendering equations. Despite their popularity, these algorithms suffer from severe ambiguities in visual data inherent to the RGB sensor, which means that although images generated with view synthesis can visually appear very believable, the underlying 3D model will often be wrong. This considerably limits the usefulness of these models in practical applications like Robotics and Extended Reality (XR), where an accurate dense 3D reconstruction otherwise would be of significant value. In this technical report, we present the vital differences between view synthesis models and 3D reconstruction models. We also comment on why a depth sensor is essential for modeling accurate geometry in general outward-facing scenes using the current paradigm of novel view synthesis methods. Focusing on the structure-from-motion task, we practically demonstrate this need by extending the Plenoxel radiance field model: Presenting an analytical differential approach for dense mapping and tracking with radiance fields based on RGB-D data without a neural network. Our method achieves state-of-the-art results in both the mapping and tracking tasks while also being faster than competing neural network-based approaches.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03404v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在神经辐射场（NeRFs）成功的基础上，近年来在新视图合成领域取得了重大进展。这些模型捕捉了场景的体积辐射场，通过使用简单、可微分的渲染方程创建了令人信服的密集真实感模型。尽管这些算法很受欢迎，但RGB传感器固有的视觉数据存在严重的模糊性，这意味着尽管通过视图合成生成的图像在视觉上看起来非常可信，但底层的3D模型往往是错误的。这大大限制了这些模型在机器人和扩展现实（XR）等实际应用中的有用性，否则，精确的密集3D重建将具有重要价值。在本技术报告中，我们介绍了视图合成模型和三维重建模型之间的重要区别。我们还评论了为什么深度传感器对于使用当前新型视图合成方法的范式在一般面向外的场景中建模精确的几何体至关重要。专注于运动任务中的结构，我们通过扩展Plenoxel辐射场模型来实际证明这一需求：在没有神经网络的情况下，基于RGB-D数据，提出了一种用于辐射场密集映射和跟踪的解析微分方法。我们的方法在映射和跟踪任务中都取得了最先进的结果，同时也比竞争对手的基于神经网络的方法更快。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03404v1" target="_blank">2307.03404v1</a>
                              </td>
                              <td>RGB-D Mapping and Tracking in a Plenoxel Radiance Field</td>
                              <td>Andreas L. Teigen</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03404v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03404v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_01817v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Human Trajectory Forecasting with Explainable Behavioral Uncertainty</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_01817v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_01817v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_01817v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Human trajectory forecasting helps to understand and predict human behaviors, enabling applications from social robots to self-driving cars, and therefore has been heavily investigated. Most existing methods can be divided into model-free and model-based methods. Model-free methods offer superior prediction accuracy but lack explainability, while model-based methods provide explainability but cannot predict well. Combining both methodologies, we propose a new Bayesian Neural Stochastic Differential Equation model BNSP-SFM, where a behavior SDE model is combined with Bayesian neural networks (BNNs). While the NNs provide superior predictive power, the SDE offers strong explainability with quantifiable uncertainty in behavior and observation. We show that BNSP-SFM achieves up to a 50% improvement in prediction accuracy, compared with 11 state-of-the-art methods. BNSP-SFM also generalizes better to drastically different scenes with different environments and crowd densities (~ 20 times higher than the testing data). Finally, BNSP-SFM can provide predictions with confidence to better explain potential causes of behaviors. The code will be released upon acceptance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_01817v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人类轨迹预测有助于理解和预测人类行为，实现从社交机器人到自动驾驶汽车的应用，因此受到了大量研究。大多数现有的方法可以分为无模型方法和基于模型的方法。无模型方法提供了优越的预测精度但缺乏可解释性，而基于模型的方法提供了可解释性但不能很好地预测。结合这两种方法，我们提出了一个新的贝叶斯神经随机微分方程模型BNSP-SFM，其中行为SDE模型与贝叶斯神经网络（BNNs）相结合。虽然神经网络提供了卓越的预测能力，但SDE提供了强大的可解释性，在行为和观察方面具有可量化的不确定性。我们表明，与11种最先进的方法相比，BNSP-SFM的预测精度提高了50%。BNSP-SFM还可以更好地推广到具有不同环境和人群密度的截然不同的场景（比测试数据高出约20倍）。最后，BNSP-SFM可以提供有信心的预测，以更好地解释行为的潜在原因。该代码将在验收后发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.01817v1" target="_blank">2307.01817v1</a>
                              </td>
                              <td>Human Trajectory Forecasting with Explainable Behavioral Uncertainty</td>
                              <td>Jiangbei Yue</td>
                              <td>2023-07-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_01817v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.01817v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16917v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Drunkard's Odometry: Estimating Camera Motion in Deforming Scenes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16917v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16917v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16917v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Estimating camera motion in deformable scenes poses a complex and open research challenge. Most existing non-rigid structure from motion techniques assume to observe also static scene parts besides deforming scene parts in order to establish an anchoring reference. However, this assumption does not hold true in certain relevant application cases such as endoscopies. Deformable odometry and SLAM pipelines, which tackle the most challenging scenario of exploratory trajectories, suffer from a lack of robustness and proper quantitative evaluation methodologies. To tackle this issue with a common benchmark, we introduce the Drunkard's Dataset, a challenging collection of synthetic data targeting visual navigation and reconstruction in deformable environments. This dataset is the first large set of exploratory camera trajectories with ground truth inside 3D scenes where every surface exhibits non-rigid deformations over time. Simulations in realistic 3D buildings lets us obtain a vast amount of data and ground truth labels, including camera poses, RGB images and depth, optical flow and normal maps at high resolution and quality. We further present a novel deformable odometry method, dubbed the Drunkard's Odometry, which decomposes optical flow estimates into rigid-body camera motion and non-rigid scene deformations. In order to validate our data, our work contains an evaluation of several baselines as well as a novel tracking error metric which does not require ground truth data. Dataset and code: https://davidrecasens.github.io/TheDrunkard'sOdometry/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16917v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在可变形场景中估计相机运动是一个复杂而开放的研究挑战。大多数现有的非刚性结构运动技术假设除了变形场景部分之外，还观察静态场景部分，以建立锚定参考。然而，这一假设在某些相关应用案例中并不成立，例如内镜。可变形里程计和SLAM管道解决了最具挑战性的探索轨迹场景，但缺乏稳健性和适当的定量评估方法。为了用一个通用的基准来解决这个问题，我们引入了Drunkard的数据集，这是一个具有挑战性的合成数据集，旨在在可变形环境中进行视觉导航和重建。该数据集是第一个在3D场景中具有地面实况的大型探索相机轨迹集，其中每个表面随着时间的推移都表现出非刚性变形。在逼真的3D建筑中进行模拟可以让我们获得大量的数据和地面实况标签，包括相机姿态、RGB图像和深度、光流和高分辨率和高质量的法线图。我们进一步提出了一种新的可变形里程计方法，称为Drunkard里程计，该方法将光流估计分解为刚体相机运动和非刚体场景变形。为了验证我们的数据，我们的工作包括对几个基线的评估，以及一种新的跟踪误差度量，该度量不需要地面实况数据。数据集和代码：https://davidrecasens.github.io/TheDrunkard'国内/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16917v1" target="_blank">2306.16917v1</a>
                              </td>
                              <td>The Drunkard's Odometry: Estimating Camera Motion in Deforming Scenes</td>
                              <td>David Recasens</td>
                              <td>2023-06-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16917v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16917v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_15667v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_15667v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_15667v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_15667v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Camera pose estimation is a long-standing computer vision problem that to date often relies on classical methods, such as handcrafted keypoint matching, RANSAC and bundle adjustment. In this paper, we propose to formulate the Structure from Motion (SfM) problem inside a probabilistic diffusion framework, modelling the conditional distribution of camera poses given input images. This novel view of an old problem has several advantages. (i) The nature of the diffusion framework mirrors the iterative procedure of bundle adjustment. (ii) The formulation allows a seamless integration of geometric constraints from epipolar geometry. (iii) It excels in typically difficult scenarios such as sparse views with wide baselines. (iv) The method can predict intrinsics and extrinsics for an arbitrary amount of images. We demonstrate that our method PoseDiffusion significantly improves over the classic SfM pipelines and the learned approaches on two real-world datasets. Finally, it is observed that our method can generalize across datasets without further training. Project page: https://posediffusion.github.io/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_15667v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>相机姿态估计是一个长期存在的计算机视觉问题，迄今为止，它通常依赖于经典的方法，如手工关键点匹配、RANSAC和束调整。在本文中，我们建议在概率扩散框架内公式化运动结构（SfM）问题，对给定输入图像的相机姿态的条件分布进行建模。这种对老问题的新颖看法有几个优点。（i） 扩散框架的性质反映了束调整的迭代过程。（ii）该公式允许来自核极几何的几何约束的无缝集成。（iii）它在典型的困难场景中表现出色，例如具有宽基线的稀疏视图。（iv）该方法可以预测任意数量的图像的内在和外在。我们在两个真实世界的数据集上证明了我们的方法PoseDiffusion比经典的SfM管道和学习的方法有了显著的改进。最后，我们观察到，我们的方法可以在不需要进一步训练的情况下在数据集之间进行推广。项目页面：https://posediffusion.github.io/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.15667v2" target="_blank">2306.15667v2</a>
                              </td>
                              <td>PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment</td>
                              <td>Jianyuan Wang</td>
                              <td>2023-06-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_15667v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.15667v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_15669v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Detector-Free Structure from Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_15669v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_15669v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_15669v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a new structure-from-motion framework to recover accurate camera poses and point clouds from unordered images. Traditional SfM systems typically rely on the successful detection of repeatable keypoints across multiple views as the first step, which is difficult for texture-poor scenes, and poor keypoint detection may break down the whole SfM system. We propose a new detector-free SfM framework to draw benefits from the recent success of detector-free matchers to avoid the early determination of keypoints, while solving the multi-view inconsistency issue of detector-free matchers. Specifically, our framework first reconstructs a coarse SfM model from quantized detector-free matches. Then, it refines the model by a novel iterative refinement pipeline, which iterates between an attention-based multi-view matching module to refine feature tracks and a geometry refinement module to improve the reconstruction accuracy. Experiments demonstrate that the proposed framework outperforms existing detector-based SfM systems on common benchmark datasets. We also collect a texture-poor SfM dataset to demonstrate the capability of our framework to reconstruct texture-poor scenes. Based on this framework, we take $\textit{first place}$ in Image Matching Challenge 2023.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_15669v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们从运动框架中提出了一种新的结构，以从无序图像中恢复准确的相机姿态和点云。传统的SfM系统通常依赖于跨多个视图的可重复关键点的成功检测作为第一步，这对于纹理较差的场景来说是困难的，并且较差的关键点检测可能会破坏整个SfM体系。我们提出了一种新的无检测器SfM框架，以从无检测器匹配器最近的成功中获益，避免早期确定关键点，同时解决无检测器匹配的多视图不一致问题。具体来说，我们的框架首先从量化的无检测器匹配中重建粗略的SfM模型。然后，它通过一种新的迭代精化流水线对模型进行精化，该流水线在基于注意力的多视图匹配模块和几何精化模块之间迭代以精化特征轨迹，从而提高重建精度。实验表明，该框架在通用基准数据集上优于现有的基于检测器的SfM系统。我们还收集了一个纹理较差的SfM数据集，以证明我们的框架重建纹理较差场景的能力。基于这个框架，我们在2023年的图像匹配挑战中获得$\textit｛first place｝$。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.15669v1" target="_blank">2306.15669v1</a>
                              </td>
                              <td>Detector-Free Structure from Motion</td>
                              <td>Xingyi He</td>
                              <td>2023-06-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_15669v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.15669v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_12770v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D Reconstruction of Spherical Images based on Incremental Structure from Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_12770v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_12770v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_12770v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>3D reconstruction plays an increasingly important role in modern photogrammetric systems. Conventional satellite or aerial-based remote sensing (RS) platforms can provide the necessary data sources for the 3D reconstruction of large-scale landforms and cities. Even with low-altitude UAVs (Unmanned Aerial Vehicles), 3D reconstruction in complicated situations, such as urban canyons and indoor scenes, is challenging due to the frequent tracking failures between camera frames and high data collection costs. Recently, spherical images have been extensively exploited due to the capability of recording surrounding environments from one camera exposure. Classical 3D reconstruction pipelines, however, cannot be used for spherical images. Besides, there exist few software packages for 3D reconstruction of spherical images. Based on the imaging geometry of spherical cameras, this study investigates the algorithms for the relative orientation using spherical correspondences, absolute orientation using 3D correspondences between scene and spherical points, and the cost functions for BA (bundle adjustment) optimization. In addition, an incremental SfM (Structure from Motion) workflow has been proposed for spherical images using the above-mentioned algorithms. The proposed solution is finally verified by using three spherical datasets captured by both consumer-grade and professional spherical cameras. The results demonstrate that the proposed SfM workflow can achieve the successful 3D reconstruction of complex scenes and provide useful clues for the implementation in open-source software packages. The source code of the designed SfM workflow would be made publicly available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_12770v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三维重建在现代摄影测量系统中发挥着越来越重要的作用。传统的卫星或航空遥感平台可以为大规模地形和城市的三维重建提供必要的数据源。即使使用低空无人机，由于相机帧之间频繁的跟踪故障和高昂的数据收集成本，在城市峡谷和室内场景等复杂情况下的3D重建也具有挑战性。最近，球形图像由于能够通过一台相机曝光记录周围环境而被广泛利用。然而，经典的3D重建管道不能用于球面图像。此外，用于球面图像的三维重建的软件包很少。基于球面相机的成像几何，研究了使用球面对应关系的相对方位、使用场景与球面点之间的3D对应关系的绝对方位以及BA（束调整）优化的成本函数的算法。此外，已经提出了使用上述算法的球面图像的增量SfM（运动结构）工作流程。通过使用消费者级和专业球形相机拍摄的三个球形数据集，最终验证了所提出的解决方案。结果表明，所提出的SfM工作流可以成功地实现复杂场景的三维重建，并为开源软件包的实现提供了有用的线索。设计的SfM工作流程的源代码将公开。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.12770v2" target="_blank">2306.12770v2</a>
                              </td>
                              <td>3D Reconstruction of Spherical Images based on Incremental Structure from Motion</td>
                              <td>San Jiang</td>
                              <td>2023-06-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_12770v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.12770v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09109v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09109v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09109v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09109v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in neural reconstruction enable high-quality 3D object reconstruction from casually captured image collections. Current techniques mostly analyze their progress on relatively simple image collections where Structure-from-Motion (SfM) techniques can provide ground-truth (GT) camera poses. We note that SfM techniques tend to fail on in-the-wild image collections such as image search results with varying backgrounds and illuminations. To enable systematic research progress on 3D reconstruction from casual image captures, we propose NAVI: a new dataset of category-agnostic image collections of objects with high-quality 3D scans along with per-image 2D-3D alignments providing near-perfect GT camera parameters. These 2D-3D alignments allow us to extract accurate derivative annotations such as dense pixel correspondences, depth and segmentation maps. We demonstrate the use of NAVI image collections on different problem settings and show that NAVI enables more thorough evaluations that were not possible with existing datasets. We believe NAVI is beneficial for systematic research progress on 3D reconstruction and correspondence estimation. Project page: https://navidataset.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09109v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经重建的最新进展使得能够从随意捕获的图像集合中重建高质量的3D对象。当前的技术主要分析它们在相对简单的图像采集上的进展，其中运动结构（SfM）技术可以提供地面实况（GT）相机姿态。我们注意到，SfM技术在野生图像集合中往往失败，例如具有不同背景和照明的图像搜索结果。为了实现从偶然图像捕获的3D重建的系统研究进展，我们提出了NAVI：一个具有高质量3D扫描的对象的类别不可知图像集合的新数据集，以及提供近乎完美的GT相机参数的每张图像2D-3D对齐。这些2D-3D比对允许我们提取精确的导数注释，例如密集像素对应、深度和分割图。我们展示了NAVI图像集合在不同问题设置中的使用，并表明NAVI能够实现现有数据集无法实现的更彻底的评估。我们相信NAVI有利于三维重建和对应关系估计的系统研究进展。项目页面：https://navidataset.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09109v1" target="_blank">2306.09109v1</a>
                              </td>
                              <td>NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations</td>
                              <td>Varun Jampani</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09109v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09109v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09012v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Yes, we CANN: Constrained Approximate Nearest Neighbors for local feature-based visual localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09012v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09012v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09012v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large-scale visual localization systems continue to rely on 3D point clouds built from image collections using structure-from-motion. While the 3D points in these models are represented using local image features, directly matching a query image's local features against the point cloud is challenging due to the scale of the nearest-neighbor search problem. Many recent approaches to visual localization have thus proposed a hybrid method, where first a global (per image) embedding is used to retrieve a small subset of database images, and local features of the query are matched only against those. It seems to have become common belief that global embeddings are critical for said image-retrieval in visual localization, despite the significant downside of having to compute two feature types for each query image. In this paper, we take a step back from this assumption and propose Constrained Approximate Nearest Neighbors (CANN), a joint solution of k-nearest-neighbors across both the geometry and appearance space using only local features. We first derive the theoretical foundation for k-nearest-neighbor retrieval across multiple metrics and then showcase how CANN improves visual localization. Our experiments on public localization benchmarks demonstrate that our method significantly outperforms both state-of-the-art global feature-based retrieval and approaches using local feature aggregation schemes. Moreover, it is an order of magnitude faster in both index and query time than feature aggregation schemes for these datasets. Code will be released.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09012v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大规模视觉定位系统继续依赖于使用来自运动的结构从图像集合构建的3D点云。虽然这些模型中的3D点是使用局部图像特征来表示的，但由于最近邻居搜索问题的规模，将查询图像的局部特征与点云直接匹配是具有挑战性的。因此，许多最近的视觉定位方法都提出了一种混合方法，其中首先使用全局（每图像）嵌入来检索数据库图像的一小部分，并且查询的局部特征仅与这些特征相匹配。人们似乎普遍认为，全局嵌入对于视觉定位中的图像检索至关重要，尽管必须为每个查询图像计算两种特征类型有很大的缺点。在本文中，我们从这一假设后退了一步，提出了约束近似最近邻（CANN），这是一种仅使用局部特征在几何和外观空间上的k个最近邻的联合解决方案。我们首先推导了跨多个度量的k近邻检索的理论基础，然后展示了CANN如何改进视觉定位。我们在公共定位基准上的实验表明，我们的方法显著优于最先进的基于全局特征的检索和使用局部特征聚合方案的方法。此外，它在索引和查询时间上都比这些数据集的特征聚合方案快一个数量级。将发布代码。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09012v1" target="_blank">2306.09012v1</a>
                              </td>
                              <td>Yes, we CANN: Constrained Approximate Nearest Neighbors for local feature-based visual localization</td>
                              <td>Dror Aiger</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09012v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09012v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_06360v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D reconstruction using Structure for Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_06360v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_06360v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_06360v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We are working towards 3D reconstruction of indoor spaces using a pair of HDR cameras in a stereo vision configuration mounted on an indoor mobile floor robot that captures various textures and spatial features as 2D images and this data is simultaneously utilized as a feed to our algorithm which will allow us to visualize the depth map.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_06360v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们正在使用安装在室内移动地板机器人上的一对立体视觉配置的HDR相机对室内空间进行3D重建，该机器人将各种纹理和空间特征捕获为2D图像，这些数据同时被用作我们算法的反馈，这将使我们能够可视化深度图。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.06360v1" target="_blank">2306.06360v1</a>
                              </td>
                              <td>3D reconstruction using Structure for Motion</td>
                              <td>Kshitij Karnawat</td>
                              <td>2023-06-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_06360v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.06360v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_05410v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_05410v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_05410v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_05410v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A critical obstacle preventing NeRF models from being deployed broadly in the wild is their reliance on accurate camera poses. Consequently, there is growing interest in extending NeRF models to jointly optimize camera poses and scene representation, which offers an alternative to off-the-shelf SfM pipelines which have well-understood failure modes. Existing approaches for unposed NeRF operate under limited assumptions, such as a prior pose distribution or coarse pose initialization, making them less effective in a general setting. In this work, we propose a novel approach, LU-NeRF, that jointly estimates camera poses and neural radiance fields with relaxed assumptions on pose configuration. Our approach operates in a local-to-global manner, where we first optimize over local subsets of the data, dubbed mini-scenes. LU-NeRF estimates local pose and geometry for this challenging few-shot task. The mini-scene poses are brought into a global reference frame through a robust pose synchronization step, where a final global optimization of pose and scene can be performed. We show our LU-NeRF pipeline outperforms prior attempts at unposed NeRF without making restrictive assumptions on the pose prior. This allows us to operate in the general SE(3) pose setting, unlike the baselines. Our results also indicate our model can be complementary to feature-based SfM pipelines as it compares favorably to COLMAP on low-texture and low-resolution images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_05410v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>阻碍NeRF模型在野外广泛部署的一个关键障碍是它们对精确相机姿势的依赖。因此，人们对扩展NeRF模型以联合优化相机姿态和场景表示越来越感兴趣，这为具有众所周知的故障模式的现成SfM管道提供了一种替代方案。现有的无基NeRF方法在有限的假设下运行，例如先前的姿态分布或粗略的姿态初始化，这使得它们在一般情况下的效果较差。在这项工作中，我们提出了一种新的方法，即LU NeRF，该方法通过对姿势配置的宽松假设来联合估计相机姿势和神经辐射场。我们的方法以局部到全局的方式运行，首先对数据的局部子集进行优化，称为迷你场景。LU NeRF估计了这项具有挑战性的少镜头任务的局部姿态和几何结构。通过稳健的姿态同步步骤，将迷你场景姿态带入全局参考帧，其中可以执行姿态和场景的最终全局优化。我们展示了我们的LU NeRF流水线在没有对姿势先验进行限制性假设的情况下，在未建模的NeRF上优于先前的尝试。这使我们能够在一般的SE（3）姿势设置中操作，而不是基线。我们的结果还表明，我们的模型可以与基于特征的SfM管道互补，因为它在低纹理和低分辨率图像上优于COLMAP。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.05410v1" target="_blank">2306.05410v1</a>
                              </td>
                              <td>LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs</td>
                              <td>Zezhou Cheng</td>
                              <td>2023-06-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_05410v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.05410v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_08422v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A vision-based autonomous UAV inspection framework for unknown tunnel construction sites with dynamic obstacles</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_08422v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_08422v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_08422v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Tunnel construction using the drill-and-blast method requires the 3D measurement of the excavation front to evaluate underbreak locations. Considering the inspection and measurement task's safety, cost, and efficiency, deploying lightweight autonomous robots, such as unmanned aerial vehicles (UAV), becomes more necessary and popular. Most of the previous works use a prior map for inspection viewpoint determination and do not consider dynamic obstacles. To maximally increase the level of autonomy, this paper proposes a vision-based UAV inspection framework for dynamic tunnel environments without using a prior map. Our approach utilizes a hierarchical planning scheme, decomposing the inspection problem into different levels. The high-level decision maker first determines the task for the robot and generates the target point. Then, the mid-level path planner finds the waypoint path and optimizes the collision-free static trajectory. Finally, the static trajectory will be fed into the low-level local planner to avoid dynamic obstacles and navigate to the target point. Besides, our framework contains a novel dynamic map module that can simultaneously track dynamic obstacles and represent static obstacles based on an RGB-D camera. After inspection, the Structure-from-Motion (SfM) pipeline is applied to generate the 3D shape of the target. To our best knowledge, this is the first time autonomous inspection has been realized in unknown and dynamic tunnel environments. Our flight experiments in a real tunnel prove that our method can autonomously inspect the tunnel excavation front surface.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_08422v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>采用钻爆法的隧道施工需要对开挖前沿进行三维测量，以评估欠挖位置。考虑到检测任务的安全性、成本和效率，部署无人机等轻型自主机器人变得更加必要和流行。以前的大多数工作都使用先验地图来确定检查视点，并且没有考虑动态障碍物。为了最大限度地提高自主性，本文提出了一种基于视觉的无人机动态隧道环境检测框架，无需使用先验地图。我们的方法采用分层规划方案，将检查问题分解为不同的级别。高级决策者首先确定机器人的任务并生成目标点。然后，中级路径规划器找到航路点路径并优化无碰撞静态轨迹。最后，静态轨迹将被输入到低级局部规划器中，以避开动态障碍并导航到目标点。此外，我们的框架包含一个新的动态地图模块，该模块可以基于RGB-D相机同时跟踪动态障碍物和表示静态障碍物。检查后，应用运动结构（SfM）管道生成目标的3D形状。据我们所知，这是首次在未知和动态的隧道环境中实现自主检测。我们在实际隧道中的飞行实验证明，我们的方法可以自主检测隧道开挖前表面。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.08422v2" target="_blank">2301.08422v2</a>
                              </td>
                              <td>A vision-based autonomous UAV inspection framework for unknown tunnel construction sites with dynamic obstacles</td>
                              <td>Zhefan Xu</td>
                              <td>2023-01-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_08422v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.08422v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01938v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Self-supervised Interest Point Detection and Description for Fisheye and Perspective Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01938v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01938v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01938v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Keypoint detection and matching is a fundamental task in many computer vision problems, from shape reconstruction, to structure from motion, to AR/VR applications and robotics. It is a well-studied problem with remarkable successes such as SIFT, and more recent deep learning approaches. While great robustness is exhibited by these techniques with respect to noise, illumination variation, and rigid motion transformations, less attention has been placed on image distortion sensitivity. In this work, we focus on the case when this is caused by the geometry of the cameras used for image acquisition, and consider the keypoint detection and matching problem between the hybrid scenario of a fisheye and a projective image. We build on a state-of-the-art approach and derive a self-supervised procedure that enables training an interest point detector and descriptor network. We also collected two new datasets for additional training and testing in this unexplored scenario, and we demonstrate that current approaches are suboptimal because they are designed to work in traditional projective conditions, while the proposed approach turns out to be the most effective.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01938v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>关键点检测和匹配是许多计算机视觉问题中的一项基本任务，从形状重建到结构从运动到AR/VR应用和机器人。这是一个研究得很好的问题，取得了显著的成功，如SIFT和最近的深度学习方法。虽然这些技术在噪声、照明变化和刚性运动变换方面表现出了很大的鲁棒性，但对图像失真敏感性的关注较少。在这项工作中，我们重点关注由用于图像采集的相机的几何形状引起的情况，并考虑鱼眼和投影图像的混合场景之间的关键点检测和匹配问题。我们建立在最先进的方法之上，并推导出一个自监督程序，该程序能够训练兴趣点检测器和描述符网络。我们还收集了两个新的数据集，用于在这个未探索的场景中进行额外的训练和测试，我们证明了当前的方法是次优的，因为它们被设计为在传统的投影条件下工作，而所提出的方法被证明是最有效的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01938v1" target="_blank">2306.01938v1</a>
                              </td>
                              <td>Self-supervised Interest Point Detection and Description for Fisheye and Perspective Images</td>
                              <td>Marcela Mera-Trujillo</td>
                              <td>2023-06-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01938v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01938v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_00180v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_00180v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_00180v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_00180v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Reconstruction of 3D neural fields from posed images has emerged as a promising method for self-supervised representation learning. The key challenge preventing the deployment of these 3D scene learners on large-scale video data is their dependence on precise camera poses from structure-from-motion, which is prohibitively expensive to run at scale. We propose a method that jointly reconstructs camera poses and 3D neural scene representations online and in a single forward pass. We estimate poses by first lifting frame-to-frame optical flow to 3D scene flow via differentiable rendering, preserving locality and shift-equivariance of the image processing backbone. SE(3) camera pose estimation is then performed via a weighted least-squares fit to the scene flow field. This formulation enables us to jointly supervise pose estimation and a generalizable neural scene representation via re-rendering the input video, and thus, train end-to-end and fully self-supervised on real-world video datasets. We demonstrate that our method performs robustly on diverse, real-world video, notably on sequences traditionally challenging to optimization-based pose estimation techniques.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_00180v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从姿态图像重建三维神经场已成为自监督表示学习的一种很有前途的方法。阻止将这些3D场景学习器部署在大规模视频数据上的关键挑战是，它们依赖于从结构到运动的精确相机姿势，这在规模上运行成本高得令人望而却步。我们提出了一种在线和单次前向联合重建相机姿态和3D神经场景表示的方法。我们通过可微分渲染将逐帧光流提升到3D场景流来估计姿态，保持图像处理主干的局部性和平移等变性。然后通过对场景流场的加权最小二乘拟合来执行SE（3）相机姿态估计。该公式使我们能够通过重新渲染输入视频来联合监督姿势估计和可推广的神经场景表示，从而在真实世界的视频数据集上进行端到端和完全自监督的训练。我们证明了我们的方法在不同的真实世界视频上表现稳健，尤其是在传统上对基于优化的姿态估计技术具有挑战性的序列上。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.00180v1" target="_blank">2306.00180v1</a>
                              </td>
                              <td>FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow</td>
                              <td>Cameron Smith</td>
                              <td>2023-05-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_00180v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.00180v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_16342v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">InterFormer: Interactive Local and Global Features Fusion for Automatic Speech Recognition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_16342v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_16342v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_16342v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The local and global features are both essential for automatic speech recognition (ASR). Many recent methods have verified that simply combining local and global features can further promote ASR performance. However, these methods pay less attention to the interaction of local and global features, and their series architectures are rigid to reflect local and global relationships. To address these issues, this paper proposes InterFormer for interactive local and global features fusion to learn a better representation for ASR. Specifically, we combine the convolution block with the transformer block in a parallel design. Besides, we propose a bidirectional feature interaction module (BFIM) and a selective fusion module (SFM) to implement the interaction and fusion of local and global features, respectively. Extensive experiments on public ASR datasets demonstrate the effectiveness of our proposed InterFormer and its superior performance over the other Transformer and Conformer models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_16342v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>局部特征和全局特征对于自动语音识别（ASR）都是必不可少的。最近的许多方法已经证明，简单地结合局部和全局特征可以进一步提高ASR性能。然而，这些方法很少关注局部和全局特征的相互作用，并且它们的串联架构是刚性的，以反映局部和全局关系。为了解决这些问题，本文提出了用于交互式局部和全局特征融合的InterFormer，以学习ASR的更好表示。具体地说，我们在并行设计中将卷积块与变换器块相结合。此外，我们提出了一个双向特征交互模块（BFIM）和一个选择性融合模块（SFM），分别实现局部和全局特征的交互和融合。在公共ASR数据集上进行的大量实验证明了我们提出的InterFormer的有效性及其优于其他Transformer和Conformer模型的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.16342v2" target="_blank">2305.16342v2</a>
                              </td>
                              <td>InterFormer: Interactive Local and Global Features Fusion for Automatic Speech Recognition</td>
                              <td>Zhi-Hao Lai</td>
                              <td>2023-05-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_16342v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.16342v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_12036v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SIDAR: Synthetic Image Dataset for Alignment & Restoration</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_12036v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_12036v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_12036v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Image alignment and image restoration are classical computer vision tasks. However, there is still a lack of datasets that provide enough data to train and evaluate end-to-end deep learning models. Obtaining ground-truth data for image alignment requires sophisticated structure-from-motion methods or optical flow systems that often do not provide enough data variance, i.e., typically providing a high number of image correspondences, while only introducing few changes of scenery within the underlying image sequences. Alternative approaches utilize random perspective distortions on existing image data. However, this only provides trivial distortions, lacking the complexity and variance of real-world scenarios. Instead, our proposed data augmentation helps to overcome the issue of data scarcity by using 3D rendering: images are added as textures onto a plane, then varying lighting conditions, shadows, and occlusions are added to the scene. The scene is rendered from multiple viewpoints, generating perspective distortions more consistent with real-world scenarios, with homographies closely resembling those of camera projections rather than randomized homographies. For each scene, we provide a sequence of distorted images with corresponding occlusion masks, homographies, and ground-truth labels. The resulting dataset can serve as a training and evaluation set for a multitude of tasks involving image alignment and artifact removal, such as deep homography estimation, dense image matching, 2D bundle adjustment, inpainting, shadow removal, denoising, content retrieval, and background subtraction. Our data generation pipeline is customizable and can be applied to any existing dataset, serving as a data augmentation to further improve the feature learning of any existing method.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_12036v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>图像对齐和图像恢复是经典的计算机视觉任务。然而，仍然缺乏提供足够数据来训练和评估端到端深度学习模型的数据集。获得用于图像对准的地面实况数据需要来自运动方法或光流系统的复杂结构，这些运动方法或光学流系统通常不能提供足够的数据方差，即，通常提供大量的图像对应，而在底层图像序列内只引入很少的风景变化。替代方法利用现有图像数据上的随机透视失真。然而，这只提供了微不足道的扭曲，缺乏现实世界场景的复杂性和多样性。相反，我们提出的数据增强通过使用3D渲染有助于克服数据稀缺的问题：将图像作为纹理添加到平面上，然后将不同的照明条件、阴影和遮挡添加到场景中。场景从多个视点渲染，生成与真实世界场景更一致的透视扭曲，单应性与相机投影的单应性非常相似，而不是随机单应性。对于每个场景，我们提供一系列失真的图像，这些图像具有相应的遮挡遮罩、单应性和基本事实标签。所得数据集可以作为涉及图像对齐和伪影去除的大量任务的训练和评估集，例如深度单应性估计、密集图像匹配、2D束调整、修复、阴影去除、去噪、内容检索和背景减法。我们的数据生成管道是可定制的，可以应用于任何现有的数据集，作为数据扩充，进一步改进任何现有方法的特征学习。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.12036v1" target="_blank">2305.12036v1</a>
                              </td>
                              <td>SIDAR: Synthetic Image Dataset for Alignment & Restoration</td>
                              <td>Monika Kwiatkowski</td>
                              <td>2023-05-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_12036v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.12036v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_08810v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AutoRecon: Automated 3D Object Discovery and Reconstruction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_08810v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_08810v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_08810v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A fully automated object reconstruction pipeline is crucial for digital content creation. While the area of 3D reconstruction has witnessed profound developments, the removal of background to obtain a clean object model still relies on different forms of manual labor, such as bounding box labeling, mask annotations, and mesh manipulations. In this paper, we propose a novel framework named AutoRecon for the automated discovery and reconstruction of an object from multi-view images. We demonstrate that foreground objects can be robustly located and segmented from SfM point clouds by leveraging self-supervised 2D vision transformer features. Then, we reconstruct decomposed neural scene representations with dense supervision provided by the decomposed point clouds, resulting in accurate object reconstruction and segmentation. Experiments on the DTU, BlendedMVS and CO3D-V2 datasets demonstrate the effectiveness and robustness of AutoRecon.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_08810v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>完全自动化的对象重建管道对于数字内容创建至关重要。虽然3D重建领域已经取得了深刻的发展，但去除背景以获得干净的对象模型仍然依赖于不同形式的手工劳动，如边界框标记、遮罩注释和网格操作。在本文中，我们提出了一个名为AutoRecon的新框架，用于从多视图图像中自动发现和重建对象。我们证明，通过利用自监督2D视觉变换器特征，可以从SfM点云中稳健地定位和分割前景对象。然后，我们在分解的点云提供的密集监督下重建分解的神经场景表示，从而实现精确的对象重建和分割。在DTU、BlendedMVS和CO3D-V2数据集上的实验证明了AutoRecon的有效性和稳健性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.08810v1" target="_blank">2305.08810v1</a>
                              </td>
                              <td>AutoRecon: Automated 3D Object Discovery and Reconstruction</td>
                              <td>Yuang Wang</td>
                              <td>2023-05-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_08810v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.08810v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_06794v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-modal Multi-level Fusion for 3D Single Object Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_06794v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_06794v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_06794v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>3D single object tracking plays a crucial role in computer vision. Mainstream methods mainly rely on point clouds to achieve geometry matching between target template and search area. However, textureless and incomplete point clouds make it difficult for single-modal trackers to distinguish objects with similar structures. To overcome the limitations of geometry matching, we propose a Multi-modal Multi-level Fusion Tracker (MMF-Track), which exploits the image texture and geometry characteristic of point clouds to track 3D target. Specifically, we first propose a Space Alignment Module (SAM) to align RGB images with point clouds in 3D space, which is the prerequisite for constructing inter-modal associations. Then, in feature interaction level, we design a Feature Interaction Module (FIM) based on dual-stream structure, which enhances intra-modal features in parallel and constructs inter-modal semantic associations. Meanwhile, in order to refine each modal feature, we introduce a Coarse-to-Fine Interaction Module (CFIM) to realize the hierarchical feature interaction at different scales. Finally, in similarity fusion level, we propose a Similarity Fusion Module (SFM) to aggregate geometry and texture clues from the target. Experiments show that our method achieves state-of-the-art performance on KITTI (39% Success and 42% Precision gains against previous multi-modal method) and is also competitive on NuScenes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_06794v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三维单目标跟踪在计算机视觉中起着至关重要的作用。主流的方法主要依靠点云来实现目标模板和搜索区域之间的几何匹配。然而，无纹理和不完整的点云使单模态跟踪器难以区分具有相似结构的对象。为了克服几何匹配的局限性，我们提出了一种多模态多级融合跟踪器（MMF Track），该跟踪器利用点云的图像纹理和几何特征来跟踪三维目标。具体来说，我们首先提出了一个空间对齐模块（SAM）来将RGB图像与3D空间中的点云对齐，这是构建模态间关联的先决条件。然后，在特征交互层面，我们设计了一个基于双流结构的特征交互模块，该模块并行增强模态内特征，构建模态间语义关联。同时，为了细化每个模态特征，我们引入了一个从粗到细的交互模块（CFIM）来实现不同尺度的层次特征交互。最后，在相似性融合层面，我们提出了一个相似性融合模块（SFM）来聚合来自目标的几何和纹理线索。实验表明，我们的方法在KITTI上实现了最先进的性能（与以前的多模态方法相比，成功率为39%，精度提高了42%），在NuScenes上也具有竞争力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.06794v1" target="_blank">2305.06794v1</a>
                              </td>
                              <td>Multi-modal Multi-level Fusion for 3D Single Object Tracking</td>
                              <td>Zhiheng Li</td>
                              <td>2023-05-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_06794v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.06794v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_05301v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Eiffel Tower: A Deep-Sea Underwater Dataset for Long-Term Visual Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_05301v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_05301v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_05301v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual localization plays an important role in the positioning and navigation of robotics systems within previously visited environments. When visits occur over long periods of time, changes in the environment related to seasons or day-night cycles present a major challenge. Under water, the sources of variability are due to other factors such as water conditions or growth of marine organisms. Yet it remains a major obstacle and a much less studied one, partly due to the lack of data. This paper presents a new deep-sea dataset to benchmark underwater long-term visual localization. The dataset is composed of images from four visits to the same hydrothermal vent edifice over the course of five years. Camera poses and a common geometry of the scene were estimated using navigation data and Structure-from-Motion. This serves as a reference when evaluating visual localization techniques. An analysis of the data provides insights about the major changes observed throughout the years. Furthermore, several well-established visual localization methods are evaluated on the dataset, showing there is still room for improvement in underwater long-term visual localization. The data is made publicly available at https://www.seanoe.org/data/00810/92226/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_05301v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉定位在机器人系统在先前访问的环境中的定位和导航中起着重要作用。当访问发生在长时间内时，与季节或昼夜周期相关的环境变化是一个重大挑战。在水下，变化的来源是由于其他因素，如水条件或海洋生物的生长。然而，它仍然是一个主要障碍，也是一个研究较少的障碍，部分原因是缺乏数据。本文提出了一个新的深海数据集，用于对水下长期视觉定位进行基准测试。该数据集由五年内四次访问同一热液喷口建筑物的图像组成。使用导航数据和“运动结构”来估计摄影机姿态和场景的常见几何体。这可作为评估视觉定位技术时的参考。对数据的分析提供了多年来观察到的主要变化的见解。此外，在数据集上评估了几种公认的视觉定位方法，表明水下长期视觉定位仍有改进空间。数据可在https://www.seanoe.org/data/00810/92226/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.05301v1" target="_blank">2305.05301v1</a>
                              </td>
                              <td>Eiffel Tower: A Deep-Sea Underwater Dataset for Long-Term Visual Localization</td>
                              <td>Clémentin Boittiaux</td>
                              <td>2023-05-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_05301v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.05301v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_05268v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rotation Synchronization via Deep Matrix Factorization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_05268v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_05268v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_05268v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we address the rotation synchronization problem, where the objective is to recover absolute rotations starting from pairwise ones, where the unknowns and the measures are represented as nodes and edges of a graph, respectively. This problem is an essential task for structure from motion and simultaneous localization and mapping. We focus on the formulation of synchronization via neural networks, which has only recently begun to be explored in the literature. Inspired by deep matrix completion, we express rotation synchronization in terms of matrix factorization with a deep neural network. Our formulation exhibits implicit regularization properties and, more importantly, is unsupervised, whereas previous deep approaches are supervised. Our experiments show that we achieve comparable accuracy to the closest competitors in most scenes, while working under weaker assumptions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_05268v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们解决了旋转同步问题，其中目标是从成对旋转开始恢复绝对旋转，其中未知数和测度分别表示为图的节点和边。这个问题是结构从运动和同时定位和映射的一个重要任务。我们专注于通过神经网络进行同步的公式化，直到最近才开始在文献中进行探索。受深度矩阵完备的启发，我们用深度神经网络的矩阵分解来表达旋转同步。我们的公式具有隐式正则化性质，更重要的是，它是无监督的，而以前的深度方法是有监督的。我们的实验表明，在大多数场景中，我们实现了与最接近的竞争对手相当的准确性，同时在较弱的假设下工作。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.05268v1" target="_blank">2305.05268v1</a>
                              </td>
                              <td>Rotation Synchronization via Deep Matrix Factorization</td>
                              <td>Gk Tejus</td>
                              <td>2023-05-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_05268v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.05268v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2210_05020v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Spectral Sparsification for Communication-Efficient Collaborative Rotation and Translation Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2210_05020v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2210_05020v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2210_05020v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose fast and communication-efficient optimization algorithms for multi-robot rotation averaging and translation estimation problems that arise from collaborative simultaneous localization and mapping (SLAM), structure-from-motion (SfM), and camera network localization applications. Our methods are based on theoretical relations between the Hessians of the underlying Riemannian optimization problems and the Laplacians of suitably weighted graphs. We leverage these results to design a collaborative solver in which robots coordinate with a central server to perform approximate second-order optimization, by solving a Laplacian system at each iteration. Crucially, our algorithms permit robots to employ spectral sparsification to sparsify intermediate dense matrices before communication, and hence provide a mechanism to trade off accuracy with communication efficiency with provable guarantees. We perform rigorous theoretical analysis of our methods and prove that they enjoy (local) linear rate of convergence. Furthermore, we show that our methods can be combined with graduated non-convexity to achieve outlier-robust estimation. Extensive experiments on real-world SLAM and SfM scenarios demonstrate the superior convergence rate and communication efficiency of our methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2210_05020v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们为多机器人旋转平均和平移估计问题提出了快速且高效的优化算法，这些问题源于协同同步定位和映射（SLAM）、运动结构（SfM）和相机网络定位应用。我们的方法基于基本黎曼优化问题的Hessians和适当加权图的Laplacian之间的理论关系。我们利用这些结果设计了一个协作求解器，在该求解器中，机器人与中央服务器协调，通过在每次迭代中求解拉普拉斯系统来执行近似的二阶优化。至关重要的是，我们的算法允许机器人在通信前使用频谱稀疏化来稀疏中间密集矩阵，因此提供了一种在通信效率与精度之间进行权衡的机制，并提供了可证明的保证。我们对我们的方法进行了严格的理论分析，并证明它们具有（局部）线性收敛率。此外，我们证明了我们的方法可以与分级非凸性相结合，以实现异常值的鲁棒估计。在真实世界的SLAM和SfM场景上进行的大量实验证明了我们的方法优越的收敛速度和通信效率。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2210.05020v3" target="_blank">2210.05020v3</a>
                              </td>
                              <td>Spectral Sparsification for Communication-Efficient Collaborative Rotation and Translation Estimation</td>
                              <td>Yulun Tian</td>
                              <td>2022-10-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2210_05020v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2210.05020v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_10664v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Comparative Neural Radiance Field (NeRF) 3D Analysis of Camera Poses from HoloLens Trajectories and Structure from Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_10664v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_10664v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_10664v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural Radiance Fields (NeRFs) are trained using a set of camera poses and associated images as input to estimate density and color values for each position. The position-dependent density learning is of particular interest for photogrammetry, enabling 3D reconstruction by querying and filtering the NeRF coordinate system based on the object density. While traditional methods like Structure from Motion are commonly used for camera pose calculation in pre-processing for NeRFs, the HoloLens offers an interesting interface for extracting the required input data directly. We present a workflow for high-resolution 3D reconstructions almost directly from HoloLens data using NeRFs. Thereby, different investigations are considered: Internal camera poses from the HoloLens trajectory via a server application, and external camera poses from Structure from Motion, both with an enhanced variant applied through pose refinement. Results show that the internal camera poses lead to NeRF convergence with a PSNR of 25\,dB with a simple rotation around the x-axis and enable a 3D reconstruction. Pose refinement enables comparable quality compared to external camera poses, resulting in improved training process with a PSNR of 27\,dB and a better 3D reconstruction. Overall, NeRF reconstructions outperform the conventional photogrammetric dense reconstruction using Multi-View Stereo in terms of completeness and level of detail.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_10664v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用一组相机姿势和相关图像作为输入来训练神经辐射场（NeRF），以估计每个位置的密度和颜色值。位置相关密度学习对摄影测量特别感兴趣，它通过基于对象密度查询和过滤NeRF坐标系来实现3D重建。虽然在NeRF的预处理中，像“运动结构”这样的传统方法通常用于相机姿态计算，但HoloLens提供了一个有趣的界面，用于直接提取所需的输入数据。我们提出了一个使用NeRF几乎直接从HoloLens数据进行高分辨率3D重建的工作流程。因此，考虑了不同的研究：通过服务器应用程序从HoloLens轨迹中获得的内部相机姿势，以及从运动中获得的结构中获得的外部相机姿势，两者都通过姿势细化应用了增强的变体。结果表明，在绕x轴简单旋转的情况下，内部相机姿态导致NeRF收敛，PSNR为25dB，并能够进行3D重建。与外部相机姿势相比，姿势细化能够实现相当的质量，从而改进训练过程，PSNR为27dB，并实现更好的3D重建。总体而言，NeRF重建在完整性和细节水平方面优于使用多视图立体的传统摄影测量密集重建。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.10664v1" target="_blank">2304.10664v1</a>
                              </td>
                              <td>A Comparative Neural Radiance Field (NeRF) 3D Analysis of Camera Poses from HoloLens Trajectories and Structure from Motion</td>
                              <td>Miriam Jäger</td>
                              <td>2023-04-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_10664v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.10664v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2103_13875v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Finding Geometric Models by Clustering in the Consensus Space</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2103_13875v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2103_13875v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2103_13875v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a new algorithm for finding an unknown number of geometric models, e.g., homographies. The problem is formalized as finding dominant model instances progressively without forming crisp point-to-model assignments. Dominant instances are found via a RANSAC-like sampling and a consolidation process driven by a model quality function considering previously proposed instances. New ones are found by clustering in the consensus space. This new formulation leads to a simple iterative algorithm with state-of-the-art accuracy while running in real-time on a number of vision problems - at least two orders of magnitude faster than the competitors on two-view motion estimation. Also, we propose a deterministic sampler reflecting the fact that real-world data tend to form spatially coherent structures. The sampler returns connected components in a progressively densified neighborhood-graph. We present a number of applications where the use of multiple geometric models improves accuracy. These include pose estimation from multiple generalized homographies; trajectory estimation of fast-moving objects; and we also propose a way of using multiple homographies in global SfM algorithms. Source code: https://github.com/danini/clustering-in-consensus-space.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2103_13875v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的算法来寻找未知数量的几何模型，例如单应性。该问题被形式化为逐步找到主导模型实例，而不形成清晰的点到模型分配。主要实例是通过类似RANSAC的采样和由考虑先前提出的实例的模型质量函数驱动的合并过程来发现的。通过在一致性空间中进行聚类来发现新的一致性。这种新的公式产生了一种简单的迭代算法，具有最先进的精度，同时实时处理许多视觉问题——在双视图运动估计方面比竞争对手快至少两个数量级。此外，我们提出了一个确定性采样器，反映了真实世界的数据往往形成空间相干结构的事实。采样器返回逐步加密的邻域图中的连接分量。我们介绍了许多应用，其中使用多个几何模型可以提高精度。这些包括来自多个广义单应性的姿态估计；快速移动物体的轨迹估计；并且我们还提出了一种在全局SfM算法中使用多个单应性的方法。源代码：https://github.com/danini/clustering-in-consensus-space.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2103.13875v2" target="_blank">2103.13875v2</a>
                              </td>
                              <td>Finding Geometric Models by Clustering in the Consensus Space</td>
                              <td>Daniel Barath</td>
                              <td>2021-03-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2103_13875v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2103.13875v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_05947v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Visual Localization using Imperfect 3D Models from the Internet</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_05947v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_05947v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_05947v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual localization is a core component in many applications, including augmented reality (AR). Localization algorithms compute the camera pose of a query image w.r.t. a scene representation, which is typically built from images. This often requires capturing and storing large amounts of data, followed by running Structure-from-Motion (SfM) algorithms. An interesting, and underexplored, source of data for building scene representations are 3D models that are readily available on the Internet, e.g., hand-drawn CAD models, 3D models generated from building footprints, or from aerial images. These models allow to perform visual localization right away without the time-consuming scene capturing and model building steps. Yet, it also comes with challenges as the available 3D models are often imperfect reflections of reality. E.g., the models might only have generic or no textures at all, might only provide a simple approximation of the scene geometry, or might be stretched. This paper studies how the imperfections of these models affect localization accuracy. We create a new benchmark for this task and provide a detailed experimental evaluation based on multiple 3D models per scene. We show that 3D models from the Internet show promise as an easy-to-obtain scene representation. At the same time, there is significant room for improvement for visual localization pipelines. To foster research on this interesting and challenging task, we release our benchmark at v-pnk.github.io/cadloc.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_05947v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉定位是包括增强现实（AR）在内的许多应用程序的核心组件。定位算法计算查询图像的相机姿态，该图像通常是根据图像构建的场景表示。这通常需要捕获和存储大量数据，然后运行运动结构（SfM）算法。用于建筑场景表示的一个有趣且未充分探索的数据源是在互联网上容易获得的3D模型，例如手绘CAD模型、从建筑足迹或从航空图像生成的3D模型。这些模型允许立即执行视觉定位，而无需耗时的场景捕捉和模型构建步骤。然而，它也带来了挑战，因为可用的3D模型往往是对现实的不完美反映。例如，模型可能只具有通用纹理或根本没有纹理，可能只提供场景几何体的简单近似，或者可能被拉伸。本文研究了这些模型的缺陷如何影响定位精度。我们为这项任务创建了一个新的基准，并基于每个场景的多个3D模型提供了详细的实验评估。我们表明，来自互联网的3D模型有望成为一种易于获得的场景表示。同时，视觉定位管道还有很大的改进空间。为了促进对这项有趣且具有挑战性的任务的研究，我们在v-pnk.github.io/cadloc上发布了我们的基准。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.05947v1" target="_blank">2304.05947v1</a>
                              </td>
                              <td>Visual Localization using Imperfect 3D Models from the Internet</td>
                              <td>Vojtech Panek</td>
                              <td>2023-04-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_05947v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.05947v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_03930v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Photometric Correction for Infrared Sensors</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_03930v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_03930v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_03930v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Infrared thermography has been widely used in several domains to capture and measure temperature distributions across surfaces and objects. This methodology can be further expanded to 3D applications if the spatial distribution of the temperature distribution is available. Structure from Motion (SfM) is a photometric range imaging technique that makes it possible to obtain 3D renderings from a cloud of 2D images. To explore the possibility of 3D reconstruction via SfM from infrared images, this article proposes a photometric correction model for infrared sensors based on temperature constancy. Photometric correction is accomplished by estimating the scene irradiance as the values from the solution to a differential equation for microbolometer pixel excitation with unknown coefficients and initial conditions. The model was integrated into an SfM framework and experimental evaluations demonstrate the contribution of the photometric correction for improving the estimates of both the camera motion and the scene structure. Further, experiments show that the reconstruction quality from the corrected infrared imagery achieves performance on par with state-of-the-art reconstruction using RGB sensors.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_03930v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>红外热成像技术已广泛应用于多个领域，用于捕捉和测量表面和物体的温度分布。如果温度分布的空间分布可用，则该方法可以进一步扩展到3D应用。运动结构（SfM）是一种光度范围成像技术，可以从2D图像云中获得3D渲染。为了探索从红外图像中通过SfM进行三维重建的可能性，本文提出了一种基于温度恒定性的红外传感器光度校正模型。光度校正是通过将场景辐照度估计为具有未知系数和初始条件的微测辐射热计像素激发的微分方程的解的值来实现的。该模型被集成到SfM框架中，实验评估证明了光度校正对改善相机运动和场景结构估计的贡献。此外，实验表明，校正后的红外图像的重建质量达到了与使用RGB传感器的最先进重建相当的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.03930v1" target="_blank">2304.03930v1</a>
                              </td>
                              <td>Photometric Correction for Infrared Sensors</td>
                              <td>Jincheng Zhang</td>
                              <td>2023-04-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_03930v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.03930v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_03560v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DualRefine: Self-Supervised Depth and Pose Estimation Through Iterative Epipolar Sampling and Refinement Toward Equilibrium</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_03560v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_03560v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_03560v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised multi-frame depth estimation achieves high accuracy by computing matching costs of pixel correspondences between adjacent frames, injecting geometric information into the network. These pixel-correspondence candidates are computed based on the relative pose estimates between the frames. Accurate pose predictions are essential for precise matching cost computation as they influence the epipolar geometry. Furthermore, improved depth estimates can, in turn, be used to align pose estimates.   Inspired by traditional structure-from-motion (SfM) principles, we propose the DualRefine model, which tightly couples depth and pose estimation through a feedback loop. Our novel update pipeline uses a deep equilibrium model framework to iteratively refine depth estimates and a hidden state of feature maps by computing local matching costs based on epipolar geometry. Importantly, we used the refined depth estimates and feature maps to compute pose updates at each step. This update in the pose estimates slowly alters the epipolar geometry during the refinement process. Experimental results on the KITTI dataset demonstrate competitive depth prediction and odometry prediction performance surpassing published self-supervised baselines.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_03560v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督多帧深度估计通过计算相邻帧之间像素对应关系的匹配代价，将几何信息注入网络，实现了高精度。这些像素对应候选是基于帧之间的相对姿态估计来计算的。精确的姿态预测对于精确的匹配成本计算至关重要，因为它们会影响核极几何。此外，改进的深度估计反过来可以用于对准姿态估计。受传统运动结构（SfM）原理的启发，我们提出了DualRefine模型，该模型通过反馈回路将深度和姿态估计紧密耦合。我们新颖的更新管道使用深度平衡模型框架，通过基于核极几何计算局部匹配成本，迭代细化深度估计和特征图的隐藏状态。重要的是，我们使用精细的深度估计和特征图来计算每一步的姿势更新。姿态估计的这种更新在细化过程中缓慢地改变了极线几何结构。KITTI数据集上的实验结果表明，竞争性深度预测和里程计预测性能超过了已公布的自监督基线。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.03560v1" target="_blank">2304.03560v1</a>
                              </td>
                              <td>DualRefine: Self-Supervised Depth and Pose Estimation Through Iterative Epipolar Sampling and Refinement Toward Equilibrium</td>
                              <td>Antyanta Bangunharcana</td>
                              <td>2023-04-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_03560v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.03560v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_03426v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Convex Minimization with Integer Minima in $\widetilde O(n^4)$ Time</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_03426v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_03426v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_03426v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Given a convex function $f$ on $\mathbb{R}^n$ with an integer minimizer, we show how to find an exact minimizer of $f$ using $O(n^2 \log n)$ calls to a separation oracle and $O(n^4 \log n)$ time. The previous best polynomial time algorithm for this problem given in [Jiang, SODA 2021, JACM 2022] achieves $\widetilde{O}(n^2)$ oracle complexity. However, the overall runtime of Jiang's algorithm is at least $\widetilde{\Omega}(n^8)$, due to expensive sub-routines such as the Lenstra-Lenstra-Lov\'asz (LLL) algorithm [Lenstra, Lenstra, Lov\'asz, Math. Ann. 1982] and random walk based cutting plane method [Bertsimas, Vempala, JACM 2004]. Our significant speedup is obtained by a nontrivial combination of a faster version of the LLL algorithm due to [Neumaier, Stehl\'e, ISSAC 2016] that gives similar guarantees, the volumetric center cutting plane method (CPM) by [Vaidya, FOCS 1989] and its fast implementation given in [Jiang, Lee, Song, Wong, STOC 2020].   For the special case of submodular function minimization (SFM), our result implies a strongly polynomial time algorithm for this problem using $O(n^3 \log n)$ calls to an evaluation oracle and $O(n^4 \log n)$ additional arithmetic operations. Both the oracle complexity and the number of arithmetic operations of our more general algorithm are better than the previous best-known runtime algorithms for this specific problem given in [Lee, Sidford, Wong, FOCS 2015] and [Dadush, V\'egh, Zambelli, SODA 2018, MOR 2021].</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_03426v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>给定$\mathbb｛R｝^n$上具有整数极小值的凸函数$f$，我们展示了如何使用对分离预言机的$O（n^2\logn）$调用和$O（n ^4\logn）$时间来找到$f$的精确极小值。[Jiang，SODA 2021，JACM 2022]中给出的该问题的先前最佳多项式时间算法实现了$\widetilde{O}（n^2）$oracle复杂度。然而，由于Lenstra-Lonstra-Lov'asz（LLL）算法[Lenstra，Lenstra，Lov'asz，Math.Ann.1982]和基于随机行走的切割平面方法[Bertsimas，Vempala，JACM 2004]等昂贵的子程序，姜算法的总体运行时间至少为$\widetilde｛\Omega｝（n^8）$。我们的显著加速是通过[Neumaier，Stehl\'e，ISSAC 2016]的LLL算法的更快版本、[Vaidya，FOCS 1989]的体积中心切割平面法（CPM）及其在[Jiang，Lee，Song，Wong，STOC 2020]中给出的快速实现的非平凡组合而获得的。对于子模函数最小化（SFM）的特殊情况，我们的结果暗示了这个问题的一个强多项式时间算法，使用$O（n^3\logn）$对评估预言机的调用和$O（n ^4\log n）$额外的算术运算。对于这一特定问题，我们更通用的算法的预言机复杂性和算术运算次数都优于[Lee，Sidford，Wong，FOCS 2015]和[Dadush，V\'egh，Zambelli，SODA 2018，MOR 2021]中给出的以前最著名的运行时算法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.03426v1" target="_blank">2304.03426v1</a>
                              </td>
                              <td>Convex Minimization with Integer Minima in $\widetilde O(n^4)$ Time</td>
                              <td>Haotian Jiang</td>
                              <td>2023-04-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_03426v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.03426v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_02420v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Semantic Validation in Structure from Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_02420v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_02420v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_02420v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Structure from Motion (SfM) challenge in computer vision is the process of recovering the 3D structure of a scene from a series of projective measurements that are calculated from a collection of 2D images, taken from different perspectives. SfM consists of three main steps; feature detection and matching, camera motion estimation, and recovery of 3D structure from estimated intrinsic and extrinsic parameters and features.   A problem encountered in SfM is that scenes lacking texture or with repetitive features can cause erroneous feature matching between frames. Semantic segmentation offers a route to validate and correct SfM models by labelling pixels in the input images with the use of a deep convolutional neural network. The semantic and geometric properties associated with classes in the scene can be taken advantage of to apply prior constraints to each class of object. The SfM pipeline COLMAP and semantic segmentation pipeline DeepLab were used. This, along with planar reconstruction of the dense model, were used to determine erroneous points that may be occluded from the calculated camera position, given the semantic label, and thus prior constraint of the reconstructed plane. Herein, semantic segmentation is integrated into SfM to apply priors on the 3D point cloud, given the object detection in the 2D input images. Additionally, the semantic labels of matched keypoints are compared and inconsistent semantically labelled points discarded. Furthermore, semantic labels on input images are used for the removal of objects associated with motion in the output SfM models. The proposed approach is evaluated on a data-set of 1102 images of a repetitive architecture scene. This project offers a novel method for improved validation of 3D SfM models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_02420v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>计算机视觉中的运动结构（SfM）挑战是从一系列投影测量中恢复场景的3D结构的过程，这些投影测量是从不同视角拍摄的2D图像集合中计算出来的。SfM包括三个主要步骤；特征检测和匹配，相机运动估计，以及从估计的内在和外在参数和特征中恢复3D结构。SfM中遇到的问题是，缺乏纹理或具有重复特征的场景可能导致帧之间的错误特征匹配。语义分割通过使用深度卷积神经网络标记输入图像中的像素，提供了一种验证和校正SfM模型的途径。可以利用与场景中的类相关联的语义和几何特性来将先验约束应用于对象的每个类。使用了SfM流水线COLMAP和语义分割流水线DeepLab。这与密集模型的平面重建一起，用于确定在给定语义标签的情况下可能从计算的相机位置遮挡的错误点，从而确定重建平面的先验约束。在此，在给定2D输入图像中的对象检测的情况下，语义分割被集成到SfM中，以在3D点云上应用先验。此外，对匹配关键点的语义标签进行比较，并丢弃语义上不一致的标记点。此外，输入图像上的语义标签用于去除与输出SfM模型中的运动相关联的对象。在重复建筑场景的1102个图像的数据集上评估所提出的方法。该项目提供了一种改进三维SfM模型验证的新方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.02420v1" target="_blank">2304.02420v1</a>
                              </td>
                              <td>Semantic Validation in Structure from Motion</td>
                              <td>Joseph Rowell</td>
                              <td>2023-04-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_02420v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.02420v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_13551v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SfM-TTR: Using Structure from Motion for Test-Time Refinement of Single-View Depth Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_13551v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_13551v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_13551v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Estimating a dense depth map from a single view is geometrically ill-posed, and state-of-the-art methods rely on learning depth's relation with visual appearance using deep neural networks. On the other hand, Structure from Motion (SfM) leverages multi-view constraints to produce very accurate but sparse maps, as matching across images is typically limited by locally discriminative texture. In this work, we combine the strengths of both approaches by proposing a novel test-time refinement (TTR) method, denoted as SfM-TTR, that boosts the performance of single-view depth networks at test time using SfM multi-view cues. Specifically, and differently from the state of the art, we use sparse SfM point clouds as test-time self-supervisory signal, fine-tuning the network encoder to learn a better representation of the test scene. Our results show how the addition of SfM-TTR to several state-of-the-art self-supervised and supervised networks improves significantly their performance, outperforming previous TTR baselines mainly based on photometric multi-view consistency. The code is available at https://github.com/serizba/SfM-TTR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_13551v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从单个视图估计密集深度图在几何上是不适定的，最先进的方法依赖于使用深度神经网络学习深度与视觉外观的关系。另一方面，运动结构（SfM）利用多视图约束来生成非常精确但稀疏的地图，因为图像之间的匹配通常受到局部判别纹理的限制。在这项工作中，我们结合了这两种方法的优势，提出了一种新的测试时间细化（TTR）方法，称为SfM-TTR，该方法在测试时使用SfM多视图线索来提高单视图深度网络的性能。具体而言，与现有技术不同的是，我们使用稀疏的SfM点云作为测试时间自监督信号，微调网络编码器以学习测试场景的更好表示。我们的结果表明，在几个最先进的自监督和监督网络中添加SfM-TTR可以显著提高其性能，优于以前主要基于光度多视图一致性的TTR基线。代码可在https://github.com/serizba/SfM-TTR.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.13551v2" target="_blank">2211.13551v2</a>
                              </td>
                              <td>SfM-TTR: Using Structure from Motion for Test-Time Refinement of Single-View Depth Networks</td>
                              <td>Sergio Izquierdo</td>
                              <td>2022-11-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_13551v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.13551v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_17504v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D Line Mapping Revisited</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_17504v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_17504v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_17504v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In contrast to sparse keypoints, a handful of line segments can concisely encode the high-level scene layout, as they often delineate the main structural elements. In addition to offering strong geometric cues, they are also omnipresent in urban landscapes and indoor scenes. Despite their apparent advantages, current line-based reconstruction methods are far behind their point-based counterparts. In this paper we aim to close the gap by introducing LIMAP, a library for 3D line mapping that robustly and efficiently creates 3D line maps from multi-view imagery. This is achieved through revisiting the degeneracy problem of line triangulation, carefully crafted scoring and track building, and exploiting structural priors such as line coincidence, parallelism, and orthogonality. Our code integrates seamlessly with existing point-based Structure-from-Motion methods and can leverage their 3D points to further improve the line reconstruction. Furthermore, as a byproduct, the method is able to recover 3D association graphs between lines and points / vanishing points (VPs). In thorough experiments, we show that LIMAP significantly outperforms existing approaches for 3D line mapping. Our robust 3D line maps also open up new research directions. We show two example applications: visual localization and bundle adjustment, where integrating lines alongside points yields the best results. Code is available at https://github.com/cvg/limap.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_17504v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>与稀疏关键点相比，少数线段可以简洁地对高级场景布局进行编码，因为它们通常描绘主要的结构元素。除了提供强烈的几何线索外，它们还在城市景观和室内场景中无处不在。尽管有明显的优势，但目前基于线的重建方法远远落后于基于点的重建方法。在本文中，我们的目标是通过引入LIMAP来缩小差距，LIMAP是一个用于3D线图绘制的库，可以从多视图图像中稳健有效地创建3D线图。这是通过重新审视线三角测量的退化问题、精心制作的评分和轨迹构建，以及利用线重合、平行和正交等结构先验来实现的。我们的代码与现有的基于点的运动结构方法无缝集成，可以利用它们的3D点来进一步改进线重建。此外，作为副产品，该方法能够恢复线和点/消失点（VP）之间的3D关联图。在深入的实验中，我们表明LIMAP显著优于现有的3D线映射方法。我们强大的3D折线图也开辟了新的研究方向。我们展示了两个示例应用程序：视觉定位和束调整，其中将线与点一起积分会产生最佳结果。代码可在https://github.com/cvg/limap.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.17504v1" target="_blank">2303.17504v1</a>
                              </td>
                              <td>3D Line Mapping Revisited</td>
                              <td>Shaohui Liu</td>
                              <td>2023-03-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_17504v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.17504v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_15069v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FeatureBooster: Boosting Feature Descriptors with a Lightweight Neural Network</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_15069v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_15069v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_15069v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce a lightweight network to improve descriptors of keypoints within the same image. The network takes the original descriptors and the geometric properties of keypoints as the input, and uses an MLP-based self-boosting stage and a Transformer-based cross-boosting stage to enhance the descriptors. The boosted descriptors can be either real-valued or binary ones. We use the proposed network to boost both hand-crafted (ORB, SIFT) and the state-of-the-art learning-based descriptors (SuperPoint, ALIKE) and evaluate them on image matching, visual localization, and structure-from-motion tasks. The results show that our method significantly improves the performance of each task, particularly in challenging cases such as large illumination changes or repetitive patterns. Our method requires only 3.2ms on desktop GPU and 27ms on embedded GPU to process 2000 features, which is fast enough to be applied to a practical system. The code and trained weights are publicly available at github.com/SJTU-ViSYS/FeatureBooster.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_15069v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们引入了一个轻量级网络来改进同一图像中关键点的描述符。该网络以原始描述符和关键点的几何特性为输入，并使用基于MLP的自提升级和基于Transformer的交叉提升级来增强描述符。增强的描述符可以是实数描述符，也可以是二进制描述符。我们使用所提出的网络来增强手工制作的（ORB，SIFT）和最先进的基于学习的描述符（SuperPoint，ALIKE），并在图像匹配、视觉定位和运动任务的结构方面对它们进行评估。结果表明，我们的方法显著提高了每个任务的性能，特别是在具有挑战性的情况下，如大的照明变化或重复模式。我们的方法只需要在台式GPU上3.2ms，在嵌入式GPU上27ms就可以处理2000个特征，这足够快，可以应用于实际系统。代码和训练过的重量可在github.com/SJTU-ViSYS/FeatureBooster上公开获取。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.15069v3" target="_blank">2211.15069v3</a>
                              </td>
                              <td>FeatureBooster: Boosting Feature Descriptors with a Lightweight Neural Network</td>
                              <td>Xinjiang Wang</td>
                              <td>2022-11-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_15069v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.15069v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_15060v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TMO: Textured Mesh Acquisition of Objects with a Mobile Device by using Differentiable Rendering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_15060v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_15060v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_15060v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present a new pipeline for acquiring a textured mesh in the wild with a single smartphone which offers access to images, depth maps, and valid poses. Our method first introduces an RGBD-aided structure from motion, which can yield filtered depth maps and refines camera poses guided by corresponding depth. Then, we adopt the neural implicit surface reconstruction method, which allows for high-quality mesh and develops a new training process for applying a regularization provided by classical multi-view stereo methods. Moreover, we apply a differentiable rendering to fine-tune incomplete texture maps and generate textures which are perceptually closer to the original scene. Our pipeline can be applied to any common objects in the real world without the need for either in-the-lab environments or accurate mask images. We demonstrate results of captured objects with complex shapes and validate our method numerically against existing 3D reconstruction and texture mapping methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_15060v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的管道，可以通过一部智能手机在野外获取纹理网格，该智能手机可以访问图像、深度图和有效姿势。我们的方法首先引入了一种基于运动的RGBD辅助结构，该结构可以生成过滤后的深度图，并根据相应的深度细化相机姿态。然后，我们采用了神经隐式曲面重建方法，该方法可以获得高质量的网格，并开发了一种新的训练过程，用于应用经典多视图立体方法提供的正则化。此外，我们应用可微分渲染来微调不完整的纹理贴图，并生成在感知上更接近原始场景的纹理。我们的管道可以应用于现实世界中的任何常见对象，而无需实验室环境或精确的掩模图像。我们展示了具有复杂形状的捕捉对象的结果，并与现有的3D重建和纹理映射方法进行了数值验证。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.15060v1" target="_blank">2303.15060v1</a>
                              </td>
                              <td>TMO: Textured Mesh Acquisition of Objects with a Mobile Device by using Differentiable Rendering</td>
                              <td>Jaehoon Choi</td>
                              <td>2023-03-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_15060v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.15060v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_12018v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Level-S$^2$fM: Structure from Motion on Neural Level Set of Implicit Surfaces</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_12018v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_12018v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_12018v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a neural incremental Structure-from-Motion (SfM) approach, Level-S$^2$fM, which estimates the camera poses and scene geometry from a set of uncalibrated images by learning coordinate MLPs for the implicit surfaces and the radiance fields from the established keypoint correspondences. Our novel formulation poses some new challenges due to inevitable two-view and few-view configurations in the incremental SfM pipeline, which complicates the optimization of coordinate MLPs for volumetric neural rendering with unknown camera poses. Nevertheless, we demonstrate that the strong inductive basis conveying in the 2D correspondences is promising to tackle those challenges by exploiting the relationship between the ray sampling schemes. Based on this, we revisit the pipeline of incremental SfM and renew the key components, including two-view geometry initialization, the camera poses registration, the 3D points triangulation, and Bundle Adjustment, with a fresh perspective based on neural implicit surfaces. By unifying the scene geometry in small MLP networks through coordinate MLPs, our Level-S$^2$fM treats the zero-level set of the implicit surface as an informative top-down regularization to manage the reconstructed 3D points, reject the outliers in correspondences via querying SDF, and refine the estimated geometries by NBA (Neural BA). Not only does our Level-S$^2$fM lead to promising results on camera pose estimation and scene geometry reconstruction, but it also shows a promising way for neural implicit rendering without knowing camera extrinsic beforehand.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_12018v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种基于运动的神经增量结构（SfM）方法，即S$^2$fM级，该方法通过从建立的关键点对应关系中学习隐式表面的坐标MLP和辐射场，从一组未校准的图像中估计相机姿态和场景几何。由于增量SfM管道中不可避免的两视图和少视图配置，我们的新公式提出了一些新的挑战，这使用于具有未知相机姿态的体积神经渲染的坐标MLP的优化变得复杂。然而，我们证明了在2D对应关系中传递的强归纳基有望通过利用射线采样方案之间的关系来解决这些挑战。基于此，我们重新审视了增量SfM的管道，并更新了关键组件，包括两视图几何初始化、相机姿态配准、3D点三角测量和束调整，以基于神经隐式曲面的全新视角。通过坐标MLP统一小型MLP网络中的场景几何结构，我们的Level-S$^2$fM将隐式曲面的零级集视为自上而下的信息正则化，以管理重建的3D点，通过查询SDF拒绝对应关系中的异常值，并通过NBA（Neural BA）细化估计的几何结构。我们的S$^2$fM级不仅在相机姿态估计和场景几何重建方面取得了有希望的结果，而且它还为神经隐式渲染提供了一种很有前途的方法，而无需事先了解相机的外在情况。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.12018v2" target="_blank">2211.12018v2</a>
                              </td>
                              <td>Level-S$^2$fM: Structure from Motion on Neural Level Set of Implicit Surfaces</td>
                              <td>Yuxi Xiao</td>
                              <td>2022-11-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_12018v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.12018v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_14840v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On the Importance of Accurate Geometry Data for Dense 3D Vision Tasks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_14840v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_14840v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_14840v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Learning-based methods to solve dense 3D vision problems typically train on 3D sensor data. The respectively used principle of measuring distances provides advantages and drawbacks. These are typically not compared nor discussed in the literature due to a lack of multi-modal datasets. Texture-less regions are problematic for structure from motion and stereo, reflective material poses issues for active sensing, and distances for translucent objects are intricate to measure with existing hardware. Training on inaccurate or corrupt data induces model bias and hampers generalisation capabilities. These effects remain unnoticed if the sensor measurement is considered as ground truth during the evaluation. This paper investigates the effect of sensor errors for the dense 3D vision tasks of depth estimation and reconstruction. We rigorously show the significant impact of sensor characteristics on the learned predictions and notice generalisation issues arising from various technologies in everyday household environments. For evaluation, we introduce a carefully designed dataset\footnote{dataset available at https://github.com/Junggy/HAMMER-dataset} comprising measurements from commodity sensors, namely D-ToF, I-ToF, passive/active stereo, and monocular RGB+P. Our study quantifies the considerable sensor noise impact and paves the way to improved dense vision estimates and targeted data fusion.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_14840v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>解决密集3D视觉问题的基于学习的方法通常基于3D传感器数据进行训练。分别使用的测量距离的原理提供了优点和缺点。由于缺乏多模态数据集，文献中通常不会对这些数据进行比较或讨论。无纹理区域对运动和立体的结构来说是有问题的，反射材料对主动传感来说是个问题，半透明物体的距离用现有硬件测量起来很复杂。对不准确或损坏的数据进行培训会导致模型偏差，阻碍泛化能力。如果在评估过程中将传感器测量视为基本事实，则这些影响不会被注意到。本文研究了传感器误差对密集三维视觉深度估计和重建任务的影响。我们严格展示了传感器特性对学习预测的重大影响，并注意到日常家庭环境中各种技术产生的泛化问题。为了进行评估，我们引入了一个精心设计的数据集\脚注｛数据集，可在https://github.com/Junggy/HAMMER-dataset}包括来自商品传感器的测量，即D-ToF、I-ToF、无源/有源立体声和单目RGB+P。我们的研究量化了传感器噪声的巨大影响，为改进密集视觉估计和有针对性的数据融合铺平了道路。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.14840v1" target="_blank">2303.14840v1</a>
                              </td>
                              <td>On the Importance of Accurate Geometry Data for Dense 3D Vision Tasks</td>
                              <td>HyunJun Jung</td>
                              <td>2023-03-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_14840v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.14840v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_13543v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BLiRF: Bandlimited Radiance Fields for Dynamic Scene Modeling</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_13543v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_13543v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_13543v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Reasoning the 3D structure of a non-rigid dynamic scene from a single moving camera is an under-constrained problem. Inspired by the remarkable progress of neural radiance fields (NeRFs) in photo-realistic novel view synthesis of static scenes, extensions have been proposed for dynamic settings. These methods heavily rely on neural priors in order to regularize the problem. In this work, we take a step back and reinvestigate how current implementations may entail deleterious effects, including limited expressiveness, entanglement of light and density fields, and sub-optimal motion localization. As a remedy, we advocate for a bridge between classic non-rigid-structure-from-motion (\nrsfm) and NeRF, enabling the well-studied priors of the former to constrain the latter. To this end, we propose a framework that factorizes time and space by formulating a scene as a composition of bandlimited, high-dimensional signals. We demonstrate compelling results across complex dynamic scenes that involve changes in lighting, texture and long-range dynamics.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_13543v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从单个运动摄像机推断非刚性动态场景的三维结构是一个欠约束问题。受神经辐射场（NeRFs）在静态场景的逼真新颖视图合成中取得的显著进展的启发，提出了动态设置的扩展。这些方法在很大程度上依赖于神经先验来正则化问题。在这项工作中，我们后退一步，重新研究当前的实现如何可能带来有害影响，包括有限的表现力、光场和密度场的纠缠以及次优运动定位。作为一种补救措施，我们主张在经典的非刚性运动结构（\nrsfm）和NeRF之间建立一座桥梁，使前者经过充分研究的先验能够约束后者。为此，我们提出了一个框架，通过将场景公式化为带限高维信号的组合，来分解时间和空间。我们在复杂的动态场景中展示了令人信服的结果，这些场景涉及照明、纹理和长程动力学的变化。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.13543v3" target="_blank">2302.13543v3</a>
                              </td>
                              <td>BLiRF: Bandlimited Radiance Fields for Dynamic Scene Modeling</td>
                              <td>Sameera Ramasinghe</td>
                              <td>2023-02-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_13543v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.13543v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_13805v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Seeing Through the Glass: Neural 3D Reconstruction of Object Inside a Transparent Container</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_13805v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_13805v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_13805v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we define a new problem of recovering the 3D geometry of an object confined in a transparent enclosure. We also propose a novel method for solving this challenging problem. Transparent enclosures pose challenges of multiple light reflections and refractions at the interface between different propagation media e.g. air or glass. These multiple reflections and refractions cause serious image distortions which invalidate the single viewpoint assumption. Hence the 3D geometry of such objects cannot be reliably reconstructed using existing methods, such as traditional structure from motion or modern neural reconstruction methods. We solve this problem by explicitly modeling the scene as two distinct sub-spaces, inside and outside the transparent enclosure. We use an existing neural reconstruction method (NeuS) that implicitly represents the geometry and appearance of the inner subspace. In order to account for complex light interactions, we develop a hybrid rendering strategy that combines volume rendering with ray tracing. We then recover the underlying geometry and appearance of the model by minimizing the difference between the real and hybrid rendered images. We evaluate our method on both synthetic and real data. Experiment results show that our method outperforms the state-of-the-art (SOTA) methods. Codes and data will be available at https://github.com/hirotong/ReNeuS</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_13805v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们定义了一个新的问题，即恢复被限制在透明外壳中的对象的三维几何结构。我们还提出了一种新的方法来解决这个具有挑战性的问题。透明外壳在不同传播介质（例如空气或玻璃）之间的界面处带来了多重光反射和折射的挑战。这些多重反射和折射会导致严重的图像失真，从而使单一视点假设无效。因此，使用现有的方法，例如传统的运动结构或现代神经重建方法，不能可靠地重建这种物体的3D几何结构。我们通过将场景明确建模为透明外壳内外两个不同的子空间来解决这个问题。我们使用现有的神经重建方法（NeuS），该方法隐式地表示内子空间的几何形状和外观。为了解决复杂的灯光交互，我们开发了一种混合渲染策略，将体积渲染与光线跟踪相结合。然后，我们通过最小化真实渲染图像和混合渲染图像之间的差异来恢复模型的基本几何结构和外观。我们根据合成数据和真实数据来评估我们的方法。实验结果表明，我们的方法优于最先进的（SOTA）方法。代码和数据将在https://github.com/hirotong/ReNeuS</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.13805v1" target="_blank">2303.13805v1</a>
                              </td>
                              <td>Seeing Through the Glass: Neural 3D Reconstruction of Object Inside a Transparent Container</td>
                              <td>Jinguang Tong</td>
                              <td>2023-03-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_13805v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.13805v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_13791v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Progressively Optimized Local Radiance Fields for Robust View Synthesis</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_13791v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_13791v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_13791v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present an algorithm for reconstructing the radiance field of a large-scale scene from a single casually captured video. The task poses two core challenges. First, most existing radiance field reconstruction approaches rely on accurate pre-estimated camera poses from Structure-from-Motion algorithms, which frequently fail on in-the-wild videos. Second, using a single, global radiance field with finite representational capacity does not scale to longer trajectories in an unbounded scene. For handling unknown poses, we jointly estimate the camera poses with radiance field in a progressive manner. We show that progressive optimization significantly improves the robustness of the reconstruction. For handling large unbounded scenes, we dynamically allocate new local radiance fields trained with frames within a temporal window. This further improves robustness (e.g., performs well even under moderate pose drifts) and allows us to scale to large scenes. Our extensive evaluation on the Tanks and Temples dataset and our collected outdoor dataset, Static Hikes, show that our approach compares favorably with the state-of-the-art.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_13791v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种从单个随意捕捉的视频中重建大规模场景辐射场的算法。这项任务提出了两个核心挑战。首先，大多数现有的辐射场重建方法都依赖于运动结构算法中准确的预估计相机姿态，而这些算法在野外视频中经常失败。其次，在无界场景中，使用具有有限表示能力的单个全局辐射场不会缩放到更长的轨迹。为了处理未知姿态，我们以渐进的方式联合估计具有辐射场的相机姿态。我们表明，渐进优化显著提高了重建的鲁棒性。为了处理大型无界场景，我们动态分配用时间窗口内的帧训练的新的局部辐射场。这进一步提高了鲁棒性（例如，即使在中等姿势漂移的情况下也表现良好），并允许我们缩放到大场景。我们对Tanks and Temples数据集和收集的户外数据集Static Hikes的广泛评估表明，我们的方法与最先进的方法相比是有利的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.13791v1" target="_blank">2303.13791v1</a>
                              </td>
                              <td>Progressively Optimized Local Radiance Fields for Robust View Synthesis</td>
                              <td>Andreas Meuleman</td>
                              <td>2023-03-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_13791v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.13791v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="LLM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2308_01906v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Reasoning in Large Language Models Through Symbolic Math Word Problems</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01906v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01906v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01906v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have revolutionized NLP by solving downstream tasks with little to no labeled data. Despite their versatile abilities, the larger question of their ability to reason remains ill-understood. This paper addresses reasoning in math word problems (MWPs) by studying symbolic versions of the numeric problems, since a symbolic expression is a "concise explanation" of the numeric answer. We create and use a symbolic version of the SVAMP dataset and find that GPT-3's davinci-002 model also has good zero-shot accuracy on symbolic MWPs. To evaluate the faithfulness of the model's reasoning, we go beyond accuracy and additionally evaluate the alignment between the final answer and the outputted reasoning, which correspond to numeric and symbolic answers respectively for MWPs. We explore a self-prompting approach to encourage the symbolic reasoning to align with the numeric answer, thus equipping the LLM with the ability to provide a concise and verifiable reasoning and making it more interpretable. Surprisingly, self-prompting also improves the symbolic accuracy to be higher than both the numeric and symbolic accuracies, thus providing an ensembling effect. The SVAMP_Sym dataset will be released for future research on symbolic math problems.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01906v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）通过解决几乎没有标记数据的下游任务，彻底改变了NLP。尽管他们多才多艺，但他们推理能力这一更大的问题仍然鲜为人知。本文通过研究数字问题的符号版本来解决数学单词问题中的推理问题，因为符号表达式是对数字答案的“简明解释”。我们创建并使用了SVAMP数据集的符号版本，发现GPT-3的davinci-002模型在符号MWP上也具有良好的零样本精度。为了评估模型推理的可信度，我们超越了准确性，还评估了最终答案和输出推理之间的一致性，这分别对应于MWP的数字和符号答案。我们探索了一种自我提示的方法，以鼓励符号推理与数字答案保持一致，从而使LLM能够提供简洁和可验证的推理，并使其更具可解释性。令人惊讶的是，自提示还将符号精度提高到高于数字和符号精度，从而提供了组合效果。SVAMP_Sym数据集将发布，用于未来对符号数学问题的研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01906v1" target="_blank">2308.01906v1</a>
                              </td>
                              <td>Reasoning in Large Language Models Through Symbolic Math Word Problems</td>
                              <td>Vedant Gaur</td>
                              <td>2023-08-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01906v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01906v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_00692v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LISA: Reasoning Segmentation via Large Language Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_00692v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_00692v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_00692v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Although perception systems have made remarkable advancements in recent years, they still rely on explicit human instruction to identify the target objects or categories before executing visual recognition tasks. Such systems lack the ability to actively reason and comprehend implicit user intentions. In this work, we propose a new segmentation task -- reasoning segmentation. The task is designed to output a segmentation mask given a complex and implicit query text. Furthermore, we establish a benchmark comprising over one thousand image-instruction pairs, incorporating intricate reasoning and world knowledge for evaluation purposes. Finally, we present LISA: large Language Instructed Segmentation Assistant, which inherits the language generation capabilities of the multi-modal Large Language Model (LLM) while also possessing the ability to produce segmentation masks. We expand the original vocabulary with a <SEG> token and propose the embedding-as-mask paradigm to unlock the segmentation capability. Remarkably, LISA can handle cases involving: 1) complex reasoning; 2) world knowledge; 3) explanatory answers; 4) multi-turn conversation. Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets. In addition, fine-tuning the model with merely 239 reasoning segmentation image-instruction pairs results in further performance enhancement. Experiments show our method not only unlocks new reasoning segmentation capabilities but also proves effective in both complex reasoning segmentation and standard referring segmentation tasks. Code, models, and demo are at https://github.com/dvlab-research/LISA.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_00692v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管感知系统近年来取得了显著的进步，但在执行视觉识别任务之前，它们仍然依赖于明确的人类指令来识别目标对象或类别。这样的系统缺乏主动推理和理解隐含用户意图的能力。在这项工作中，我们提出了一个新的分割任务——推理分割。该任务被设计为在给定复杂且隐含的查询文本的情况下输出分段掩码。此外，我们建立了一个由1000多个图像指令对组成的基准，将复杂的推理和世界知识结合起来进行评估。最后，我们提出了LISA：大型语言指令分割助手，它继承了多模态大型语言模型（LLM）的语言生成能力，同时也具有生成分割掩码的能力。我们用<SEG>标记扩展了原始词汇表，并提出了嵌入作为掩码的范式来解锁分割能力。值得注意的是，LISA可以处理以下案件：1）复杂推理；2） 世界知识；3） 解释性答案；4） 多回合对话。此外，当专门在无推理数据集上训练时，它展示了强大的零样本能力。此外，仅用239个推理分割图像指令对来微调模型会进一步提高性能。实验表明，我们的方法不仅释放了新的推理分割能力，而且在复杂推理分割和标准参考分割任务中都是有效的。代码、模型和演示位于https://github.com/dvlab-research/LISA.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.00692v2" target="_blank">2308.00692v2</a>
                              </td>
                              <td>LISA: Reasoning Segmentation via Large Language Model</td>
                              <td>Xin Lai</td>
                              <td>2023-08-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_00692v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.00692v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01862v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Wider and Deeper LLM Networks are Fairer LLM Evaluators</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01862v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01862v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01862v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Measuring the quality of responses generated by LLMs is a challenging task, particularly when it comes to evaluating whether the response is aligned with human preference. A novel approach involves using the LLM itself to make evaluation and stabilizing the results through multiple independent evaluations, similar to a single-layer narrow LLM network. This network consists of a fixed number of neurons, with each neuron being the same LLM. In this paper, we draw upon the extensive research on deep neural networks to explore whether deeper and wider networks can lead to fairer evaluations. Specifically, inspired by the observation that different neurons in a neural network are responsible for detecting different concepts, we first adaptively generate as many neuron roles as possible for each evaluation sample. Each perspective corresponds to the role of a specific LLM neuron in the first layer. In subsequent layers, we follow the idea that higher layers in deep networks are responsible for more comprehensive features, each layer receives representations from all neurons in the previous layer, integrating the locally learned evaluation information to obtain a more comprehensive evaluation result. Interestingly, this network design resembles the process of academic paper reviewing. To validate the effectiveness of our method, we construct the largest and most diverse English evaluation benchmark LLMEval$^2$ for LLM evaluators, comprising 15 tasks, 8 abilities, and 2,553 samples. Experimental results demonstrate that a wider network (involving many reviewers) with 2 layers (one round of discussion) performs the best, improving kappa correlation coefficient from 0.28 to 0.34. We also leverage WideDeep to aid in the assessment of Chinese LLMs, which has accelerated the evaluation time by 4.6 times, resulting in a 60% cost saving. WideDeep achieves a remarkable 93% agreement level among humans.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01862v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>衡量LLM产生的反应的质量是一项具有挑战性的任务，尤其是在评估反应是否符合人类偏好时。一种新的方法包括使用LLM本身进行评估，并通过多个独立的评估来稳定结果，类似于单层窄LLM网络。该网络由固定数量的神经元组成，每个神经元都是相同的LLM。在本文中，我们利用对深度神经网络的广泛研究来探索更深更宽的网络是否可以带来更公平的评估。具体来说，受神经网络中不同神经元负责检测不同概念的观察结果的启发，我们首先为每个评估样本自适应地生成尽可能多的神经元角色。每个视角对应于第一层中特定LLM神经元的作用。在随后的层中，我们遵循这样的想法，即深度网络中的更高层负责更全面的特征，每一层都接收来自前一层中所有神经元的表示，整合本地学习的评估信息，以获得更全面的评估结果。有趣的是，这种网络设计类似于学术论文审查的过程。为了验证我们方法的有效性，我们为LLM评估者构建了最大、最多样化的英语评估基准LLMEval$^2$，包括15项任务、8项能力和2553个样本。实验结果表明，具有2层（一轮讨论）的更广泛的网络（涉及许多评审员）表现最好，将kappa相关系数从0.28提高到0.34。我们还利用WideDeep来帮助评估中国LLM，这将评估时间缩短了4.6倍，从而节省了60%的成本。WideDeep在人类中达到了93%的一致性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01862v1" target="_blank">2308.01862v1</a>
                              </td>
                              <td>Wider and Deeper LLM Networks are Fairer LLM Evaluators</td>
                              <td>Xinghua Zhang</td>
                              <td>2023-08-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01862v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01862v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01861v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01861v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01861v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01861v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this work, we make the first attempt to evaluate LLMs in a more challenging code generation scenario, i.e. class-level code generation. We first manually construct the first class-level code generation benchmark ClassEval of 100 class-level Python code generation tasks with approximately 500 person-hours. Based on it, we then perform the first study of 11 state-of-the-art LLMs on class-level code generation. Based on our results, we have the following main findings. First, we find that all existing LLMs show much worse performance on class-level code generation compared to on standalone method-level code generation benchmarks like HumanEval; and the method-level coding ability cannot equivalently reflect the class-level coding ability among LLMs. Second, we find that GPT-4 and GPT-3.5 still exhibit dominate superior than other LLMs on class-level code generation, and the second-tier models includes Instruct-Starcoder, Instruct-Codegen, and Wizardcoder with very similar performance. Third, we find that generating the entire class all at once (i.e. holistic generation strategy) is the best generation strategy only for GPT-4 and GPT-3.5, while method-by-method generation (i.e. incremental and compositional) is better strategies for the other models with limited ability of understanding long instructions and utilizing the middle information. Lastly, we find the limited model ability of generating method-dependent code and discuss the frequent error types in generated classes. Our benchmark is available at https://github.com/FudanSELab/ClassEval.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01861v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项工作中，我们首次尝试在更具挑战性的代码生成场景中评估LLM，即类级代码生成。我们首先手动构建了第一个类级代码生成基准ClassEval，该基准包含100个类级Python代码生成任务，约500人时。在此基础上，我们对11种最先进的LLM在类级代码生成方面进行了首次研究。根据我们的研究结果，我们有以下主要发现。首先，我们发现，与HumanEval等独立方法级代码生成基准测试相比，所有现有的LLM在类级代码生成上的性能都要差得多；并且方法级编码能力不能等效地反映LLM之间的类级编码能力。其次，我们发现GPT-4和GPT-3.5在类级代码生成方面仍然表现出优于其他LLM的优势，第二层模型包括性能非常相似的Directive Starcoder、Directive Codegen和Wizardcoder。第三，我们发现，一次生成整个类（即整体生成策略）是仅适用于GPT-4和GPT-3.5的最佳生成策略，而对于理解长指令和利用中间信息能力有限的其他模型，逐方法生成（即增量和组合）是更好的策略。最后，我们发现了生成方法相关代码的有限模型能力，并讨论了生成类中常见的错误类型。我们的基准可在https://github.com/FudanSELab/ClassEval.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01861v1" target="_blank">2308.01861v1</a>
                              </td>
                              <td>ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation</td>
                              <td>Xueying Du</td>
                              <td>2023-08-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01861v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01861v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01846v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">XNLP: An Interactive Demonstration System for Universal Structured NLP</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01846v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01846v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01846v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Structured Natural Language Processing (XNLP) is an important subset of NLP that entails understanding the underlying semantic or syntactic structure of texts, which serves as a foundational component for many downstream applications. Despite certain recent efforts to explore universal solutions for specific categories of XNLP tasks, a comprehensive and effective approach for unifying all XNLP tasks long remains underdeveloped. In the meanwhile, while XNLP demonstration systems are vital for researchers exploring various XNLP tasks, existing platforms can be limited to, e.g., supporting few XNLP tasks, lacking interactivity and universalness. To this end, we propose an advanced XNLP demonstration platform, where we propose leveraging LLM to achieve universal XNLP, with one model for all with high generalizability. Overall, our system advances in multiple aspects, including universal XNLP modeling, high performance, interpretability, scalability, and interactivity, providing a unified platform for exploring diverse XNLP tasks in the community. XNLP is online: https://xnlp.haofei.vip</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01846v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>结构化自然语言处理（XNLP）是NLP的一个重要子集，它需要理解文本的潜在语义或句法结构，这是许多下游应用的基础组件。尽管最近为探索特定类别的XNLP任务的通用解决方案做出了某些努力，但长期以来，统一所有XNLP工作的全面有效的方法仍然不成熟。与此同时，尽管XNLP演示系统对于研究人员探索各种XNLP任务至关重要，但现有平台可能仅限于支持少数XNLP工作，缺乏交互性和通用性。为此，我们提出了一个先进的XNLP演示平台，在该平台上，我们建议利用LLM实现通用XNLP，其中一个模型适用于所有模型，具有高度的可推广性。总体而言，我们的系统在多个方面取得了进步，包括通用的XNLP建模、高性能、可解释性、可扩展性和交互性，为探索社区中的各种XNLP任务提供了一个统一的平台。XNLP已联机：https://xnlp.haofei.vip</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01846v1" target="_blank">2308.01846v1</a>
                              </td>
                              <td>XNLP: An Interactive Demonstration System for Universal Structured NLP</td>
                              <td>Hao Fei</td>
                              <td>2023-08-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01846v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01846v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01834v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Capability of Large Language Models to Measure Psychiatric Functioning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01834v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01834v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01834v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The current work investigates the capability of Large language models (LLMs) that are explicitly trained on large corpuses of medical knowledge (Med-PaLM 2) to predict psychiatric functioning from patient interviews and clinical descriptions without being trained to do so. To assess this, n = 145 depression and n =115 PTSD assessments and n = 46 clinical case studies across high prevalence/high comorbidity disorders (Depressive, Anxiety, Psychotic, trauma and stress, Addictive disorders) were analyzed using prompts to extract estimated clinical scores and diagnoses. Results demonstrate that Med-PaLM 2 is capable of assessing psychiatric functioning across a range of psychiatric conditions with the strongest performance being the prediction of depression scores based on standardized assessments (Accuracy range= 0.80 - 0.84) which were statistically indistinguishable from human clinical raters t(1,144) = 1.20; p = 0.23. Results show the potential for general clinical language models to flexibly predict psychiatric risk based on free descriptions of functioning from both patients and clinicians.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01834v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>目前的工作调查了在大型医学知识库（Med PaLM 2）上明确训练的大型语言模型（LLM）在没有经过训练的情况下，通过患者访谈和临床描述预测精神功能的能力，使用提示提取估计的临床评分和诊断，分析了145例抑郁症和115例创伤后应激障碍评估，以及46例高患病率/高共病性疾病（抑郁、焦虑、精神病、创伤和压力、成瘾性疾病）的临床病例研究。结果表明，Med PaLM 2能够评估一系列精神疾病的精神功能，最强大的表现是基于标准化评估的抑郁评分预测（准确度范围=0.80-0.84），在统计学上与人类临床评分t（1144）=1.20无法区分；p＝0.23。结果显示，通用临床语言模型有潜力根据患者和临床医生对功能的免费描述灵活预测精神风险。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01834v1" target="_blank">2308.01834v1</a>
                              </td>
                              <td>The Capability of Large Language Models to Measure Psychiatric Functioning</td>
                              <td>Isaac R. Galatzer-Levy</td>
                              <td>2023-08-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01834v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01834v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01825v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Scaling Relationship on Learning Mathematical Reasoning with Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01825v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01825v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01825v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Mathematical reasoning is a challenging task for large language models (LLMs), while the scaling relationship of it with respect to LLM capacity is under-explored. In this paper, we investigate how the pre-training loss, supervised data amount, and augmented data amount influence the reasoning performances of a supervised LLM. We find that pre-training loss is a better indicator of the model's performance than the model's parameter count. We apply supervised fine-tuning (SFT) with different amounts of supervised data and empirically find a log-linear relation between data amount and model performance, and we find better models improve less with enlarged supervised datasets. To augment more data samples for improving model performances without any human effort, we propose to apply Rejection sampling Fine-Tuning (RFT). RFT uses supervised models to generate and collect correct reasoning paths as augmented fine-tuning datasets. We find with augmented samples containing more distinct reasoning paths, RFT improves mathematical reasoning performance more for LLMs. We also find RFT brings more improvement for less performant LLMs. Furthermore, we combine rejection samples from multiple models which push LLaMA-7B to an accuracy of 49.3% and outperforms the supervised fine-tuning (SFT) accuracy of 35.9% significantly.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01825v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对于大型语言模型（LLM）来说，数学推理是一项具有挑战性的任务，而它与LLM容量的比例关系还没有得到充分的探索。在本文中，我们研究了预训练损失、监督数据量和增强数据量如何影响监督LLM的推理性能。我们发现，与模型的参数计数相比，预训练损失是更好的模型性能指标。我们将监督微调（SFT）应用于不同数量的监督数据，并根据经验发现数据量与模型性能之间存在对数线性关系，我们发现，随着监督数据集的扩大，更好的模型改进更少。为了在不需要任何人力的情况下增加更多的数据样本以提高模型性能，我们建议应用拒绝采样微调（RFT）。RFT使用监督模型来生成和收集正确的推理路径作为增强的微调数据集。我们发现，当增广样本包含更多不同的推理路径时，RFT可以更好地提高LLM的数学推理性能。我们还发现，RFT为性能较差的LLM带来了更多的改进。此外，我们结合了来自多个模型的拒绝样本，将LLaMA-7B的准确率推高至49.3%，并显著优于35.9%的监督微调（SFT）准确率。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01825v1" target="_blank">2308.01825v1</a>
                              </td>
                              <td>Scaling Relationship on Learning Mathematical Reasoning with Large Language Models</td>
                              <td>Zheng Yuan</td>
                              <td>2023-08-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01825v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01825v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_06556v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Are LLMs All You Need for Task-Oriented Dialogue?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_06556v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_06556v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_06556v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Instructions-tuned Large Language Models (LLMs) gained recently huge popularity thanks to their ability to interact with users through conversation. In this work we aim to evaluate their ability to complete multi-turn tasks and interact with external databases in the context of established task-oriented dialogue benchmarks. We show that for explicit belief state tracking, LLMs underperform compared to specialized task-specific models. Nevertheless, they show ability to guide the dialogue to successful ending if given correct slot values. Furthermore this ability improves with access to true belief state distribution or in-domain examples.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_06556v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>指令调优的大型语言模型（LLM）由于能够通过对话与用户交互，最近大受欢迎。在这项工作中，我们旨在评估他们在既定的面向任务的对话基准的背景下完成多回合任务并与外部数据库互动的能力。我们发现，对于明确的信念状态跟踪，LLM与专门的任务特定模型相比表现不佳。尽管如此，如果给出正确的槽值，它们显示出引导对话成功结束的能力。此外，这种能力随着对真实信念状态分布或领域内示例的访问而提高。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.06556v2" target="_blank">2304.06556v2</a>
                              </td>
                              <td>Are LLMs All You Need for Task-Oriented Dialogue?</td>
                              <td>Vojtěch Hudeček</td>
                              <td>2023-04-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_06556v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.06556v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_06548v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Inductive reasoning in humans and large language models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_06548v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_06548v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_06548v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The impressive recent performance of large language models has led many to wonder to what extent they can serve as models of general intelligence or are similar to human cognition. We address this issue by applying GPT-3.5 and GPT-4 to a classic problem in human inductive reasoning known as property induction. Over two experiments, we elicit human judgments on a range of property induction tasks spanning multiple domains. Although GPT-3.5 struggles to capture many aspects of human behaviour, GPT-4 is much more successful: for the most part, its performance qualitatively matches that of humans, and the only notable exception is its failure to capture the phenomenon of premise non-monotonicity. Our work demonstrates that property induction allows for interesting comparisons between human and machine intelligence and provides two large datasets that can serve as benchmarks for future work in this vein.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_06548v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型最近令人印象深刻的表现让许多人想知道，它们在多大程度上可以作为一般智力的模型，或者与人类认知相似。我们通过将GPT-3.5和GPT-4应用于人类归纳推理中的一个经典问题，即属性归纳来解决这个问题。通过两个实验，我们得出了人类对一系列跨越多个领域的属性归纳任务的判断。尽管GPT-3.5很难捕捉到人类行为的许多方面，但GPT-4要成功得多：在大多数情况下，它的性能在质量上与人类相匹配，唯一值得注意的例外是它未能捕捉到前提非单调性现象。我们的工作表明，属性归纳法可以在人类和机器智能之间进行有趣的比较，并提供了两个大型数据集，可以作为未来这方面工作的基准。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.06548v2" target="_blank">2306.06548v2</a>
                              </td>
                              <td>Inductive reasoning in humans and large language models</td>
                              <td>Simon J. Han</td>
                              <td>2023-06-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_06548v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.06548v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_02897v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">An automatically discovered chain-of-thought prompt generalizes to novel models and datasets</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_02897v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_02897v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_02897v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Emergent chain-of-thought (CoT) reasoning capabilities promise to improve performance and explainability of large language models (LLMs). However, uncertainties remain about how reasoning strategies formulated for previous model generations generalize to new model generations and different datasets. In this small-scale study, we compare different reasoning strategies induced by zero-shot prompting across six recently released LLMs (davinci-002, davinci-003, GPT-3.5-turbo, GPT-4, Flan-T5-xxl and Cohere command-xlarge) on a mixture of six question-answering datasets, including datasets from scientific and medical domains. Our findings demonstrate that while some variations in effectiveness occur, gains from CoT reasoning strategies remain robust across different models and datasets. GPT-4 has the most benefit from current state-of-the-art reasoning strategies and exhibits the best performance by applying a prompt previously discovered through automated discovery.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_02897v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>突发思维链（CoT）推理能力有望提高大型语言模型（LLM）的性能和可解释性。然而，对于为前几代模型制定的推理策略如何推广到新的模型代和不同的数据集，仍然存在不确定性。在这项小规模研究中，我们在六个问答数据集（包括科学和医学领域的数据集）的混合数据集上，比较了最近发布的六个LLM（davinci-002、davinci-003、GPT-3.5-turbo、GPT-4、Flan-T5-xml和Cohere command-xlarg）中零样本提示引发的不同推理策略。我们的研究结果表明，虽然有效性发生了一些变化，但CoT推理策略的收益在不同的模型和数据集中仍然稳健。GPT-4从当前最先进的推理策略中获益最大，并通过应用先前通过自动发现发现的提示显示出最佳性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.02897v2" target="_blank">2305.02897v2</a>
                              </td>
                              <td>An automatically discovered chain-of-thought prompt generalizes to novel models and datasets</td>
                              <td>Konstantin Hebenstreit</td>
                              <td>2023-05-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_02897v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.02897v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01776v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Does Correction Remain An Problem For Large Language Models?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01776v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01776v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01776v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>As large language models, such as GPT, continue to advance the capabilities of natural language processing (NLP), the question arises: does the problem of correction still persist? This paper investigates the role of correction in the context of large language models by conducting two experiments. The first experiment focuses on correction as a standalone task, employing few-shot learning techniques with GPT-like models for error correction. The second experiment explores the notion of correction as a preparatory task for other NLP tasks, examining whether large language models can tolerate and perform adequately on texts containing certain levels of noise or errors. By addressing these experiments, we aim to shed light on the significance of correction in the era of large language models and its implications for various NLP applications.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01776v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着GPT等大型语言模型不断提高自然语言处理（NLP）的能力，问题来了：纠正问题仍然存在吗？本文通过两个实验研究了在大型语言模型的上下文中纠正的作用。第一个实验侧重于将校正作为一项独立任务，使用具有类似GPT模型的少量镜头学习技术进行纠错。第二个实验探讨了校正的概念，将其作为其他NLP任务的准备任务，检验大型语言模型是否能够容忍并在包含一定程度的噪声或错误的文本上充分执行。通过处理这些实验，我们旨在阐明在大型语言模型时代校正的重要性及其对各种NLP应用的影响。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01776v1" target="_blank">2308.01776v1</a>
                              </td>
                              <td>Does Correction Remain An Problem For Large Language Models?</td>
                              <td>Xiaowu Zhang</td>
                              <td>2023-08-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01776v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01776v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_06689v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Self-planning Code Generation with Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_06689v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_06689v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_06689v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Although large language models have demonstrated impressive ability in code generation, they are still struggling to address the complicated intent provided by humans. It is widely acknowledged that humans typically employ planning to decompose complex problems and schedule the solution steps prior to implementation. Thus we introduce planning into code generation to help the model understand complex intent and reduce the difficulty of problem solving. This paper proposes a self-planning code generation method with large language model, which consists of two phases, namely planning phase and implementation phase. Specifically, in the planning phase, the language model plans out the solution steps from the intent combined with in-context learning. Then it enters the implementation phase, where the model generates code step by step, guided by the solution steps. The effectiveness of self-planning code generation has been rigorously evaluated on multiple code generation datasets and the results have demonstrated a marked superiority over naive direct generation approaches with language model. The improvement in performance is substantial, highlighting the significance of self-planning in code generation tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_06689v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管大型语言模型在代码生成方面表现出了令人印象深刻的能力，但它们仍在努力解决人类提供的复杂意图。人们普遍认为，人类通常采用计划来分解复杂的问题，并在实施之前安排解决步骤。因此，我们在代码生成中引入了规划，以帮助模型理解复杂的意图并降低问题解决的难度。本文提出了一种具有大型语言模型的自规划代码生成方法，该方法由两个阶段组成，即规划阶段和实现阶段。具体来说，在规划阶段，语言模型从意图出发，结合上下文学习，规划出解决方案的步骤。然后进入实现阶段，模型在解决方案步骤的指导下逐步生成代码。在多个代码生成数据集上对自规划代码生成的有效性进行了严格评估，结果表明，与使用语言模型的天真直接生成方法相比，自规划代码产生的有效性具有显著优势。性能的提高是实质性的，突出了代码生成任务中自我规划的重要性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.06689v2" target="_blank">2303.06689v2</a>
                              </td>
                              <td>Self-planning Code Generation with Large Language Models</td>
                              <td>Xue Jiang</td>
                              <td>2023-03-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_06689v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.06689v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01741v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Supply chain emission estimation using large language models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01741v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01741v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01741v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large enterprises face a crucial imperative to achieve the Sustainable Development Goals (SDGs), especially goal 13, which focuses on combating climate change and its impacts. To mitigate the effects of climate change, reducing enterprise Scope 3 (supply chain emissions) is vital, as it accounts for more than 90\% of total emission inventories. However, tracking Scope 3 emissions proves challenging, as data must be collected from thousands of upstream and downstream suppliers.To address the above mentioned challenges, we propose a first-of-a-kind framework that uses domain-adapted NLP foundation models to estimate Scope 3 emissions, by utilizing financial transactions as a proxy for purchased goods and services. We compared the performance of the proposed framework with the state-of-art text classification models such as TF-IDF, word2Vec, and Zero shot learning. Our results show that the domain-adapted foundation model outperforms state-of-the-art text mining techniques and performs as well as a subject matter expert (SME). The proposed framework could accelerate the Scope 3 estimation at Enterprise scale and will help to take appropriate climate actions to achieve SDG 13.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01741v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型企业迫切需要实现可持续发展目标，特别是目标13，该目标侧重于应对气候变化及其影响。为了减轻气候变化的影响，减少企业范围3（供应链排放）至关重要，因为它占总排放清单的90%以上。然而，跟踪范围3的排放量具有挑战性，因为必须从数千个上游和下游供应商那里收集数据。为了应对上述挑战，我们提出了一个一流的框架，该框架使用适用于领域的NLP基础模型来估计范围3的排放量，方法是利用金融交易作为购买商品和服务的代理。我们将所提出的框架的性能与最先进的文本分类模型（如TF-IDF、word2Vec和零镜头学习）进行了比较。我们的结果表明，适用于领域的基础模型优于最先进的文本挖掘技术，表现与主题专家（SME）一样好。拟议的框架可以加快企业规模的范围3估计，并将有助于采取适当的气候行动来实现可持续发展目标13。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01741v1" target="_blank">2308.01741v1</a>
                              </td>
                              <td>Supply chain emission estimation using large language models</td>
                              <td>Ayush Jain</td>
                              <td>2023-08-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01741v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01741v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01734v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Ambient Adventures: Teaching ChatGPT on Developing Complex Stories</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01734v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01734v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01734v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Imaginative play is an area of creativity that could allow robots to engage with the world around them in a much more personified way. Imaginary play can be seen as taking real objects and locations and using them as imaginary objects and locations in virtual scenarios. We adopted the story generation capability of large language models (LLMs) to obtain the stories used for imaginary play with human-written prompts. Those generated stories will be simplified and mapped into action sequences that can guide the agent in imaginary play. To evaluate whether the agent can successfully finish the imaginary play, we also designed a text adventure game to simulate a house as the playground for the agent to interact.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01734v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>想象游戏是一个创造性的领域，它可以让机器人以一种更加拟人化的方式与周围的世界互动。想象游戏可以被视为在虚拟场景中获取真实的物体和位置，并将其用作想象的物体和地点。我们采用了大型语言模型（LLM）的故事生成能力，以获得用于具有人类书面提示的想象游戏的故事。这些生成的故事将被简化并映射到动作序列中，这些动作序列可以指导代理人进行想象中的游戏。为了评估代理人是否能成功完成想象中的游戏，我们还设计了一款文本冒险游戏，模拟一所房子作为代理人互动的游乐场。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01734v1" target="_blank">2308.01734v1</a>
                              </td>
                              <td>Ambient Adventures: Teaching ChatGPT on Developing Complex Stories</td>
                              <td>Zexin Chen</td>
                              <td>2023-08-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01734v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01734v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01727v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Local Large Language Models for Complex Structured Medical Tasks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01727v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01727v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01727v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper introduces an approach that combines the language reasoning capabilities of large language models (LLMs) with the benefits of local training to tackle complex, domain-specific tasks. Specifically, the authors demonstrate their approach by extracting structured condition codes from pathology reports. The proposed approach utilizes local LLMs, which can be fine-tuned to respond to specific generative instructions and provide structured outputs. The authors collected a dataset of over 150k uncurated surgical pathology reports, containing gross descriptions, final diagnoses, and condition codes. They trained different model architectures, including LLaMA, BERT and LongFormer and evaluated their performance. The results show that the LLaMA-based models significantly outperform BERT-style models across all evaluated metrics, even with extremely reduced precision. The LLaMA models performed especially well with large datasets, demonstrating their ability to handle complex, multi-label tasks. Overall, this work presents an effective approach for utilizing LLMs to perform domain-specific tasks using accessible hardware, with potential applications in the medical domain, where complex data extraction and classification are required.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01727v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了一种方法，该方法将大型语言模型（LLM）的语言推理能力与本地训练的优势相结合，以处理复杂的、特定领域的任务。具体来说，作者通过从病理报告中提取结构化的条件代码来展示他们的方法。所提出的方法利用了局部LLM，可以对其进行微调以响应特定的生成指令并提供结构化输出。作者收集了超过15万份未分级手术病理报告的数据集，其中包括总体描述、最终诊断和状况代码。他们训练了不同的模型体系结构，包括LLaMA、BERT和LongFormer，并评估了它们的性能。结果表明，在所有评估指标中，基于LLaMA的模型显著优于BERT风格的模型，即使精度极低。LLaMA模型在大型数据集中表现特别好，证明了它们处理复杂、多标签任务的能力。总的来说，这项工作为利用LLM使用可访问的硬件执行特定领域的任务提供了一种有效的方法，在需要复杂数据提取和分类的医疗领域具有潜在的应用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01727v1" target="_blank">2308.01727v1</a>
                              </td>
                              <td>Local Large Language Models for Complex Structured Medical Tasks</td>
                              <td>V. K. Cody Bumgardner</td>
                              <td>2023-08-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01727v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01727v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01684v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Baby's CoThought: Leveraging Large Language Models for Enhanced Reasoning in Compact Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01684v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01684v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01684v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) demonstrate remarkable performance on a variety of Natural Language Understanding (NLU) tasks, primarily due to their in-context learning ability. This ability is utilized in our proposed "CoThought" pipeline, which efficiently trains smaller "baby" language models (BabyLMs) by leveraging the Chain of Thought (CoT) prompting of LLMs. Our pipeline restructures a dataset of less than 100M in size using GPT-3.5-turbo, transforming it into task-oriented, human-readable texts that are comparable to the school texts for language learners. The BabyLM is then pretrained on this restructured dataset in a RoBERTa (Liu et al., 2019) fashion. In evaluations across 4 benchmarks, our BabyLM outperforms the RoBERTa-base in 10 linguistic, NLU, and question answering tasks by more than 3 points, showing superior ability to extract contextual information. These results suggest that compact LMs pretrained on small, LLM-restructured data can better understand tasks and achieve improved performance. The code for data processing and model training is available at: https://github.com/oooranz/Baby-CoThought.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01684v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）在各种自然语言理解（NLU）任务中表现出非凡的性能，这主要归功于它们的上下文学习能力。这种能力在我们提出的“CoThought”管道中得到了利用，该管道通过利用LLM的思想链（CoT）提示来有效地训练较小的“婴儿”语言模型（BabyLMs）。我们的管道使用GPT-3.5-turbo重构了一个大小小于100M的数据集，将其转换为面向任务的、人类可读的文本，与语言学习者的学校文本相当。然后，以RoBERTa（Liu et al.，2019）的方式在该重组数据集上对BabyLM进行预训练。在4个基准的评估中，我们的BabyLM在10项语言、NLU和问答任务中比RoBERTa基础高出3分以上，显示出提取上下文信息的卓越能力。这些结果表明，在LLM重构的小数据上预训练的紧凑LMs可以更好地理解任务并提高性能。数据处理和模型训练的代码可在以下位置获得：https://github.com/oooranz/Baby-CoThought.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01684v1" target="_blank">2308.01684v1</a>
                              </td>
                              <td>Baby's CoThought: Leveraging Large Language Models for Enhanced Reasoning in Compact Models</td>
                              <td>Zheyu Zhang</td>
                              <td>2023-08-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01684v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01684v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01666v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Evaluating ChatGPT text-mining of clinical records for obesity monitoring</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01666v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01666v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01666v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Background: Veterinary clinical narratives remain a largely untapped resource for addressing complex diseases. Here we compare the ability of a large language model (ChatGPT) and a previously developed regular expression (RegexT) to identify overweight body condition scores (BCS) in veterinary narratives. Methods: BCS values were extracted from 4,415 anonymised clinical narratives using either RegexT or by appending the narrative to a prompt sent to ChatGPT coercing the model to return the BCS information. Data were manually reviewed for comparison. Results: The precision of RegexT was higher (100%, 95% CI 94.81-100%) than the ChatGPT (89.3%; 95% CI82.75-93.64%). However, the recall of ChatGPT (100%. 95% CI 96.18-100%) was considerably higher than that of RegexT (72.6%, 95% CI 63.92-79.94%). Limitations: Subtle prompt engineering is needed to improve ChatGPT output. Conclusions: Large language models create diverse opportunities and, whilst complex, present an intuitive interface to information but require careful implementation to avoid unpredictable errors.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01666v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>背景：兽医临床叙述在很大程度上仍然是解决复杂疾病的未开发资源。在这里，我们比较了大型语言模型（ChatGPT）和先前开发的正则表达式（RegexT）在兽医叙述中识别超重身体状况评分（BCS）的能力。方法：使用RegexT或将叙述附加到发送给ChatGPT的提示中，强制模型返回BCS信息，从4415个匿名临床叙述中提取BCS值。手动审查数据以进行比较。结果：RegexT的准确度（100%，95%CI 94.81-100%）高于ChatGPT（89.3%；95%CI 82.75-93.64%）。然而，ChatGPT的召回率（100%.95%CI 96.18-100%）显著高于RegexT（72.6%，95%CI 63.92-79.94%）。局限性：需要精细的即时工程来提高ChatGPT输出。结论：大型语言模型创造了各种各样的机会，虽然很复杂，但它提供了一个直观的信息界面，但需要仔细实施，以避免不可预测的错误。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01666v1" target="_blank">2308.01666v1</a>
                              </td>
                              <td>Evaluating ChatGPT text-mining of clinical records for obesity monitoring</td>
                              <td>Ivo S. Fins</td>
                              <td>2023-08-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01666v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01666v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01589v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Holy Grail 2.0: From Natural Language to Constraint Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01589v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01589v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01589v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Twenty-seven years ago, E. Freuder highlighted that "Constraint programming represents one of the closest approaches computer science has yet made to the Holy Grail of programming: the user states the problem, the computer solves it". Nowadays, CP users have great modeling tools available (like Minizinc and CPMpy), allowing them to formulate the problem and then let a solver do the rest of the job, getting closer to the stated goal. However, this still requires the CP user to know the formalism and respect it. Another significant challenge lies in the expertise required to effectively model combinatorial problems. All this limits the wider adoption of CP. In this position paper, we investigate a possible approach to leverage pre-trained Large Language Models to extract models from textual problem descriptions. More specifically, we take inspiration from the Natural Language Processing for Optimization (NL4OPT) challenge and present early results with a decomposition-based prompting approach to GPT Models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01589v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>27年前，E.Freuder强调，“约束编程是计算机科学迄今为止最接近编程圣杯的方法之一：用户陈述问题，计算机解决问题”。如今，CP用户有很好的建模工具（如Minizinc和CPMpy），可以让他们制定问题，然后让求解器完成剩下的工作，离既定目标越来越近。然而，这仍然需要CP用户了解并尊重形式主义。另一个重大挑战在于有效建模组合问题所需的专业知识。所有这些都限制了CP的广泛采用。在这篇立场论文中，我们研究了一种可能的方法，利用预先训练的大型语言模型从文本问题描述中提取模型。更具体地说，我们从优化的自然语言处理（NL4OPT）挑战中获得了灵感，并用基于分解的GPT模型提示方法给出了早期结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01589v1" target="_blank">2308.01589v1</a>
                              </td>
                              <td>Holy Grail 2.0: From Natural Language to Constraint Models</td>
                              <td>Dimos Tsouros</td>
                              <td>2023-08-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01589v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01589v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01555v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Mani-GPT: A Generative Model for Interactive Robotic Manipulation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01555v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01555v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01555v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In real-world scenarios, human dialogues are multi-round and diverse. Furthermore, human instructions can be unclear and human responses are unrestricted. Interactive robots face difficulties in understanding human intents and generating suitable strategies for assisting individuals through manipulation. In this article, we propose Mani-GPT, a Generative Pre-trained Transformer (GPT) for interactive robotic manipulation. The proposed model has the ability to understand the environment through object information, understand human intent through dialogues, generate natural language responses to human input, and generate appropriate manipulation plans to assist the human. This makes the human-robot interaction more natural and humanized. In our experiment, Mani-GPT outperforms existing algorithms with an accuracy of 84.6% in intent recognition and decision-making for actions. Furthermore, it demonstrates satisfying performance in real-world dialogue tests with users, achieving an average response accuracy of 70%.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01555v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在现实世界的场景中，人类对话是多方面和多样化的。此外，人类的指令可能不明确，人类的反应也不受限制。交互式机器人在理解人类意图和生成合适的策略以通过操纵来帮助个体方面面临困难。在本文中，我们提出了Mani-GPT，一种用于交互式机器人操作的生成预训练转换器（GPT）。所提出的模型能够通过物体信息理解环境，通过对话理解人类意图，对人类输入产生自然语言反应，并生成适当的操作计划来帮助人类。这使得人机交互更加自然和人性化。在我们的实验中，Mani-GPT在意图识别和行动决策方面优于现有算法，准确率为84.6%。此外，它在与用户的真实世界对话测试中表现出令人满意的性能，实现了70%的平均响应准确率。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01555v1" target="_blank">2308.01555v1</a>
                              </td>
                              <td>Mani-GPT: A Generative Model for Interactive Robotic Manipulation</td>
                              <td>Zhe Zhang</td>
                              <td>2023-08-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01555v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01555v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01191v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Understanding the Capability of Large Language Models on Code Clone Detection: A Survey</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01191v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01191v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01191v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Code cloning, the duplication of code fragments, is common in software development. While some reuse aids productivity, excessive cloning hurts maintainability and introduces bugs. Hence, automatic code clone detection is vital. Meanwhile, large language models (LLMs) possess diverse code-related knowledge, making them versatile for various software engineering challenges. However, LLMs' performance in code clone detection is unclear and needs more study for accurate assessment. In this paper, we provide the first comprehensive evaluation of LLMs for clone detection, covering different clone types, languages, and prompts. We find advanced LLMs excel in detecting complex semantic clones, surpassing existing methods. Adding intermediate reasoning steps via chain-of-thought prompts noticeably enhances performance. Additionally, representing code as vector embeddings, especially with text encoders, effectively aids clone detection.Lastly, the ability of LLMs to detect code clones differs among various programming languages. Our study suggests that LLMs have potential for clone detection due to their language capabilities, offering insights for developing robust LLM-based methods to enhance software engineering.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01191v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>代码克隆，即代码片段的复制，在软件开发中很常见。虽然有些重用有助于提高生产效率，但过多的克隆会损害可维护性并引入错误。因此，自动代码克隆检测至关重要。同时，大型语言模型（LLM）拥有多种与代码相关的知识，使其能够应对各种软件工程挑战。然而，LLM在代码克隆检测方面的性能尚不清楚，需要更多的研究来进行准确的评估。在本文中，我们首次对用于克隆检测的LLM进行了全面评估，涵盖了不同的克隆类型、语言和提示。我们发现高级LLM在检测复杂语义克隆方面表现出色，超过了现有的方法。通过思维链提示添加中间推理步骤显著提高了性能。此外，将代码表示为矢量嵌入，特别是使用文本编码器，可以有效地帮助克隆检测。最后，LLM检测代码克隆的能力在不同的编程语言中有所不同。我们的研究表明，LLM由于其语言能力而具有克隆检测的潜力，为开发基于LLM的强大方法以增强软件工程提供了见解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01191v2" target="_blank">2308.01191v2</a>
                              </td>
                              <td>Towards Understanding the Capability of Large Language Models on Code Clone Detection: A Survey</td>
                              <td>Shihan Dou</td>
                              <td>2023-08-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01191v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01191v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01542v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Memory Sandbox: Transparent and Interactive Memory Management for Conversational Agents</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01542v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01542v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01542v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The recent advent of large language models (LLM) has resulted in high-performing conversational agents such as chatGPT. These agents must remember key information from an ongoing conversation to provide responses that are contextually relevant to the user. However, these agents have limited memory and can be distracted by irrelevant parts of the conversation. While many strategies exist to manage conversational memory, users currently lack affordances for viewing and controlling what the agent remembers, resulting in a poor mental model and conversational breakdowns. In this paper, we present Memory Sandbox, an interactive system and design probe that allows users to manage the conversational memory of LLM-powered agents. By treating memories as data objects that can be viewed, manipulated, recorded, summarized, and shared across conversations, Memory Sandbox provides interaction affordances for users to manage how the agent should `see' the conversation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01542v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近出现的大型语言模型（LLM）产生了高性能的会话代理，如chatGPT。这些代理必须记住正在进行的对话中的关键信息，以提供与用户上下文相关的响应。然而，这些代理人的记忆力有限，可能会被谈话中不相关的部分分散注意力。虽然存在许多管理会话记忆的策略，但用户目前缺乏查看和控制代理记忆的可供性，导致心理模型不佳和会话崩溃。在本文中，我们介绍了Memory Sandbox，这是一个交互式系统和设计探针，允许用户管理LLM代理的会话内存。通过将内存视为可以在对话中查看、操作、记录、总结和共享的数据对象，Memory Sandbox为用户提供了交互启示，以管理代理应该如何“查看”对话。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01542v1" target="_blank">2308.01542v1</a>
                              </td>
                              <td>Memory Sandbox: Transparent and Interactive Memory Management for Conversational Agents</td>
                              <td>Ziheng Huang</td>
                              <td>2023-08-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01542v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01542v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_16680v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_16680v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_16680v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_16680v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Diffusion models and large language models have emerged as leading-edge generative models and have sparked a revolutionary impact on various aspects of human life. However, the practical implementation of these models has also exposed inherent risks, highlighting their dual nature and raising concerns regarding their trustworthiness. Despite the abundance of literature on this subject, a comprehensive survey specifically delving into the intersection of large-scale generative models and their trustworthiness remains largely absent. To bridge this gap, This paper investigates both the long-standing and emerging threats associated with these models across four fundamental dimensions: privacy, security, fairness, and responsibility. In this way, we construct an extensive map outlining the trustworthiness of these models, while also providing practical recommendations and identifying future directions. These efforts are crucial for promoting the trustworthy deployment of these models, ultimately benefiting society as a whole.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_16680v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>扩散模型和大语言模型已经成为前沿的生成模型，并对人类生活的各个方面产生了革命性的影响。然而，这些模式的实际实施也暴露出固有的风险，突出了其双重性，并引发了人们对其可信度的担忧。尽管有大量关于这一主题的文献，但一项专门研究大规模生成模型及其可信度的交叉点的全面调查在很大程度上仍然缺乏。为了弥补这一差距，本文从隐私、安全、公平和责任四个基本维度调查了与这些模型相关的长期和新出现的威胁。通过这种方式，我们构建了一个广泛的地图，概述了这些模型的可信度，同时也提供了实用的建议和确定了未来的方向。这些努力对于促进这些模式的可靠部署至关重要，最终使整个社会受益。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.16680v2" target="_blank">2307.16680v2</a>
                              </td>
                              <td>On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey</td>
                              <td>Mingyuan Fan</td>
                              <td>2023-07-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_16680v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.16680v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01497v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Large Language Model Displays Emergent Ability to Interpret Novel Literary Metaphors</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01497v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01497v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01497v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in the performance of large language models (LLMs) have sparked debate over whether, given sufficient training, high-level human abilities emerge in such generic forms of artificial intelligence (AI). Despite the exceptional performance of LLMs on a wide range of tasks involving natural language processing and reasoning, there has been sharp disagreement as to whether their abilities extend to more creative human abilities. A core example is the ability to interpret novel metaphors. Given the enormous and non-curated text corpora used to train LLMs, a serious obstacle to designing tests is the requirement of finding novel yet high-quality metaphors that are unlikely to have been included in the training data. Here we assessed the ability of GPT-4, a state-of-the-art large language model, to provide natural-language interpretations of novel literary metaphors drawn from Serbian poetry and translated into English. Despite exhibiting no signs of having been exposed to these metaphors previously, the AI system consistently produced detailed and incisive interpretations. Human judge - blind to the fact that an AI model was involved - rated metaphor interpretations generated by GPT-4 as superior to those provided by a group of college students. In interpreting reversed metaphors, GPT-4, as well as humans, exhibited signs of sensitivity to the Gricean cooperative principle. These results indicate that LLMs such as GPT-4 have acquired an emergent ability to interpret complex novel metaphors.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01497v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）性能的最新进展引发了人们的争论，即如果经过足够的训练，高水平的人类能力是否会以这种通用形式的人工智能（AI）出现。尽管LLM在涉及自然语言处理和推理的广泛任务中表现出色，但对于它们的能力是否延伸到更具创造性的人类能力，一直存在着尖锐的分歧。一个核心的例子是解读小说隐喻的能力。考虑到用于训练LLM的庞大且非策划的文本语料库，设计测试的一个严重障碍是需要找到新颖但高质量的隐喻，而这些隐喻不太可能包含在训练数据中。在这里，我们评估了GPT-4（一种最先进的大型语言模型）对从塞尔维亚诗歌中提取并翻译成英语的小说文学隐喻进行自然语言解释的能力。尽管之前没有表现出接触过这些隐喻的迹象，但人工智能系统始终产生了详细而深刻的解释。人类法官无视人工智能模型的参与，认为GPT-4产生的隐喻解释优于一群大学生提供的隐喻解释。在解释反向隐喻时，GPT-4和人类一样，表现出对Gricean合作原则的敏感性。这些结果表明，像GPT-4这样的LLM已经获得了解释复杂新颖隐喻的新兴能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01497v1" target="_blank">2308.01497v1</a>
                              </td>
                              <td>Large Language Model Displays Emergent Ability to Interpret Novel Literary Metaphors</td>
                              <td>Nicholas Ichien</td>
                              <td>2023-08-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01497v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01497v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_01358v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Challenging the appearance of machine intelligence: Cognitive bias in LLMs and Best Practices for Adoption</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_01358v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_01358v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_01358v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Assessments of algorithmic bias in large language models (LLMs) are generally catered to uncovering systemic discrimination based on protected characteristics such as sex and ethnicity. However, there are over 180 documented cognitive biases that pervade human reasoning and decision making that are routinely ignored when discussing the ethical complexities of AI. We demonstrate the presence of these cognitive biases in LLMs and discuss the implications of using biased reasoning under the guise of expertise. We call for stronger education, risk management, and continued research as widespread adoption of this technology increases. Finally, we close with a set of best practices for when and how to employ this technology as widespread adoption continues to grow.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_01358v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对大型语言模型（LLM）中算法偏见的评估通常是为了揭示基于性别和种族等受保护特征的系统性歧视。然而，在讨论人工智能的伦理复杂性时，人类推理和决策中普遍存在180多种公认的认知偏见，而这些偏见通常被忽视。我们证明了LLM中存在这些认知偏见，并讨论了以专业知识为幌子使用有偏见推理的含义。随着这项技术的广泛采用，我们呼吁加强教育、风险管理和持续研究。最后，随着这项技术的广泛采用，我们将以一系列最佳实践来结束这项技术何时以及如何使用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.01358v2" target="_blank">2304.01358v2</a>
                              </td>
                              <td>Challenging the appearance of machine intelligence: Cognitive bias in LLMs and Best Practices for Adoption</td>
                              <td>Alaina N. Talboy</td>
                              <td>2023-04-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_01358v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.01358v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01408v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">UPB at IberLEF-2023 AuTexTification: Detection of Machine-Generated Text using Transformer Ensembles</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01408v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01408v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01408v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper describes the solutions submitted by the UPB team to the AuTexTification shared task, featured as part of IberLEF-2023. Our team participated in the first subtask, identifying text documents produced by large language models instead of humans. The organizers provided a bilingual dataset for this subtask, comprising English and Spanish texts covering multiple domains, such as legal texts, social media posts, and how-to articles. We experimented mostly with deep learning models based on Transformers, as well as training techniques such as multi-task learning and virtual adversarial training to obtain better results. We submitted three runs, two of which consisted of ensemble models. Our best-performing model achieved macro F1-scores of 66.63% on the English dataset and 67.10% on the Spanish dataset.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01408v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文描述了UPB团队提交给AuTexTification共享任务的解决方案，该任务是IberLEF-2023的一部分。我们的团队参与了第一个子任务，识别由大型语言模型而非人类生成的文本文档。组织者为这一子任务提供了一个双语数据集，包括涵盖多个领域的英语和西班牙语文本，如法律文本、社交媒体帖子和操作文章。我们主要试验了基于Transformers的深度学习模型，以及多任务学习和虚拟对抗性训练等训练技术，以获得更好的结果。我们提交了三次跑步，其中两次由合奏模型组成。我们表现最好的模型在英语数据集和西班牙语数据集上分别获得了66.63%和67.10%的宏观F1分数。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01408v1" target="_blank">2308.01408v1</a>
                              </td>
                              <td>UPB at IberLEF-2023 AuTexTification: Detection of Machine-Generated Text using Transformer Ensembles</td>
                              <td>Andrei-Alexandru Preda</td>
                              <td>2023-08-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01408v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01408v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_04370v5_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">OpenAGI: When LLM Meets Domain Experts</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_04370v5_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_04370v5_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_04370v5_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Human intelligence excels at combining basic skills to solve complex tasks. This capability is vital for Artificial Intelligence (AI) and should be embedded in comprehensive intelligent models, enabling them to harness expert models for complex task-solving towards Artificial General Intelligence (AGI). Large Language Models (LLMs) show promising learning and reasoning abilities, and can effectively use external models, tools or APIs to tackle complex problems. In this work, we introduce OpenAGI, an open-source AGI research platform designed for multi-step, real-world tasks. Specifically, OpenAGI uses a dual strategy, integrating standard benchmark tasks for benchmarking and evaluation, and open-ended tasks including more expandable models, tools or APIs for creative problem-solving. Tasks are presented as natural language queries to the LLM, which then selects and executes appropriate models. We also propose a Reinforcement Learning from Task Feedback (RLTF) mechanism that uses task results to improve the LLM's ability, which creates a self-improving AI feedback loop. While we acknowledge that AGI is a broad and multifaceted research challenge with no singularly defined solution path, the integration of LLMs with domain-specific expert models, inspired by mirroring the blend of general and specialized intelligence in humans, offers a promising approach towards AGI. We are open-sourcing the OpenAGI project's code, dataset, benchmarks, evaluation methods, and demo to foster community involvement in AGI advancement: https://github.com/agiresearch/OpenAGI.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_04370v5_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人类的智慧擅长将基本技能结合起来解决复杂的任务。这种能力对人工智能（AI）至关重要，应该嵌入到综合智能模型中，使他们能够利用专家模型来解决人工通用智能（AGI）的复杂任务。大型语言模型（LLM）具有良好的学习和推理能力，可以有效地使用外部模型、工具或API来解决复杂问题。在这项工作中，我们介绍了OpenAGI，这是一个开源的AGI研究平台，专为多步骤、真实世界的任务设计。具体而言，OpenAGI使用双重策略，集成了用于基准测试和评估的标准基准任务，以及用于创造性解决问题的开放式任务，包括更可扩展的模型、工具或API。任务以自然语言查询的形式呈现给LLM，然后LLM选择并执行适当的模型。我们还提出了一种从任务反馈中强化学习（RLTF）机制，该机制使用任务结果来提高LLM的能力，从而创建了一个自我改进的AI反馈回路。虽然我们承认AGI是一个广泛而多方面的研究挑战，没有单独定义的解决方案路径，但LLM与特定领域专家模型的集成，受到人类通用智能和专业智能混合的启发，为AGI提供了一种很有前途的方法。我们正在开源OpenAGI项目的代码、数据集、基准测试、评估方法和演示，以促进社区参与AGI的发展：https://github.com/agiresearch/OpenAGI.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.04370v5" target="_blank">2304.04370v5</a>
                              </td>
                              <td>OpenAGI: When LLM Meets Domain Experts</td>
                              <td>Yingqiang Ge</td>
                              <td>2023-04-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_04370v5_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.04370v5" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01317v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ELIXR: Towards a general purpose X-ray artificial intelligence system through alignment of large language models and radiology vision encoders</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01317v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01317v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01317v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Our approach, which we call Embeddings for Language/Image-aligned X-Rays, or ELIXR, leverages a language-aligned image encoder combined or grafted onto a fixed LLM, PaLM 2, to perform a broad range of tasks. We train this lightweight adapter architecture using images paired with corresponding free-text radiology reports from the MIMIC-CXR dataset. ELIXR achieved state-of-the-art performance on zero-shot chest X-ray (CXR) classification (mean AUC of 0.850 across 13 findings), data-efficient CXR classification (mean AUCs of 0.893 and 0.898 across five findings (atelectasis, cardiomegaly, consolidation, pleural effusion, and pulmonary edema) for 1% (~2,200 images) and 10% (~22,000 images) training data), and semantic search (0.76 normalized discounted cumulative gain (NDCG) across nineteen queries, including perfect retrieval on twelve of them). Compared to existing data-efficient methods including supervised contrastive learning (SupCon), ELIXR required two orders of magnitude less data to reach similar performance. ELIXR also showed promise on CXR vision-language tasks, demonstrating overall accuracies of 58.7% and 62.5% on visual question answering and report quality assurance tasks, respectively. These results suggest that ELIXR is a robust and versatile approach to CXR AI.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01317v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们的方法，我们称之为语言/图像对齐X射线嵌入，或ELIXR，利用组合或移植到固定LLM PaLM 2上的语言对齐图像编码器来执行广泛的任务。我们使用来自MIMIC-CXR数据集的图像与相应的自由文本放射学报告配对来训练这种轻量级适配器架构。ELIXR在零样本胸部X射线（CXR）分类（13个检查结果的平均AUC为0.850）、数据有效的CXR分类（1%（约2200张图像）和10%（约22000张图像）训练数据的5个检查结果（肺不张、心脏肿大、巩固、胸腔积液和肺水肿）的平均AUCs为0.893和0.898）方面取得了最先进的性能，和语义搜索（19个查询的0.76标准化贴现累积增益（NDCG），包括其中12个查询的完美检索）。与包括监督对比学习（SupCon）在内的现有数据高效方法相比，ELIXR需要少两个数量级的数据才能达到类似的性能。ELIXR在CXR视觉语言任务上也表现出了希望，在视觉问答和报告质量保证任务上的总体准确率分别为58.7%和62.5%。这些结果表明，ELIXR是一种稳健且通用的CXR AI方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01317v1" target="_blank">2308.01317v1</a>
                              </td>
                              <td>ELIXR: Towards a general purpose X-ray artificial intelligence system through alignment of large language models and radiology vision encoders</td>
                              <td>Shawn Xu</td>
                              <td>2023-08-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01317v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01317v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01284v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fighting Fire with Fire: Can ChatGPT Detect AI-generated Text?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01284v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01284v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01284v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) such as ChatGPT are increasingly being used for various use cases, including text content generation at scale. Although detection methods for such AI-generated text exist already, we investigate ChatGPT's performance as a detector on such AI-generated text, inspired by works that use ChatGPT as a data labeler or annotator. We evaluate the zero-shot performance of ChatGPT in the task of human-written vs. AI-generated text detection, and perform experiments on publicly available datasets. We empirically investigate if ChatGPT is symmetrically effective in detecting AI-generated or human-written text. Our findings provide insight on how ChatGPT and similar LLMs may be leveraged in automated detection pipelines by simply focusing on solving a specific aspect of the problem and deriving the rest from that solution. All code and data is available at \url{https://github.com/AmritaBh/ChatGPT-as-Detector}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01284v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>诸如ChatGPT之类的大型语言模型（LLM）正越来越多地用于各种用例，包括大规模的文本内容生成。尽管这种人工智能生成的文本的检测方法已经存在，但我们研究了ChatGPT作为人工智能生成文本检测器的性能，其灵感来自于将ChatGPT用作数据标记器或注释器的工作。我们评估了ChatGPT在人工书写与人工智能生成文本检测任务中的零样本性能，并在公开可用的数据集上进行了实验。我们实证研究了ChatGPT在检测人工智能生成或人类书写的文本方面是否对称有效。我们的发现深入了解了ChatGPT和类似LLM如何在自动化检测管道中发挥作用，只需专注于解决问题的特定方面，并从该解决方案中获得其余部分。所有代码和数据都可以在\url上找到{https://github.com/AmritaBh/ChatGPT-as-Detector}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01284v1" target="_blank">2308.01284v1</a>
                              </td>
                              <td>Fighting Fire with Fire: Can ChatGPT Detect AI-generated Text?</td>
                              <td>Amrita Bhattacharjee</td>
                              <td>2023-08-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01284v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01284v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01264v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exploring the psychology of GPT-4's Moral and Legal Reasoning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01264v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01264v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01264v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models have been used as the foundation of highly sophisticated artificial intelligences, capable of delivering human-like responses to probes about legal and moral issues. However, these models are unreliable guides to their own inner workings, and even the engineering teams behind their creation are unable to explain exactly how they came to develop all of the capabilities they currently have. The emerging field of machine psychology seeks to gain insight into the processes and concepts that these models possess. In this paper, we employ the methods of psychology to probe into GPT-4's moral and legal reasoning. More specifically, we investigate the similarities and differences between GPT-4 and humans when it comes to intentionality ascriptions, judgments about causation, the morality of deception, moral foundations, the impact of moral luck on legal judgments, the concept of consent, and rule violation judgments. We find high correlations between human and AI responses, but also several significant systematic differences between them. We conclude with a discussion of the philosophical implications of our findings.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01264v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型已被用作高度复杂的人工智能的基础，能够对法律和道德问题的调查做出类似人类的反应。然而，这些模型是不可靠的内部工作指南，即使是他们创建的工程团队也无法准确解释他们是如何开发出目前所有的能力的。机器心理学的新兴领域试图深入了解这些模型所具有的过程和概念。本文运用心理学的方法对GPT-4的道德和法律推理进行了探讨。更具体地说，我们调查了GPT-4与人类在意向归属、因果关系判断、欺骗道德、道德基础、道德运气对法律判断的影响、同意概念和违反规则判断方面的异同。我们发现人类和人工智能反应之间存在高度相关性，但它们之间也存在一些显著的系统差异。最后，我们讨论了我们发现的哲学含义。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01264v1" target="_blank">2308.01264v1</a>
                              </td>
                              <td>Exploring the psychology of GPT-4's Moral and Legal Reasoning</td>
                              <td>Guilherme F. C. F. Almeida</td>
                              <td>2023-08-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01264v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01264v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01263v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01263v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01263v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01263v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. This motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. However, there is a tension between these two objectives, since harmlessness requires models to refuse complying with unsafe prompts, and thus not be helpful. Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. In this paper, we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a structured and systematic way. In its current form, XSTest comprises 200 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with. We describe XSTest's creation and composition, and use the test suite to highlight systematic failure modes in a recently-released state-of-the-art language model.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01263v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>如果没有适当的保护措施，大型语言模型将很容易遵循恶意指令并生成有毒内容。这激励了安全工作，如红队和大规模反馈学习，旨在使模型既有用又无害。然而，这两个目标之间存在紧张关系，因为无害化要求模型拒绝遵守不安全的提示，因此没有帮助。最近的轶事证据表明，一些模型可能平衡不佳，因此，如果他们使用与不安全提示类似的语言或提及敏感话题，即使是明显安全的提示也会被拒绝。在本文中，我们引入了一个名为XSTest的新测试套件，以结构化和系统化的方式识别此类异常安全行为。在目前的形式中，XSTest包括10种提示类型的200个安全提示，经过良好校准的模型不应拒绝遵守这些提示。我们描述了XSTest的创建和组成，并使用测试套件在最近发布的最先进的语言模型中突出显示系统故障模式。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01263v1" target="_blank">2308.01263v1</a>
                              </td>
                              <td>XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models</td>
                              <td>Paul Röttger</td>
                              <td>2023-08-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01263v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01263v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01240v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Evaluating Instruction-Tuned Large Language Models on Code Comprehension and Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01240v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01240v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01240v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this work, we evaluate 10 open-source instructed LLMs on four representative code comprehension and generation tasks. We have the following main findings. First, for the zero-shot setting, instructed LLMs are very competitive on code comprehension and generation tasks and sometimes even better than small SOTA models specifically fine-tuned on each downstream task. We also find that larger instructed LLMs are not always better on code-related tasks. Second, for the few-shot setting, we find that adding demonstration examples substantially helps instructed LLMs perform better on most code comprehension and generation tasks; however, the examples would sometimes induce unstable or even worse performance. Furthermore, we find widely-used BM25-based shot selection strategy significantly outperforms the basic random selection or fixed selection only on generation problems. Third, for the fine-tuning setting, we find that fine-tuning could further improve the model performance on downstream code comprehension and generation tasks compared to the zero-shot/one-shot performance. In addition, after being fine-tuned on the same downstream task dataset, instructed LLMs outperform both the small SOTA models and similar-scaled LLMs without instruction tuning. Based on our findings, we further present practical implications on model and usage recommendation, performance and cost trade-offs, and future direction.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01240v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项工作中，我们评估了10个开源指导LLM的四个代表性代码理解和生成任务。我们有以下主要发现。首先，对于零样本设置，指示的LLM在代码理解和生成任务上非常有竞争力，有时甚至比在每个下游任务上专门微调的小型SOTA模型更好。我们还发现，较大的指令LLM在代码相关任务中并不总是更好。其次，对于少数镜头设置，我们发现添加演示示例大大有助于指导LLM在大多数代码理解和生成任务中表现得更好；然而，这些例子有时会导致性能不稳定甚至更差。此外，我们发现广泛使用的基于BM25的镜头选择策略仅在生成问题上显著优于基本随机选择或固定选择。第三，对于微调设置，我们发现与零样本/单次性能相比，微调可以进一步提高下游代码理解和生成任务的模型性能。此外，在对同一下游任务数据集进行微调后，指令LLM在没有指令调优的情况下优于小型SOTA模型和类似规模的LLM。基于我们的发现，我们进一步提出了模型和使用建议、性能和成本权衡以及未来方向的实际意义。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01240v1" target="_blank">2308.01240v1</a>
                              </td>
                              <td>Evaluating Instruction-Tuned Large Language Models on Code Comprehension and Generation</td>
                              <td>Zhiqiang Yuan</td>
                              <td>2023-08-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01240v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01240v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01222v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Calibration in Deep Learning: A Survey of the State-of-the-Art</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01222v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01222v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01222v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Calibrating deep neural models plays an important role in building reliable, robust AI systems in safety-critical applications. Recent work has shown that modern neural networks that possess high predictive capability are poorly calibrated and produce unreliable model predictions. Though deep learning models achieve remarkable performance on various benchmarks, the study of model calibration and reliability is relatively underexplored. Ideal deep models should have not only high predictive performance but also be well calibrated. There have been some recent methods proposed to calibrate deep models by using different mechanisms. In this survey, we review the state-of-the-art calibration methods and provide an understanding of their principles for performing model calibration. First, we start with the definition of model calibration and explain the root causes of model miscalibration. Then we introduce the key metrics that can measure this aspect. It is followed by a summary of calibration methods that we roughly classified into four categories: post-hoc calibration, regularization methods, uncertainty estimation, and composition methods. We also covered some recent advancements in calibrating large models, particularly large language models (LLMs). Finally, we discuss some open issues, challenges, and potential directions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01222v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>校准深度神经模型在安全关键应用中构建可靠、稳健的人工智能系统方面发挥着重要作用。最近的工作表明，具有高预测能力的现代神经网络校准较差，并且产生不可靠的模型预测。尽管深度学习模型在各种基准上取得了显著的性能，但对模型校准和可靠性的研究相对较少。理想的深度模型不仅应该具有高预测性能，而且应该经过良好的校准。最近提出了一些通过使用不同机制来校准深度模型的方法。在这项调查中，我们回顾了最先进的校准方法，并了解了它们执行模型校准的原理。首先，我们从模型校准的定义开始，解释了模型校准错误的根本原因。然后，我们介绍了可以衡量这一方面的关键指标。接下来是校准方法的总结，我们大致将其分为四类：事后校准、正则化方法、不确定性估计和合成方法。我们还介绍了在校准大型模型，特别是大型语言模型（LLM）方面的一些最新进展。最后，我们讨论了一些悬而未决的问题、挑战和潜在的方向。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01222v1" target="_blank">2308.01222v1</a>
                              </td>
                              <td>Calibration in Deep Learning: A Survey of the State-of-the-Art</td>
                              <td>Cheng Wang</td>
                              <td>2023-08-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01222v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01222v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01154v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Arithmetic with Language Models: from Memorization to Computation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01154v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01154v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01154v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A better understanding of the emergent computation and problem-solving capabilities of recent large language models is of paramount importance to further improve them and broaden their applicability. This work investigates how a language model, trained to predict the next token, can perform arithmetic computations generalizing beyond training data. Binary addition and multiplication constitute a good testbed for this purpose, since they require a very small vocabulary and exhibit relevant input/output discontinuities making smooth input interpolation ineffective for novel data. We successfully trained a light language model to learn these tasks and ran a number of experiments to investigate the extrapolation capabilities and internal information processing. Our findings support the hypotheses that the language model works as an Encoding-Regression-Decoding machine where the computation takes place in the value space once the input token representation is mapped to an appropriate internal representation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01154v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>更好地理解最近的大型语言模型的紧急计算和解决问题的能力对于进一步改进它们并扩大其适用性至关重要。这项工作研究了一个经过训练以预测下一个令牌的语言模型如何在训练数据之外进行算术计算。二进制加法和乘法构成了一个很好的测试平台，因为它们需要非常小的词汇表，并且表现出相关的输入/输出不连续性，使得平滑的输入插值对新数据无效。我们成功地训练了一个轻语言模型来学习这些任务，并进行了大量实验来研究推断能力和内部信息处理。我们的发现支持了这样的假设，即语言模型充当编码回归解码机，一旦输入令牌表示映射到适当的内部表示，计算就在值空间中进行。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01154v1" target="_blank">2308.01154v1</a>
                              </td>
                              <td>Arithmetic with Language Models: from Memorization to Computation</td>
                              <td>Davide Maltoni</td>
                              <td>2023-08-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01154v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01154v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_15386v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Vistaar: Diverse Benchmarks and Training Sets for Indian Language ASR</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_15386v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_15386v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_15386v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Improving ASR systems is necessary to make new LLM-based use-cases accessible to people across the globe. In this paper, we focus on Indian languages, and make the case that diverse benchmarks are required to evaluate and improve ASR systems for Indian languages. To address this, we collate Vistaar as a set of 59 benchmarks across various language and domain combinations, on which we evaluate 3 publicly available ASR systems and 2 commercial systems. We also train IndicWhisper models by fine-tuning the Whisper models on publicly available training datasets across 12 Indian languages totalling to 10.7K hours. We show that IndicWhisper significantly improves on considered ASR systems on the Vistaar benchmark. Indeed, IndicWhisper has the lowest WER in 39 out of the 59 benchmarks, with an average reduction of 4.1 WER. We open-source all datasets, code and models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_15386v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>改进ASR系统是必要的，以使全球各地的人们都能访问基于LLM的新用例。在本文中，我们关注印度语言，并提出需要不同的基准来评估和改进印度语言的ASR系统。为了解决这一问题，我们将Vistaar整理为一组59个不同语言和领域组合的基准，在此基础上评估了3个公开可用的ASR系统和2个商业系统。我们还通过在12种印度语言的公开训练数据集上微调Whisper模型来训练IndicWhisper模式，总计107K小时。我们表明，IndicWhisper在Vistaar基准上显著改进了所考虑的ASR系统。事实上，IndicWhisper在59个基准中的39个基准中拥有最低的WER，平均减少了4.1个WER。我们开放所有数据集、代码和模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.15386v2" target="_blank">2305.15386v2</a>
                              </td>
                              <td>Vistaar: Diverse Benchmarks and Training Sets for Indian Language ASR</td>
                              <td>Kaushal Santosh Bhogale</td>
                              <td>2023-05-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_15386v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.15386v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_03701v5_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LMEye: An Interactive Perception Network for Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_03701v5_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_03701v5_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_03701v5_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Training a Large Visual Language Model (LVLM) from scratch, like GPT-4, is resource-intensive. Our paper presents a play-and-plug module for Large Language Models (LLMs), namely Interactive Perception Network (IPN), aiming to achieve a LVLM by incorporating the image understanding capability into LLMs. Previous methods incorporate visual information into LLMs with a simple visual mapping network, where the image feature is projected into the embedding space of LLMs via a linear layer. Such mapping network projects the image feature once yet does not consider the interaction between the image and the human input query. Hence, the obtained visual information with no connections with human intention may be inadequate for LLMs to make intention-following responses, which we term as static visual information. IPN addresses this issue by allowing the LLM to request the desired visual information aligned with various human instructions, which we term as the dynamic interaction between the LLM and visual information. Specifically, IPN consists of a simple visual mapping network to provide the basic perception of an image for LLMs. It also contains additional modules responsible for acquiring requests from LLMs, performing request-based visual information interaction, and transmitting the resulting interacted visual information to LLMs, respectively. In this way, LLMs act to understand the human query, deliver the corresponding request to the request-based visual information interaction module, and generate the response based on the interleaved multimodal information. We evaluate IPN through extensive experiments on multimodal question answering, reasoning, and so on, demonstrating that it significantly improves the zero-shot performance of LVLMs on various multimodal tasks compared to previous methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_03701v5_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从头开始训练大型视觉语言模型（LVLM）（如GPT-4）是资源密集型的。本文提出了一个用于大型语言模型（LLM）的即插即用模块，即交互式感知网络（IPN），旨在通过将图像理解能力融入LLM来实现LVLM。以前的方法通过简单的视觉映射网络将视觉信息结合到LLM中，其中图像特征通过线性层投影到LLM的嵌入空间中。这种映射网络对图像特征进行一次投影，但没有考虑图像和人类输入查询之间的交互。因此，所获得的与人类意图无关的视觉信息可能不足以使LLM做出意图跟随反应，我们称之为静态视觉信息。IPN通过允许LLM请求与各种人类指令一致的所需视觉信息来解决这个问题，我们称之为LLM和视觉信息之间的动态交互。具体而言，IPN由一个简单的视觉映射网络组成，为LLM提供图像的基本感知。它还包含额外的模块，分别负责从LLM获取请求，执行基于请求的视觉信息交互，并将生成的交互视觉信息传输给LLM。通过这种方式，LLM用于理解人类查询，将相应的请求传递到基于请求的视觉信息交互模块，并基于交织的多模式信息生成响应。我们通过对多模式问答、推理等的大量实验来评估IPN，表明与以前的方法相比，它显著提高了LVLMs在各种多模式任务上的零样本性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.03701v5" target="_blank">2305.03701v5</a>
                              </td>
                              <td>LMEye: An Interactive Perception Network for Large Language Models</td>
                              <td>Yunxin Li</td>
                              <td>2023-05-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_03701v5_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.03701v5" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_05206v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Thinking Fast and Slow in Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_05206v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_05206v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_05206v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Therefore, it is of great importance to evaluate their emerging abilities. In this study, we show that LLMs like GPT-3 exhibit behavior that strikingly resembles human-like intuition - and the cognitive errors that come with it. However, LLMs with higher cognitive capabilities, in particular ChatGPT and GPT-4, learned to avoid succumbing to these errors and perform in a hyperrational manner. For our experiments, we probe LLMs with the Cognitive Reflection Test (CRT) as well as semantic illusions that were originally designed to investigate intuitive decision-making in humans. Our study demonstrates that investigating LLMs with methods from psychology has the potential to reveal otherwise unknown emergent traits.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_05206v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）目前处于将人工智能系统与人类通信和日常生活交织在一起的前沿。因此，评估他们的新兴能力具有重要意义。在这项研究中，我们发现，像GPT-3这样的LLM表现出与人类直觉惊人相似的行为，以及随之而来的认知错误。然而，具有更高认知能力的LLM，特别是ChatGPT和GPT-4，学会了避免屈服于这些错误，并以超理性的方式表现。在我们的实验中，我们用认知反射测试（CRT）以及最初设计用于研究人类直觉决策的语义错觉来探究LLM。我们的研究表明，用心理学的方法调查LLM有可能揭示其他未知的突发特征。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.05206v2" target="_blank">2212.05206v2</a>
                              </td>
                              <td>Thinking Fast and Slow in Large Language Models</td>
                              <td>Thilo Hagendorff</td>
                              <td>2022-12-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_05206v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.05206v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_16230v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Private Watermark for Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_16230v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_16230v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_16230v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, text watermarking algorithms for large language models (LLMs) have been mitigating the potential harms of text generated by the LLMs, including fake news and copyright issues. However, the watermark detection of current text algorithms requires the key from the generation process, making them susceptible to breaches and counterfeiting. In this work, we propose the first private watermarking algorithm, which extends the current text watermarking algorithms by using two different neural networks respectively for watermark generation and detection, rather than using the same key at both stages. Meanwhile, part of the parameters of the watermark generation and detection networks are shared, which makes the detection network achieve a high accuracy very efficiently. Experiments show that our algorithm ensures high detection accuracy with minimal impact on generation and detection speed, due to the small parameter size of both networks. Additionally, our subsequent analysis demonstrates the difficulty of reverting the watermark generation rules from the detection network.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_16230v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，用于大型语言模型（LLM）的文本水印算法已经减轻了LLM生成的文本的潜在危害，包括假新闻和版权问题。然而，当前文本算法的水印检测从生成过程中就需要密钥，这使得它们很容易被破解和伪造。在这项工作中，我们提出了第一个私有水印算法，它通过使用两个不同的神经网络分别用于水印生成和检测来扩展当前的文本水印算法，而不是在两个阶段使用相同的密钥。同时，水印生成和检测网络的部分参数是共享的，这使得检测网络非常有效地实现了高精度。实验表明，由于两个网络的参数大小较小，我们的算法确保了高检测精度，同时对生成和检测速度的影响最小。此外，我们随后的分析证明了从检测网络恢复水印生成规则的困难。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.16230v2" target="_blank">2307.16230v2</a>
                              </td>
                              <td>A Private Watermark for Large Language Models</td>
                              <td>Aiwei Liu</td>
                              <td>2023-07-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_16230v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.16230v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_00436v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_00436v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_00436v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_00436v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The recent progress in large language models (LLMs), especially the invention of chain-of-thoughts (CoT) prompting, makes it possible to solve reasoning problems. However, even the strongest LLMs are still struggling with more complicated problems that require non-linear thinking and multi-step reasoning. In this work, we explore whether LLMs have the ability to recognize their own errors, without resorting to external resources. In particular, we investigate whether they can be used to identify individual errors within a step-by-step reasoning. To this end, we propose a zero-shot verification scheme to recognize such errors. We then use this verification scheme to improve question-answering performance, by using it to perform weighted voting on different generated answers. We test the method on three math datasets-GSM8K, MathQA, and MATH-and find that it successfully recognizes errors and, in turn, increases final predictive performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_00436v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）的最新进展，特别是思想链（CoT）提示的发明，使解决推理问题成为可能。然而，即使是最强大的LLM仍在与更复杂的问题作斗争，这些问题需要非线性思维和多步骤推理。在这项工作中，我们探索LLM是否有能力在不求助于外部资源的情况下识别自己的错误。特别是，我们研究了它们是否可以用于在逐步推理中识别单个错误。为此，我们提出了一种零样本验证方案来识别这种错误。然后，我们使用这种验证方案，通过使用它对不同生成的答案进行加权投票，来提高问答性能。我们在三个数学数据集-GSM8K、MathQA和math上测试了该方法，发现它成功地识别了错误，从而提高了最终的预测性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.00436v2" target="_blank">2308.00436v2</a>
                              </td>
                              <td>SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning</td>
                              <td>Ning Miao</td>
                              <td>2023-08-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_00436v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.00436v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_16125v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_16125v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_16125v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_16125v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 18 models across all 12 dimensions, covering both the spatial and temporal understanding. By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research. We will launch and consistently maintain a leaderboard to provide a platform for the community to assess and investigate model capability.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_16125v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于强大的大型语言模型（LLM），近年来生成的多模态大型语言模型作为一个关键的研究领域，在理解和生成方面表现出了非凡的能力。在这项工作中，我们通过引入一个名为SEED Bench的基准，将MLLMs中生成理解的评估作为对生成模型进行全面评估的初步步骤。SEED Bench由19K个多选题组成，带有准确的人工注释（比现有基准大6个），涵盖12个评估维度，包括对图像和视频模态的理解。我们开发了一个高级管道，用于生成针对特定评估维度的多项选择题，集成了自动过滤和手动验证过程。多选题具有源自人工注释的基本事实选项，可以客观有效地评估模型性能，从而消除评估过程中人工或GPT干预的需要。我们进一步评估了18个模型在所有12个维度上的性能，涵盖了空间和时间理解。通过评估结果揭示现有MLLM的局限性，我们的目标是SEED Bench为激励未来的研究提供见解。我们将推出并持续维护排行榜，为社区提供评估和调查模型能力的平台。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.16125v2" target="_blank">2307.16125v2</a>
                              </td>
                              <td>SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension</td>
                              <td>Bohao Li</td>
                              <td>2023-07-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_16125v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.16125v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_16648v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LLMs4OL: Large Language Models for Ontology Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_16648v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_16648v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_16648v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose the LLMs4OL approach, which utilizes Large Language Models (LLMs) for Ontology Learning (OL). LLMs have shown significant advancements in natural language processing, demonstrating their ability to capture complex language patterns in different knowledge domains. Our LLMs4OL paradigm investigates the following hypothesis: \textit{Can LLMs effectively apply their language pattern capturing capability to OL, which involves automatically extracting and structuring knowledge from natural language text?} To test this hypothesis, we conduct a comprehensive evaluation using the zero-shot prompting method. We evaluate nine different LLM model families for three main OL tasks: term typing, taxonomy discovery, and extraction of non-taxonomic relations. Additionally, the evaluations encompass diverse genres of ontological knowledge, including lexicosemantic knowledge in WordNet, geographical knowledge in GeoNames, and medical knowledge in UMLS.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_16648v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了LLMs4OL方法，该方法利用大型语言模型（LLM）进行本体学习（OL）。LLM在自然语言处理方面取得了重大进展，证明了它们在不同知识领域捕捉复杂语言模式的能力。我们的LLMs4OL范式研究了以下假设：\textit｛LLM能否有效地将其语言模式捕获能力应用于OL，这涉及到从自然语言文本中自动提取和结构化知识？｝为了检验这一假设，我们使用零样本提示方法进行了全面评估。我们评估了九个不同的LLM模型家族的三个主要OL任务：术语分型、分类学发现和非分类学关系的提取。此外，评估涵盖了不同类型的本体论知识，包括WordNet中的词典语义知识、GeoNames中的地理知识和UMLS中的医学知识。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.16648v2" target="_blank">2307.16648v2</a>
                              </td>
                              <td>LLMs4OL: Large Language Models for Ontology Learning</td>
                              <td>Hamed Babaei Giglou</td>
                              <td>2023-07-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_16648v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.16648v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03109v6_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Survey on Evaluation of Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03109v6_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03109v6_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03109v6_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03109v6_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）由于其在各种应用中前所未有的性能，在学术界和工业界都越来越受欢迎。随着LLM在研究和日常使用中继续发挥重要作用，其评估变得越来越重要，不仅在任务层面，而且在社会层面，以更好地了解其潜在风险。在过去的几年里，我们从不同的角度对LLM进行了大量的研究。本文对LLM的这些评估方法进行了全面的回顾，重点关注三个关键维度：评估什么、在哪里评估以及如何评估。首先，我们从评估任务的角度进行了概述，包括一般的自然语言处理任务、推理、医学使用、伦理学、教育、自然科学和社会科学、代理应用和其他领域。其次，我们通过深入研究评估方法和基准来回答“在哪里”和“如何”问题，这些方法和基准是评估LLM绩效的关键组成部分。然后，我们总结了LLM在不同任务中的成功和失败案例。最后，我们阐明了LLM评估未来面临的几个挑战。我们的目标是为LLM评估领域的研究人员提供宝贵的见解，从而帮助开发更熟练的LLM。我们的重点是，评估应被视为一门重要学科，以更好地帮助LLM的发展。我们一贯将相关开源材料保存在：https://github.com/MLGroupJLU/LLM-eval-survey.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03109v6" target="_blank">2307.03109v6</a>
                              </td>
                              <td>A Survey on Evaluation of Large Language Models</td>
                              <td>Yupeng Chang</td>
                              <td>2023-07-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03109v6_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03109v6" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_00081v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Semantically Enriched Embeddings for Knowledge Graph Completion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_00081v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_00081v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_00081v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Embedding based Knowledge Graph (KG) Completion has gained much attention over the past few years. Most of the current algorithms consider a KG as a multidirectional labeled graph and lack the ability to capture the semantics underlying the schematic information. In a separate development, a vast amount of information has been captured within the Large Language Models (LLMs) which has revolutionized the field of Artificial Intelligence. KGs could benefit from these LLMs and vice versa. This vision paper discusses the existing algorithms for KG completion based on the variations for generating KG embeddings. It starts with discussing various KG completion algorithms such as transductive and inductive link prediction and entity type prediction algorithms. It then moves on to the algorithms utilizing type information within the KGs, LLMs, and finally to algorithms capturing the semantics represented in different description logic axioms. We conclude the paper with a critical reflection on the current state of work in the community and give recommendations for future directions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_00081v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于嵌入的知识图（KG）完成在过去几年中受到了广泛的关注。目前的大多数算法都认为KG是一个多方向的标记图，并且缺乏捕获原理图信息背后语义的能力。在另一个发展中，大量信息已在大型语言模型（LLM）中捕获，这使人工智能领域发生了革命性的变化。幼儿园可以从这些LLM中受益，反之亦然。这篇视觉论文讨论了现有的基于生成KG嵌入的变体的KG完成算法。它首先讨论了各种KG完成算法，如转导和归纳链路预测和实体类型预测算法。然后，它转到利用KGs、LLM中的类型信息的算法，最后转到捕获不同描述逻辑公理中表示的语义的算法。最后，我们对社区的工作现状进行了批判性反思，并对未来的方向提出了建议。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.00081v2" target="_blank">2308.00081v2</a>
                              </td>
                              <td>Towards Semantically Enriched Embeddings for Knowledge Graph Completion</td>
                              <td>Mehwish Alam</td>
                              <td>2023-07-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_00081v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.00081v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03393v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03393v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03393v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03393v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Learning on Graphs has attracted immense attention due to its wide real-world applications. The most popular pipeline for learning on graphs with textual node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes shallow text embedding as initial node representations, which has limitations in general knowledge and profound semantic understanding. In recent years, Large Language Models (LLMs) have been proven to possess extensive common knowledge and powerful semantic comprehension abilities that have revolutionized existing workflows to handle text data. In this paper, we aim to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigate two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text attributes with their massive knowledge and then generate predictions through GNNs. The latter attempts to directly employ LLMs as standalone predictors. We conduct comprehensive and systematical studies on these two pipelines under various settings. From comprehensive empirical results, we make original observations and find new insights that open new possibilities and suggest promising directions to leverage LLMs for learning on graphs. Our codes and datasets are available at https://github.com/CurryTang/Graph-LLM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03393v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>图形学习由于其在现实世界中的广泛应用而引起了极大的关注。在具有文本节点属性的图上学习最流行的管道主要依赖于图神经网络（GNN），并利用浅文本嵌入作为初始节点表示，这在一般知识和深刻的语义理解方面具有局限性。近年来，大型语言模型（LLM）已被证明具有广泛的公共知识和强大的语义理解能力，这彻底改变了现有的处理文本数据的工作流程。在本文中，我们旨在探索LLM在图机器学习中的潜力，特别是在节点分类任务中，并研究两种可能的管道：LLM作为增强器和LLM作为预测器。前者利用LLM利用其海量知识增强节点的文本属性，然后通过GNN生成预测。后者试图直接使用LLM作为独立的预测因子。我们在不同的环境下对这两条管道进行了全面、系统的研究。从全面的实证结果中，我们进行了原始的观察，发现了新的见解，这些见解开辟了新的可能性，并提出了利用LLM在图上学习的有希望的方向。我们的代码和数据集可在https://github.com/CurryTang/Graph-LLM.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03393v3" target="_blank">2307.03393v3</a>
                              </td>
                              <td>Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs</td>
                              <td>Zhikai Chen</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03393v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03393v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_00352v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MetaGPT: Meta Programming for Multi-Agent Collaborative Framework</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_00352v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_00352v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_00352v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, remarkable progress has been made in automated task-solving through the use of multi-agents driven by large language models (LLMs). However, existing works primarily focuses on simple tasks lacking exploration and investigation in complicated tasks mainly due to the hallucination problem. This kind of hallucination gets amplified infinitely as multiple intelligent agents interact with each other, resulting in failures when tackling complicated problems.Therefore, we introduce MetaGPT, an innovative framework that infuses effective human workflows as a meta programming approach into LLM-driven multi-agent collaboration. In particular, MetaGPT first encodes Standardized Operating Procedures (SOPs) into prompts, fostering structured coordination. And then, it further mandates modular outputs, bestowing agents with domain expertise paralleling human professionals to validate outputs and reduce compounded errors. In this way, MetaGPT leverages the assembly line work model to assign diverse roles to various agents, thus establishing a framework that can effectively and cohesively deconstruct complex multi-agent collaborative problems. Our experiments conducted on collaborative software engineering tasks illustrate MetaGPT's capability in producing comprehensive solutions with higher coherence relative to existing conversational and chat-based multi-agent systems. This underscores the potential of incorporating human domain knowledge into multi-agents, thus opening up novel avenues for grappling with intricate real-world challenges. The GitHub repository of this project is made publicly available on: https://github.com/geekan/MetaGPT</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_00352v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，通过使用由大型语言模型（LLM）驱动的多代理，在自动任务解决方面取得了显著进展。然而，现有的工作主要集中在简单的任务上，缺乏对复杂任务的探索和调查，主要是由于幻觉问题。当多个智能代理相互作用时，这种幻觉会被无限放大，导致在处理复杂问题时失败。因此，我们引入了MetaGPT，这是一种创新框架，将有效的人类工作流作为元编程方法注入LLM驱动的多代理协作中。特别是，MetaGPT首先将标准化操作程序（SOP）编码为提示，促进结构化协调。然后，它进一步强制模块化输出，赋予代理与人类专业人员平行的领域专业知识，以验证输出并减少复合错误。通过这种方式，MetaGPT利用装配线工作模型为各种代理分配不同的角色，从而建立了一个能够有效、连贯地解构复杂多智能体协作问题的框架。我们在协作软件工程任务上进行的实验表明，相对于现有的会话和基于聊天的多智能体系统，MetaGPT能够产生具有更高一致性的综合解决方案。这突出了将人类领域知识融入多智能体的潜力，从而为应对复杂的现实世界挑战开辟了新的途径。该项目的GitHub存储库在以下网站上公开：https://github.com/geekan/MetaGPT</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.00352v2" target="_blank">2308.00352v2</a>
                              </td>
                              <td>MetaGPT: Meta Programming for Multi-Agent Collaborative Framework</td>
                              <td>Sirui Hong</td>
                              <td>2023-08-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_00352v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.00352v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_14233v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Large Language Models are Strong Zero-Shot Retriever</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_14233v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_14233v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_14233v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this work, we propose a simple method that applies a large language model (LLM) to large-scale retrieval in zero-shot scenarios. Our method, the Language language model as Retriever (LameR), is built upon no other neural models but an LLM, while breaking brute-force combinations of retrievers with LLMs and lifting the performance of zero-shot retrieval to be very competitive on benchmark datasets. Essentially, we propose to augment a query with its potential answers by prompting LLMs with a composition of the query and the query's in-domain candidates. The candidates, regardless of correct or wrong, are obtained by a vanilla retrieval procedure on the target collection. As a part of the prompts, they are likely to help LLM generate more precise answers by pattern imitation or candidate summarization. Even if all the candidates are wrong, the prompts at least make LLM aware of in-collection patterns and genres. Moreover, due to the low performance of a self-supervised retriever, the LLM-based query augmentation becomes less effective as the retriever bottlenecks the whole pipeline. Therefore, we propose to leverage a non-parametric lexicon-based method (e.g., BM25) as the retrieval module to capture query-document overlap in a literal fashion. As such, LameR makes the retrieval procedure transparent to the LLM, thus circumventing the performance bottleneck.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_14233v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项工作中，我们提出了一种简单的方法，将大型语言模型（LLM）应用于零样本场景中的大规模检索。我们的方法，即作为检索器的语言语言模型（LameR），不是建立在其他神经模型上，而是建立在LLM上，同时打破了检索器与LLM的野蛮力量组合，并提高了零样本检索的性能，使其在基准数据集上具有很强的竞争力。从本质上讲，我们建议通过提示LLM组合查询和域内候选查询来增强查询的潜在答案。候选者，无论正确与否，都是通过目标集合上的普通检索程序获得的。作为提示的一部分，它们可能通过模式模仿或候选摘要来帮助LLM生成更精确的答案。即使所有的候选者都错了，提示至少让LLM意识到集合中的模式和流派。此外，由于自监督检索器的低性能，基于LLM的查询扩充变得不那么有效，因为检索器会阻塞整个管道。因此，我们建议利用基于非参数词典的方法（例如，BM25）作为检索模块，以文字的方式捕获查询文档重叠。因此，LameR使检索过程对LLM透明，从而规避了性能瓶颈。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.14233v2" target="_blank">2304.14233v2</a>
                              </td>
                              <td>Large Language Models are Strong Zero-Shot Retriever</td>
                              <td>Tao Shen</td>
                              <td>2023-04-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_14233v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.14233v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_16039v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_16039v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_16039v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_16039v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A key technology for the development of large language models (LLMs) involves instruction tuning that helps align the models' responses with human expectations to realize impressive learning abilities. Two major approaches for instruction tuning characterize supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), which are currently applied to produce the best commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for research and development efforts, various instruction-tuned open-source LLMs have also been introduced recently, e.g., Alpaca, Vicuna, to name a few. However, existing open-source LLMs have only been instruction-tuned for English and a few popular languages, thus hindering their impacts and accessibility to many other languages in the world. Among a few very recent work to explore instruction tuning for LLMs in multiple languages, SFT has been used as the only approach to instruction-tune LLMs for multiple languages. This has left a significant gap for fine-tuned LLMs based on RLHF in diverse languages and raised important questions on how RLHF can boost the performance of multilingual instruction tuning. To overcome this issue, we present Okapi, the first system with instruction-tuned LLMs based on RLHF for multiple languages. Okapi introduces instruction and response-ranked data in 26 diverse languages to facilitate the experiments and development of future multilingual LLM research. We also present benchmark datasets to enable the evaluation of generative LLMs in multiple languages. Our experiments demonstrate the advantages of RLHF for multilingual instruction over SFT for different base models and datasets. Our framework and resources are released at https://github.com/nlp-uoregon/Okapi.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_16039v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>开发大型语言模型（LLM）的一项关键技术涉及教学调整，这有助于将模型的反应与人类的期望相一致，以实现令人印象深刻的学习能力。指令调整的两种主要方法是监督微调（SFT）和来自人类反馈的强化学习（RLHF），这两种方法目前被用于产生最佳的商业LLM（例如，ChatGPT）。为了提高LLM在研发工作中的可访问性，最近还引入了各种基于指令的开源LLM，例如Alpaca、Vicuna等。然而，现有的开源LLM只针对英语和少数流行语言进行了教学调整，从而阻碍了它们对世界上许多其他语言的影响和访问。在最近几项探索多种语言LLM指令调优的工作中，SFT已被用作多种语言LLMs指令调优的唯一方法。这为在不同语言中基于RLHF的微调LLM留下了巨大的空白，并提出了RLHF如何提高多语言指令调整性能的重要问题。为了克服这个问题，我们提出了Okapi，这是第一个针对多种语言的基于RLHF的指令调优LLM系统。Okapi介绍了26种不同语言的指令和响应排名数据，以促进未来多语言LLM研究的实验和发展。我们还提供了基准数据集，以便能够评估多种语言中的生成LLM。我们的实验证明了RLHF在不同基础模型和数据集的多语言教学中优于SFT的优势。我们的框架和资源发布于https://github.com/nlp-uoregon/Okapi.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.16039v2" target="_blank">2307.16039v2</a>
                              </td>
                              <td>Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback</td>
                              <td>Viet Dac Lai</td>
                              <td>2023-07-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_16039v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.16039v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_00127v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DiviML: A Module-based Heuristic for Mapping Neural Networks onto Heterogeneous Platforms</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_00127v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_00127v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_00127v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Datacenters are increasingly becoming heterogeneous, and are starting to include specialized hardware for networking, video processing, and especially deep learning. To leverage the heterogeneous compute capability of modern datacenters, we develop an approach for compiler-level partitioning of deep neural networks (DNNs) onto multiple interconnected hardware devices. We present a general framework for heterogeneous DNN compilation, offering automatic partitioning and device mapping. Our scheduler integrates both an exact solver, through a mixed integer linear programming (MILP) formulation, and a modularity-based heuristic for scalability. Furthermore, we propose a theoretical lower bound formula for the optimal solution, which enables the assessment of the heuristic solutions' quality. We evaluate our scheduler in optimizing both conventional DNNs and randomly-wired neural networks, subject to latency and throughput constraints, on a heterogeneous system comprised of a CPU and two distinct GPUs. Compared to na\"ively running DNNs on the fastest GPU, he proposed framework can achieve more than 3$\times$ times lower latency and up to 2.9$\times$ higher throughput by automatically leveraging both data and model parallelism to deploy DNNs on our sample heterogeneous server node. Moreover, our modularity-based "splitting" heuristic improves the solution runtime up to 395$\times$ without noticeably sacrificing solution quality compared to an exact MILP solution, and outperforms all other heuristics by 30-60% solution quality. Finally, our case study shows how we can extend our framework to schedule large language models across multiple heterogeneous servers by exploiting symmetry in the hardware setup. Our code can be easily plugged in to existing frameworks, and is available at https://github.com/abdelfattah-lab/diviml.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_00127v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>数据中心正变得越来越异构，并开始包括用于网络、视频处理，尤其是深度学习的专用硬件。为了利用现代数据中心的异构计算能力，我们开发了一种将深度神经网络（DNN）划分到多个互连硬件设备上的编译器级方法。我们提出了一个用于异构DNN编译的通用框架，提供自动分区和设备映射。我们的调度器通过混合整数线性规划（MILP）公式集成了精确求解器，并集成了基于模块化的启发式算法以实现可扩展性。此外，我们提出了最优解的理论下界公式，该公式能够评估启发式解的质量。我们在由一个CPU和两个不同的GPU组成的异构系统上，在受到延迟和吞吐量约束的情况下，评估了我们的调度器在优化传统DNN和随机连接的神经网络方面的性能。与在最快的GPU上实际运行DNN相比，他提出的框架通过自动利用数据和模型并行性在我们的示例异构服务器节点上部署DNN，可以实现3倍以上的延迟和2.9倍以上的吞吐量“与精确的MILP解决方案相比，启发式算法在不显著牺牲解决方案质量的情况下，将解决方案运行时间提高到395$\times$，并比所有其他启发式算法高出30-60%的解决方案质量。最后，我们的案例研究表明，我们可以通过利用硬件设置中的对称性，扩展我们的框架，在多个异构服务器上调度大型语言模型可以很容易地插入到现有的框架中，并且可以在https://github.com/abdelfattah-lab/diviml.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>46</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.00127v2" target="_blank">2308.00127v2</a>
                              </td>
                              <td>DiviML: A Module-based Heuristic for Mapping Neural Networks onto Heterogeneous Platforms</td>
                              <td>Yassine Ghannane</td>
                              <td>2023-07-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_00127v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.00127v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="SAM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2308_01727v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Local Large Language Models for Complex Structured Medical Tasks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01727v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01727v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01727v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper introduces an approach that combines the language reasoning capabilities of large language models (LLMs) with the benefits of local training to tackle complex, domain-specific tasks. Specifically, the authors demonstrate their approach by extracting structured condition codes from pathology reports. The proposed approach utilizes local LLMs, which can be fine-tuned to respond to specific generative instructions and provide structured outputs. The authors collected a dataset of over 150k uncurated surgical pathology reports, containing gross descriptions, final diagnoses, and condition codes. They trained different model architectures, including LLaMA, BERT and LongFormer and evaluated their performance. The results show that the LLaMA-based models significantly outperform BERT-style models across all evaluated metrics, even with extremely reduced precision. The LLaMA models performed especially well with large datasets, demonstrating their ability to handle complex, multi-label tasks. Overall, this work presents an effective approach for utilizing LLMs to perform domain-specific tasks using accessible hardware, with potential applications in the medical domain, where complex data extraction and classification are required.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01727v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了一种方法，该方法将大型语言模型（LLM）的语言推理能力与本地训练的优势相结合，以处理复杂的、特定领域的任务。具体来说，作者通过从病理报告中提取结构化的条件代码来展示他们的方法。所提出的方法利用了局部LLM，可以对其进行微调以响应特定的生成指令并提供结构化输出。作者收集了超过15万份未分级手术病理报告的数据集，其中包括总体描述、最终诊断和状况代码。他们训练了不同的模型体系结构，包括LLaMA、BERT和LongFormer，并评估了它们的性能。结果表明，在所有评估指标中，基于LLaMA的模型显著优于BERT风格的模型，即使精度极低。LLaMA模型在大型数据集中表现特别好，证明了它们处理复杂、多标签任务的能力。总的来说，这项工作为利用LLM使用可访问的硬件执行特定领域的任务提供了一种有效的方法，在需要复杂数据提取和分类的医疗领域具有潜在的应用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01727v1" target="_blank">2308.01727v1</a>
                              </td>
                              <td>Local Large Language Models for Complex Structured Medical Tasks</td>
                              <td>V. K. Cody Bumgardner</td>
                              <td>2023-08-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01727v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01727v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_00675v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_00675v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_00675v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_00675v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Today, large language models (LLMs) are taught to use new tools by providing a few demonstrations of the tool's usage. Unfortunately, demonstrations are hard to acquire, and can result in undesirable biased usage if the wrong demonstration is chosen. Even in the rare scenario that demonstrations are readily available, there is no principled selection protocol to determine how many and which ones to provide. As tasks grow more complex, the selection search grows combinatorially and invariably becomes intractable. Our work provides an alternative to demonstrations: tool documentation. We advocate the use of tool documentation, descriptions for the individual tool usage, over demonstrations. We substantiate our claim through three main empirical findings on 6 tasks across both vision and language modalities. First, on existing benchmarks, zero-shot prompts with only tool documentation are sufficient for eliciting proper tool usage, achieving performance on par with few-shot prompts. Second, on a newly collected realistic tool-use dataset with hundreds of available tool APIs, we show that tool documentation is significantly more valuable than demonstrations, with zero-shot documentation significantly outperforming few-shot without documentation. Third, we highlight the benefits of tool documentations by tackling image generation and video tracking using just-released unseen state-of-the-art models as tools. Finally, we highlight the possibility of using tool documentation to automatically enable new applications: by using nothing more than the documentation of GroundingDino, Stable Diffusion, XMem, and SAM, LLMs can re-invent the functionalities of the just-released Grounded-SAM and Track Anything models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_00675v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>如今，大型语言模型（LLM）通过提供一些工具用法的演示来学习使用新工具。不幸的是，演示很难获得，如果选择了错误的演示，可能会导致不希望的有偏见的使用。即使在极少数情况下，演示很容易获得，也没有原则性的选择协议来确定提供多少演示以及提供哪些演示。随着任务变得越来越复杂，选择搜索组合增长，并且总是变得棘手。我们的工作提供了一种替代演示的方法：工具文档。我们提倡使用工具文档，对单个工具使用的描述，而不是演示。我们通过对视觉和语言模式中的6项任务的三个主要实证发现来证实我们的说法。首先，在现有的基准测试中，只有工具文档的零样本提示就足以引发正确的工具使用，实现与很少的零热点提示相当的性能。其次，在一个新收集的具有数百个可用工具API的真实工具使用数据集上，我们表明工具文档比演示更有价值，零样本文档显著优于没有文档的少数快照。第三，我们强调了工具文档的好处，通过使用刚刚发布的未公开的最先进模型作为工具来处理图像生成和视频跟踪。最后，我们强调了使用工具文档自动启用新应用程序的可能性：通过只使用GroundingDino、Stable Diffusion、XMem和SAM的文档，LLM可以重新发明刚刚发布的Grounded SAM和Track Anything模型的功能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.00675v1" target="_blank">2308.00675v1</a>
                              </td>
                              <td>Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models</td>
                              <td>Cheng-Yu Hsieh</td>
                              <td>2023-08-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_00675v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.00675v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2206_06468v6_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Stable Relationships</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2206_06468v6_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2206_06468v6_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2206_06468v6_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We study a dynamic model of the relationship between two people where the states depend on the "power" in the relationship. We perform a comprehensive analysis of stability of the system, and determine a set of conditions under which stable relationships are possible. In particular, stable relationships can occur if both people are dominant, but the sum of dominances is below a bound determined by the model's parameters. Stable relationships can also occur if one person is dominant and the other is submissive, provided the level of dominance exceeds the level of submissiveness but not beyond a threshold. We also conclude that a stable relationship is not possible if both people are submissive. While our model is motivated by a social or romantic relationship, it can also be applied to professional or business relationships as well as diplomatic relationships between nations.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2206_06468v6_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们研究了两个人关系的动态模型，其中国家取决于关系中的“权力”。我们对系统的稳定性进行了全面的分析，并确定了一组稳定关系可能存在的条件。特别是，如果两个人都占主导地位，但主导地位的总和低于模型参数确定的界限，则可以发生稳定的关系。如果一个人占主导地位，另一个人顺从，那么稳定的关系也会发生，前提是主导地位的水平超过了顺从的水平，但没有超过阈值。我们还得出结论，如果两个人都顺从，那么稳定的关系是不可能的。虽然我们的模式是由社会或浪漫关系驱动的，但它也可以应用于职业或商业关系以及国家之间的外交关系。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2206.06468v6" target="_blank">2206.06468v6</a>
                              </td>
                              <td>Stable Relationships</td>
                              <td>Sam Ganzfried</td>
                              <td>2022-06-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2206_06468v6_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2206.06468v6" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_16709v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multilingual context-based pronunciation learning for Text-to-Speech</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_16709v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_16709v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_16709v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Phonetic information and linguistic knowledge are an essential component of a Text-to-speech (TTS) front-end. Given a language, a lexicon can be collected offline and Grapheme-to-Phoneme (G2P) relationships are usually modeled in order to predict the pronunciation for out-of-vocabulary (OOV) words. Additionally, post-lexical phonology, often defined in the form of rule-based systems, is used to correct pronunciation within or between words. In this work we showcase a multilingual unified front-end system that addresses any pronunciation related task, typically handled by separate modules. We evaluate the proposed model on G2P conversion and other language-specific challenges, such as homograph and polyphones disambiguation, post-lexical rules and implicit diacritization. We find that the multilingual model is competitive across languages and tasks, however, some trade-offs exists when compared to equivalent monolingual solutions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_16709v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语音信息和语言知识是TTS前端的重要组成部分。给定一种语言，可以离线收集词典，通常对词素与音素（G2P）的关系进行建模，以预测词汇表外（OOV）单词的发音。此外，后词汇音韵学通常以基于规则的系统的形式定义，用于纠正单词内部或单词之间的发音。在这项工作中，我们展示了一个多语言统一前端系统，该系统可以处理任何与发音相关的任务，通常由单独的模块处理。我们评估了所提出的G2P转换模型和其他语言特有的挑战，如同形词和复调词的消歧、后词汇规则和隐式变音。我们发现，多语言模型在不同语言和任务之间具有竞争力，然而，与等效的单语解决方案相比，存在一些权衡。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.16709v1" target="_blank">2307.16709v1</a>
                              </td>
                              <td>Multilingual context-based pronunciation learning for Text-to-Speech</td>
                              <td>Giulia Comini</td>
                              <td>2023-07-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_16709v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.16709v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_16704v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Lookbehind Optimizer: k steps back, 1 step forward</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_16704v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_16704v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_16704v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Lookahead optimizer improves the training stability of deep neural networks by having a set of fast weights that "look ahead" to guide the descent direction. Here, we combine this idea with sharpness-aware minimization (SAM) to stabilize its multi-step variant and improve the loss-sharpness trade-off. We propose Lookbehind, which computes $k$ gradient ascent steps ("looking behind") at each iteration and combine the gradients to bias the descent step toward flatter minima. We apply Lookbehind on top of two popular sharpness-aware training methods -- SAM and adaptive SAM (ASAM) -- and show that our approach leads to a myriad of benefits across a variety of tasks and training regimes. Particularly, we show increased generalization performance, greater robustness against noisy weights, and higher tolerance to catastrophic forgetting in lifelong learning settings.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_16704v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>展望优化器通过具有一组“展望”以引导下降方向的快速权重，提高了深度神经网络的训练稳定性。在这里，我们将这一想法与清晰度感知最小化（SAM）相结合，以稳定其多步骤变体并改善损失清晰度权衡。我们提出了Lookbacking，它在每次迭代时计算$k$梯度上升步长（“Lookbacking”），并将梯度组合起来，使下降步长偏向更平坦的最小值。我们在两种流行的敏锐度感知训练方法（SAM和自适应SAM（ASAM））的基础上应用了Lookbacking，并表明我们的方法在各种任务和训练制度中带来了无数好处。特别是，我们展示了在终身学习环境中提高的泛化性能、对噪声权重的更强鲁棒性以及对灾难性遗忘的更高容忍度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.16704v1" target="_blank">2307.16704v1</a>
                              </td>
                              <td>Lookbehind Optimizer: k steps back, 1 step forward</td>
                              <td>Gonçalo Mordido</td>
                              <td>2023-07-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_16704v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.16704v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_16586v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAMFlow: Eliminating Any Fragmentation in Optical Flow with Segment Anything Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_16586v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_16586v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_16586v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Optical flow estimation aims to find the 2D dense motion field between two frames. Due to the limitation of model structures and training datasets, existing methods often rely too much on local clues and ignore the integrity of objects, resulting in fragmented motion estimation. We notice that the recently famous Segment Anything Model (SAM) demonstrates a strong ability to segment complete objects, which is suitable for solving the fragmentation problem in optical flow estimation. We thus propose a solution to embed the frozen SAM image encoder into FlowFormer to enhance object perception. To address the challenge of in-depth utilizing SAM in non-segmentation tasks like optical flow estimation, we propose an Optical Flow Task-Specific Adaption scheme, including a Context Fusion Module to fuse the SAM encoder with the optical flow context encoder, and a Context Adaption Module to adapt the SAM features for optical flow task with Learned Task-Specific Embedding. Our proposed SAMFlow model reaches 0.86/2.10 clean/final EPE and 3.55/12.32 EPE/F1-all on Sintel and KITTI-15 training set, surpassing Flowformer by 8.5%/9.9% and 13.2%/16.3%. Furthermore, our model achieves state-of-the-art performance on the Sintel and KITTI-15 benchmarks, ranking #1 among all two-frame methods on Sintel clean pass.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_16586v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>光流估计旨在找到两帧之间的2D密集运动场。由于模型结构和训练数据集的限制，现有的方法往往过于依赖局部线索，忽略了对象的完整性，导致运动估计碎片化。我们注意到，最近著名的分段任意模型（SAM）展示了对完整对象进行分段的强大能力，适用于解决光流估计中的分段问题。因此，我们提出了一种将冻结的SAM图像编码器嵌入FlowFormer中以增强对象感知的解决方案。为了解决在光流估计等非分割任务中深入利用SAM的挑战，我们提出了一种光流任务特定自适应方案，包括将SAM编码器与光流上下文编码器融合的上下文融合模块，以及通过学习任务特定嵌入将SAM特征适配于光流任务的上下文自适应模块。我们提出的SAMFlow模型在Sintel和KITTI-15训练集上达到了0.86/2.10的清洁/最终EPE和3.55/12.32的EPE/F1，分别超过Flowformer 8.5%/9.9%和13.2%/16.3%。此外，我们的模型在Sintl和KITTI-15基准上实现了最先进的性能，在Sintel清洁通道的所有两种框架方法中排名第一。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.16586v1" target="_blank">2307.16586v1</a>
                              </td>
                              <td>SAMFlow: Eliminating Any Fragmentation in Optical Flow with Segment Anything Model</td>
                              <td>Shili Zhou</td>
                              <td>2023-07-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_16586v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.16586v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_10286v4_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Enlighten Anything: When Segment Anything Model Meets Low-Light Image Enhancement</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_10286v4_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_10286v4_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_10286v4_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Image restoration is a low-level visual task, and most CNN methods are designed as black boxes, lacking transparency and intrinsic aesthetics. Many unsupervised approaches ignore the degradation of visible information in low-light scenes, which will seriously affect the aggregation of complementary information and also make the fusion algorithm unable to produce satisfactory fusion results under extreme conditions. In this paper, we propose Enlighten-anything, which is able to enhance and fuse the semantic intent of SAM segmentation with low-light images to obtain fused images with good visual perception. The generalization ability of unsupervised learning is greatly improved, and experiments on LOL dataset are conducted to show that our method improves 3db in PSNR over baseline and 8 in SSIM. Zero-shot learning of SAM introduces a powerful aid for unsupervised low-light enhancement. The source code of Enlighten Anything can be obtained from https://github.com/zhangbaijin/enlighten-anything</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_10286v4_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>图像恢复是一项低级的视觉任务，大多数CNN方法都被设计成黑匣子，缺乏透明度和内在的美感。许多无监督方法忽略了弱光场景中可见信息的退化，这将严重影响互补信息的聚合，也使融合算法在极端条件下无法产生令人满意的融合结果。在本文中，我们提出了Enlighten anything，它能够增强和融合低光图像的SAM分割的语义意图，以获得具有良好视觉感知的融合图像。无监督学习的泛化能力大大提高，在LOL数据集上进行的实验表明，我们的方法在PSNR中比基线提高了3db，在SSIM中提高了8。SAM的零样本学习为无监督低光增强引入了强大的帮助。Enlighten Anything的源代码可以从https://github.com/zhangbaijin/enlighten-anything</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.10286v4" target="_blank">2306.10286v4</a>
                              </td>
                              <td>Enlighten Anything: When Segment Anything Model Meets Low-Light Image Enhancement</td>
                              <td>Qihan Zhao</td>
                              <td>2023-06-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_10286v4_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.10286v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_00093v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Task-Oriented Channel Attention for Fine-Grained Few-Shot Classification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_00093v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_00093v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_00093v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The difficulty of the fine-grained image classification mainly comes from a shared overall appearance across classes. Thus, recognizing discriminative details, such as eyes and beaks for birds, is a key in the task. However, this is particularly challenging when training data is limited. To address this, we propose Task Discrepancy Maximization (TDM), a task-oriented channel attention method tailored for fine-grained few-shot classification with two novel modules Support Attention Module (SAM) and Query Attention Module (QAM). SAM highlights channels encoding class-wise discriminative features, while QAM assigns higher weights to object-relevant channels of the query. Based on these submodules, TDM produces task-adaptive features by focusing on channels encoding class-discriminative details and possessed by the query at the same time, for accurate class-sensitive similarity measure between support and query instances. While TDM influences high-level feature maps by task-adaptive calibration of channel-wise importance, we further introduce Instance Attention Module (IAM) operating in intermediate layers of feature extractors to instance-wisely highlight object-relevant channels, by extending QAM. The merits of TDM and IAM and their complementary benefits are experimentally validated in fine-grained few-shot classification tasks. Moreover, IAM is also shown to be effective in coarse-grained and cross-domain few-shot classifications.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_00093v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>细粒度图像分类的困难主要来自于类之间共享的整体外观。因此，识别辨别细节，如鸟类的眼睛和喙，是这项任务的关键。然而，当训练数据有限时，这尤其具有挑战性。为了解决这一问题，我们提出了任务差异最大化（TDM），这是一种面向任务的通道注意力方法，专门用于细粒度的少镜头分类，具有两个新的模块支持注意力模块（SAM）和查询注意力模块（QAM）。SAM突出显示编码类判别特征的通道，而QAM为查询的对象相关通道分配更高的权重。基于这些子模块，TDM通过关注编码类判别细节并同时被查询所拥有的信道来产生任务自适应特征，以实现支持和查询实例之间准确的类敏感相似性度量。虽然TDM通过信道重要性的任务自适应校准来影响高级特征图，但我们进一步引入了在特征提取器的中间层中操作的实例注意模块（IAM），通过扩展QAM来实例明智地突出对象相关信道。TDM和IAM的优点及其互补优势在细粒度少镜头分类任务中得到了实验验证。此外，IAM在粗粒度和跨域少炮分类中也被证明是有效的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.00093v1" target="_blank">2308.00093v1</a>
                              </td>
                              <td>Task-Oriented Channel Attention for Fine-Grained Few-Shot Classification</td>
                              <td>SuBeen Lee</td>
                              <td>2023-07-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_00093v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.00093v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_01986v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">USTC FLICAR: A Sensors Fusion Dataset of LiDAR-Inertial-Camera for Heavy-duty Autonomous Aerial Work Robots</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_01986v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_01986v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_01986v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we present the USTC FLICAR Dataset, which is dedicated to the development of simultaneous localization and mapping and precise 3D reconstruction of the workspace for heavy-duty autonomous aerial work robots. In recent years, numerous public datasets have played significant roles in the advancement of autonomous cars and unmanned aerial vehicles (UAVs). However, these two platforms differ from aerial work robots: UAVs are limited in their payload capacity, while cars are restricted to two-dimensional movements. To fill this gap, we create the "Giraffe" mapping robot based on a bucket truck, which is equipped with a variety of well-calibrated and synchronized sensors: four 3D LiDARs, two stereo cameras, two monocular cameras, Inertial Measurement Units (IMUs), and a GNSS/INS system. A laser tracker is used to record the millimeter-level ground truth positions. We also make its ground twin, the "Okapi" mapping robot, to gather data for comparison. The proposed dataset extends the typical autonomous driving sensing suite to aerial scenes, demonstrating the potential of combining autonomous driving perception systems with bucket trucks to create a versatile autonomous aerial working platform. Moreover, based on the Segment Anything Model (SAM), we produce the Semantic FLICAR dataset, which provides fine-grained semantic segmentation annotations for multimodal continuous data in both temporal and spatial dimensions. The dataset is available for download at: https://ustc-flicar.github.io/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_01986v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们介绍了USTC FLICAR数据集，该数据集致力于开发重型自主航空工作机器人工作空间的同时定位和映射以及精确的三维重建。近年来，大量公共数据集在自动驾驶汽车和无人机的发展中发挥了重要作用。然而，这两个平台不同于空中作业机器人：无人机的有效载荷能力有限，而汽车仅限于二维运动。为了填补这一空白，我们创建了基于斗式卡车的“长颈鹿”测绘机器人，该机器人配备了各种校准良好且同步的传感器：四个3D激光雷达、两个立体相机、两个单眼相机、惯性测量单元（IMU）和一个GNSS/INS系统。激光跟踪器用于记录毫米级地面实况位置。我们还制造了它的地面双胞胎“奥卡皮”测绘机器人，以收集数据进行比较。所提出的数据集将典型的自动驾驶感知套件扩展到空中场景，展示了将自动驾驶感知系统与斗式卡车相结合以创建多功能自动空中工作平台的潜力。此外，基于Segment Anything Model（SAM），我们生成了Semantic FLICAR数据集，该数据集在时间和空间维度上为多模式连续数据提供了细粒度的语义分割注释。数据集可在以下位置下载：https://ustc-flicar.github.io/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.01986v2" target="_blank">2304.01986v2</a>
                              </td>
                              <td>USTC FLICAR: A Sensors Fusion Dataset of LiDAR-Inertial-Camera for Heavy-duty Autonomous Aerial Work Robots</td>
                              <td>Ziming Wang</td>
                              <td>2023-04-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_01986v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.01986v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_14620v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NeRF-Det: Learning Geometry-Aware Volumetric Representation for Multi-View 3D Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_14620v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_14620v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_14620v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present NeRF-Det, a novel method for indoor 3D detection with posed RGB images as input. Unlike existing indoor 3D detection methods that struggle to model scene geometry, our method makes novel use of NeRF in an end-to-end manner to explicitly estimate 3D geometry, thereby improving 3D detection performance. Specifically, to avoid the significant extra latency associated with per-scene optimization of NeRF, we introduce sufficient geometry priors to enhance the generalizability of NeRF-MLP. Furthermore, we subtly connect the detection and NeRF branches through a shared MLP, enabling an efficient adaptation of NeRF to detection and yielding geometry-aware volumetric representations for 3D detection. Our method outperforms state-of-the-arts by 3.9 mAP and 3.1 mAP on the ScanNet and ARKITScenes benchmarks, respectively. We provide extensive analysis to shed light on how NeRF-Det works. As a result of our joint-training design, NeRF-Det is able to generalize well to unseen scenes for object detection, view synthesis, and depth estimation tasks without requiring per-scene optimization. Code is available at \url{https://github.com/facebookresearch/NeRF-Det}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_14620v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了NeRF Det，这是一种新的室内3D检测方法，以摆出的RGB图像为输入。与现有难以对场景几何建模的室内3D检测方法不同，我们的方法以端到端的方式新颖地使用NeRF来明确估计3D几何，从而提高3D检测性能。具体来说，为了避免与NeRF的每场景优化相关的显著额外延迟，我们引入了足够的几何先验来增强NeRF MLP的可推广性。此外，我们通过共享MLP巧妙地将检测和NeRF分支连接起来，使NeRF能够有效地适应检测，并产生用于3D检测的几何感知体积表示。在ScanNet和ARKITScenes基准测试中，我们的方法分别比现有技术高出3.9mAP和3.1mAP。我们提供了广泛的分析，以阐明NeRF Det是如何工作的。由于我们的联合训练设计，NeRF Det能够很好地推广到看不见的场景，用于对象检测、视图合成和深度估计任务，而无需对每个场景进行优化。代码位于\url{https://github.com/facebookresearch/NeRF-Det}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.14620v1" target="_blank">2307.14620v1</a>
                              </td>
                              <td>NeRF-Det: Learning Geometry-Aware Volumetric Representation for Multi-View 3D Object Detection</td>
                              <td>Chenfeng Xu</td>
                              <td>2023-07-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_14620v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.14620v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_08579v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Scale-Aware Modulation Meet Transformer</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_08579v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_08579v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_08579v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a new vision Transformer, Scale-Aware Modulation Transformer (SMT), that can handle various downstream tasks efficiently by combining the convolutional network and vision Transformer. The proposed Scale-Aware Modulation (SAM) in the SMT includes two primary novel designs. Firstly, we introduce the Multi-Head Mixed Convolution (MHMC) module, which can capture multi-scale features and expand the receptive field. Secondly, we propose the Scale-Aware Aggregation (SAA) module, which is lightweight but effective, enabling information fusion across different heads. By leveraging these two modules, convolutional modulation is further enhanced. Furthermore, in contrast to prior works that utilized modulations throughout all stages to build an attention-free network, we propose an Evolutionary Hybrid Network (EHN), which can effectively simulate the shift from capturing local to global dependencies as the network becomes deeper, resulting in superior performance. Extensive experiments demonstrate that SMT significantly outperforms existing state-of-the-art models across a wide range of visual tasks. Specifically, SMT with 11.5M / 2.4GFLOPs and 32M / 7.7GFLOPs can achieve 82.2% and 84.3% top-1 accuracy on ImageNet-1K, respectively. After pretrained on ImageNet-22K in 224^2 resolution, it attains 87.1% and 88.1% top-1 accuracy when finetuned with resolution 224^2 and 384^2, respectively. For object detection with Mask R-CNN, the SMT base trained with 1x and 3x schedule outperforms the Swin Transformer counterpart by 4.2 and 1.3 mAP on COCO, respectively. For semantic segmentation with UPerNet, the SMT base test at single- and multi-scale surpasses Swin by 2.0 and 1.1 mIoU respectively on the ADE20K.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_08579v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种新的视觉转换器——尺度感知调制转换器（SMT），它可以通过将卷积网络和视觉转换器相结合来有效地处理各种下游任务。SMT中提出的尺度感知调制（SAM）包括两种主要的新颖设计。首先，我们介绍了多头混合卷积（MHMC）模块，该模块可以捕捉多尺度特征并扩展感受野。其次，我们提出了规模感知聚合（SAA）模块，该模块重量轻但有效，能够实现不同头部的信息融合。通过利用这两个模块，卷积调制得到了进一步增强。此外，与之前在所有阶段都使用调制来构建无注意力网络的工作相比，我们提出了一种进化混合网络（EHN），它可以有效地模拟随着网络变得更深而从捕获局部依赖性到全局依赖性的转变，从而获得卓越的性能。大量实验表明，SMT在广泛的视觉任务中显著优于现有的最先进的模型。具体而言，具有11.5M/2.4GFLOP和32M/7.7GFLOP的SMT在ImageNet-1K上分别可以实现82.2%和84.3%的前1精度。在ImageNet-22K上以224^2分辨率进行预训练后，当以224^2和384^2分辨率进行微调时，它分别达到87.1%和88.1%的前1级精度。对于使用Mask R-CNN的对象检测，使用1x和3x调度训练的SMT基础在COCO上分别比Swin Transformer对应的基础高4.2和1.3mAP。对于UPerNet的语义分割，在ADE20K上，单尺度和多尺度的SMT基测试分别超过Swin 2.0和1.1mIoU。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.08579v2" target="_blank">2307.08579v2</a>
                              </td>
                              <td>Scale-Aware Modulation Meet Transformer</td>
                              <td>Weifeng Lin</td>
                              <td>2023-07-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_08579v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.08579v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_13974v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Tracking Anything in High Quality</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_13974v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_13974v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_13974v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual object tracking is a fundamental video task in computer vision. Recently, the notably increasing power of perception algorithms allows the unification of single/multiobject and box/mask-based tracking. Among them, the Segment Anything Model (SAM) attracts much attention. In this report, we propose HQTrack, a framework for High Quality Tracking anything in videos. HQTrack mainly consists of a video multi-object segmenter (VMOS) and a mask refiner (MR). Given the object to be tracked in the initial frame of a video, VMOS propagates the object masks to the current frame. The mask results at this stage are not accurate enough since VMOS is trained on several closeset video object segmentation (VOS) datasets, which has limited ability to generalize to complex and corner scenes. To further improve the quality of tracking masks, a pretrained MR model is employed to refine the tracking results. As a compelling testament to the effectiveness of our paradigm, without employing any tricks such as test-time data augmentations and model ensemble, HQTrack ranks the 2nd place in the Visual Object Tracking and Segmentation (VOTS2023) challenge. Code and models are available at https://github.com/jiawen-zhu/HQTrack.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_13974v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉对象跟踪是计算机视觉中的一项基本视频任务。最近，感知算法的能力显著增强，使得基于单/多对象和盒/掩模的跟踪得以统一。其中，分段任意模型（SAM）备受关注。在这份报告中，我们提出了HQTrack，这是一个高质量跟踪视频中任何内容的框架。HQTrack主要由视频多对象分割器（VMOS）和掩模细化器（MR）组成。给定要在视频的初始帧中跟踪的对象，VMOS将对象掩码传播到当前帧。这一阶段的掩模结果不够准确，因为VMOS是在几个闭集视频对象分割（VOS）数据集上训练的，其推广到复杂和角落场景的能力有限。为了进一步提高跟踪掩模的质量，采用预训练的MR模型来细化跟踪结果。作为我们范式有效性的有力证明，在没有使用任何技巧（如测试时间数据增强和模型集成）的情况下，HQTrack在视觉对象跟踪和分割（VOTS2023）挑战中排名第二。代码和型号可在https://github.com/jiawen-zhu/HQTrack.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.13974v1" target="_blank">2307.13974v1</a>
                              </td>
                              <td>Tracking Anything in High Quality</td>
                              <td>Jiawen Zhu</td>
                              <td>2023-07-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_13974v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.13974v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_13240v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fashion Matrix: Editing Photos by Just Talking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_13240v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_13240v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_13240v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The utilization of Large Language Models (LLMs) for the construction of AI systems has garnered significant attention across diverse fields. The extension of LLMs to the domain of fashion holds substantial commercial potential but also inherent challenges due to the intricate semantic interactions in fashion-related generation. To address this issue, we developed a hierarchical AI system called Fashion Matrix dedicated to editing photos by just talking. This system facilitates diverse prompt-driven tasks, encompassing garment or accessory replacement, recoloring, addition, and removal. Specifically, Fashion Matrix employs LLM as its foundational support and engages in iterative interactions with users. It employs a range of Semantic Segmentation Models (e.g., Grounded-SAM, MattingAnything, etc.) to delineate the specific editing masks based on user instructions. Subsequently, Visual Foundation Models (e.g., Stable Diffusion, ControlNet, etc.) are leveraged to generate edited images from text prompts and masks, thereby facilitating the automation of fashion editing processes. Experiments demonstrate the outstanding ability of Fashion Matrix to explores the collaborative potential of functionally diverse pre-trained models in the domain of fashion editing.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_13240v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）在人工智能系统构建中的应用已经在各个领域引起了极大的关注。LLM向时尚领域的扩展具有巨大的商业潜力，但由于时尚相关生成中复杂的语义交互，也带来了固有的挑战。为了解决这个问题，我们开发了一个名为Fashion Matrix的分层人工智能系统，专门通过聊天来编辑照片。该系统方便了各种即时驱动的任务，包括服装或配件更换、重新着色、添加和移除。具体而言，Fashion Matrix采用LLM作为其基础支持，并与用户进行迭代交互。它采用了一系列语义分割模型（例如，Grounded SAM、MattingAnything等）来根据用户指令描绘特定的编辑掩码。随后，利用Visual Foundation模型（例如，Stable Diffusion、ControlNet等）从文本提示和掩码生成编辑后的图像，从而促进时尚编辑过程的自动化。实验证明了Fashion Matrix在时尚编辑领域探索功能多样的预训练模特合作潜力的卓越能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.13240v1" target="_blank">2307.13240v1</a>
                              </td>
                              <td>Fashion Matrix: Editing Photos by Just Talking</td>
                              <td>Zheng Chong</td>
                              <td>2023-07-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_13240v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.13240v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_11768v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Question Decomposition Improves the Faithfulness of Model-Generated Reasoning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_11768v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_11768v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_11768v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>As large language models (LLMs) perform more difficult tasks, it becomes harder to verify the correctness and safety of their behavior. One approach to help with this issue is to prompt LLMs to externalize their reasoning, e.g., by having them generate step-by-step reasoning as they answer a question (Chain-of-Thought; CoT). The reasoning may enable us to check the process that models use to perform tasks. However, this approach relies on the stated reasoning faithfully reflecting the model's actual reasoning, which is not always the case. To improve over the faithfulness of CoT reasoning, we have models generate reasoning by decomposing questions into subquestions. Decomposition-based methods achieve strong performance on question-answering tasks, sometimes approaching that of CoT while improving the faithfulness of the model's stated reasoning on several recently-proposed metrics. By forcing the model to answer simpler subquestions in separate contexts, we greatly increase the faithfulness of model-generated reasoning over CoT, while still achieving some of the performance gains of CoT. Our results show it is possible to improve the faithfulness of model-generated reasoning; continued improvements may lead to reasoning that enables us to verify the correctness and safety of LLM behavior.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_11768v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着大型语言模型（LLM）执行更困难的任务，验证其行为的正确性和安全性变得更加困难。帮助解决这个问题的一种方法是促使LLM将他们的推理外部化，例如，让他们在回答问题时生成逐步推理（思维链；CoT）。推理可能使我们能够检查模型用于执行任务的过程。然而，这种方法依赖于所陈述的推理，忠实地反映了模型的实际推理，而事实并非总是如此。为了提高CoT推理的可信度，我们有模型通过将问题分解为子问题来生成推理。基于分解的方法在问答任务上取得了很强的性能，有时接近CoT的性能，同时提高了模型在最近提出的几个指标上所陈述推理的可信度。通过强迫模型在不同的上下文中回答更简单的子问题，我们大大提高了模型生成推理相对于CoT的忠实性，同时仍然实现了CoT的一些性能增益。我们的结果表明，提高模型生成推理的忠实性是可能的；持续的改进可能导致推理，使我们能够验证LLM行为的正确性和安全性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.11768v2" target="_blank">2307.11768v2</a>
                              </td>
                              <td>Question Decomposition Improves the Faithfulness of Model-Generated Reasoning</td>
                              <td>Ansh Radhakrishnan</td>
                              <td>2023-07-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_11768v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.11768v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_13159v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RoboChop: Autonomous Framework for Fruit and Vegetable Chopping Leveraging Foundational Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_13159v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_13159v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_13159v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With the goal of developing fully autonomous cooking robots, developing robust systems that can chop a wide variety of objects is important. Existing approaches focus primarily on the low-level dynamics of the cutting action, which overlooks some of the practical real-world challenges of implementing autonomous cutting systems. In this work we propose an autonomous framework to sequence together action primitives for the purpose of chopping fruits and vegetables on a cluttered cutting board. We present a novel technique to leverage vision foundational models SAM and YOLO to accurately detect, segment, and track fruits and vegetables as they visually change through the sequences of chops, finetuning YOLO on a novel dataset of whole and chopped fruits and vegetables. In our experiments, we demonstrate that our simple pipeline is able to reliably chop a variety of fruits and vegetables ranging in size, appearance, and texture, meeting a variety of chopping specifications, including fruit type, number of slices, and types of slices.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_13159v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>为了开发完全自主的烹饪机器人，开发能够切割各种物体的鲁棒系统非常重要。现有的方法主要关注切割动作的低水平动态，忽略了实现自主切割系统的一些现实挑战。在这项工作中，我们提出了一个自主框架，将动作原语排列在一起，以在杂乱的砧板上切水果和蔬菜。我们提出了一种新技术，利用视觉基础模型SAM和YOLO来准确检测、分割和跟踪水果和蔬菜在排骨序列中的视觉变化，在完整和切碎的水果和蔬菜的新数据集上微调YOLO。在我们的实验中，我们证明了我们的简单管道能够可靠地切碎各种大小、外观和质地的水果和蔬菜，满足各种切碎规范，包括水果类型、切片数量和切片类型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.13159v1" target="_blank">2307.13159v1</a>
                              </td>
                              <td>RoboChop: Autonomous Framework for Fruit and Vegetable Chopping Leveraging Foundational Models</td>
                              <td>Atharva Dikshit</td>
                              <td>2023-07-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_13159v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.13159v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_12674v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Industrial Segment Anything -- a Case Study in Aircraft Manufacturing, Intralogistics, Maintenance, Repair, and Overhaul</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_12674v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_12674v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_12674v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Deploying deep learning-based applications in specialized domains like the aircraft production industry typically suffers from the training data availability problem. Only a few datasets represent non-everyday objects, situations, and tasks. Recent advantages in research around Vision Foundation Models (VFM) opened a new area of tasks and models with high generalization capabilities in non-semantic and semantic predictions. As recently demonstrated by the Segment Anything Project, exploiting VFM's zero-shot capabilities is a promising direction in tackling the boundaries spanned by data, context, and sensor variety. Although, investigating its application within specific domains is subject to ongoing research. This paper contributes here by surveying applications of the SAM in aircraft production-specific use cases. We include manufacturing, intralogistics, as well as maintenance, repair, and overhaul processes, also representing a variety of other neighboring industrial domains. Besides presenting the various use cases, we further discuss the injection of domain knowledge.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_12674v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在飞机生产行业等专业领域部署基于深度学习的应用程序通常会遇到训练数据可用性问题。只有少数数据集表示非日常对象、情况和任务。最近围绕视觉基础模型（VFM）的研究优势开辟了一个新的任务和模型领域，该领域在非语义和语义预测方面具有较高的泛化能力。正如Segment Anything Project最近所证明的那样，利用VFM的零样本功能是解决数据、上下文和传感器多样性所跨越的边界的一个很有前途的方向。尽管如此，研究其在特定领域的应用仍有待于不断的研究。本文通过调查SAM在飞机生产特定用例中的应用做出了贡献。我们包括制造、内部物流以及维护、维修和大修流程，也代表了其他各种邻近的工业领域。除了介绍各种用例外，我们还进一步讨论了领域知识的注入。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.12674v1" target="_blank">2307.12674v1</a>
                              </td>
                              <td>Industrial Segment Anything -- a Case Study in Aircraft Manufacturing, Intralogistics, Maintenance, Repair, and Overhaul</td>
                              <td>Keno Moenck</td>
                              <td>2023-07-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_12674v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.12674v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_02745v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Analysis of Dynamic Voronoi Diagrams in the Hilbert Metric</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_02745v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_02745v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_02745v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Hilbert metric is a projective metric defined on a convex body which generalizes the Cayley-Klein model of hyperbolic geometry to any convex set. In this paper we analyze Hilbert Voronoi diagrams in the Dynamic setting. In addition we introduce dynamic visualization software for Voronoi diagrams in the Hilbert metric on user specified convex polygons.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_02745v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Hilbert度量是定义在凸体上的投影度量，它将双曲几何的Cayley-Klein模型推广到任何凸集。本文分析了动态环境下的Hilbert-Voronoi图。此外，我们还介绍了用户指定凸多边形上Hilbert度量中Voronoi图的动态可视化软件。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.02745v3" target="_blank">2304.02745v3</a>
                              </td>
                              <td>Analysis of Dynamic Voronoi Diagrams in the Hilbert Metric</td>
                              <td>Madeline Bumpus</td>
                              <td>2023-04-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_02745v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.02745v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_11783v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A novel integrated method of detection-grasping for specific object based on the box coordinate matching</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_11783v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_11783v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_11783v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>To better care for the elderly and disabled, it is essential for service robots to have an effective fusion method of object detection and grasp estimation. However, limited research has been observed on the combination of object detection and grasp estimation. To overcome this technical difficulty, a novel integrated method of detection-grasping for specific object based on the box coordinate matching is proposed in this paper. Firstly, the SOLOv2 instance segmentation model is improved by adding channel attention module (CAM) and spatial attention module (SAM). Then, the atrous spatial pyramid pooling (ASPP) and CAM are added to the generative residual convolutional neural network (GR-CNN) model to optimize grasp estimation. Furthermore, a detection-grasping integrated algorithm based on box coordinate matching (DG-BCM) is proposed to obtain the fusion model of object detection and grasp estimation. For verification, experiments on object detection and grasp estimation are conducted separately to verify the superiority of improved models. Additionally, grasping tasks for several specific objects are implemented on a simulation platform, demonstrating the feasibility and effectiveness of DG-BCM algorithm proposed in this paper.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_11783v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>为了更好地照顾老年人和残疾人，服务机器人必须有一种有效的目标检测和抓取估计的融合方法。然而，对物体检测和抓取估计相结合的研究有限。为了克服这一技术难点，本文提出了一种基于盒坐标匹配的特定物体检测抓取集成方法。首先，通过添加通道注意力模块（CAM）和空间注意力模块（SAM）对SOLOv2实例分割模型进行了改进。然后，将萎缩空间金字塔池（ASPP）和CAM添加到生成残差卷积神经网络（GR-CNN）模型中，以优化抓取估计。此外，提出了一种基于盒坐标匹配的检测-抓取集成算法（DG-BCM），以获得目标检测和抓取估计的融合模型。为了验证，分别进行了物体检测和抓取估计实验，以验证改进模型的优越性。此外，在仿真平台上实现了对几个特定对象的抓取任务，验证了本文提出的DG-BCM算法的可行性和有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.11783v1" target="_blank">2307.11783v1</a>
                              </td>
                              <td>A novel integrated method of detection-grasping for specific object based on the box coordinate matching</td>
                              <td>Zongmin Liu</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_11783v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.11783v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_00362v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Kernelization for Finding Lineal Topologies (Depth-First Spanning Trees) with Many or Few Leaves</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_00362v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_00362v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_00362v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>For a given graph $G$, a depth-first search (DFS) tree $T$ of $G$ is an $r$-rooted spanning tree such that every edge of $G$ is either an edge of $T$ or is between a \textit{descendant} and an \textit{ancestor} in $T$. A graph $G$ together with a DFS tree is called a \textit{lineal topology} $\mathcal{T} = (G, r, T)$. Sam et al. (2023) initiated study of the parameterized complexity of the \textsc{Min-LLT} and \textsc{Max-LLT} problems which ask, given a graph $G$ and an integer $k\geq 0$, whether $G$ has a DFS tree with at most $k$ and at least $k$ leaves, respectively. Particularly, they showed that for the dual parameterization, where the tasks are to find DFS trees with at least $n-k$ and at most $n-k$ leaves, respectively, these problems are fixed-parameter tractable when parameterized by $k$. However, the proofs were based on Courcelle's theorem, thereby making the running times a tower of exponentials. We prove that both problems admit polynomial kernels with $\Oh(k^3)$ vertices. In particular, this implies FPT algorithms running in $k^{\Oh(k)}\cdot n^{O(1)}$ time. We achieve these results by making use of a $\Oh(k)$-sized vertex cover structure associated with each problem. This also allows us to demonstrate polynomial kernels for \textsc{Min-LLT} and \textsc{Max-LLT} for the structural parameterization by the vertex cover number.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_00362v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对于给定的图$G$，$G$的深度优先搜索（DFS）树$T$是一个$r$根的生成树，使得$G$中的每条边要么是$T$的边，要么在$T$中的\textit｛subscendance｝和\textit{祖先｝之间。图形$G$与DFS树一起称为\textit｛linear topology｝$\mathcal｛T｝=（G，r，T）$。Sam等人（2023）开始研究\textsc｛Min LLT｝和\textsc｝Max LLT问题的参数化复杂性，这些问题询问，给定图$G$和整数$k\geq 0$，$G$是否具有分别具有最多$k$和至少$k$叶的DFS树。特别地，他们表明，对于对偶参数化，其中任务是分别找到叶数至少为$n-k$和最多为$n-k$的DFS树，当用$k$参数化时，这些问题是固定参数可处理的。然而，这些证明是基于Courcelle定理，从而使运行时间成为指数塔。我们证明了这两个问题都允许具有$\Oh（k^3）$顶点的多项式核。特别是，这意味着FPT算法在$k^｛\Oh（k）｝\cdot n^｛O（1）｝$时间内运行。我们通过使用与每个问题相关的$\Oh（k）$大小的顶点覆盖结构来实现这些结果。这也使我们能够证明\textsc｛Min LLT｝和\textsc｝的多项式核，用于通过顶点覆盖数进行结构参数化。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.00362v2" target="_blank">2307.00362v2</a>
                              </td>
                              <td>Kernelization for Finding Lineal Topologies (Depth-First Spanning Trees) with Many or Few Leaves</td>
                              <td>Emmanuel Sam</td>
                              <td>2023-07-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_00362v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.00362v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10515v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Gaussian Partial Information Decomposition: Bias Correction and Application to High-dimensional Data</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10515v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10515v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10515v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in neuroscientific experimental techniques have enabled us to simultaneously record the activity of thousands of neurons across multiple brain regions. This has led to a growing need for computational tools capable of analyzing how task-relevant information is represented and communicated between several brain regions. Partial information decompositions (PIDs) have emerged as one such tool, quantifying how much unique, redundant and synergistic information two or more brain regions carry about a task-relevant message. However, computing PIDs is computationally challenging in practice, and statistical issues such as the bias and variance of estimates remain largely unexplored. In this paper, we propose a new method for efficiently computing and estimating a PID definition on multivariate Gaussian distributions. We show empirically that our method satisfies an intuitive additivity property, and recovers the ground truth in a battery of canonical examples, even at high dimensionality. We also propose and evaluate, for the first time, a method to correct the bias in PID estimates at finite sample sizes. Finally, we demonstrate that our Gaussian PID effectively characterizes inter-areal interactions in the mouse brain, revealing higher redundancy between visual areas when a stimulus is behaviorally relevant.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10515v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经科学实验技术的最新进展使我们能够同时记录大脑多个区域数千个神经元的活动。这导致人们越来越需要能够分析任务相关信息如何在几个大脑区域之间表示和交流的计算工具。部分信息分解（PID）已经成为一种这样的工具，可以量化两个或多个大脑区域在任务相关信息中携带的独特、冗余和协同信息。然而，计算PID在实践中在计算上具有挑战性，并且估计的偏差和方差等统计问题在很大程度上仍未得到探索。在本文中，我们提出了一种新的方法来有效地计算和估计多元高斯分布上的PID定义。我们从经验上证明，我们的方法满足直观的可加性性质，并在一组典型例子中恢复了基本事实，即使在高维下也是如此。我们还首次提出并评估了一种在有限样本量下校正PID估计偏差的方法。最后，我们证明了我们的高斯PID有效地表征了小鼠大脑中区域间的相互作用，当刺激与行为相关时，显示了视觉区域之间更高的冗余度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10515v1" target="_blank">2307.10515v1</a>
                              </td>
                              <td>Gaussian Partial Information Decomposition: Bias Correction and Application to High-dimensional Data</td>
                              <td>Praveen Venkatesh</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10515v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10515v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09874v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Agricultural Robotic System: The Automation of Detection and Speech Control</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09874v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09874v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09874v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Agriculture industries often face challenges in manual tasks such as planting, harvesting, fertilizing, and detection, which can be time consuming and prone to errors. The "Agricultural Robotic System" project addresses these issues through a modular design that integrates advanced visual, speech recognition, and robotic technologies. This system is comprised of separate but interconnected modules for vision detection and speech recognition, creating a flexible and adaptable solution. The vision detection module uses computer vision techniques, trained on YOLOv5 and deployed on the Jetson Nano in TensorRT format, to accurately detect and identify different items. A robotic arm module then precisely controls the picking up of seedlings or seeds, and arranges them in specific locations. The speech recognition module enhances intelligent human robot interaction, allowing for efficient and intuitive control of the system. This modular approach improves the efficiency and accuracy of agricultural tasks, demonstrating the potential of robotics in the agricultural industry.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09874v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>农业行业在种植、收割、施肥和检测等手动任务中经常面临挑战，这些任务可能耗时且容易出错。“农业机器人系统”项目通过集成先进视觉、语音识别和机器人技术的模块化设计来解决这些问题。该系统由用于视觉检测和语音识别的独立但互连的模块组成，创造了一个灵活且适应性强的解决方案。视觉检测模块使用计算机视觉技术，在YOLOv5上进行训练，并以TensorRT格式部署在Jetson Nano上，以准确检测和识别不同的物品。然后，一个机械臂模块精确控制幼苗或种子的拾取，并将它们安排在特定的位置。语音识别模块增强了智能人机交互，实现了对系统的高效直观控制。这种模块化方法提高了农业任务的效率和准确性，展示了机器人在农业行业的潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09874v1" target="_blank">2307.09874v1</a>
                              </td>
                              <td>Agricultural Robotic System: The Automation of Detection and Speech Control</td>
                              <td>Yang Wenkai</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09874v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09874v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07873v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Why Does Little Robustness Help? Understanding Adversarial Transferability From Surrogate Training</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07873v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07873v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07873v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Adversarial examples (AEs) for DNNs have been shown to be transferable: AEs that successfully fool white-box surrogate models can also deceive other black-box models with different architectures. Although a bunch of empirical studies have provided guidance on generating highly transferable AEs, many of these findings lack explanations and even lead to inconsistent advice. In this paper, we take a further step towards understanding adversarial transferability, with a particular focus on surrogate aspects. Starting from the intriguing little robustness phenomenon, where models adversarially trained with mildly perturbed adversarial samples can serve as better surrogates, we attribute it to a trade-off between two predominant factors: model smoothness and gradient similarity. Our investigations focus on their joint effects, rather than their separate correlations with transferability. Through a series of theoretical and empirical analyses, we conjecture that the data distribution shift in adversarial training explains the degradation of gradient similarity. Building on these insights, we explore the impacts of data augmentation and gradient regularization on transferability and identify that the trade-off generally exists in the various training mechanisms, thus building a comprehensive blueprint for the regulation mechanism behind transferability. Finally, we provide a general route for constructing better surrogates to boost transferability which optimizes both model smoothness and gradient similarity simultaneously, e.g., the combination of input gradient regularization and sharpness-aware minimization (SAM), validated by extensive experiments. In summary, we call for attention to the united impacts of these two factors for launching effective transfer attacks, rather than optimizing one while ignoring the other, and emphasize the crucial role of manipulating surrogate models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07873v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>DNN的对抗性例子（AE）已被证明是可转移的：成功欺骗白盒代理模型的AE也可以欺骗其他具有不同架构的黑盒模型。尽管一系列实证研究为产生高度可转移的AE提供了指导，但其中许多发现缺乏解释，甚至导致建议不一致。在本文中，我们朝着理解对抗性可转移性迈出了进一步的一步，特别关注代理方面。从有趣的小鲁棒性现象开始，用轻度扰动的对抗性样本进行对抗性训练的模型可以作为更好的替代品，我们将其归因于两个主要因素之间的权衡：模型平滑性和梯度相似性。我们的研究重点是它们的联合效应，而不是它们与可转移性的单独相关性。通过一系列理论和实证分析，我们推测对抗性训练中的数据分布变化解释了梯度相似性的退化。基于这些见解，我们探索了数据扩充和梯度正则化对可转移性的影响，并确定了这种权衡通常存在于各种训练机制中，从而为可转移性背后的调节机制构建了一个全面的蓝图。最后，我们提供了一种构建更好的替代物以提高可转移性的通用途径，该途径同时优化了模型的平滑性和梯度相似性，例如，输入梯度正则化和清晰度感知最小化（SAM）的组合，并通过大量实验进行了验证。总之，我们呼吁关注这两个因素对发起有效转移攻击的联合影响，而不是优化其中一个而忽略另一个，并强调操纵代理模型的关键作用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07873v2" target="_blank">2307.07873v2</a>
                              </td>
                              <td>Why Does Little Robustness Help? Understanding Adversarial Transferability From Surrogate Training</td>
                              <td>Yechao Zhang</td>
                              <td>2023-07-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07873v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07873v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09727v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAMConvex: Fast Discrete Optimization for CT Registration using Self-supervised Anatomical Embedding and Correlation Pyramid</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09727v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09727v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09727v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Estimating displacement vector field via a cost volume computed in the feature space has shown great success in image registration, but it suffers excessive computation burdens. Moreover, existing feature descriptors only extract local features incapable of representing the global semantic information, which is especially important for solving large transformations. To address the discussed issues, we propose SAMConvex, a fast coarse-to-fine discrete optimization method for CT registration that includes a decoupled convex optimization procedure to obtain deformation fields based on a self-supervised anatomical embedding (SAM) feature extractor that captures both local and global information. To be specific, SAMConvex extracts per-voxel features and builds 6D correlation volumes based on SAM features, and iteratively updates a flow field by performing lookups on the correlation volumes with a coarse-to-fine scheme. SAMConvex outperforms the state-of-the-art learning-based methods and optimization-based methods over two inter-patient registration datasets (Abdomen CT and HeadNeck CT) and one intra-patient registration dataset (Lung CT). Moreover, as an optimization-based method, SAMConvex only takes $\sim2$s ($\sim5s$ with instance optimization) for one paired images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09727v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>通过在特征空间中计算的代价体积来估计位移矢量场在图像配准中取得了巨大成功，但它承受了过多的计算负担。此外，现有的特征描述符只提取无法表示全局语义信息的局部特征，这对于解决大型变换尤为重要。为了解决所讨论的问题，我们提出了SAM凸面，这是一种用于CT配准的快速粗到细离散优化方法，包括一个解耦的凸优化过程，以基于捕获局部和全局信息的自监督解剖嵌入（SAM）特征提取器来获得变形场。具体而言，SAM凸面提取每个体素的特征，并基于SAM特征构建6D相关体积，并通过使用从粗到细的方案对相关体积执行查找来迭代更新流场。在两个患者间配准数据集（腹部CT和头颈部CT）和一个患者内配准数据集中（肺部CT），SAM凸面优于最先进的基于学习的方法和基于优化的方法。此外，作为一种基于优化的方法，SAM凸面对一对图像只需要$\sim2$s（带有实例优化的$\sim5s$）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09727v1" target="_blank">2307.09727v1</a>
                              </td>
                              <td>SAMConvex: Fast Discrete Optimization for CT Registration using Self-supervised Anatomical Embedding and Correlation Pyramid</td>
                              <td>Zi Li</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09727v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09727v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09701v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Efficiency Pentathlon: A Standardized Arena for Efficiency Evaluation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09701v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09701v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09701v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Rising computational demands of modern natural language processing (NLP) systems have increased the barrier to entry for cutting-edge research while posing serious environmental concerns. Yet, progress on model efficiency has been impeded by practical challenges in model evaluation and comparison. For example, hardware is challenging to control due to disparate levels of accessibility across different institutions. Moreover, improvements in metrics such as FLOPs often fail to translate to progress in real-world applications. In response, we introduce Pentathlon, a benchmark for holistic and realistic evaluation of model efficiency. Pentathlon focuses on inference, which accounts for a majority of the compute in a model's lifecycle. It offers a strictly-controlled hardware platform, and is designed to mirror real-world applications scenarios. It incorporates a suite of metrics that target different aspects of efficiency, including latency, throughput, memory overhead, and energy consumption. Pentathlon also comes with a software library that can be seamlessly integrated into any codebase and enable evaluation. As a standardized and centralized evaluation platform, Pentathlon can drastically reduce the workload to make fair and reproducible efficiency comparisons. While initially focused on natural language processing (NLP) models, Pentathlon is designed to allow flexible extension to other fields. We envision Pentathlon will stimulate algorithmic innovations in building efficient models, and foster an increased awareness of the social and environmental implications in the development of future-generation NLP models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09701v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现代自然语言处理（NLP）系统日益增长的计算需求增加了尖端研究的进入壁垒，同时也带来了严重的环境问题。然而，模型评估和比较方面的实际挑战阻碍了模型效率方面的进展。例如，由于不同机构的可访问性水平不同，硬件很难控制。此外，FLOP等指标的改进往往无法转化为现实应用程序的进步。作为回应，我们介绍了五项，这是一个对模型效率进行全面和现实评估的基准。Pentathlon专注于推理，推理在模型生命周期中占计算的大部分。它提供了一个严格控制的硬件平台，旨在反映真实世界的应用场景。它包含了一套针对效率不同方面的指标，包括延迟、吞吐量、内存开销和能耗。Pentathlon还配备了一个软件库，可以无缝集成到任何代码库中并进行评估。五项作为一个标准化、集中化的评估平台，可以大幅减少工作量，进行公平、可重复的效率比较。虽然最初专注于自然语言处理（NLP）模型，但Pentathlon的设计允许灵活扩展到其他领域。我们设想五项运动将在构建高效模型方面激发算法创新，并在开发未来一代NLP模型时提高对社会和环境影响的认识。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09701v1" target="_blank">2307.09701v1</a>
                              </td>
                              <td>Efficiency Pentathlon: A Standardized Arena for Efficiency Evaluation</td>
                              <td>Hao Peng</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09701v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09701v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09383v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Soundly Handling Linearity</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09383v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09383v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09383v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a novel approach to soundly combining linear types with effect handlers. Linear type systems statically ensure that resources such as file handles are used exactly once. Effect handlers provide a modular programming abstraction for implementing features ranging from exceptions to concurrency. Whereas linear type systems bake in the assumption that continuations are invoked exactly once, effect handlers allow continuations to be discarded or invoked more than once. This mismatch leads to soundness bugs in existing systems such as the programming language Links, which combines linearity (for session types) with effect handlers. We introduce control flow linearity as a means to ensure that continuations are used in accordance with the linearity of any resources they capture, ruling out such soundness bugs.   We formalise control flow linearity in a System F-style core calculus Feffpop equipped with linear types, effect types, and effect handlers. We define a linearity-aware semantics to formally prove that Feffpop preserves the integrity of linear values in the sense that no linear value is discarded or duplicated. In order to show that control flow linearity can be made practical, we adapt Links based on the design of Feffpop, in doing so fixing a long-standing soundness bug.   Finally, to better expose the potential of control flow linearity, we define an ML-style core calculus Qeffpop, based on qualified types, which requires no programmer provided annotations, and instead relies entirely on type inference to infer control flow linearity. Both linearity and effects are captured by qualified types. Qeffpop overcomes a number of practical limitations of Feffpop, supporting abstraction over linearity, linearity dependencies between type variables, and a much more fine-grained notion of control flow linearity.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09383v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种将线性类型与效果处理程序完美结合的新方法。线性类型系统静态地确保文件句柄等资源只使用一次。效果处理程序提供了一个模块化编程抽象，用于实现从异常到并发的各种功能。线性类型系统是在假设连续性只被调用一次的情况下烘焙的，而效果处理程序允许连续性被丢弃或多次调用。这种不匹配导致了现有系统中的健全性错误，例如编程语言Links，它将线性（针对会话类型）与效果处理程序相结合。我们引入控制流线性作为一种手段，以确保根据所捕获的任何资源的线性来使用连续性，从而排除此类健全性错误。我们在系统F型核心演算Feffpop中正式化了控制流线性，该演算配备了线性类型、效果类型和效果处理程序。我们定义了一个线性感知语义，以正式证明Feffpop在没有线性值被丢弃或重复的意义上保持了线性值的完整性。为了表明控制流线性可以变得实用，我们在Feffpop的设计基础上对Links进行了调整，从而修复了一个长期存在的健全性缺陷。最后，为了更好地揭示控制流线性的潜力，我们基于合格类型定义了ML风格的核心演算Qeffpop，它不需要程序员提供的注释，而是完全依赖类型推理来推断控制流线性。线性和效果都由合格的类型捕获。Qeffpop克服了Feffpop的许多实际限制，支持对线性的抽象、类型变量之间的线性依赖性，以及更细粒度的控制流线性概念。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09383v1" target="_blank">2307.09383v1</a>
                              </td>
                              <td>Soundly Handling Linearity</td>
                              <td>Wenhao Tang</td>
                              <td>2023-07-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09383v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09383v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="CLIP"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2305_13501v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LaDI-VTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_13501v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_13501v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_13501v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The rapidly evolving fields of e-commerce and metaverse continue to seek innovative approaches to enhance the consumer experience. At the same time, recent advancements in the development of diffusion models have enabled generative networks to create remarkably realistic images. In this context, image-based virtual try-on, which consists in generating a novel image of a target model wearing a given in-shop garment, has yet to capitalize on the potential of these powerful generative solutions. This work introduces LaDI-VTON, the first Latent Diffusion textual Inversion-enhanced model for the Virtual Try-ON task. The proposed architecture relies on a latent diffusion model extended with a novel additional autoencoder module that exploits learnable skip connections to enhance the generation process preserving the model's characteristics. To effectively maintain the texture and details of the in-shop garment, we propose a textual inversion component that can map the visual features of the garment to the CLIP token embedding space and thus generate a set of pseudo-word token embeddings capable of conditioning the generation process. Experimental results on Dress Code and VITON-HD datasets demonstrate that our approach outperforms the competitors by a consistent margin, achieving a significant milestone for the task. Source code and trained models are publicly available at: https://github.com/miccunifi/ladi-vton.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_13501v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>快速发展的电子商务和元宇宙领域继续寻求创新的方法来增强消费者体验。与此同时，扩散模型发展的最新进展使生成网络能够创造出非常逼真的图像。在这种情况下，基于图像的虚拟试穿，包括生成穿着给定店内服装的目标模特的新图像，尚未利用这些强大的生成解决方案的潜力。这项工作介绍了LaDI VTON，这是第一个用于虚拟Try-ON任务的潜在扩散文本反转增强模型。所提出的体系结构依赖于一个潜在扩散模型，该模型扩展了一个新的附加自动编码器模块，该模块利用可学习的跳跃连接来增强生成过程，从而保持模型的特性。为了有效地维护店内服装的纹理和细节，我们提出了一种文本反转组件，该组件可以将服装的视觉特征映射到CLIP令牌嵌入空间，从而生成一组能够调节生成过程的伪单词令牌嵌入。在Dress Code和VITON-HD数据集上的实验结果表明，我们的方法以一致的优势优于竞争对手，实现了任务的一个重要里程碑。源代码和经过训练的模型可在以下网站上公开获取：https://github.com/miccunifi/ladi-vton.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.13501v3" target="_blank">2305.13501v3</a>
                              </td>
                              <td>LaDI-VTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On</td>
                              <td>Davide Morelli</td>
                              <td>2023-05-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_13501v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.13501v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_12678v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Uni-Fusion: Universal Continuous Mapping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_12678v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_12678v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_12678v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present Uni-Fusion, a universal continuous mapping framework for surfaces, surface properties (color, infrared, etc.) and more (latent features in CLIP embedding space, etc.). We propose the first universal implicit encoding model that supports encoding of both geometry and different types of properties (RGB, infrared, features, etc.) without requiring any training. Based on this, our framework divides the point cloud into regular grid voxels and generates a latent feature in each voxel to form a Latent Implicit Map (LIM) for geometries and arbitrary properties. Then, by fusing a local LIM frame-wisely into a global LIM, an incremental reconstruction is achieved. Encoded with corresponding types of data, our Latent Implicit Map is capable of generating continuous surfaces, surface property fields, surface feature fields, and all other possible options. To demonstrate the capabilities of our model, we implement three applications: (1) incremental reconstruction for surfaces and color (2) 2D-to-3D transfer of fabricated properties (3) open-vocabulary scene understanding by creating a text CLIP feature field on surfaces. We evaluate Uni-Fusion by comparing it in corresponding applications, from which Uni-Fusion shows high-flexibility in various applications while performing best or being competitive. The project page of Uni-Fusion is available at https://jarrome.github.io/Uni-Fusion/ .</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_12678v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了Uni-Fusion，这是一个适用于表面、表面属性（颜色、红外等）等（CLIP嵌入空间中的潜在特征等）的通用连续映射框架。我们提出了第一个通用隐式编码模型，该模型支持对几何体和不同类型的属性（RGB、红外、特征等）进行编码，而无需任何训练。基于此，我们的框架将点云划分为规则网格体素，并在每个体素中生成一个潜在特征，以形成几何图形和任意属性的潜在隐式映射（LIM）。然后，通过将局部LIM帧明智地融合为全局LIM，实现了增量重建。用相应类型的数据编码，我们的潜在隐式映射能够生成连续曲面、曲面属性字段、曲面特征字段和所有其他可能的选项。为了证明我们模型的能力，我们实现了三个应用程序：（1）表面和颜色的增量重建（2）制造属性的二维到三维转换（3）通过在表面上创建文本CLIP特征字段来理解开放词汇场景。我们通过在相应应用中进行比较来评估Uni-Fusion，从中可以看出，Uni-FFusion在各种应用中表现出很高的灵活性，同时表现最好或具有竞争力。Uni Fusion的项目页面可在https://jarrome.github.io/Uni-Fusion/。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.12678v2" target="_blank">2303.12678v2</a>
                              </td>
                              <td>Uni-Fusion: Universal Continuous Mapping</td>
                              <td>Yijun Yuan</td>
                              <td>2023-03-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_12678v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.12678v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01655v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DiffColor: Toward High Fidelity Text-Guided Image Colorization with Diffusion Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01655v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01655v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01655v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent data-driven image colorization methods have enabled automatic or reference-based colorization, while still suffering from unsatisfactory and inaccurate object-level color control. To address these issues, we propose a new method called DiffColor that leverages the power of pre-trained diffusion models to recover vivid colors conditioned on a prompt text, without any additional inputs. DiffColor mainly contains two stages: colorization with generative color prior and in-context controllable colorization. Specifically, we first fine-tune a pre-trained text-to-image model to generate colorized images using a CLIP-based contrastive loss. Then we try to obtain an optimized text embedding aligning the colorized image and the text prompt, and a fine-tuned diffusion model enabling high-quality image reconstruction. Our method can produce vivid and diverse colors with a few iterations, and keep the structure and background intact while having colors well-aligned with the target language guidance. Moreover, our method allows for in-context colorization, i.e., producing different colorization results by modifying prompt texts without any fine-tuning, and can achieve object-level controllable colorization results. Extensive experiments and user studies demonstrate that DiffColor outperforms previous works in terms of visual quality, color fidelity, and diversity of colorization options.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01655v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近的数据驱动图像彩色化方法已经实现了自动或基于参考的彩色化，同时仍然存在不令人满意和不准确的对象级颜色控制的问题。为了解决这些问题，我们提出了一种称为DiffColor的新方法，该方法利用预先训练的扩散模型的能力，在没有任何额外输入的情况下，恢复以提示文本为条件的生动颜色。DiffColor主要包括两个阶段：生成颜色先验的着色和上下文可控的着色。具体来说，我们首先使用基于CLIP的对比损失来微调预先训练的文本到图像模型，以生成彩色图像。然后，我们试图获得一个优化的文本嵌入，将彩色图像和文本提示对齐，并获得一个微调的扩散模型，以实现高质量的图像重建。我们的方法可以通过几次迭代产生生动多样的颜色，并保持结构和背景的完整性，同时使颜色与目标语言指导保持一致。此外，我们的方法允许上下文中的着色，即通过修改提示文本而不进行任何微调来产生不同的着色结果，并且可以实现对象级可控的着色结果。大量的实验和用户研究表明，DiffColor在视觉质量、色彩保真度和着色选项的多样性方面优于以前的作品。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01655v1" target="_blank">2308.01655v1</a>
                              </td>
                              <td>DiffColor: Toward High Fidelity Text-Guided Image Colorization with Diffusion Models</td>
                              <td>Jianxin Lin</td>
                              <td>2023-08-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01655v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01655v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_12726v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Explainable In-the-Wild Video Quality Assessment: A Database and a Language-Prompted Approach</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_12726v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_12726v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_12726v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The proliferation of in-the-wild videos has greatly expanded the Video Quality Assessment (VQA) problem. Unlike early definitions that usually focus on limited distortion types, VQA on in-the-wild videos is especially challenging as it could be affected by complicated factors, including various distortions and diverse contents. Though subjective studies have collected overall quality scores for these videos, how the abstract quality scores relate with specific factors is still obscure, hindering VQA methods from more concrete quality evaluations (e.g. sharpness of a video). To solve this problem, we collect over two million opinions on 4,543 in-the-wild videos on 13 dimensions of quality-related factors, including in-capture authentic distortions (e.g. motion blur, noise, flicker), errors introduced by compression and transmission, and higher-level experiences on semantic contents and aesthetic issues (e.g. composition, camera trajectory), to establish the multi-dimensional Maxwell database. Specifically, we ask the subjects to label among a positive, a negative, and a neutral choice for each dimension. These explanation-level opinions allow us to measure the relationships between specific quality factors and abstract subjective quality ratings, and to benchmark different categories of VQA algorithms on each dimension, so as to more comprehensively analyze their strengths and weaknesses. Furthermore, we propose the MaxVQA, a language-prompted VQA approach that modifies vision-language foundation model CLIP to better capture important quality issues as observed in our analyses. The MaxVQA can jointly evaluate various specific quality factors and final quality scores with state-of-the-art accuracy on all dimensions, and superb generalization ability on existing datasets. Code and data available at https://github.com/VQAssessment/MaxVQA.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_12726v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>野生视频的激增极大地扩展了视频质量评估（VQA）问题。与通常关注有限失真类型的早期定义不同，野生视频上的VQA尤其具有挑战性，因为它可能受到复杂因素的影响，包括各种失真和不同的内容。尽管主观研究已经收集了这些视频的总体质量分数，但抽象质量分数与特定因素的关系仍然不清楚，阻碍了VQA方法进行更具体的质量评估（例如视频的清晰度）。为了解决这个问题，我们收集了超过200万条关于4543个野生视频的意见，涉及13个维度的质量相关因素，包括捕获中的真实失真（如运动模糊、噪声、闪烁）、压缩和传输引入的错误，以及语义内容和美学问题（如构图、相机轨迹）的更高层次体验，建立多维Maxwell数据库。具体来说，我们要求受试者在每个维度上进行积极、消极和中立的选择。这些解释层面的意见使我们能够衡量特定质量因素与抽象主观质量评级之间的关系，并在每个维度上对不同类别的VQA算法进行基准测试，从而更全面地分析它们的优缺点。此外，我们提出了MaxVQA，这是一种语言提示的VQA方法，它修改了视觉语言基础模型CLIP，以更好地捕捉我们分析中观察到的重要质量问题。MaxVQA可以联合评估各种特定的质量因素和最终质量分数，在所有维度上都具有最先进的准确性，并在现有数据集上具有卓越的泛化能力。代码和数据可在https://github.com/VQAssessment/MaxVQA.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.12726v2" target="_blank">2305.12726v2</a>
                              </td>
                              <td>Towards Explainable In-the-Wild Video Quality Assessment: A Database and a Language-Prompted Approach</td>
                              <td>Haoning Wu</td>
                              <td>2023-05-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_12726v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.12726v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01618v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Survey on Deep Learning-based Spatio-temporal Action Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01618v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01618v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01618v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Spatio-temporal action detection (STAD) aims to classify the actions present in a video and localize them in space and time. It has become a particularly active area of research in computer vision because of its explosively emerging real-world applications, such as autonomous driving, visual surveillance, entertainment, etc. Many efforts have been devoted in recent years to building a robust and effective framework for STAD. This paper provides a comprehensive review of the state-of-the-art deep learning-based methods for STAD. Firstly, a taxonomy is developed to organize these methods. Next, the linking algorithms, which aim to associate the frame- or clip-level detection results together to form action tubes, are reviewed. Then, the commonly used benchmark datasets and evaluation metrics are introduced, and the performance of state-of-the-art models is compared. At last, this paper is concluded, and a set of potential research directions of STAD are discussed.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01618v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>时空动作检测（STAD）旨在对视频中存在的动作进行分类，并在空间和时间上对其进行定位。由于其爆炸性的现实世界应用，如自动驾驶、视觉监控、娱乐等，它已成为计算机视觉研究中一个特别活跃的领域。近年来，人们致力于为STAD构建一个强大有效的框架。本文全面回顾了最先进的基于深度学习的STAD方法。首先，开发了一个分类法来组织这些方法。接下来，回顾了链接算法，该算法旨在将帧或剪辑级别的检测结果关联在一起以形成动作管。然后，介绍了常用的基准数据集和评估指标，并比较了现有模型的性能。最后，对本文进行了总结，并对STAD的一系列潜在研究方向进行了探讨。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01618v1" target="_blank">2308.01618v1</a>
                              </td>
                              <td>A Survey on Deep Learning-based Spatio-temporal Action Detection</td>
                              <td>Peng Wang</td>
                              <td>2023-08-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01618v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01618v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01532v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multimodal Adaptation of CLIP for Few-Shot Action Recognition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01532v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01532v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01532v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Applying large-scale pre-trained visual models like CLIP to few-shot action recognition tasks can benefit performance and efficiency. Utilizing the "pre-training, fine-tuning" paradigm makes it possible to avoid training a network from scratch, which can be time-consuming and resource-intensive. However, this method has two drawbacks. First, limited labeled samples for few-shot action recognition necessitate minimizing the number of tunable parameters to mitigate over-fitting, also leading to inadequate fine-tuning that increases resource consumption and may disrupt the generalized representation of models. Second, the video's extra-temporal dimension challenges few-shot recognition's effective temporal modeling, while pre-trained visual models are usually image models. This paper proposes a novel method called Multimodal Adaptation of CLIP (MA-CLIP) to address these issues. It adapts CLIP for few-shot action recognition by adding lightweight adapters, which can minimize the number of learnable parameters and enable the model to transfer across different tasks quickly. The adapters we design can combine information from video-text multimodal sources for task-oriented spatiotemporal modeling, which is fast, efficient, and has low training costs. Additionally, based on the attention mechanism, we design a text-guided prototype construction module that can fully utilize video-text information to enhance the representation of video prototypes. Our MA-CLIP is plug-and-play, which can be used in any different few-shot action recognition temporal alignment metric.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01532v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>将像CLIP这样的大规模预先训练的视觉模型应用于少数镜头动作识别任务可以提高性能和效率。利用“预训练、微调”模式可以避免从头开始训练网络，这可能是耗时和资源密集型的。然而，这种方法有两个缺点。首先，用于少镜头动作识别的有限标记样本需要最小化可调参数的数量以缓解过度拟合，这也导致了不充分的微调，从而增加了资源消耗，并可能破坏模型的广义表示。其次，视频的额外时间维度挑战了少镜头识别的有效时间建模，而预先训练的视觉模型通常是图像模型。本文提出了一种新的方法，称为CLIP的多模式自适应（MA-CLIP）来解决这些问题。它通过添加轻量级适配器，使CLIP适用于少镜头动作识别，这可以最大限度地减少可学习参数的数量，并使模型能够在不同任务之间快速转移。我们设计的适配器可以结合来自视频-文本多模式源的信息进行面向任务的时空建模，这是快速、高效且训练成本低的。此外，基于注意力机制，我们设计了一个文本引导的原型构建模块，该模块可以充分利用视频文本信息来增强视频原型的表示。我们的MA-CLIP是即插即用的，可以用于任何不同的少镜头动作识别时间对齐度量。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01532v1" target="_blank">2308.01532v1</a>
                              </td>
                              <td>Multimodal Adaptation of CLIP for Few-Shot Action Recognition</td>
                              <td>Jiazheng Xing</td>
                              <td>2023-08-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01532v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01532v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_06267v4_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_06267v4_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_06267v4_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_06267v4_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The ability to quickly learn a new task with minimal instruction - known as few-shot learning - is a central aspect of intelligent agents. Classical few-shot benchmarks make use of few-shot samples from a single modality, but such samples may not be sufficient to characterize an entire concept class. In contrast, humans use cross-modal information to learn new concepts efficiently. In this work, we demonstrate that one can indeed build a better ${\bf visual}$ dog classifier by ${\bf read}$ing about dogs and ${\bf listen}$ing to them bark. To do so, we exploit the fact that recent multimodal foundation models such as CLIP are inherently cross-modal, mapping different modalities to the same representation space. Specifically, we propose a simple cross-modal adaptation approach that learns from few-shot examples spanning different modalities. By repurposing class names as additional one-shot training samples, we achieve SOTA results with an embarrassingly simple linear classifier for vision-language adaptation. Furthermore, we show that our approach can benefit existing methods such as prefix tuning, adapters, and classifier ensembling. Finally, to explore other modalities beyond vision and language, we construct the first (to our knowledge) audiovisual few-shot benchmark and use cross-modal training to improve the performance of both image and audio classification.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_06267v4_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>用最少的指令快速学习新任务的能力——被称为少镜头学习——是智能代理的一个核心方面。经典的少镜头基准使用来自单个模态的少镜头样本，但这样的样本可能不足以表征整个概念类。相反，人类使用跨模态信息来有效地学习新概念。在这项工作中，我们证明了通过${\bf-read}$ing关于狗和${\bb-list}$ing听它们吠叫，确实可以构建更好的${\bf visual}$dog分类器。为此，我们利用了这样一个事实，即最近的多模态基础模型（如CLIP）本质上是跨模态的，将不同的模态映射到相同的表示空间。具体而言，我们提出了一种简单的跨模态自适应方法，该方法从跨越不同模态的少数镜头示例中学习。通过将类名重新调整为额外的一次性训练样本，我们用一个简单得令人尴尬的视觉语言自适应线性分类器实现了SOTA结果。此外，我们还表明，我们的方法可以受益于现有的方法，如前缀调整、适配器和分类器组合。最后，为了探索视觉和语言之外的其他模式，我们构建了第一个（据我们所知）视听少镜头基准，并使用跨模式训练来提高图像和音频分类的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.06267v4" target="_blank">2301.06267v4</a>
                              </td>
                              <td>Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models</td>
                              <td>Zhiqiu Lin</td>
                              <td>2023-01-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_06267v4_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.06267v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01313v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01313v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01313v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01313v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>CLIP, as a foundational vision language model, is widely used in zero-shot image classification due to its ability to understand various visual concepts and natural language descriptions. However, how to fully leverage CLIP's unprecedented human-like understanding capabilities to achieve better zero-shot classification is still an open question. This paper draws inspiration from the human visual perception process: a modern neuroscience view suggests that in classifying an object, humans first infer its class-independent attributes (e.g., background and orientation) which help separate the foreground object from the background, and then make decisions based on this information. Inspired by this, we observe that providing CLIP with contextual attributes improves zero-shot classification and mitigates reliance on spurious features. We also observe that CLIP itself can reasonably infer the attributes from an image. With these observations, we propose a training-free, two-step zero-shot classification method named PerceptionCLIP. Given an image, it first infers contextual attributes (e.g., background) and then performs object classification conditioning on them. Our experiments show that PerceptionCLIP achieves better generalization, group robustness, and better interpretability. For example, PerceptionCLIP with ViT-L/14 improves the worst group accuracy by 16.5% on the Waterbirds dataset and by 3.5% on CelebA.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01313v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>CLIP作为一种基础的视觉语言模型，由于能够理解各种视觉概念和自然语言描述，在零样本图像分类中得到了广泛的应用。然而，如何充分利用CLIP前所未有的人性化理解能力来实现更好的零样本分类仍然是一个悬而未决的问题。本文从人类视觉感知过程中获得了灵感：现代神经科学观点认为，在对物体进行分类时，人类首先推断出其独立于类别的属性（如背景和方向），这些属性有助于将前景物体与背景分离，然后根据这些信息做出决策。受此启发，我们观察到，为CLIP提供上下文属性可以改进零样本分类，并减少对虚假特征的依赖。我们还观察到CLIP本身可以从图像中合理地推断属性。根据这些观察结果，我们提出了一种无训练的两步零样本分类方法，称为感知CLIP。给定图像，它首先推断上下文属性（例如背景），然后对它们进行对象分类条件处理。我们的实验表明，PerceptionCLIP实现了更好的泛化、组鲁棒性和更好的可解释性。例如，使用ViT-L/14的PerceptionCLIP在Waterbirds数据集上将最差组的准确率提高了16.5%，在CelebA上提高了3.5%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01313v1" target="_blank">2308.01313v1</a>
                              </td>
                              <td>More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes</td>
                              <td>Bang An</td>
                              <td>2023-08-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01313v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01313v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01217v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TeachCLIP: Multi-Grained Teaching for Efficient Text-to-Video Retrieval</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01217v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01217v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01217v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>For text-to-video retrieval (T2VR), which aims to retrieve unlabeled videos by ad-hoc textual queries, CLIP-based methods are dominating. Compared to CLIP4Clip which is efficient and compact, the state-of-the-art models tend to compute video-text similarity by fine-grained cross-modal feature interaction and matching, putting their scalability for large-scale T2VR into doubt. For efficient T2VR, we propose TeachCLIP with multi-grained teaching to let a CLIP4Clip based student network learn from more advanced yet computationally heavy models such as X-CLIP, TS2-Net and X-Pool . To improve the student's learning capability, we add an Attentional frame-Feature Aggregation (AFA) block, which by design adds no extra storage/computation overhead at the retrieval stage. While attentive weights produced by AFA are commonly used for combining frame-level features, we propose a novel use of the weights to let them imitate frame-text relevance estimated by the teacher network. As such, AFA provides a fine-grained learning (teaching) channel for the student (teacher). Extensive experiments on multiple public datasets justify the viability of the proposed method.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01217v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对于旨在通过特定文本查询检索未标记视频的文本到视频检索（T2VR），基于CLIP的方法占主导地位。与高效紧凑的CLIP4Clip相比，最先进的模型倾向于通过细粒度的跨模态特征交互和匹配来计算视频文本相似性，这使其在大规模T2VR中的可扩展性受到质疑。对于高效的T2VR，我们提出了具有多粒度教学的TeachCLIP，让基于CLIP4Clip的学生网络学习更先进但计算量大的模型，如X-CLIP、TS2-Net和X-Pool。为了提高学生的学习能力，我们添加了一个注意帧特征聚合（AFA）块，该块在检索阶段不增加额外的存储/计算开销。虽然AFA产生的注意力权重通常用于组合帧级特征，但我们提出了一种新颖的权重用法，让它们模仿教师网络估计的帧文本相关性。因此，AFA为学生（教师）提供了一个细粒度的学习（教学）渠道。在多个公共数据集上进行的大量实验证明了所提出方法的可行性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01217v1" target="_blank">2308.01217v1</a>
                              </td>
                              <td>TeachCLIP: Multi-Grained Teaching for Efficient Text-to-Video Retrieval</td>
                              <td>Kaibin Tian</td>
                              <td>2023-08-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01217v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01217v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_00909v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rethinking Similarity Search: Embracing Smarter Mechanisms over Smarter Data</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_00909v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_00909v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_00909v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this vision paper, we propose a shift in perspective for improving the effectiveness of similarity search. Rather than focusing solely on enhancing the data quality, particularly machine learning-generated embeddings, we advocate for a more comprehensive approach that also enhances the underpinning search mechanisms. We highlight three novel avenues that call for a redefinition of the similarity search problem: exploiting implicit data structures and distributions, engaging users in an iterative feedback loop, and moving beyond a single query vector. These novel pathways have gained relevance in emerging applications such as large-scale language models, video clip retrieval, and data labeling. We discuss the corresponding research challenges posed by these new problem areas and share insights from our preliminary discoveries.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_00909v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这篇前瞻性的论文中，我们提出了一个视角的转变，以提高相似性搜索的有效性。我们主张采用一种更全面的方法来增强基础搜索机制，而不是仅仅关注提高数据质量，特别是机器学习生成的嵌入。我们强调了三种需要重新定义相似性搜索问题的新途径：利用隐含的数据结构和分布，让用户参与迭代反馈循环，以及超越单个查询向量。这些新途径在大规模语言模型、视频片段检索和数据标记等新兴应用中获得了相关性。我们讨论了这些新问题领域带来的相应研究挑战，并分享了我们初步发现的见解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.00909v1" target="_blank">2308.00909v1</a>
                              </td>
                              <td>Rethinking Similarity Search: Embracing Smarter Mechanisms over Smarter Data</td>
                              <td>Renzhi Wu</td>
                              <td>2023-08-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_00909v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.00909v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04192v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04192v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04192v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04192v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video question--answering is a fundamental task in the field of video understanding. Although current vision--language models (VLMs) equipped with Video Transformers have enabled temporal modeling and yielded superior results, they are at the cost of huge computational power and thus too expensive to deploy in real-time application scenarios. An economical workaround only samples a small portion of frames to represent the main content of that video and tune an image--text model on these sampled frames. Recent video understanding models usually randomly sample a set of frames or clips, regardless of internal correlations between their visual contents, nor their relevance to the problem. We argue that such kinds of aimless sampling may omit the key frames from which the correct answer can be deduced, and the situation gets worse when the sampling sparsity increases, which always happens as the video lengths increase. To mitigate this issue, we propose two frame sampling strategies, namely the most domain frames (MDF) and most implied frames (MIF), to maximally preserve those frames that are most likely vital to the given questions. MDF passively minimizes the risk of key frame omission in a bootstrap manner, while MIS actively searches key frames customized for each video--question pair with the assistance of auxiliary models. The experimental results on three public datasets from three advanced VLMs (CLIP, GIT and All-in-one) demonstrate that our proposed strategies can boost the performance for image--text pretrained models. The source codes pertaining to the method proposed in this paper are publicly available at https://github.com/declare-lab/sas-vqa.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04192v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频问答是视频理解领域的一项基本任务。尽管目前配备了视频转换器的视觉语言模型（VLM）已经实现了时间建模并产生了优越的结果，但它们是以巨大的计算能力为代价的，因此在实时应用场景中部署成本太高。一种经济的解决方法只对一小部分帧进行采样，以表示视频的主要内容，并在这些采样帧上调整图像-文本模型。最近的视频理解模型通常随机采样一组帧或剪辑，而不管它们的视觉内容之间的内部相关性，也不管它们与问题的相关性。我们认为，这种无目标的采样可能会忽略可以推断出正确答案的关键帧，并且当采样稀疏性增加时，情况会变得更糟，而这种情况总是随着视频长度的增加而发生。为了缓解这个问题，我们提出了两种帧采样策略，即最域帧（MDF）和最隐含帧（MIF），以最大限度地保留那些对给定问题最重要的帧。MDF以引导的方式被动地将关键帧遗漏的风险降至最低，而MIS则在辅助模型的帮助下主动搜索为每个视频定制的关键帧——问题对。在三个高级VLM（CLIP、GIT和All-in-one）的三个公共数据集上的实验结果表明，我们提出的策略可以提高图像-文本预训练模型的性能。与本文中提出的方法有关的源代码可在https://github.com/declare-lab/sas-vqa.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04192v3" target="_blank">2307.04192v3</a>
                              </td>
                              <td>SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering</td>
                              <td>Wei Han</td>
                              <td>2023-07-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04192v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04192v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_00541v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Detecting Cloud Presence in Satellite Images Using the RGB-based CLIP Vision-Language Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_00541v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_00541v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_00541v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This work explores capabilities of the pre-trained CLIP vision-language model to identify satellite images affected by clouds. Several approaches to using the model to perform cloud presence detection are proposed and evaluated, including a purely zero-shot operation with text prompts and several fine-tuning approaches. Furthermore, the transferability of the methods across different datasets and sensor types (Sentinel-2 and Landsat-8) is tested. The results that CLIP can achieve non-trivial performance on the cloud presence detection task with apparent capability to generalise across sensing modalities and sensing bands. It is also found that a low-cost fine-tuning stage leads to a strong increase in true negative rate. The results demonstrate that the representations learned by the CLIP model can be useful for satellite image processing tasks involving clouds.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_00541v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>这项工作探索了预先训练的CLIP视觉语言模型识别受云影响的卫星图像的能力。提出并评估了使用该模型进行云存在检测的几种方法，包括带有文本提示的纯零样本操作和几种微调方法。此外，还测试了这些方法在不同数据集和传感器类型（Sentinel-2和Landsat-8）之间的可转移性。结果表明，CLIP可以在云存在检测任务上实现非平凡的性能，具有跨感知模态和感知频带进行泛化的明显能力。研究还发现，低成本的微调阶段会导致真负率的大幅增加。结果表明，CLIP模型学习的表示可以用于涉及云的卫星图像处理任务。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.00541v1" target="_blank">2308.00541v1</a>
                              </td>
                              <td>Detecting Cloud Presence in Satellite Images Using the RGB-based CLIP Vision-Language Model</td>
                              <td>Mikolaj Czerkawski</td>
                              <td>2023-08-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_00541v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.00541v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_00462v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Context-Aware Talking-Head Video Editing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_00462v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_00462v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_00462v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Talking-head video editing aims to efficiently insert, delete, and substitute the word of a pre-recorded video through a text transcript editor. The key challenge for this task is obtaining an editing model that generates new talking-head video clips which simultaneously have accurate lip synchronization and motion smoothness. Previous approaches, including 3DMM-based (3D Morphable Model) methods and NeRF-based (Neural Radiance Field) methods, are sub-optimal in that they either require minutes of source videos and days of training time or lack the disentangled control of verbal (e.g., lip motion) and non-verbal (e.g., head pose and expression) representations for video clip insertion. In this work, we fully utilize the video context to design a novel framework for talking-head video editing, which achieves efficiency, disentangled motion control, and sequential smoothness. Specifically, we decompose this framework to motion prediction and motion-conditioned rendering: (1) We first design an animation prediction module that efficiently obtains smooth and lip-sync motion sequences conditioned on the driven speech. This module adopts a non-autoregressive network to obtain context prior and improve the prediction efficiency, and it learns a speech-animation mapping prior with better generalization to novel speech from a multi-identity video dataset. (2) We then introduce a neural rendering module to synthesize the photo-realistic and full-head video frames given the predicted motion sequence. This module adopts a pre-trained head topology and uses only few frames for efficient fine-tuning to obtain a person-specific rendering model. Extensive experiments demonstrate that our method efficiently achieves smoother editing results with higher image quality and lip accuracy using less data than previous methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_00462v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Talking head视频编辑旨在通过文本转录编辑器有效地插入、删除和替换预先录制的视频中的单词。这项任务的关键挑战是获得一个编辑模型，该模型可以生成新的会说话的头部视频剪辑，同时具有精确的嘴唇同步和运动平滑度。以前的方法，包括基于3DMM的（3D变形模型）方法和基于NeRF的（神经辐射场）方法，都是次优的，因为它们要么需要几分钟的源视频和几天的训练时间，要么缺乏对视频剪辑插入的语言（如嘴唇运动）和非语言（如头部姿势和表情）表示的复杂控制。在这项工作中，我们充分利用视频上下文设计了一个新的谈话式头部视频编辑框架，该框架实现了效率、解纠缠的运动控制和顺序平滑。具体来说，我们将该框架分解为运动预测和运动条件渲染：（1）我们首先设计了一个动画预测模块，该模块可以有效地获得基于驱动语音的平滑和唇同步运动序列。该模块采用非自回归网络来获取上下文先验，提高预测效率，并从多身份视频数据集中学习具有更好泛化能力的语音动画映射先验。（2） 然后，在给定预测的运动序列的情况下，我们引入了一个神经渲染模块来合成照片逼真度和全头视频帧。该模块采用预先训练的头部拓扑结构，仅使用少量帧进行有效微调，以获得特定于个人的渲染模型。大量实验表明，与以前的方法相比，我们的方法使用更少的数据，以更高的图像质量和嘴唇精度有效地实现了更平滑的编辑结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.00462v1" target="_blank">2308.00462v1</a>
                              </td>
                              <td>Context-Aware Talking-Head Video Editing</td>
                              <td>Songlin Yang</td>
                              <td>2023-08-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_00462v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.00462v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_11710v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Controlled and Conditional Text to Image Generation with Diffusion Prior</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_11710v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_11710v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_11710v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Denoising Diffusion models have shown remarkable performance in generating diverse, high quality images from text. Numerous techniques have been proposed on top of or in alignment with models like Stable Diffusion and Imagen that generate images directly from text. A lesser explored approach is DALLE-2's two step process comprising a Diffusion Prior that generates a CLIP image embedding from text and a Diffusion Decoder that generates an image from a CLIP image embedding. We explore the capabilities of the Diffusion Prior and the advantages of an intermediate CLIP representation. We observe that Diffusion Prior can be used in a memory and compute efficient way to constrain the generation to a specific domain without altering the larger Diffusion Decoder. Moreover, we show that the Diffusion Prior can be trained with additional conditional information such as color histogram to further control the generation. We show quantitatively and qualitatively that the proposed approaches perform better than prompt engineering for domain specific generation and existing baselines for color conditioned generation. We believe that our observations and results will instigate further research into the diffusion prior and uncover more of its capabilities.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_11710v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>去噪扩散模型在从文本生成多样化的高质量图像方面表现出了显著的性能。在诸如Stable Diffusion和Imagen之类的直接从文本生成图像的模型之上或与之对齐，已经提出了许多技术。较少探索的方法是DALLE-2的两步过程，包括从文本生成CLIP图像嵌入的扩散先验和从CLIP图像植入生成图像的扩散解码器。我们探讨了扩散先验的能力和中间CLIP表示的优势。我们观察到，扩散先验可以在存储器中使用，并以有效的方式计算，以将生成约束到特定域，而不改变较大的扩散解码器。此外，我们还表明，可以使用额外的条件信息（如颜色直方图）来训练扩散先验，以进一步控制生成。我们从数量和质量上表明，所提出的方法在特定领域的生成和颜色条件生成的现有基线方面比即时工程表现得更好。我们相信，我们的观察和结果将推动对扩散先验的进一步研究，并揭示其更多的能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.11710v2" target="_blank">2302.11710v2</a>
                              </td>
                              <td>Controlled and Conditional Text to Image Generation with Diffusion Prior</td>
                              <td>Pranav Aggarwal</td>
                              <td>2023-02-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_11710v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.11710v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2210_11603v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3DALL-E: Integrating Text-to-Image AI in 3D Design Workflows</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2210_11603v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2210_11603v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2210_11603v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Text-to-image AI are capable of generating novel images for inspiration, but their applications for 3D design workflows and how designers can build 3D models using AI-provided inspiration have not yet been explored. To investigate this, we integrated DALL-E, GPT-3, and CLIP within a CAD software in 3DALL-E, a plugin that generates 2D image inspiration for 3D design. 3DALL-E allows users to construct text and image prompts based on what they are modeling. In a study with 13 designers, we found that designers saw great potential in 3DALL-E within their workflows and could use text-to-image AI to produce reference images, prevent design fixation, and inspire design considerations. We elaborate on prompting patterns observed across 3D modeling tasks and provide measures of prompt complexity observed across participants. From our findings, we discuss how 3DALL-E can merge with existing generative design workflows and propose prompt bibliographies as a form of human-AI design history.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2210_11603v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文本到图像的人工智能能够生成新颖的图像来获得灵感，但它们在3D设计工作流程中的应用，以及设计师如何利用人工智能提供的灵感构建3D模型，尚未得到探索。为了研究这一点，我们在3DALL-E的CAD软件中集成了DALL-E、GPT-3和CLIP，3DALL-E是一个为3D设计生成2D图像灵感的插件。3DALL-E允许用户根据建模内容构建文本和图像提示。在一项针对13名设计师的研究中，我们发现设计师在他们的工作流程中看到了3DALL-E的巨大潜力，可以使用文本到图像的人工智能来生成参考图像，防止设计固定，并激发设计考虑。我们详细阐述了在三维建模任务中观察到的提示模式，并提供了在参与者中观察到提示复杂性的测量。根据我们的研究结果，我们讨论了3DALL-E如何与现有的生成设计工作流相结合，并提出了作为人类人工智能设计历史形式的即时书目。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2210.11603v2" target="_blank">2210.11603v2</a>
                              </td>
                              <td>3DALL-E: Integrating Text-to-Image AI in 3D Design Workflows</td>
                              <td>Vivian Liu</td>
                              <td>2022-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2210_11603v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2210.11603v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_16715v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">UniVTG: Towards Unified Video-Language Temporal Grounding</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_16715v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_16715v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_16715v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video Temporal Grounding (VTG), which aims to ground target clips from videos (such as consecutive intervals or disjoint shots) according to custom language queries (e.g., sentences or words), is key for video browsing on social media. Most methods in this direction develop taskspecific models that are trained with type-specific labels, such as moment retrieval (time interval) and highlight detection (worthiness curve), which limits their abilities to generalize to various VTG tasks and labels. In this paper, we propose to Unify the diverse VTG labels and tasks, dubbed UniVTG, along three directions: Firstly, we revisit a wide range of VTG labels and tasks and define a unified formulation. Based on this, we develop data annotation schemes to create scalable pseudo supervision. Secondly, we develop an effective and flexible grounding model capable of addressing each task and making full use of each label. Lastly, thanks to the unified framework, we are able to unlock temporal grounding pretraining from large-scale diverse labels and develop stronger grounding abilities e.g., zero-shot grounding. Extensive experiments on three tasks (moment retrieval, highlight detection and video summarization) across seven datasets (QVHighlights, Charades-STA, TACoS, Ego4D, YouTube Highlights, TVSum, and QFVS) demonstrate the effectiveness and flexibility of our proposed framework. The codes are available at https://github.com/showlab/UniVTG.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_16715v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频时间接地（VTG）旨在根据自定义语言查询（如句子或单词）接地视频中的目标片段（如连续间隔或不相交的镜头），是社交媒体上视频浏览的关键。这个方向上的大多数方法都开发了使用特定类型标签训练的特定任务模型，如时刻检索（时间间隔）和高亮检测（价值曲线），这限制了它们推广到各种VTG任务和标签的能力。在本文中，我们建议沿着三个方向统一不同的VTG标签和任务，称为UniVTG：首先，我们重新审视了广泛的VTG标记和任务，并定义了一个统一的公式。在此基础上，我们开发了数据注释方案来创建可扩展的伪监督。其次，我们开发了一个有效而灵活的基础模型，能够处理每一项任务并充分利用每一个标签。最后，由于统一的框架，我们能够从大规模的不同标签中解锁时间接地预训练，并开发更强的接地能力，例如零样本接地。在七个数据集（QVHighlights、Charades STA、TACoS、Ego4D、YouTube Highlights、TVSum和QFVS）上对三项任务（时刻检索、亮点检测和视频摘要）进行了广泛的实验，证明了我们提出的框架的有效性和灵活性。代码可在https://github.com/showlab/UniVTG.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.16715v1" target="_blank">2307.16715v1</a>
                              </td>
                              <td>UniVTG: Towards Unified Video-Language Temporal Grounding</td>
                              <td>Kevin Qinghong Lin</td>
                              <td>2023-07-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_16715v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.16715v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_16686v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Guiding Image Captioning Models Toward More Specific Captions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_16686v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_16686v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_16686v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Image captioning is conventionally formulated as the task of generating captions for images that match the distribution of reference image-caption pairs. However, reference captions in standard captioning datasets are short and may not uniquely identify the images they describe. These problems are further exacerbated when models are trained directly on image-alt text pairs collected from the internet. In this work, we show that it is possible to generate more specific captions with minimal changes to the training process. We implement classifier-free guidance for an autoregressive captioning model by fine-tuning it to estimate both conditional and unconditional distributions over captions. The guidance scale applied at decoding controls a trade-off between maximizing $p(\mathrm{caption}|\mathrm{image})$ and $p(\mathrm{image}|\mathrm{caption})$. Compared to standard greedy decoding, decoding with a guidance scale of 2 substantially improves reference-free metrics such as CLIPScore (0.808 vs. 0.775) and caption$\to$image retrieval performance in the CLIP embedding space (recall@1 44.6% vs. 26.5%), but worsens standard reference-based captioning metrics (e.g., CIDEr 78.6 vs 126.1). We further explore the use of language models to guide the decoding process, obtaining small improvements over the Pareto frontier of reference-free vs. reference-based captioning metrics that arises from classifier-free guidance, and substantially improving the quality of captions generated from a model trained only on minimally curated web data.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_16686v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>图像字幕通常被公式化为生成与参考图像字幕对的分布匹配的图像的字幕的任务。然而，标准字幕数据集中的参考字幕很短，可能无法唯一识别它们所描述的图像。当模型直接在从互联网收集的图像-alt-文本对上进行训练时，这些问题会进一步加剧。在这项工作中，我们表明，在对训练过程进行最小更改的情况下，可以生成更具体的字幕。我们通过对自回归字幕模型进行微调来估计字幕上的条件分布和无条件分布，从而实现了对其无分类器指导。解码时应用的指导量表控制了最大化$p（\mathrm｛caption｝|\mathrm{image｝）$和$p（\athrm{image}|\mathrm{caption}）$之间的权衡。与标准贪婪解码相比，指导量表为2的解码大大提高了无参考指标，如CLIP嵌入空间中的CLIPScore（0.808 vs.0.775）和字幕$\to$图像检索性能(recall@144.6%对26.5%），但恶化了基于参考的标准字幕度量（例如，CIDEr 78.6对126.1）。我们进一步探索了使用语言模型来指导解码过程，在无参考与基于参考的字幕度量的Pareto边界上获得了小的改进，这是由无分类器指导产生的，以及显著地提高了从仅在最小策划的网络数据上训练的模型生成的字幕的质量。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.16686v1" target="_blank">2307.16686v1</a>
                              </td>
                              <td>Guiding Image Captioning Models Toward More Specific Captions</td>
                              <td>Simon Kornblith</td>
                              <td>2023-07-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_16686v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.16686v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_16634v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CDUL: CLIP-Driven Unsupervised Learning for Multi-Label Image Classification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_16634v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_16634v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_16634v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a CLIP-based unsupervised learning method for annotation-free multi-label image classification, including three stages: initialization, training, and inference. At the initialization stage, we take full advantage of the powerful CLIP model and propose a novel approach to extend CLIP for multi-label predictions based on global-local image-text similarity aggregation. To be more specific, we split each image into snippets and leverage CLIP to generate the similarity vector for the whole image (global) as well as each snippet (local). Then a similarity aggregator is introduced to leverage the global and local similarity vectors. Using the aggregated similarity scores as the initial pseudo labels at the training stage, we propose an optimization framework to train the parameters of the classification network and refine pseudo labels for unobserved labels. During inference, only the classification network is used to predict the labels of the input image. Extensive experiments show that our method outperforms state-of-the-art unsupervised methods on MS-COCO, PASCAL VOC 2007, PASCAL VOC 2012, and NUS datasets and even achieves comparable results to weakly supervised classification methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_16634v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种基于CLIP的无标注多标签图像分类无监督学习方法，包括初始化、训练和推理三个阶段。在初始化阶段，我们充分利用了强大的CLIP模型，提出了一种基于全局局部图像-文本相似性聚合的扩展CLIP用于多标签预测的新方法。更具体地说，我们将每个图像分割成片段，并利用CLIP生成整个图像（全局）和每个片段（局部）的相似性向量。然后引入相似性聚合器来利用全局和局部相似性向量。在训练阶段，使用聚合的相似性得分作为初始伪标签，我们提出了一个优化框架来训练分类网络的参数，并为未观察到的标签细化伪标签。在推理过程中，仅使用分类网络来预测输入图像的标签。大量实验表明，我们的方法在MS-COCO、PASCAL VOC 2007、PASCAL VOC2012和NUS数据集上优于最先进的无监督方法，甚至取得了与弱监督分类方法相当的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.16634v1" target="_blank">2307.16634v1</a>
                              </td>
                              <td>CDUL: CLIP-Driven Unsupervised Learning for Multi-Label Image Classification</td>
                              <td>Rabab Abdelfattah</td>
                              <td>2023-07-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_16634v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.16634v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_16565v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Unbalanced Motion: Part-Decoupling Network for Video Portrait Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_16565v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_16565v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_16565v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video portrait segmentation (VPS), aiming at segmenting prominent foreground portraits from video frames, has received much attention in recent years. However, simplicity of existing VPS datasets leads to a limitation on extensive research of the task. In this work, we propose a new intricate large-scale Multi-scene Video Portrait Segmentation dataset MVPS consisting of 101 video clips in 7 scenario categories, in which 10,843 sampled frames are finely annotated at pixel level. The dataset has diverse scenes and complicated background environments, which is the most complex dataset in VPS to our best knowledge. Through the observation of a large number of videos with portraits during dataset construction, we find that due to the joint structure of human body, motion of portraits is part-associated, which leads that different parts are relatively independent in motion. That is, motion of different parts of the portraits is unbalanced. Towards this unbalance, an intuitive and reasonable idea is that different motion states in portraits can be better exploited by decoupling the portraits into parts. To achieve this, we propose a Part-Decoupling Network (PDNet) for video portrait segmentation. Specifically, an Inter-frame Part-Discriminated Attention (IPDA) module is proposed which unsupervisely segments portrait into parts and utilizes different attentiveness on discriminative features specified to each different part. In this way, appropriate attention can be imposed to portrait parts with unbalanced motion to extract part-discriminated correlations, so that the portraits can be segmented more accurately. Experimental results demonstrate that our method achieves leading performance with the comparison to state-of-the-art methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_16565v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频人像分割（VPS）旨在从视频帧中分割出突出的前景人像，近年来备受关注。然而，现有VPS数据集的简单性导致了对该任务的广泛研究受到限制。在这项工作中，我们提出了一个新的复杂的大规模多场景视频人像分割数据集MVPS，该数据集由7个场景类别的101个视频片段组成，其中10843个采样帧在像素级进行了精细注释。该数据集具有多样化的场景和复杂的背景环境，据我们所知，这是VPS中最复杂的数据集。通过在数据集构建过程中对大量带有肖像的视频的观察，我们发现由于人体的关节结构，肖像的运动是部分关联的，这导致不同的部分在运动中相对独立。也就是说，肖像画不同部分的运动是不平衡的。对于这种不平衡，一个直观合理的想法是，通过将肖像分解为多个部分，可以更好地利用肖像中的不同运动状态。为了实现这一点，我们提出了一种用于视频人像分割的部分解耦网络（PDNet）。具体而言，提出了一种帧间部分判别注意（IPDA）模块，该模块将肖像非监督地分割成多个部分，并对每个不同部分指定的判别特征利用不同的注意。通过这种方式，可以对运动不平衡的人像部分施加适当的关注，以提取区分部分的相关性，从而可以更准确地分割人像。实验结果表明，和最先进的方法相比，我们的方法取得了领先的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.16565v1" target="_blank">2307.16565v1</a>
                              </td>
                              <td>Towards Unbalanced Motion: Part-Decoupling Network for Video Portrait Segmentation</td>
                              <td>Tianshu Yu</td>
                              <td>2023-07-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_16565v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.16565v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_16372v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LP-MusicCaps: LLM-Based Pseudo Music Captioning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_16372v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_16372v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_16372v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Automatic music captioning, which generates natural language descriptions for given music tracks, holds significant potential for enhancing the understanding and organization of large volumes of musical data. Despite its importance, researchers face challenges due to the costly and time-consuming collection process of existing music-language datasets, which are limited in size. To address this data scarcity issue, we propose the use of large language models (LLMs) to artificially generate the description sentences from large-scale tag datasets. This results in approximately 2.2M captions paired with 0.5M audio clips. We term it Large Language Model based Pseudo music caption dataset, shortly, LP-MusicCaps. We conduct a systemic evaluation of the large-scale music captioning dataset with various quantitative evaluation metrics used in the field of natural language processing as well as human evaluation. In addition, we trained a transformer-based music captioning model with the dataset and evaluated it under zero-shot and transfer-learning settings. The results demonstrate that our proposed approach outperforms the supervised baseline model.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_16372v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自动音乐字幕为给定的音乐曲目生成自然语言描述，在增强对大量音乐数据的理解和组织方面具有重要潜力。尽管它很重要，但由于现有音乐语言数据集的收集过程成本高昂且耗时，研究人员面临着挑战，这些数据集的大小有限。为了解决这一数据稀缺问题，我们建议使用大型语言模型（LLM）从大规模标签数据集中人工生成描述语句。这导致大约220万个字幕与0.5万个音频片段配对。我们称之为基于大型语言模型的伪音乐字幕数据集，简称LP MusicCaps。我们使用自然语言处理和人类评估领域中使用的各种定量评估指标，对大型音乐字幕数据集进行了系统评估。此外，我们用数据集训练了一个基于变换器的音乐字幕模型，并在零样本和转移学习设置下对其进行了评估。结果表明，我们提出的方法优于监督基线模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.16372v1" target="_blank">2307.16372v1</a>
                              </td>
                              <td>LP-MusicCaps: LLM-Based Pseudo Music Captioning</td>
                              <td>SeungHeon Doh</td>
                              <td>2023-07-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_16372v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.16372v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07694v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Evaluation of Deep Reinforcement Learning Algorithms for Portfolio Optimisation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07694v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07694v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07694v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We evaluate benchmark deep reinforcement learning (DRL) algorithms on the task of portfolio optimisation under a simulator. The simulator is based on correlated geometric Brownian motion (GBM) with the Bertsimas-Lo (BL) market impact model. Using the Kelly criterion (log utility) as the objective, we can analytically derive the optimal policy without market impact and use it as an upper bound to measure performance when including market impact. We found that the off-policy algorithms DDPG, TD3 and SAC were unable to learn the right Q function due to the noisy rewards and therefore perform poorly. The on-policy algorithms PPO and A2C, with the use of generalised advantage estimation (GAE), were able to deal with the noise and derive a close to optimal policy. The clipping variant of PPO was found to be important in preventing the policy from deviating from the optimal once converged. In a more challenging environment where we have regime changes in the GBM parameters, we found that PPO, combined with a hidden Markov model (HMM) to learn and predict the regime context, is able to learn different policies adapted to each regime. Overall, we find that the sample complexity of these algorithms is too high, requiring more than 2m steps to learn a good policy in the simplest setting, which is equivalent to almost 8,000 years of daily prices.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07694v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们在模拟器下评估了投资组合优化任务的基准深度强化学习（DRL）算法。该模拟器基于相关几何布朗运动（GBM）和Bertsimas-Lo（BL）市场影响模型。使用Kelly准则（对数效用）作为目标，我们可以分析推导出没有市场影响的最优政策，并将其作为衡量包括市场影响时的绩效的上界。我们发现，由于有噪声的奖励，策略外算法DDPG、TD3和SAC无法学习正确的Q函数，因此性能较差。策略上算法PPO和A2C使用广义优势估计（GAE），能够处理噪声并导出接近最优的策略。PPO的剪裁变体被发现在防止策略一旦收敛就偏离最优时是重要的。在GBM参数发生制度变化的更具挑战性的环境中，我们发现PPO与隐马尔可夫模型（HMM）相结合来学习和预测制度上下文，能够学习适合每个制度的不同政策。总的来说，我们发现这些算法的样本复杂度太高，需要200多万步才能在最简单的设置中学习一个好的策略，这相当于近8000年的日常价格。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07694v2" target="_blank">2307.07694v2</a>
                              </td>
                              <td>Evaluation of Deep Reinforcement Learning Algorithms for Portfolio Optimisation</td>
                              <td>Chung I Lu</td>
                              <td>2023-07-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07694v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07694v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2208_14649v4_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Injecting Image Details into CLIP's Feature Space</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2208_14649v4_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2208_14649v4_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2208_14649v4_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Although CLIP-like Visual Language Models provide a functional joint feature space for image and text, due to the limitation of the CILP-like model's image input size (e.g., 224), subtle details are lost in the feature representation if we input high-resolution images (e.g., 2240). In this work, we introduce an efficient framework that can produce a single feature representation for a high-resolution image that injects image details and shares the same semantic space as the original CLIP. In the framework, we train a feature fusing model based on CLIP features extracted from a carefully designed image patch method that can cover objects of any scale, weakly supervised by image-agnostic class prompted queries. We validate our framework by retrieving images from class prompted queries on the real world and synthetic datasets, showing significant performance improvement on these tasks. Furthermore, to fully demonstrate our framework's detail retrieval ability, we construct a CLEVR-like synthetic dataset called CLVER-DS, which is fully annotated and has a controllable object scale.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2208_14649v4_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管类CLIP视觉语言模型为图像和文本提供了功能性的联合特征空间，但由于类CILP模型的图像输入大小（例如224）的限制，如果我们输入高分辨率图像（例如2240），则在特征表示中会丢失细微的细节。在这项工作中，我们介绍了一个有效的框架，该框架可以为高分辨率图像生成单一特征表示，该图像注入图像细节，并与原始CLIP共享相同的语义空间。在该框架中，我们基于从精心设计的图像补丁方法中提取的CLIP特征来训练特征融合模型，该方法可以覆盖任何规模的对象，受图像不可知类提示查询的弱监督。我们通过从真实世界和合成数据集上的类提示查询中检索图像来验证我们的框架，显示出在这些任务上的显著性能改进。此外，为了充分展示我们框架的细节检索能力，我们构建了一个类似CLEVR的合成数据集，称为CLVER-DS，该数据集经过充分注释，具有可控的对象规模。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2208.14649v4" target="_blank">2208.14649v4</a>
                              </td>
                              <td>Injecting Image Details into CLIP's Feature Space</td>
                              <td>Zilun Zhang</td>
                              <td>2022-08-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2208_14649v4_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2208.14649v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_16204v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Open-Set Domain Adaptation with Visual-Language Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_16204v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_16204v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_16204v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Unsupervised domain adaptation (UDA) has proven to be very effective in transferring knowledge obtained from a source domain with labeled data to a target domain with unlabeled data. Owing to the lack of labeled data in the target domain and the possible presence of unknown classes, open-set domain adaptation (ODA) has emerged as a potential solution to identify these classes during the training phase. Although existing ODA approaches aim to solve the distribution shifts between the source and target domains, most methods fine-tuned ImageNet pre-trained models on the source domain with the adaptation on the target domain. Recent visual-language foundation models (VLFM), such as Contrastive Language-Image Pre-Training (CLIP), are robust to many distribution shifts and, therefore, should substantially improve the performance of ODA. In this work, we explore generic ways to adopt CLIP, a popular VLFM, for ODA. We investigate the performance of zero-shot prediction using CLIP, and then propose an entropy optimization strategy to assist the ODA models with the outputs of CLIP. The proposed approach achieves state-of-the-art results on various benchmarks, demonstrating its effectiveness in addressing the ODA problem.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_16204v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>无监督域自适应（UDA）已被证明在将从具有标记数据的源域获得的知识转移到具有未标记数据的目标域方面非常有效。由于目标域中缺乏标记数据，并且可能存在未知类，因此开放集域自适应（ODA）已成为在训练阶段识别这些类的潜在解决方案。尽管现有的ODA方法旨在解决源域和目标域之间的分布变化，但大多数方法都在源域上微调ImageNet预训练模型，并在目标域上进行自适应。最近的视觉语言基础模型（VLFM），如对比语言图像预训练（CLIP），对许多分布变化都是稳健的，因此应该大大提高ODA的性能。在这项工作中，我们探索了采用CLIP（一种流行的VLFM）进行ODA的通用方法。我们研究了使用CLIP的零样本预测的性能，然后提出了一种熵优化策略来帮助具有CLIP输出的ODA模型。拟议的方法在各种基准上取得了最先进的成果，表明其在解决官方发展援助问题方面的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.16204v1" target="_blank">2307.16204v1</a>
                              </td>
                              <td>Open-Set Domain Adaptation with Visual-Language Foundation Models</td>
                              <td>Qing Yu</td>
                              <td>2023-07-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_16204v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.16204v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_15971v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">You Can Backdoor Personalized Federated Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_15971v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_15971v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_15971v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Backdoor attacks pose a significant threat to the security of federated learning systems. However, existing research primarily focuses on backdoor attacks and defenses within the generic FL scenario, where all clients collaborate to train a single global model. \citet{qin2023revisiting} conduct the first study of backdoor attacks in the personalized federated learning (pFL) scenario, where each client constructs a personalized model based on its local data. Notably, the study demonstrates that pFL methods with partial model-sharing can significantly boost robustness against backdoor attacks. In this paper, we whistleblow that pFL methods with partial model-sharing are still vulnerable to backdoor attacks in the absence of any defense. We propose three backdoor attack methods: BapFL, BapFL+, and Gen-BapFL, and we empirically demonstrate that they can effectively attack the pFL methods. Specifically, the key principle of BapFL lies in maintaining clean local parameters while implanting the backdoor into the global parameters. BapFL+ generalizes the attack success to benign clients by introducing Gaussian noise to the local parameters. Furthermore, we assume the collaboration of malicious clients and propose Gen-BapFL, which leverages meta-learning techniques to further enhances attack generalization. We evaluate our proposed attack methods against two classic pFL methods with partial model-sharing, FedPer and LG-FedAvg. Extensive experiments on four FL benchmark datasets demonstrate the effectiveness of our proposed attack methods. Additionally, we assess the defense efficacy of various defense strategies against our proposed attacks and find that Gradient Norm-Clipping is particularly effective. It is crucial to note that pFL method is not always secure in the presence of backdoor attacks, and we hope to inspire further research on attack and defense in pFL scenarios.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_15971v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>后门攻击对联合学习系统的安全性构成了重大威胁。然而，现有的研究主要集中在通用FL场景中的后门攻击和防御，在该场景中，所有客户都合作训练一个单一的全局模型。\citet{qin2023revisiting}首次研究了个性化联合学习（pFL）场景中的后门攻击，其中每个客户端基于其本地数据构建个性化模型。值得注意的是，该研究表明，具有部分模型共享的pFL方法可以显著提高对后门攻击的鲁棒性。在本文中，我们发现，在没有任何防御的情况下，具有部分模型共享的pFL方法仍然容易受到后门攻击。我们提出了三种后门攻击方法：BapFL、BapFL+和Gen BapFL，并实证证明它们可以有效地攻击pFL方法。具体而言，BapFL的关键原理在于在将后门植入全局参数的同时保持干净的局部参数。BapFL+通过在局部参数中引入高斯噪声，将攻击成功推广到良性客户端。此外，我们假设恶意客户端的协作，并提出了Gen BapFL，它利用元学习技术来进一步增强攻击泛化能力。我们针对两种具有部分模型共享的经典pFL方法FedPer和LG FedAvg对我们提出的攻击方法进行了评估。在四个FL基准数据集上的大量实验证明了我们提出的攻击方法的有效性。此外，我们评估了各种防御策略对我们提出的攻击的防御效果，发现梯度范数裁剪特别有效。需要注意的是，在存在后门攻击的情况下，pFL方法并不总是安全的，我们希望能启发对pFL场景中的攻击和防御的进一步研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.15971v1" target="_blank">2307.15971v1</a>
                              </td>
                              <td>You Can Backdoor Personalized Federated Learning</td>
                              <td>Tiandi Ye</td>
                              <td>2023-07-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_15971v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.15971v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_15904v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Sat2Cap: Mapping Fine-Grained Textual Descriptions from Satellite Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_15904v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_15904v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_15904v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a novel weakly supervised approach for creating maps using free-form textual descriptions (or captions). We refer to this new line of work of creating textual maps as zero-shot mapping. Prior works have approached mapping tasks by developing models that predict over a fixed set of attributes using overhead imagery. However, these models are very restrictive as they can only solve highly specific tasks for which they were trained. Mapping text, on the other hand, allows us to solve a large variety of mapping problems with minimal restrictions. To achieve this, we train a contrastive learning framework called Sat2Cap on a new large-scale dataset of paired overhead and ground-level images. For a given location, our model predicts the expected CLIP embedding of the ground-level scenery. Sat2Cap is also conditioned on temporal information, enabling it to learn dynamic concepts that vary over time. Our experimental results demonstrate that our models successfully capture fine-grained concepts and effectively adapt to temporal variations. Our approach does not require any text-labeled data making the training easily scalable. The code, dataset, and models will be made publicly available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_15904v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的弱监督方法，用于使用自由形式的文本描述（或字幕）创建地图。我们将创建文本映射的这一新工作称为零样本映射。先前的工作通过开发模型来处理映射任务，该模型使用头顶图像预测固定的一组属性。然而，这些模型的限制性很强，因为它们只能解决它们所训练的高度特定的任务。另一方面，映射文本使我们能够以最小的限制来解决各种各样的映射问题。为了实现这一点，我们在一个由成对的头顶和地面图像组成的新的大规模数据集上训练了一个名为Sat2Cap的对比学习框架。对于给定的位置，我们的模型预测了地面景观的预期CLIP嵌入。Sat2Cap还以时间信息为条件，使其能够学习随时间变化的动态概念。我们的实验结果表明，我们的模型成功地捕捉了细粒度的概念，并有效地适应了时间变化。我们的方法不需要任何文本标记数据，使训练易于扩展。代码、数据集和模型将公开。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.15904v1" target="_blank">2307.15904v1</a>
                              </td>
                              <td>Sat2Cap: Mapping Fine-Grained Textual Descriptions from Satellite Images</td>
                              <td>Aayush Dhakal</td>
                              <td>2023-07-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_15904v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.15904v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_15640v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLIP Brings Better Features to Visual Aesthetics Learners</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_15640v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_15640v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_15640v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The success of pre-training approaches on a variety of downstream tasks has revitalized the field of computer vision. Image aesthetics assessment (IAA) is one of the ideal application scenarios for such methods due to subjective and expensive labeling procedure. In this work, an unified and flexible two-phase \textbf{C}LIP-based \textbf{S}emi-supervised \textbf{K}nowledge \textbf{D}istillation paradigm is proposed, namely \textbf{\textit{CSKD}}. Specifically, we first integrate and leverage a multi-source unlabeled dataset to align rich features between a given visual encoder and an off-the-shelf CLIP image encoder via feature alignment loss. Notably, the given visual encoder is not limited by size or structure and, once well-trained, it can seamlessly serve as a better visual aesthetic learner for both student and teacher. In the second phase, the unlabeled data is also utilized in semi-supervised IAA learning to further boost student model performance when applied in latency-sensitive production scenarios. By analyzing the attention distance and entropy before and after feature alignment, we notice an alleviation of feature collapse issue, which in turn showcase the necessity of feature alignment instead of training directly based on CLIP image encoder. Extensive experiments indicate the superiority of CSKD, which achieves state-of-the-art performance on multiple widely used IAA benchmarks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_15640v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>各种下游任务的预训练方法的成功振兴了计算机视觉领域。图像美学评估（IAA）是此类方法的理想应用场景之一，因为其标记过程主观且昂贵。在这项工作中，提出了一个统一而灵活的基于两阶段的\textbf｛C｝LIP的\textbf｛S｝emi监督\textbf{K｝确认\textbf{D｝识别范式，即\textbf｛\textit｛CSKD｝｝。具体而言，我们首先集成并利用多源未标记数据集，通过特征对齐丢失来对齐给定视觉编码器和现成的CLIP图像编码器之间的丰富特征。值得注意的是，给定的视觉编码器不受尺寸或结构的限制，一旦训练有素，它可以无缝地为学生和教师提供更好的视觉美学学习者。在第二阶段，未标记的数据也用于半监督IAA学习，以在延迟敏感的生产场景中应用时进一步提高学生模型的性能。通过分析特征对齐前后的注意距离和熵，我们注意到特征折叠问题的缓解，这反过来表明了特征对齐的必要性，而不是直接基于CLIP图像编码器进行训练。大量实验表明了CSKD的优越性，它在多个广泛使用的IAA基准上实现了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.15640v1" target="_blank">2307.15640v1</a>
                              </td>
                              <td>CLIP Brings Better Features to Visual Aesthetics Learners</td>
                              <td>Liwu Xu</td>
                              <td>2023-07-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_15640v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.15640v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_15460v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Cross-Modal Concept Learning and Inference for Vision-Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_15460v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_15460v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_15460v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large-scale pre-trained Vision-Language Models (VLMs), such as CLIP, establish the correlation between texts and images, achieving remarkable success on various downstream tasks with fine-tuning. In existing fine-tuning methods, the class-specific text description is matched against the whole image. We recognize that this whole image matching is not effective since images from the same class often contain a set of different semantic objects, and an object further consists of a set of semantic parts or concepts. Individual semantic parts or concepts may appear in image samples from different classes. To address this issue, in this paper, we develop a new method called cross-model concept learning and inference (CCLI). Using the powerful text-image correlation capability of CLIP, our method automatically learns a large set of distinctive visual concepts from images using a set of semantic text concepts. Based on these visual concepts, we construct a discriminative representation of images and learn a concept inference network to perform downstream image classification tasks, such as few-shot learning and domain generalization. Extensive experimental results demonstrate that our CCLI method is able to improve the performance upon the current state-of-the-art methods by large margins, for example, by up to 8.0% improvement on few-shot learning and by up to 1.3% for domain generalization.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_15460v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大规模预训练的视觉语言模型（VLM），如CLIP，建立了文本和图像之间的相关性，通过微调在各种下游任务上取得了显著的成功。在现有的微调方法中，类特定的文本描述与整个图像相匹配。我们认识到，这种全图像匹配是无效的，因为来自同一类的图像通常包含一组不同的语义对象，并且对象进一步由一组语义部分或概念组成。单独的语义部分或概念可以出现在来自不同类别的图像样本中。为了解决这个问题，在本文中，我们开发了一种新的方法，称为跨模型概念学习和推理（CCLI）。利用CLIP强大的文本-图像关联能力，我们的方法使用一组语义文本概念从图像中自动学习一大组独特的视觉概念。基于这些视觉概念，我们构建了图像的判别表示，并学习了一个概念推理网络来执行下游的图像分类任务，如少镜头学习和领域泛化。大量的实验结果表明，我们的CCLI方法能够在当前最先进的方法的基础上大幅提高性能，例如，在少镜头学习方面提高了8.0%，在领域泛化方面提高了1.3%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.15460v1" target="_blank">2307.15460v1</a>
                              </td>
                              <td>Cross-Modal Concept Learning and Inference for Vision-Language Models</td>
                              <td>Yi Zhang</td>
                              <td>2023-07-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_15460v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.15460v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_15344v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Improving Audio-Text Retrieval via Hierarchical Cross-Modal Interaction and Auxiliary Captions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_15344v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_15344v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_15344v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Most existing audio-text retrieval (ATR) methods focus on constructing contrastive pairs between whole audio clips and complete caption sentences, while ignoring fine-grained cross-modal relationships, e.g., short segments and phrases or frames and words. In this paper, we introduce a hierarchical cross-modal interaction (HCI) method for ATR by simultaneously exploring clip-sentence, segment-phrase, and frame-word relationships, achieving a comprehensive multi-modal semantic comparison. Besides, we also present a novel ATR framework that leverages auxiliary captions (AC) generated by a pretrained captioner to perform feature interaction between audio and generated captions, which yields enhanced audio representations and is complementary to the original ATR matching branch. The audio and generated captions can also form new audio-text pairs as data augmentation for training. Experiments show that our HCI significantly improves the ATR performance. Moreover, our AC framework also shows stable performance gains on multiple datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_15344v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大多数现有的音频文本检索（ATR）方法侧重于在整个音频片段和完整的字幕句子之间构建对比对，而忽略了细粒度的跨模态关系，例如短片段和短语或框架和单词。在本文中，我们介绍了一种用于ATR的分层跨模态交互（HCI）方法，通过同时探索剪贴句、分段短语和框架词的关系，实现了全面的多模态语义比较。此外，我们还提出了一种新的ATR框架，该框架利用由预训练的字幕机生成的辅助字幕（AC）来执行音频和生成的字幕之间的特征交互，这产生了增强的音频表示，并且是对原始ATR匹配分支的补充。音频和生成的字幕还可以形成新的音频文本对，作为用于训练的数据扩充。实验表明，我们的HCI显著提高了ATR的性能。此外，我们的AC框架在多个数据集上也显示出稳定的性能提升。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.15344v1" target="_blank">2307.15344v1</a>
                              </td>
                              <td>Improving Audio-Text Retrieval via Hierarchical Cross-Modal Interaction and Auxiliary Captions</td>
                              <td>Yifei Xin</td>
                              <td>2023-07-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_15344v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.15344v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_14750v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exploring Annotation-free Image Captioning with Retrieval-augmented Pseudo Sentence Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_14750v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_14750v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_14750v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Training an image captioner without annotated image-sentence pairs has gained traction in recent years. Previous approaches can be categorized into two strategies: crawling sentences from mismatching corpora and aligning them with the given images as pseudo annotations, or pre-training the captioner using external image-text pairs. However, the aligning setting seems to reach its performance limit due to the quality problem of pairs, and pre-training requires significant computational resources. To address these challenges, we propose a new strategy ``LPM + retrieval-augmented learning" where the prior knowledge from large pre-trained models (LPMs) is leveraged as supervision, and a retrieval process is integrated to further reinforce its effectiveness. Specifically, we introduce Retrieval-augmented Pseudo Sentence Generation (RaPSG), which adopts an efficient approach to retrieve highly relevant short region descriptions from the mismatching corpora and use them to generate a variety of pseudo sentences with distinct representations as well as high quality via LPMs. In addition, a fluency filter and a CLIP-guided training objective are further introduced to facilitate model optimization. Experimental results demonstrate that our method surpasses the SOTA pre-training model (Flamingo3B) by achieving a CIDEr score of 78.1 (+5.1) while utilizing only 0.3% of its trainable parameters (1.3B VS 33M). Importantly, our approach eliminates the need of computationally expensive pre-training processes on external datasets (e.g., the requirement of 312M image-text pairs for Flamingo3B). We further show that with a simple extension, the generated pseudo sentences can be deployed as weak supervision to boost the 1% semi-supervised image caption benchmark up to 93.4 CIDEr score (+8.9) which showcases the versatility and effectiveness of our approach.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_14750v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，训练一个没有注释的图像句子对的图像字幕已经获得了关注。以前的方法可以分为两种策略：从不匹配的语料库中抓取句子，并将它们与给定的图像对齐作为伪注释，或者使用外部图像-文本对预训练字幕作者。然而，由于配对的质量问题，对齐设置似乎达到了其性能极限，并且预训练需要大量的计算资源。为了应对这些挑战，我们提出了一种新的策略“LPM+检索增强学习”，利用来自大型预训练模型（LPM）的先验知识作为监督，并集成检索过程以进一步增强其有效性。具体而言，我们引入了检索增强伪句子生成（RaPSG），它采用了一种有效的方法来从不匹配的语料库中检索高度相关的短区域描述，并使用它们通过LPM生成各种具有不同表示和高质量的伪句子。此外，还进一步引入了流利度过滤器和CLIP引导的训练目标，以促进模型优化。实验结果表明，我们的方法超过了SOTA预训练模型（Flamingo3B），实现了78.1（+5.1）的CIDEr得分，同时仅利用了0.3%的可训练参数（1.3B VS 33M）。重要的是，我们的方法消除了在外部数据集上计算昂贵的预训练过程的需要（例如，Flamingo3B需要312M个图像-文本对）。我们进一步表明，通过简单的扩展，生成的伪语句可以作为弱监督来部署，以将1%的半监督图像字幕基准提高到93.4 CIDEr分数（+8.9），这展示了我们方法的多功能性和有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.14750v2" target="_blank">2307.14750v2</a>
                              </td>
                              <td>Exploring Annotation-free Image Captioning with Retrieval-augmented Pseudo Sentence Generation</td>
                              <td>Zhiyuan Li</td>
                              <td>2023-07-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_14750v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.14750v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_15220v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Multi-modal Representations by Watching Hundreds of Surgical Video Lectures</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_15220v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_15220v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_15220v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advancements in surgical computer vision applications have been driven by fully-supervised methods, primarily using only visual data. These methods rely on manually annotated surgical videos to predict a fixed set of object categories, limiting their generalizability to unseen surgical procedures and downstream tasks. In this work, we put forward the idea that the surgical video lectures available through open surgical e-learning platforms can provide effective supervisory signals for multi-modal representation learning without relying on manual annotations. We address the surgery-specific linguistic challenges present in surgical video lectures by employing multiple complementary automatic speech recognition systems to generate text transcriptions. We then present a novel method, SurgVLP - Surgical Vision Language Pre-training, for multi-modal representation learning. SurgVLP constructs a new contrastive learning objective to align video clip embeddings with the corresponding multiple text embeddings by bringing them together within a joint latent space. To effectively show the representation capability of the learned joint latent space, we introduce several vision-and-language tasks for surgery, such as text-based video retrieval, temporal activity grounding, and video captioning, as benchmarks for evaluation. We further demonstrate that without using any labeled ground truth, our approach can be employed for traditional vision-only surgical downstream tasks, such as surgical tool, phase, and triplet recognition. The code will be made available at https://github.com/CAMMA-public/SurgVLP</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_15220v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>外科计算机视觉应用的最新进展是由完全监督的方法驱动的，主要使用视觉数据。这些方法依赖于手动注释的手术视频来预测一组固定的对象类别，将其可推广性限制在看不见的手术过程和下游任务中。在这项工作中，我们提出了这样的想法，即通过开放的外科电子学习平台提供的外科视频讲座可以为多模态表示学习提供有效的监督信号，而不依赖于手动注释。我们通过使用多个互补的自动语音识别系统来生成文本转录，来解决外科视频讲座中存在的外科特定语言挑战。然后，我们提出了一种新的方法，SurgVLP-外科视觉语言预训练，用于多模态表征学习。SurgVLP构建了一个新的对比学习目标，通过在联合潜在空间内将视频片段嵌入与相应的多个文本嵌入对齐。为了有效地展示学习的关节潜在空间的表示能力，我们引入了几种用于手术的视觉和语言任务，如基于文本的视频检索、时间活动基础和视频字幕，作为评估的基准。我们进一步证明，在不使用任何标记的基本事实的情况下，我们的方法可以用于传统的仅视觉的手术下游任务，如手术工具、相位和三元组识别。该代码将在https://github.com/CAMMA-public/SurgVLP</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.15220v1" target="_blank">2307.15220v1</a>
                              </td>
                              <td>Learning Multi-modal Representations by Watching Hundreds of Surgical Video Lectures</td>
                              <td>Kun Yuan</td>
                              <td>2023-07-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_15220v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.15220v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_06350v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">VITR: Augmenting Vision Transformers with Relation-Focused Learning for Cross-Modal Information Retrieval</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_06350v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_06350v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_06350v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The relations expressed in user queries are vital for cross-modal information retrieval. Relation-focused cross-modal retrieval aims to retrieve information that corresponds to these relations, enabling effective retrieval across different modalities. Pre-trained networks, such as Contrastive Language-Image Pre-training (CLIP), have gained significant attention and acclaim for their exceptional performance in various cross-modal learning tasks. However, the Vision Transformer (ViT) used in these networks is limited in its ability to focus on image region relations. Specifically, ViT is trained to match images with relevant descriptions at the global level, without considering the alignment between image regions and descriptions. This paper introduces VITR, a novel network that enhances ViT by extracting and reasoning about image region relations based on a local encoder. VITR is comprised of two key components. Firstly, it extends the capabilities of ViT-based cross-modal networks by enabling them to extract and reason with region relations present in images. Secondly, VITR incorporates a fusion module that combines the reasoned results with global knowledge to predict similarity scores between images and descriptions. The proposed VITR network was evaluated through experiments on the tasks of relation-focused cross-modal information retrieval. The results derived from the analysis of the RefCOCOg, CLEVR, and Flickr30K datasets demonstrated that the proposed VITR network consistently outperforms state-of-the-art networks in image-to-text and text-to-image retrieval.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_06350v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>用户查询中表达的关系对于跨模态信息检索至关重要。以关系为中心的跨模态检索旨在检索与这些关系相对应的信息，从而实现跨不同模态的有效检索。经过预训练的网络，如对比语言图像预训练（CLIP），因其在各种跨模态学习任务中的出色表现而获得了极大的关注和赞誉。然而，在这些网络中使用的视觉转换器（ViT）在关注图像区域关系的能力方面受到限制。具体而言，ViT被训练为在全局级别将图像与相关描述匹配，而不考虑图像区域和描述之间的对齐。本文介绍了一种新的网络VITR，它通过基于局部编码器的图像区域关系提取和推理来增强ViT。VITR由两个关键组成部分组成。首先，它扩展了基于ViT的跨模态网络的能力，使它们能够提取图像中存在的区域关系并进行推理。其次，VITR结合了一个融合模块，该模块将推理结果与全局知识相结合，以预测图像和描述之间的相似性得分。通过对以关系为中心的跨模态信息检索任务的实验，对所提出的VITR网络进行了评估。对RefCOCOg、CLEVR和Flickr30K数据集的分析结果表明，所提出的VITR网络在图像到文本和文本到图像检索方面始终优于最先进的网络。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.06350v3" target="_blank">2302.06350v3</a>
                              </td>
                              <td>VITR: Augmenting Vision Transformers with Relation-Focused Learning for Cross-Modal Information Retrieval</td>
                              <td>Yan Gong</td>
                              <td>2023-02-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_06350v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.06350v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09233v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Augmenting CLIP with Improved Visio-Linguistic Reasoning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09233v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09233v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09233v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Image-text contrastive models such as CLIP are useful for a variety of downstream applications including zero-shot classification, image-text retrieval and transfer learning. However, these contrastively trained vision-language models often fail on compositional visio-linguistic tasks such as Winoground with performance equivalent to random chance. In our paper, we address this issue and propose a sample-efficient light-weight method called SDS-CLIP to improve the compositional visio-linguistic reasoning capabilities of CLIP. The core idea of our method is to use differentiable image parameterizations to fine-tune CLIP with a distillation objective from large text-to-image generative models such as Stable-Diffusion which are relatively good at visio-linguistic reasoning tasks. On the challenging Winoground compositional reasoning benchmark, our method improves the absolute visio-linguistic performance of different CLIP models by up to 7%, while on the ARO dataset, our method improves the visio-linguistic performance by upto 3%. As a byproduct of inducing visio-linguistic reasoning into CLIP, we also find that the zero-shot performance improves marginally on a variety of downstream datasets. Our method reinforces that carefully designed distillation objectives from generative models can be leveraged to extend existing contrastive image-text models with improved visio-linguistic reasoning capabilities.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09233v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>诸如CLIP的图像-文本对比模型可用于各种下游应用，包括零样本分类、图像-文本检索和迁移学习。然而，这些经过对比训练的视觉语言模型在组合视觉语言任务（如Winoground）中往往失败，其性能相当于随机机会。在我们的论文中，我们解决了这个问题，并提出了一种称为SDS-CLIP的样本有效的轻量级方法，以提高CLIP的组合视觉语言推理能力。我们方法的核心思想是使用可微图像参数化来微调CLIP，其目标是从大文本到图像生成模型（如稳定扩散），这些模型相对擅长视觉语言推理任务。在具有挑战性的Winoground组合推理基准上，我们的方法将不同CLIP模型的视觉语言绝对性能提高了7%，而在ARO数据集上，我们方法将视觉语言性能提高了3%。作为将视觉-语言推理引入CLIP的副产品，我们还发现，在各种下游数据集上，零样本性能略有提高。我们的方法强化了从生成模型中精心设计的提取目标可以用来扩展现有的对比图像-文本模型，并提高视觉-语言推理能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09233v2" target="_blank">2307.09233v2</a>
                              </td>
                              <td>Augmenting CLIP with Improved Visio-Linguistic Reasoning</td>
                              <td>Samyadeep Basu</td>
                              <td>2023-07-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09233v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09233v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_15064v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Self-Supervised Visual Acoustic Matching</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_15064v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_15064v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_15064v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Acoustic matching aims to re-synthesize an audio clip to sound as if it were recorded in a target acoustic environment. Existing methods assume access to paired training data, where the audio is observed in both source and target environments, but this limits the diversity of training data or requires the use of simulated data or heuristics to create paired samples. We propose a self-supervised approach to visual acoustic matching where training samples include only the target scene image and audio -- without acoustically mismatched source audio for reference. Our approach jointly learns to disentangle room acoustics and re-synthesize audio into the target environment, via a conditional GAN framework and a novel metric that quantifies the level of residual acoustic information in the de-biased audio. Training with either in-the-wild web data or simulated data, we demonstrate it outperforms the state-of-the-art on multiple challenging datasets and a wide variety of real-world audio and environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_15064v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>声学匹配旨在将音频片段重新合成为声音，就好像它是在目标声学环境中录制的一样。现有方法假设访问配对训练数据，其中在源和目标环境中都观察到音频，但这限制了训练数据的多样性，或者需要使用模拟数据或启发式方法来创建配对样本。我们提出了一种自监督的视觉声学匹配方法，其中训练样本仅包括目标场景图像和音频，而没有声学不匹配的源音频作为参考。我们的方法通过条件GAN框架和量化去偏音频中残余声学信息水平的新度量，共同学习将房间声学分解并将音频重新合成到目标环境中。使用野外网络数据或模拟数据进行训练，我们证明它在多个具有挑战性的数据集和各种真实世界的音频和环境中都优于最先进的技术。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.15064v1" target="_blank">2307.15064v1</a>
                              </td>
                              <td>Self-Supervised Visual Acoustic Matching</td>
                              <td>Arjun Somayazulu</td>
                              <td>2023-07-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_15064v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.15064v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_15049v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Regularized Mask Tuning: Uncovering Hidden Knowledge in Pre-trained Vision-Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_15049v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_15049v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_15049v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Prompt tuning and adapter tuning have shown great potential in transferring pre-trained vision-language models (VLMs) to various downstream tasks. In this work, we design a new type of tuning method, termed as regularized mask tuning, which masks the network parameters through a learnable selection. Inspired by neural pathways, we argue that the knowledge required by a downstream task already exists in the pre-trained weights but just gets concealed in the upstream pre-training stage. To bring the useful knowledge back into light, we first identify a set of parameters that are important to a given downstream task, then attach a binary mask to each parameter, and finally optimize these masks on the downstream data with the parameters frozen. When updating the mask, we introduce a novel gradient dropout strategy to regularize the parameter selection, in order to prevent the model from forgetting old knowledge and overfitting the downstream data. Experimental results on 11 datasets demonstrate the consistent superiority of our method over previous alternatives. It is noteworthy that we manage to deliver 18.73% performance improvement compared to the zero-shot CLIP via masking an average of only 2.56% parameters. Furthermore, our method is synergistic with most existing parameter-efficient tuning methods and can boost the performance on top of them. Project page can be found here (https://wuw2019.github.io/RMT/).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_15049v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>即时调整和适配器调整在将预先训练的视觉语言模型（VLM）转移到各种下游任务方面显示出巨大的潜力。在这项工作中，我们设计了一种新型的调谐方法，称为正则掩码调谐，它通过可学习的选择来屏蔽网络参数。受神经通路的启发，我们认为下游任务所需的知识已经存在于预训练的权重中，但只是在上游预训练阶段被掩盖了。为了让有用的知识重见天日，我们首先识别一组对给定下游任务很重要的参数，然后为每个参数附加一个二进制掩码，最后在冻结参数的情况下优化下游数据上的这些掩码。在更新掩码时，我们引入了一种新的梯度丢弃策略来正则化参数选择，以防止模型忘记旧知识和过拟合下游数据。在11个数据集上的实验结果表明，与以前的替代方案相比，我们的方法具有一致的优越性。值得注意的是，与零样本CLIP相比，我们通过屏蔽平均仅2.56%的参数，实现了18.73%的性能改进。此外，我们的方法与大多数现有的参数有效调谐方法具有协同作用，可以在它们之上提高性能。项目页面可在此处找到(https://wuw2019.github.io/RMT/)。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.15049v1" target="_blank">2307.15049v1</a>
                              </td>
                              <td>Regularized Mask Tuning: Uncovering Hidden Knowledge in Pre-trained Vision-Language Models</td>
                              <td>Kecheng Zheng</td>
                              <td>2023-07-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_15049v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.15049v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_14768v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Gloss-free Sign Language Translation: Improving from Visual-Language Pretraining</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_14768v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_14768v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_14768v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Sign Language Translation (SLT) is a challenging task due to its cross-domain nature, involving the translation of visual-gestural language to text. Many previous methods employ an intermediate representation, i.e., gloss sequences, to facilitate SLT, thus transforming it into a two-stage task of sign language recognition (SLR) followed by sign language translation (SLT). However, the scarcity of gloss-annotated sign language data, combined with the information bottleneck in the mid-level gloss representation, has hindered the further development of the SLT task. To address this challenge, we propose a novel Gloss-Free SLT based on Visual-Language Pretraining (GFSLT-VLP), which improves SLT by inheriting language-oriented prior knowledge from pre-trained models, without any gloss annotation assistance. Our approach involves two stages: (i) integrating Contrastive Language-Image Pre-training (CLIP) with masked self-supervised learning to create pre-tasks that bridge the semantic gap between visual and textual representations and restore masked sentences, and (ii) constructing an end-to-end architecture with an encoder-decoder-like structure that inherits the parameters of the pre-trained Visual Encoder and Text Decoder from the first stage. The seamless combination of these novel designs forms a robust sign language representation and significantly improves gloss-free sign language translation. In particular, we have achieved unprecedented improvements in terms of BLEU-4 score on the PHOENIX14T dataset (>+5) and the CSL-Daily dataset (>+3) compared to state-of-the-art gloss-free SLT methods. Furthermore, our approach also achieves competitive results on the PHOENIX14T dataset when compared with most of the gloss-based methods. Our code is available at https://github.com/zhoubenjia/GFSLT-VLP.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_14768v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>手语翻译是一项具有跨领域性质的具有挑战性的任务，涉及视觉手势语到文本的翻译。以前的许多方法都使用中间表示，即光泽序列，来促进SLT，从而将其转化为手语识别（SLR）和手语翻译（SLT）的两阶段任务。然而，带光泽注释的手语数据的稀缺性，加上中级光泽表示中的信息瓶颈，阻碍了SLT任务的进一步发展。为了应对这一挑战，我们提出了一种新的基于视觉语言预训练的无光泽SLT（GFSLT-VLP），它通过从预训练的模型中继承面向语言的先验知识来改进SLT，而不需要任何光泽注释辅助。我们的方法包括两个阶段：（i）将对比语言图像预训练（CLIP）与掩蔽自监督学习相结合，以创建预任务，弥合视觉和文本表征之间的语义差距，并恢复掩蔽句子，以及（ii）构建具有类似编码器-解码器的结构的端到端架构，该结构继承来自第一阶段的预训练的视觉编码器和文本解码器的参数。这些新颖设计的无缝结合形成了强大的手语表达，并显著提高了无光泽的手语翻译。特别是，与最先进的无光泽SLT方法相比，我们在PHOENIX14T数据集（>+5）和CSL Daily数据集（>>3）的BLEU-4评分方面取得了前所未有的改进。此外，与大多数基于光泽的方法相比，我们的方法在PHOENIX14T数据集上也取得了有竞争力的结果。我们的代码可在https://github.com/zhoubenjia/GFSLT-VLP.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.14768v1" target="_blank">2307.14768v1</a>
                              </td>
                              <td>Gloss-free Sign Language Translation: Improving from Visual-Language Pretraining</td>
                              <td>Benjia Zhou</td>
                              <td>2023-07-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_14768v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.14768v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_11934v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LAMP: Leveraging Language Prompts for Multi-person Pose Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_11934v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_11934v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_11934v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Human-centric visual understanding is an important desideratum for effective human-robot interaction. In order to navigate crowded public places, social robots must be able to interpret the activity of the surrounding humans. This paper addresses one key aspect of human-centric visual understanding, multi-person pose estimation. Achieving good performance on multi-person pose estimation in crowded scenes is difficult due to the challenges of occluded joints and instance separation. In order to tackle these challenges and overcome the limitations of image features in representing invisible body parts, we propose a novel prompt-based pose inference strategy called LAMP (Language Assisted Multi-person Pose estimation). By utilizing the text representations generated by a well-trained language model (CLIP), LAMP can facilitate the understanding of poses on the instance and joint levels, and learn more robust visual representations that are less susceptible to occlusion. This paper demonstrates that language-supervised training boosts the performance of single-stage multi-person pose estimation, and both instance-level and joint-level prompts are valuable for training. The code is available at https://github.com/shengnanh20/LAMP.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_11934v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>以人为中心的视觉理解是实现人机有效交互的重要要求。为了在拥挤的公共场所导航，社交机器人必须能够解读周围人类的活动。本文讨论了以人为中心的视觉理解的一个关键方面，即多人姿势估计。由于关节遮挡和实例分离的挑战，在拥挤场景中实现良好的多人姿态估计性能是困难的。为了应对这些挑战，并克服图像特征在表示不可见身体部位方面的局限性，我们提出了一种新的基于提示的姿势推断策略，称为LAMP（语言辅助多人姿势估计）。通过利用训练有素的语言模型（CLIP）生成的文本表示，LAMP可以促进对实例和关节级别的姿势的理解，并学习更健壮的视觉表示，这些视觉表示不太容易被遮挡。本文证明，语言监督训练提高了单阶段多人姿势估计的性能，实例级和联合级提示对训练都有价值。代码可在https://github.com/shengnanh20/LAMP.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.11934v2" target="_blank">2307.11934v2</a>
                              </td>
                              <td>LAMP: Leveraging Language Prompts for Multi-person Pose Estimation</td>
                              <td>Shengnan Hu</td>
                              <td>2023-07-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_11934v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.11934v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_14240v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Boon: A Neural Search Engine for Cross-Modal Information Retrieval</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_14240v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_14240v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_14240v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual-Semantic Embedding (VSE) networks can help search engines better understand the meaning behind visual content and associate it with relevant textual information, leading to more accurate search results. VSE networks can be used in cross-modal search engines to embed image and textual descriptions in a shared space, enabling image-to-text and text-to-image retrieval tasks. However, the full potential of VSE networks for search engines has yet to be fully explored. This paper presents Boon, a novel cross-modal search engine that combines two state-of-the-art networks: the GPT-3.5-turbo large language model, and the VSE network VITR (VIsion Transformers with Relation-focused learning) to enhance the engine's capabilities in extracting and reasoning with regional relationships in images. VITR employs encoders from CLIP that were trained with 400 million image-description pairs and it was fine-turned on the RefCOCOg dataset. Boon's neural-based components serve as its main functionalities: 1) a 'cross-modal search engine' that enables end-users to perform image-to-text and text-to-image retrieval. 2) a 'multi-lingual conversational AI' component that enables the end-user to converse about one or more images selected by the end-user. Such a feature makes the search engine accessible to a wide audience, including those with visual impairments. 3) Boon is multi-lingual and can take queries and handle conversations about images in multiple languages. Boon was implemented using the Django and PyTorch frameworks. The interface and capabilities of the Boon search engine are demonstrated using the RefCOCOg dataset, and the engine's ability to search for multimedia through the web is facilitated by Google's API.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_14240v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉语义嵌入（VSE）网络可以帮助搜索引擎更好地理解视觉内容背后的含义，并将其与相关文本信息相关联，从而获得更准确的搜索结果。VSE网络可以用于跨模态搜索引擎，在共享空间中嵌入图像和文本描述，实现图像到文本和文本到图像的检索任务。然而，VSE网络在搜索引擎中的全部潜力还有待充分探索。本文介绍了Boon，一种新型的跨模态搜索引擎，它结合了两个最先进的网络：GPT-3.5-turbo大型语言模型和VSE网络VITR（具有关系集中学习的VIsion Transformers），以增强引擎提取和推理图像中区域关系的能力。VITR采用了CLIP的编码器，这些编码器经过4亿个图像描述对的训练，并在RefCOCOg数据集上进行了微调。Boon基于神经的组件是其主要功能：1）“跨模态搜索引擎”，使最终用户能够执行图像到文本和文本到图像的检索。2） “多语言对话AI”组件，使最终用户能够就最终用户选择的一个或多个图像进行对话。这样的功能使搜索引擎能够被广泛的受众访问，包括那些有视觉障碍的人。3） Boon会说多种语言，可以用多种语言处理有关图像的查询和对话。Boon是使用Django和PyTorch框架实现的。Boon搜索引擎的界面和功能通过RefCOCOOg数据集进行了演示，谷歌的API促进了该引擎通过网络搜索多媒体的能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.14240v1" target="_blank">2307.14240v1</a>
                              </td>
                              <td>Boon: A Neural Search Engine for Cross-Modal Information Retrieval</td>
                              <td>Yan Gong</td>
                              <td>2023-07-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_14240v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.14240v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_14063v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ECO: Ensembling Context Optimization for Vision-Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_14063v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_14063v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_14063v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Image recognition has recently witnessed a paradigm shift, where vision-language models are now used to perform few-shot classification based on textual prompts. Among these, the CLIP model has shown remarkable capabilities for zero-shot transfer by matching an image and a custom textual prompt in its latent space. This has paved the way for several works that focus on engineering or learning textual contexts for maximizing CLIP's classification capabilities. In this paper, we follow this trend by learning an ensemble of prompts for image classification. We show that learning diverse and possibly shorter contexts improves considerably and consistently the results rather than relying on a single trainable prompt. In particular, we report better few-shot capabilities with no additional cost at inference time. We demonstrate the capabilities of our approach on 11 different benchmarks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_14063v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>图像识别最近发生了范式转变，现在使用视觉语言模型基于文本提示进行少量镜头分类。其中，CLIP模型通过在其潜在空间中匹配图像和自定义文本提示，显示了零样本转移的显著能力。这为一些专注于工程或学习文本上下文的工作铺平了道路，以最大限度地提高CLIP的分类能力。在本文中，我们通过学习图像分类的提示集合来遵循这一趋势。我们发现，学习多样化的、可能更短的上下文可以显著且一致地提高结果，而不是依赖于单一的可训练提示。特别是，我们报告了更好的少镜头能力，在推理时没有额外的成本。我们在11个不同的基准上展示了我们的方法的能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.14063v1" target="_blank">2307.14063v1</a>
                              </td>
                              <td>ECO: Ensembling Context Optimization for Vision-Language Models</td>
                              <td>Lorenzo Agnolucci</td>
                              <td>2023-07-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_14063v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.14063v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_18120v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TD-GEM: Text-Driven Garment Editing Mapper</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_18120v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_18120v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_18120v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Language-based fashion image editing allows users to try out variations of desired garments through provided text prompts. Inspired by research on manipulating latent representations in StyleCLIP and HairCLIP, we focus on these latent spaces for editing fashion items of full-body human datasets. Currently, there is a gap in handling fashion image editing due to the complexity of garment shapes and textures and the diversity of human poses. In this paper, we propose an editing optimizer scheme method called Text-Driven Garment Editing Mapper (TD-GEM), aiming to edit fashion items in a disentangled way. To this end, we initially obtain a latent representation of an image through generative adversarial network inversions such as Encoder for Editing (e4e) or Pivotal Tuning Inversion (PTI) for more accurate results. An optimization-based Contrastive Language-Image Pre-training (CLIP) is then utilized to guide the latent representation of a fashion image in the direction of a target attribute expressed in terms of a text prompt. Our TD-GEM manipulates the image accurately according to the target attribute, while other parts of the image are kept untouched. In the experiments, we evaluate TD-GEM on two different attributes (i.e., "color" and "sleeve length"), which effectively generates realistic images compared to the recent manipulation schemes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_18120v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于语言的时尚图像编辑允许用户通过提供的文本提示来尝试所需服装的变体。受StyleCLIP和HairCLIP中操纵潜在表征的研究启发，我们专注于编辑全身人体数据集时尚项目的这些潜在空间。目前，由于服装形状和纹理的复杂性以及人类姿势的多样性，在处理时尚图像编辑方面存在差距。在本文中，我们提出了一种编辑优化器方案，称为文本驱动服装编辑映射器（TD-GEM），旨在以一种解开纠缠的方式编辑时尚项目。为此，我们最初通过生成对抗性网络反转（如编辑编码器（e4e）或关键调整反转（PTI））来获得图像的潜在表示，以获得更准确的结果。然后，利用基于优化的对比语言图像预训练（CLIP）来引导时尚图像在文本提示中表达的目标属性的方向上的潜在表示。我们的TD-GEM根据目标属性准确地操纵图像，而图像的其他部分保持不变。在实验中，我们对TD-GEM的两个不同属性（即“颜色”和“袖长”）进行了评估，与最近的操作方案相比，TD-GEM有效地生成了逼真的图像。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.18120v2" target="_blank">2305.18120v2</a>
                              </td>
                              <td>TD-GEM: Text-Driven Garment Editing Mapper</td>
                              <td>Reza Dadfar</td>
                              <td>2023-05-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_18120v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.18120v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_15786v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">HOICLIP: Efficient Knowledge Transfer for HOI Detection with Vision-Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_15786v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_15786v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_15786v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Human-Object Interaction (HOI) detection aims to localize human-object pairs and recognize their interactions. Recently, Contrastive Language-Image Pre-training (CLIP) has shown great potential in providing interaction prior for HOI detectors via knowledge distillation. However, such approaches often rely on large-scale training data and suffer from inferior performance under few/zero-shot scenarios. In this paper, we propose a novel HOI detection framework that efficiently extracts prior knowledge from CLIP and achieves better generalization. In detail, we first introduce a novel interaction decoder to extract informative regions in the visual feature map of CLIP via a cross-attention mechanism, which is then fused with the detection backbone by a knowledge integration block for more accurate human-object pair detection. In addition, prior knowledge in CLIP text encoder is leveraged to generate a classifier by embedding HOI descriptions. To distinguish fine-grained interactions, we build a verb classifier from training data via visual semantic arithmetic and a lightweight verb representation adapter. Furthermore, we propose a training-free enhancement to exploit global HOI predictions from CLIP. Extensive experiments demonstrate that our method outperforms the state of the art by a large margin on various settings, e.g. +4.04 mAP on HICO-Det. The source code is available in https://github.com/Artanic30/HOICLIP.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_15786v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人-物交互（HOI）检测旨在定位人-物对并识别它们的交互。近年来，对比语言图像预训练（CLIP）在通过知识提取为HOI检测器提供交互先验方面显示出巨大的潜力。然而，这种方法通常依赖于大规模训练数据，并且在少数/零样本场景下性能较差。在本文中，我们提出了一种新的HOI检测框架，该框架能够有效地从CLIP中提取先验知识，并实现更好的泛化。详细地说，我们首先介绍了一种新的交互解码器，通过交叉注意力机制提取CLIP视觉特征图中的信息区域，然后通过知识集成块将其与检测主干融合，以实现更准确的人-物对检测。此外，CLIP文本编码器中的先验知识通过嵌入HOI描述来生成分类器。为了区分细粒度的交互，我们通过视觉语义算法和轻量级动词表示适配器从训练数据中构建了一个动词分类器。此外，我们提出了一种无训练增强，以利用CLIP的全局HOI预测。大量实验表明，我们的方法在各种设置上都大大优于现有技术，例如HICO-Det上的+4.04mAP。源代码位于https://github.com/Artanic30/HOICLIP.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.15786v3" target="_blank">2303.15786v3</a>
                              </td>
                              <td>HOICLIP: Efficient Knowledge Transfer for HOI Detection with Vision-Language Models</td>
                              <td>Shan Ning</td>
                              <td>2023-03-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_15786v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.15786v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_13697v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Benchmarking and Analyzing Generative Data for Visual Recognition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_13697v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_13697v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_13697v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Advancements in large pre-trained generative models have expanded their potential as effective data generators in visual recognition. This work delves into the impact of generative images, primarily comparing paradigms that harness external data (\ie generative \vs retrieval \vs original).   Our key contributions are: \textbf{1) GenBench Construction:} We devise \textbf{GenBench}, a broad benchmark comprising 22 datasets with 2548 categories, to appraise generative data across various visual recognition tasks. \textbf{2) CLER Score:} To address the insufficient correlation of existing metrics (\eg, FID, CLIP score) with downstream recognition performance, we propose \textbf{CLER}, a training-free metric indicating generative data's efficiency for recognition tasks prior to training. \textbf{3) New Baselines:} Comparisons of generative data with retrieved data from the same external pool help to elucidate the unique traits of generative data. \textbf{4) External Knowledge Injection:} By fine-tuning special token embeddings for each category via Textual Inversion, performance improves across 17 datasets, except when dealing with low-resolution reference images.   Our exhaustive benchmark and analysis spotlight generative data's promise in visual recognition, while identifying key challenges for future investigation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_13697v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型预训练生成模型的进步扩大了它们作为视觉识别中有效数据生成器的潜力。这项工作深入研究了生成图像的影响，主要比较了利用外部数据的范式（即生成图像与检索图像与原始图像）。我们的主要贡献是：\textbf｛1）GenBench构建：｝我们设计了\textbf｛GenBench｝，这是一个由22个数据集和2548个类别组成的广泛基准，用于评估各种视觉识别任务中的生成数据。\textbf｛2）CLER分数：｝为了解决现有指标（例如FID、CLIP分数）与下游识别性能之间的相关性不足的问题，我们提出了\textbf{CLER｝，这是一个无训练的指标，用于指示生成数据在训练前对识别任务的效率。\textbf｛3）新基线：｝将生成数据与从同一外部池检索到的数据进行比较有助于阐明生成数据的独特特征。\textbf｛4）外部知识注入：｝通过文本反转微调每个类别的特殊标记嵌入，除处理低分辨率参考图像外，17个数据集的性能都有所提高。我们详尽的基准测试和分析突出了生成数据在视觉识别方面的前景，同时确定了未来调查的关键挑战。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.13697v1" target="_blank">2307.13697v1</a>
                              </td>
                              <td>Benchmarking and Analyzing Generative Data for Visual Recognition</td>
                              <td>Bo Li</td>
                              <td>2023-07-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_13697v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.13697v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_13681v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Visual Language of Fabrics</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_13681v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_13681v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_13681v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce text2fabric, a novel dataset that links free-text descriptions to various fabric materials. The dataset comprises 15,000 natural language descriptions associated to 3,000 corresponding images of fabric materials. Traditionally, material descriptions come in the form of tags/keywords, which limits their expressivity, induces pre-existing knowledge of the appropriate vocabulary, and ultimately leads to a chopped description system. Therefore, we study the use of free-text as a more appropriate way to describe material appearance, taking the use case of fabrics as a common item that non-experts may often deal with. Based on the analysis of the dataset, we identify a compact lexicon, set of attributes and key structure that emerge from the descriptions. This allows us to accurately understand how people describe fabrics and draw directions for generalization to other types of materials. We also show that our dataset enables specializing large vision-language models such as CLIP, creating a meaningful latent space for fabric appearance, and significantly improving applications such as fine-grained material retrieval and automatic captioning.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_13681v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了text2fabric，这是一个新颖的数据集，将自由文本描述链接到各种织物材料。该数据集包括15000个自然语言描述，与3000个织物材料的相应图像相关。传统上，材料描述以标签/关键词的形式出现，这限制了它们的表达能力，诱导了对适当词汇的预先存在的知识，并最终导致了一个支离破碎的描述系统。因此，我们研究使用自由文本作为描述材料外观的一种更合适的方式，将织物的用例作为非专家可能经常处理的常见项目。基于对数据集的分析，我们确定了一个紧凑的词典、一组属性和描述中出现的关键结构。这使我们能够准确地理解人们是如何描述织物的，并为推广到其他类型的材料指明方向。我们还表明，我们的数据集能够专门化大型视觉语言模型，如CLIP，为织物外观创造一个有意义的潜在空间，并显著改进细粒度材料检索和自动字幕等应用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.13681v1" target="_blank">2307.13681v1</a>
                              </td>
                              <td>The Visual Language of Fabrics</td>
                              <td>Valentin Deschaintre</td>
                              <td>2023-07-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_13681v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.13681v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_13680v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">High Probability Analysis for Non-Convex Stochastic Optimization with Clipping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_13680v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_13680v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_13680v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Gradient clipping is a commonly used technique to stabilize the training process of neural networks. A growing body of studies has shown that gradient clipping is a promising technique for dealing with the heavy-tailed behavior that emerged in stochastic optimization as well. While gradient clipping is significant, its theoretical guarantees are scarce. Most theoretical guarantees only provide an in-expectation analysis and only focus on optimization performance. In this paper, we provide high probability analysis in the non-convex setting and derive the optimization bound and the generalization bound simultaneously for popular stochastic optimization algorithms with gradient clipping, including stochastic gradient descent and its variants of momentum and adaptive stepsizes. With the gradient clipping, we study a heavy-tailed assumption that the gradients only have bounded $\alpha$-th moments for some $\alpha \in (1, 2]$, which is much weaker than the standard bounded second-moment assumption. Overall, our study provides a relatively complete picture for the theoretical guarantee of stochastic optimization algorithms with clipping.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_13680v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>梯度裁剪是一种常用的稳定神经网络训练过程的技术。越来越多的研究表明，梯度裁剪是一种很有前途的技术，可以处理随机优化中出现的重尾行为。虽然梯度削波是重要的，但其理论保证很少。大多数理论保证只提供预期分析，只关注优化性能。在本文中，我们在非凸设置中提供了高概率分析，并同时导出了具有梯度裁剪的流行随机优化算法的优化界和推广界，包括随机梯度下降及其动量和自适应步长的变体。利用梯度裁剪，我们研究了一个重尾假设，即对于某些$\alpha\in（1,2]$），梯度只有有界的$\alph$-阶矩，这比标准的有界二阶矩假设弱得多。总之，我们的研究为具有裁剪的随机优化算法的理论保证提供了一个相对完整的画面。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.13680v1" target="_blank">2307.13680v1</a>
                              </td>
                              <td>High Probability Analysis for Non-Convex Stochastic Optimization with Clipping</td>
                              <td>Shaojie Li</td>
                              <td>2023-07-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_13680v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.13680v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_05796v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Generalizing DP-SGD with Shuffling and Batch Clipping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_05796v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_05796v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_05796v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Classical differential private DP-SGD implements individual clipping with random subsampling, which forces a mini-batch SGD approach. We provide a general differential private algorithmic framework that goes beyond DP-SGD and allows any possible first order optimizers (e.g., classical SGD and momentum based SGD approaches) in combination with batch clipping, which clips an aggregate of computed gradients rather than summing clipped gradients (as is done in individual clipping). The framework also admits sampling techniques beyond random subsampling such as shuffling. Our DP analysis follows the $f$-DP approach and introduces a new proof technique which allows us to derive simple closed form expressions and to also analyse group privacy. In particular, for $E$ epochs work and groups of size $g$, we show a $\sqrt{g E}$ DP dependency for batch clipping with shuffling.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_05796v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>经典的差分专用DP-SGD通过随机子采样实现了单独的削波，这强制采用了小批量SGD方法。我们提供了一个通用的差分私有算法框架，该框架超越了DP-SGD，并允许任何可能的一阶优化器（例如，经典的SGD和基于动量的SGD方法）与批量剪裁相结合，批量剪裁是对计算的梯度的集合进行剪裁，而不是对剪裁的梯度求和（如在单独剪裁中所做的那样）。该框架还允许使用随机子采样之外的采样技术，如混洗。我们的DP分析遵循$f$-DP方法，并引入了一种新的证明技术，该技术允许我们导出简单的闭式表达式，还可以分析组隐私。特别是，对于$E$划时代的工作和$g$大小的组，我们展示了一个$\sqrt｛gE｝$DP依赖关系，用于带有shuffling的批量裁剪。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.05796v3" target="_blank">2212.05796v3</a>
                              </td>
                              <td>Generalizing DP-SGD with Shuffling and Batch Clipping</td>
                              <td>Marten van Dijk</td>
                              <td>2022-12-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_05796v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.05796v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_14108v4_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DataComp: In search of the next generation of multimodal datasets</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_14108v4_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_14108v4_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_14108v4_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multimodal datasets are a critical component in recent breakthroughs such as Stable Diffusion and GPT-4, yet their design does not receive the same research attention as model architectures or training algorithms. To address this shortcoming in the ML ecosystem, we introduce DataComp, a testbed for dataset experiments centered around a new candidate pool of 12.8 billion image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing the resulting model on 38 downstream test sets. Our benchmark consists of multiple compute scales spanning four orders of magnitude, which enables the study of scaling trends and makes the benchmark accessible to researchers with varying resources. Our baseline experiments show that the DataComp workflow leads to better training sets. In particular, our best baseline, DataComp-1B, enables training a CLIP ViT-L/14 from scratch to 79.2% zero-shot accuracy on ImageNet, outperforming OpenAI's CLIP ViT-L/14 by 3.7 percentage points while using the same training procedure and compute. We release DataComp and all accompanying code at www.datacomp.ai.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_14108v4_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多模式数据集是最近突破的关键组成部分，如稳定扩散和GPT-4，但它们的设计并没有像模型架构或训练算法那样受到研究关注。为了解决ML生态系统中的这一缺点，我们引入了DataComp，这是一个数据集实验的测试平台，围绕着来自Common Crawl的128亿个图像-文本对的新候选池进行。我们基准测试的参与者设计新的过滤技术或策划新的数据源，然后通过运行我们的标准化CLIP训练代码并在38个下游测试集上测试结果模型来评估他们的新数据集。我们的基准由跨越四个数量级的多个计算量表组成，这使得能够研究缩放趋势，并使具有不同资源的研究人员能够访问该基准。我们的基线实验表明，DataComp工作流可以产生更好的训练集。特别是，我们的最佳基线DataComp-1B能够在ImageNet上将CLIP ViT-L/14从零开始训练到79.2%的零样本精度，在使用相同的训练程序和计算时，比OpenAI的CLIP ViT-L/14高3.7个百分点。我们在www.DataComp.ai上发布DataComp和所有附带代码。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.14108v4" target="_blank">2304.14108v4</a>
                              </td>
                              <td>DataComp: In search of the next generation of multimodal datasets</td>
                              <td>Samir Yitzhak Gadre</td>
                              <td>2023-04-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_14108v4_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.14108v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_13136v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Does Progress On Object Recognition Benchmarks Improve Real-World Generalization?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_13136v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_13136v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_13136v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>For more than a decade, researchers have measured progress in object recognition on ImageNet-based generalization benchmarks such as ImageNet-A, -C, and -R. Recent advances in foundation models, trained on orders of magnitude more data, have begun to saturate these standard benchmarks, but remain brittle in practice. This suggests standard benchmarks, which tend to focus on predefined or synthetic changes, may not be sufficient for measuring real world generalization. Consequently, we propose studying generalization across geography as a more realistic measure of progress using two datasets of objects from households across the globe. We conduct an extensive empirical evaluation of progress across nearly 100 vision models up to most recent foundation models. We first identify a progress gap between standard benchmarks and real-world, geographical shifts: progress on ImageNet results in up to 2.5x more progress on standard generalization benchmarks than real-world distribution shifts. Second, we study model generalization across geographies by measuring the disparities in performance across regions, a more fine-grained measure of real world generalization. We observe all models have large geographic disparities, even foundation CLIP models, with differences of 7-20% in accuracy between regions. Counter to modern intuition, we discover progress on standard benchmarks fails to improve geographic disparities and often exacerbates them: geographic disparities between the least performant models and today's best models have more than tripled. Our results suggest scaling alone is insufficient for consistent robustness to real-world distribution shifts. Finally, we highlight in early experiments how simple last layer retraining on more representative, curated data can complement scaling as a promising direction of future work, reducing geographic disparity on both benchmarks by over two-thirds.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_13136v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>十多年来，研究人员一直在基于ImageNet的泛化基准（如ImageNet-a、-C和-R）上测量对象识别的进展。基础模型的最新进展，基于数量级以上的数据进行训练，已经开始使这些标准基准饱和，但在实践中仍然很脆弱。这表明，倾向于关注预定义或合成变化的标准基准可能不足以衡量现实世界的普遍性。因此，我们建议使用来自全球家庭的两个对象数据集，研究跨地理的概括，作为一种更现实的进步衡量标准。我们对从最近的基础模型到近100个愿景模型的进展进行了广泛的实证评估。我们首先确定了标准基准和现实世界的地理变化之间的进展差距：ImageNet上的进展导致标准泛化基准的进展是现实世界分布变化的2.5倍。其次，我们通过测量区域间性能的差异来研究跨地理区域的模型泛化，这是对现实世界泛化的一种更细粒度的测量。我们观察到，所有模型都有很大的地理差异，甚至是基础CLIP模型，区域之间的准确率差异为7-20%。与现代直觉相反，我们发现在标准基准方面取得的进展并不能改善地理差异，而且往往会加剧这种差异：表现最差的模型和当今最好的模型之间的地理差异增加了两倍多。我们的结果表明，单独的缩放不足以对真实世界的分布变化保持一致的稳健性。最后，我们在早期实验中强调，对更具代表性、更具策划性的数据进行简单的最后一层再培训，可以作为未来工作的一个有希望的方向来补充扩展，将两个基准的地理差异减少三分之二以上。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.13136v1" target="_blank">2307.13136v1</a>
                              </td>
                              <td>Does Progress On Object Recognition Benchmarks Improve Real-World Generalization?</td>
                              <td>Megan Richards</td>
                              <td>2023-07-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_13136v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.13136v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_12980v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_12980v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_12980v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_12980v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Prompt engineering is a technique that involves augmenting a large pre-trained model with task-specific hints, known as prompts, to adapt the model to new tasks. Prompts can be created manually as natural language instructions or generated automatically as either natural language instructions or vector representations. Prompt engineering enables the ability to perform predictions based solely on prompts without updating model parameters, and the easier application of large pre-trained models in real-world tasks. In past years, Prompt engineering has been well-studied in natural language processing. Recently, it has also been intensively studied in vision-language modeling. However, there is currently a lack of a systematic overview of prompt engineering on pre-trained vision-language models. This paper aims to provide a comprehensive survey of cutting-edge research in prompt engineering on three types of vision-language models: multimodal-to-text generation models (e.g. Flamingo), image-text matching models (e.g. CLIP), and text-to-image generation models (e.g. Stable Diffusion). For each type of model, a brief model summary, prompting methods, prompting-based applications, and the corresponding responsibility and integrity issues are summarized and discussed. Furthermore, the commonalities and differences between prompting on vision-language models, language models, and vision models are also discussed. The challenges, future directions, and research opportunities are summarized to foster future research on this topic.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_12980v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>提示工程是一种技术，它涉及用特定于任务的提示（称为提示）来增强预先训练的大型模型，以使模型适应新任务。提示可以手动创建为自然语言指令，也可以自动生成为自然语言指示或矢量表示。提示工程使您能够在不更新模型参数的情况下仅根据提示执行预测，并使大型预训练模型更容易应用于现实世界的任务。在过去的几年里，提示工程在自然语言处理中得到了很好的研究。近年来，它在视觉语言建模中也得到了深入的研究。然而，目前缺乏对预先训练的视觉语言模型的即时工程的系统概述。本文旨在对提示工程中三种类型的视觉语言模型的前沿研究进行全面综述：多模式到文本生成模型（如Flamingo）、图像-文本匹配模型（如CLIP）和文本到图像生成模型（例如Stable Diffusion）。对于每种类型的模型，都会简要总结和讨论模型摘要、提示方法、基于提示的应用程序以及相应的责任和完整性问题。此外，还讨论了视觉语言模型、语言模型和视觉模型提示的异同。总结了挑战、未来方向和研究机会，以促进未来对该主题的研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>46</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.12980v1" target="_blank">2307.12980v1</a>
                              </td>
                              <td>A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models</td>
                              <td>Jindong Gu</td>
                              <td>2023-07-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_12980v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.12980v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="DINO"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2307_11067v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CNOS: A Strong Baseline for CAD-based Novel Object Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_11067v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_11067v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_11067v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a simple three-stage approach to segment unseen objects in RGB images using their CAD models. Leveraging recent powerful foundation models, DINOv2 and Segment Anything, we create descriptors and generate proposals, including binary masks for a given input RGB image. By matching proposals with reference descriptors created from CAD models, we achieve precise object ID assignment along with modal masks. We experimentally demonstrate that our method achieves state-of-the-art results in CAD-based novel object segmentation, surpassing existing approaches on the seven core datasets of the BOP challenge by 19.8% AP using the same BOP evaluation protocol. Our source code is available at https://github.com/nv-nguyen/cnos.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_11067v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种简单的三阶段方法，使用RGB图像中的CAD模型来分割看不见的对象。利用最近强大的基础模型DINOv2和Segment Anything，我们创建描述符并生成建议，包括给定输入RGB图像的二进制掩码。通过将方案与从CAD模型创建的参考描述符相匹配，我们实现了精确的对象ID分配以及模式掩码。我们通过实验证明，我们的方法在基于CAD的新对象分割中取得了最先进的结果，使用相同的BOP评估协议，在BOP挑战的七个核心数据集上超过了现有方法19.8%AP。我们的源代码可在https://github.com/nv-nguyen/cnos.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.11067v2" target="_blank">2307.11067v2</a>
                              </td>
                              <td>CNOS: A Strong Baseline for CAD-based Novel Object Segmentation</td>
                              <td>Van Nguyen Nguyen</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_11067v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.11067v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2006_01236v5_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Aperiodicity, Star-freeness, and First-order Logic Definability of Structured Context-Free Languages</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2006_01236v5_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2006_01236v5_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2006_01236v5_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A classic result in formal language theory is the equivalence among non-counting, or aperiodic, regular languages, and languages defined through star-free regular expressions, or first-order logic. Past attempts to extend this result beyond the realm of regular languages have met with difficulties: for instance it is known that star-free tree languages may violate the non-counting property and there are aperiodic tree languages that cannot be defined through first-order logic. We extend such classic equivalence results to a significant family of deterministic context-free languages, the operator-precedence languages (OPL), which strictly includes the widely investigated visibly pushdown, alias input-driven, family and other structured context-free languages. The OP model originated in the '60s for defining programming languages and is still used by high performance compilers; its rich algebraic properties have been investigated initially in connection with grammar learning and recently completed with further closure properties and with monadic second order logic definition. We introduce an extension of regular expressions, the OP-expressions (OPE) which define the OPLs and, under the star-free hypothesis, define first-order definable and non-counting OPLs. Then, we prove, through a fairly articulated grammar transformation, that aperiodic OPLs are first-order definable. Thus, the classic equivalence of star-freeness, aperiodicity, and first-order definability is established for the large and powerful class of OPLs. We argue that the same approach can be exploited to obtain analogous results for visibly pushdown languages too.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2006_01236v5_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>形式语言理论中的一个经典结果是非计数或非周期的正则语言与通过无星正则表达式或一阶逻辑定义的语言之间的等价。过去试图将这一结果扩展到正则语言之外的尝试遇到了困难：例如，已知无星树语言可能违反不计数性质，并且存在无法通过一阶逻辑定义的非周期树语言。我们将这种经典等价结果扩展到一个重要的确定上下文无关语言家族，即算子优先语言（OPL），它严格包括广泛研究的可见下推、别名输入驱动、家族和其他结构化上下文无关语言。OP模型起源于60年代，用于定义编程语言，目前仍被高性能编译器使用；它丰富的代数性质最初是在语法学习中研究的，最近又完成了进一步的闭包性质和一元二阶逻辑定义。我们引入了正则表达式的一个扩展，即OP表达式（OPE），它定义了OPL，并且在无星假设下，定义了一阶可定义和不计数的OPL。然后，我们通过一个相当清晰的语法转换证明了非周期OPL是一阶可定义的。因此，对于大而有力的OPL类，建立了星自由度、非周期性和一阶可定义性的经典等价性。我们认为，同样的方法也可以用于获得明显下推语言的类似结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2006.01236v5" target="_blank">2006.01236v5</a>
                              </td>
                              <td>Aperiodicity, Star-freeness, and First-order Logic Definability of Structured Context-Free Languages</td>
                              <td>Dino Mandrioli</td>
                              <td>2020-06-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2006_01236v5_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2006.01236v5" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10907v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Role of Entropy and Reconstruction in Multi-View Self-Supervised Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10907v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10907v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10907v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The mechanisms behind the success of multi-view self-supervised learning (MVSSL) are not yet fully understood. Contrastive MVSSL methods have been studied through the lens of InfoNCE, a lower bound of the Mutual Information (MI). However, the relation between other MVSSL methods and MI remains unclear. We consider a different lower bound on the MI consisting of an entropy and a reconstruction term (ER), and analyze the main MVSSL families through its lens. Through this ER bound, we show that clustering-based methods such as DeepCluster and SwAV maximize the MI. We also re-interpret the mechanisms of distillation-based approaches such as BYOL and DINO, showing that they explicitly maximize the reconstruction term and implicitly encourage a stable entropy, and we confirm this empirically. We show that replacing the objectives of common MVSSL methods with this ER bound achieves competitive performance, while making them stable when training with smaller batch sizes or smaller exponential moving average (EMA) coefficients.   Github repo: https://github.com/apple/ml-entropy-reconstruction.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10907v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多视角自我监督学习（MVSSL）成功背后的机制尚不完全清楚。通过互信息（MI）的下界InfoNCE的视角研究了MVSSL的对比方法。然而，其他MVSSL方法与MI之间的关系仍不清楚。我们考虑了由熵和重建项（ER）组成的MI的不同下界，并通过其透镜分析了主要的MVSSL族。通过这个ER界，我们证明了基于聚类的方法，如DeepCluster和SwAV，最大化了MI。我们还重新解释了基于蒸馏的方法（如BYOL和DINO）的机制，表明它们显式地最大化了重建项，隐式地鼓励了稳定的熵，我们从经验上证实了这一点。我们表明，用该ER界取代常见MVSSL方法的目标可以获得有竞争力的性能，同时在使用较小的批量或较小的指数移动平均（EMA）系数进行训练时使其稳定。Github回购：https://github.com/apple/ml-entropy-reconstruction.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10907v1" target="_blank">2307.10907v1</a>
                              </td>
                              <td>The Role of Entropy and Reconstruction in Multi-View Self-Supervised Learning</td>
                              <td>Borja Rodríguez-Gálvez</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10907v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10907v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03376v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Weakly-supervised Contrastive Learning for Unsupervised Object Discovery</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03376v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03376v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03376v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Unsupervised object discovery (UOD) refers to the task of discriminating the whole region of objects from the background within a scene without relying on labeled datasets, which benefits the task of bounding-box-level localization and pixel-level segmentation. This task is promising due to its ability to discover objects in a generic manner. We roughly categorise existing techniques into two main directions, namely the generative solutions based on image resynthesis, and the clustering methods based on self-supervised models. We have observed that the former heavily relies on the quality of image reconstruction, while the latter shows limitations in effectively modeling semantic correlations. To directly target at object discovery, we focus on the latter approach and propose a novel solution by incorporating weakly-supervised contrastive learning (WCL) to enhance semantic information exploration. We design a semantic-guided self-supervised learning model to extract high-level semantic features from images, which is achieved by fine-tuning the feature encoder of a self-supervised model, namely DINO, via WCL. Subsequently, we introduce Principal Component Analysis (PCA) to localize object regions. The principal projection direction, corresponding to the maximal eigenvalue, serves as an indicator of the object region(s). Extensive experiments on benchmark unsupervised object discovery datasets demonstrate the effectiveness of our proposed solution. The source code and experimental results are publicly available via our project page at https://github.com/npucvr/WSCUOD.git.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03376v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>无监督对象发现（UOD）是指在不依赖标记数据集的情况下，从场景内的背景中区分整个对象区域的任务，这有利于边界框级定位和像素级分割的任务。这项任务很有前景，因为它能够以通用的方式发现对象。我们将现有技术大致分为两个主要方向，即基于图像再合成的生成解决方案和基于自监督模型的聚类方法。我们观察到，前者在很大程度上依赖于图像重建的质量，而后者在有效建模语义相关性方面表现出局限性。为了直接针对对象发现，我们专注于后一种方法，并通过结合弱监督对比学习（WCL）来增强语义信息探索，提出了一种新的解决方案。我们设计了一个语义引导的自监督学习模型来从图像中提取高级语义特征，这是通过WCL微调自监督模型（即DINO）的特征编码器来实现的。随后，我们引入主成分分析（PCA）来定位对象区域。与最大特征值相对应的主投影方向用作对象区域的指示符。在基准无监督对象发现数据集上进行的大量实验证明了我们提出的解决方案的有效性。源代码和实验结果可通过我们的项目页面公开获取，网址为https://github.com/npucvr/WSCUOD.git.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03376v1" target="_blank">2307.03376v1</a>
                              </td>
                              <td>Weakly-supervised Contrastive Learning for Unsupervised Object Discovery</td>
                              <td>Yunqiu Lv</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03376v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03376v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_08069v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DETRs Beat YOLOs on Real-time Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_08069v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_08069v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_08069v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, end-to-end transformer-based detectors~(DETRs) have achieved remarkable performance. However, the issue of the high computational cost of DETRs has not been effectively addressed, limiting their practical application and preventing them from fully exploiting the benefits of no post-processing, such as non-maximum suppression (NMS). In this paper, we first analyze the influence of NMS in modern real-time object detectors on inference speed, and establish an end-to-end speed benchmark. To avoid the inference delay caused by NMS, we propose a Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge. Specifically, we design an efficient hybrid encoder to efficiently process multi-scale features by decoupling the intra-scale interaction and cross-scale fusion, and propose IoU-aware query selection to improve the initialization of object queries. In addition, our proposed detector supports flexibly adjustment of the inference speed by using different decoder layers without the need for retraining, which facilitates the practical application of real-time object detectors. Our RT-DETR-L achieves 53.0% AP on COCO val2017 and 114 FPS on T4 GPU, while RT-DETR-X achieves 54.8% AP and 74 FPS, outperforming all YOLO detectors of the same scale in both speed and accuracy. Furthermore, our RT-DETR-R50 achieves 53.1% AP and 108 FPS, outperforming DINO-Deformable-DETR-R50 by 2.2% AP in accuracy and by about 21 times in FPS. ource code and pre-trained models are available at https://github.com/lyuwenyu/RT-DETR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_08069v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，基于变压器的端到端检测器（DETR）取得了显著的性能。然而，DETR的高计算成本问题尚未得到有效解决，这限制了它们的实际应用，并使它们无法充分利用无后处理的好处，例如非最大值抑制（NMS）。本文首先分析了现代实时对象检测器中NMS对推理速度的影响，并建立了端到端速度基准。为了避免NMS引起的推理延迟，我们提出了一种实时检测TRansformer（RT-DETR），这是我们所知的第一个实时端到端对象检测器。具体而言，我们设计了一种高效的混合编码器，通过解耦尺度内交互和跨尺度融合来高效处理多尺度特征，并提出了IoU感知查询选择，以提高对象查询的初始化能力。此外，我们提出的检测器支持通过使用不同的解码器层来灵活调整推理速度，而不需要重新训练，这有助于实时对象检测器的实际应用。我们的RT-DETR-L在COCO val2017上实现了53.0%的AP，在T4 GPU上实现了114 FPS，而RT-DETR-X实现了54.8%的AP和74 FPS，在速度和精度方面都优于相同规模的所有YOLO检测器。此外，我们的RT-DETR-R50实现了53.1%的AP和108 FPS，在精度上比DINO-Deformable-DETR-R5高出2.2%的AP，在FPS上高出约21倍。源代码和预训练模型可在https://github.com/lyuwenyu/RT-DETR.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.08069v2" target="_blank">2304.08069v2</a>
                              </td>
                              <td>DETRs Beat YOLOs on Real-time Object Detection</td>
                              <td>Wenyu Lv</td>
                              <td>2023-04-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_08069v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.08069v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_06211v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_06211v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_06211v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_06211v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segment anything model (SAM) developed by Meta AI Research has recently attracted significant attention. Trained on a large segmentation dataset of over 1 billion masks, SAM is capable of segmenting any object on a certain image. In the original SAM work, the authors turned to zero-short transfer tasks (like edge detection) for evaluating the performance of SAM. Recently, numerous works have attempted to investigate the performance of SAM in various scenarios to recognize and segment objects. Moreover, numerous projects have emerged to show the versatility of SAM as a foundation model by combining it with other models, like Grounding DINO, Stable Diffusion, ChatGPT, etc. With the relevant papers and projects increasing exponentially, it is challenging for the readers to catch up with the development of SAM. To this end, this work conducts the first yet comprehensive survey on SAM. This is an ongoing project and we intend to update the manuscript on a regular basis. Therefore, readers are welcome to contact us if they complete new works related to SAM so that we can include them in our next version.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_06211v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Meta AI Research开发的分段任意模型（SAM）最近引起了人们的极大关注。SAM在超过10亿个掩码的大型分割数据集上进行训练，能够分割特定图像上的任何对象。在最初的SAM工作中，作者转向零短转移任务（如边缘检测）来评估SAM的性能。最近，许多工作试图研究SAM在各种场景中的性能，以识别和分割对象。此外，通过将SAM与其他模型相结合，如Grounding DINO、Stable Diffusion、ChatGPT等，已经出现了许多项目来展示SAM作为基础模型的多功能性。随着相关论文和项目呈指数级增长，读者很难跟上SAM的发展。为此，本工作首次对SAM进行了全面的调查。这是一个正在进行的项目，我们打算定期更新手稿。因此，如果读者完成了与SAM相关的新作品，欢迎与我们联系，以便我们将其纳入下一版本。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.06211v3" target="_blank">2306.06211v3</a>
                              </td>
                              <td>A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering</td>
                              <td>Chaoning Zhang</td>
                              <td>2023-05-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_06211v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.06211v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09165v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DEYOv2: Rank Feature with Greedy Matching for End-to-End Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09165v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09165v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09165v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a novel object detector called DEYOv2, an improved version of the first-generation DEYO (DETR with YOLO) model. DEYOv2, similar to its predecessor, DEYOv2 employs a progressive reasoning approach to accelerate model training and enhance performance. The study delves into the limitations of one-to-one matching in optimization and proposes solutions to effectively address the issue, such as Rank Feature and Greedy Matching. This approach enables the third stage of DEYOv2 to maximize information acquisition from the first and second stages without needing NMS, achieving end-to-end optimization. By combining dense queries, sparse queries, one-to-many matching, and one-to-one matching, DEYOv2 leverages the advantages of each method. It outperforms all existing query-based end-to-end detectors under the same settings. When using ResNet-50 as the backbone and multi-scale features on the COCO dataset, DEYOv2 achieves 51.1 AP and 51.8 AP in 12 and 24 epochs, respectively. Compared to the end-to-end model DINO, DEYOv2 provides significant performance gains of 2.1 AP and 1.4 AP in the two epoch settings. To the best of our knowledge, DEYOv2 is the first fully end-to-end object detector that combines the respective strengths of classical detectors and query-based detectors.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09165v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种称为DEYOv2的新型物体检测器，它是第一代DEYO（DETR with YOLO）模型的改进版本。与前代类似，DEYOv2采用渐进式推理方法来加速模型训练并提高性能。该研究深入探讨了一对一匹配在优化中的局限性，并提出了有效解决该问题的解决方案，如秩特征和贪婪匹配。这种方法使DEYOv2的第三阶段能够最大限度地从第一和第二阶段获取信息，而无需NMS，实现端到端优化。通过组合密集查询、稀疏查询、一对多匹配和一对一匹配，DEYOv2充分利用了每种方法的优势。在相同设置下，它的性能优于所有现有的基于查询的端到端检测器。当在COCO数据集上使用ResNet-50作为主干和多尺度特征时，DEYOv2在12个和24个时期分别实现了51.1个AP和51.8个AP。与端到端模型DINO相比，DEYOv2在两个历元设置中提供了2.1 AP和1.4 AP的显著性能提升。据我们所知，DEYOv2是第一个完全端到端的对象检测器，它结合了经典检测器和基于查询的检测器的各自优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09165v2" target="_blank">2306.09165v2</a>
                              </td>
                              <td>DEYOv2: Rank Feature with Greedy Matching for End-to-End Object Detection</td>
                              <td>Haodong Ouyang</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09165v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09165v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_12860v4_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DETRs with Collaborative Hybrid Assignments Training</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_12860v4_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_12860v4_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_12860v4_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we provide the observation that too few queries assigned as positive samples in DETR with one-to-one set matching leads to sparse supervisions on the encoder's output which considerably hurt the discriminative feature learning of the encoder and vice visa for attention learning in the decoder. To alleviate this, we present a novel collaborative hybrid assignments training scheme, namely Co-DETR, to learn more efficient and effective DETR-based detectors from versatile label assignment manners. This new training scheme can easily enhance the encoder's learning ability in end-to-end detectors by training the multiple parallel auxiliary heads supervised by one-to-many label assignments such as ATSS and Faster RCNN. In addition, we conduct extra customized positive queries by extracting the positive coordinates from these auxiliary heads to improve the training efficiency of positive samples in the decoder. In inference, these auxiliary heads are discarded and thus our method introduces no additional parameters and computational cost to the original detector while requiring no hand-crafted non-maximum suppression (NMS). We conduct extensive experiments to evaluate the effectiveness of the proposed approach on DETR variants, including DAB-DETR, Deformable-DETR, and DINO-Deformable-DETR. Specifically, we improve the basic Deformable-DETR by 5.8% AP in 12-epoch training and 3.2% AP in 36-epoch training. The state-of-the-art DINO-Deformable-DETR with Swin-L can still be improved from 58.5% to 59.5% AP on COCO val. Surprisingly, incorporated with ViT-L backbone, we achieve 65.6% AP on COCO test-dev, outperforming previous methods with much fewer model sizes. Codes will be available at https://github.com/Sense-X/Co-DETR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_12860v4_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们观察到，在具有一对一集匹配的DETR中，被分配为正样本的查询太少，导致对编码器输出的稀疏监督，这大大损害了编码器的判别特征学习，反之亦然。为了缓解这种情况，我们提出了一种新的协作混合分配训练方案，即Co-DETR，以从通用的标签分配方式中学习更高效、更有效的基于DETR的检测器。这种新的训练方案可以通过训练由一对多标签分配（如ATSS和Faster RCNN）监督的多个并行辅助头，轻松增强编码器在端到端检测器中的学习能力。此外，我们通过从这些辅助头中提取正坐标来进行额外定制的正查询，以提高解码器中正样本的训练效率。在推断中，这些辅助头被丢弃，因此我们的方法在不需要手工制作的非最大值抑制（NMS）的同时，没有给原始检测器引入额外的参数和计算成本。我们进行了广泛的实验来评估所提出的方法对DETR变体的有效性，包括DAB-DETR、可变形DETR和DINO可变形DETER。具体来说，我们在12个历元训练中将基本的可变形DETR提高了5.8%AP，在36个历元的训练中提高了3.2%。最先进的带Swin-L的DINO可变形DETR在COCO上的AP仍然可以从58.5%提高到59.5%。令人惊讶的是，与ViT-L主干相结合，我们在COCO测试开发上实现了65.6%的AP，优于以前的方法，模型尺寸要小得多。代码将在https://github.com/Sense-X/Co-DETR.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.12860v4" target="_blank">2211.12860v4</a>
                              </td>
                              <td>DETRs with Collaborative Hybrid Assignments Training</td>
                              <td>Zhuofan Zong</td>
                              <td>2022-11-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_12860v4_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.12860v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_15472v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Taming Detection Transformers for Medical Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_15472v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_15472v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_15472v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The accurate detection of suspicious regions in medical images is an error-prone and time-consuming process required by many routinely performed diagnostic procedures. To support clinicians during this difficult task, several automated solutions were proposed relying on complex methods with many hyperparameters. In this study, we investigate the feasibility of DEtection TRansformer (DETR) models for volumetric medical object detection. In contrast to previous works, these models directly predict a set of objects without relying on the design of anchors or manual heuristics such as non-maximum-suppression to detect objects. We show by conducting extensive experiments with three models, namely DETR, Conditional DETR, and DINO DETR on four data sets (CADA, RibFrac, KiTS19, and LIDC) that these set prediction models can perform on par with or even better than currently existing methods. DINO DETR, the best-performing model in our experiments demonstrates this by outperforming a strong anchor-based one-stage detector, Retina U-Net, on three out of four data sets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_15472v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>准确检测医学图像中的可疑区域是一个容易出错且耗时的过程，这是许多常规诊断程序所要求的。为了在这项艰巨的任务中为临床医生提供支持，提出了几种基于具有许多超参数的复杂方法的自动化解决方案。在这项研究中，我们研究了DEtection TRansformer（DETR）模型用于体积医疗对象检测的可行性。与之前的工作相比，这些模型直接预测一组对象，而不依赖于锚的设计或手动启发式（如非最大值抑制）来检测对象。我们通过在四个数据集（CADA、RibFrac、KiTS19和LIDC）上对三个模型（即DETR、Conditional DETR和DINO DETR）进行广泛的实验表明，这些集合预测模型的性能可以与当前现有的方法相当，甚至更好。DINO DETR，我们实验中性能最好的模型，通过在四分之三的数据集上优于基于强锚的一级检测器Retina U-Net，证明了这一点。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.15472v1" target="_blank">2306.15472v1</a>
                              </td>
                              <td>Taming Detection Transformers for Medical Object Detection</td>
                              <td>Marc K. Ickler</td>
                              <td>2023-06-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_15472v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.15472v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_13723v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Social AI and the Challenges of the Human-AI Ecosystem</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_13723v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_13723v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_13723v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The rise of large-scale socio-technical systems in which humans interact with artificial intelligence (AI) systems (including assistants and recommenders, in short AIs) multiplies the opportunity for the emergence of collective phenomena and tipping points, with unexpected, possibly unintended, consequences. For example, navigation systems' suggestions may create chaos if too many drivers are directed on the same route, and personalised recommendations on social media may amplify polarisation, filter bubbles, and radicalisation. On the other hand, we may learn how to foster the "wisdom of crowds" and collective action effects to face social and environmental challenges. In order to understand the impact of AI on socio-technical systems and design next-generation AIs that team with humans to help overcome societal problems rather than exacerbate them, we propose to build the foundations of Social AI at the intersection of Complex Systems, Network Science and AI. In this perspective paper, we discuss the main open questions in Social AI, outlining possible technical and scientific challenges and suggesting research avenues.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_13723v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人类与人工智能（AI）系统（包括助手和推荐人，简称AI）互动的大规模社会技术系统的兴起，增加了集体现象和临界点出现的机会，带来了意想不到的、可能是无意的后果。例如，如果太多司机在同一条路线上行驶，导航系统的建议可能会造成混乱，而社交媒体上的个性化建议可能会放大两极分化、过滤泡沫和激进化。另一方面，我们可以学习如何培养“群体智慧”和集体行动效应，以应对社会和环境挑战。为了理解人工智能对社会技术系统的影响，并设计下一代人工智能，与人类合作，帮助克服而不是加剧社会问题，我们建议在复杂系统、网络科学和人工智能的交叉点上建立社会人工智能的基础，概述可能的技术和科学挑战，并提出研究途径。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.13723v1" target="_blank">2306.13723v1</a>
                              </td>
                              <td>Social AI and the Challenges of the Human-AI Ecosystem</td>
                              <td>Dino Pedreschi</td>
                              <td>2023-06-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_13723v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.13723v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_13337v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Patch-Level Contrasting without Patch Correspondence for Accurate and Dense Contrastive Representation Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_13337v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_13337v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_13337v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose ADCLR: A ccurate and D ense Contrastive Representation Learning, a novel self-supervised learning framework for learning accurate and dense vision representation. To extract spatial-sensitive information, ADCLR introduces query patches for contrasting in addition with global contrasting. Compared with previous dense contrasting methods, ADCLR mainly enjoys three merits: i) achieving both global-discriminative and spatial-sensitive representation, ii) model-efficient (no extra parameters in addition to the global contrasting baseline), and iii) correspondence-free and thus simpler to implement. Our approach achieves new state-of-the-art performance for contrastive methods. On classification tasks, for ViT-S, ADCLR achieves 77.5% top-1 accuracy on ImageNet with linear probing, outperforming our baseline (DINO) without our devised techniques as plug-in, by 0.5%. For ViT-B, ADCLR achieves 79.8%, 84.0% accuracy on ImageNet by linear probing and finetune, outperforming iBOT by 0.3%, 0.2% accuracy. For dense tasks, on MS-COCO, ADCLR achieves significant improvements of 44.3% AP on object detection, 39.7% AP on instance segmentation, outperforming previous SOTA method SelfPatch by 2.2% and 1.2%, respectively. On ADE20K, ADCLR outperforms SelfPatch by 1.0% mIoU, 1.2% mAcc on the segme</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_13337v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的自监督学习框架ADCLR：准确度和密集度对比表示学习，用于学习准确和密集的视觉表示。为了提取空间敏感信息，ADCLR除了引入全局对比之外，还引入了用于对比的查询补丁。与以前的密集对比方法相比，ADCLR主要有三个优点：i）同时实现全局判别和空间敏感表示，ii）模型有效（除了全局对比基线之外没有额外的参数），以及iii）无对应，因此实现更简单。我们的方法为对比方法实现了最先进的性能。在分类任务方面，对于ViT-S，ADCLR在使用线性探测的ImageNet上实现了77.5%的前1级准确率，比没有我们设计的技术作为插件的基线（DINO）高出0.5%。对于ViT-B，ADCLR通过线性探测和微调在ImageNet上分别实现了79.8%和84.0%的准确率，分别比iBOT高出0.3%和0.2%的准确率。对于密集任务，在MS-COCO上，ADCLR在对象检测上实现了44.3%的AP，在实例分割上实现了39.7%的AP，分别比以前的SOTA方法SelfPatch提高了2.2%和1.2%。在ADE20K上，ADCLR比SelfPatch高出1.0%mIoU，在segme上高出1.2%mAcc</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.13337v1" target="_blank">2306.13337v1</a>
                              </td>
                              <td>Patch-Level Contrasting without Patch Correspondence for Accurate and Dense Contrastive Representation Learning</td>
                              <td>Shaofeng Zhang</td>
                              <td>2023-06-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_13337v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.13337v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09346v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rosetta Neurons: Mining the Common Units in a Model Zoo</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09346v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09346v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09346v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Do different neural networks, trained for various vision tasks, share some common representations? In this paper, we demonstrate the existence of common features we call "Rosetta Neurons" across a range of models with different architectures, different tasks (generative and discriminative), and different types of supervision (class-supervised, text-supervised, self-supervised). We present an algorithm for mining a dictionary of Rosetta Neurons across several popular vision models: Class Supervised-ResNet50, DINO-ResNet50, DINO-ViT, MAE, CLIP-ResNet50, BigGAN, StyleGAN-2, StyleGAN-XL. Our findings suggest that certain visual concepts and structures are inherently embedded in the natural world and can be learned by different models regardless of the specific task or architecture, and without the use of semantic labels. We can visualize shared concepts directly due to generative models included in our analysis. The Rosetta Neurons facilitate model-to-model translation enabling various inversion-based manipulations, including cross-class alignments, shifting, zooming, and more, without the need for specialized training.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09346v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>为各种视觉任务训练的不同神经网络是否共享一些共同的表示？在本文中，我们在一系列具有不同架构、不同任务（生成和判别）和不同类型监督（类监督、文本监督、自监督）的模型中证明了我们称之为“罗塞塔神经元”的共同特征的存在。我们提出了一种在几种流行的视觉模型中挖掘罗塞塔神经元字典的算法：Class Supervisored-ResNet50、DINO-ResNet50、DINO-ViT、MAE、CLIP-ResNet50，BigGAN、StyleGAN-2、StyleGAN-XL。我们的研究结果表明，某些视觉概念和结构固有地嵌入在自然世界中，无论具体任务或架构如何，都可以通过不同的模型学习，而无需使用语义标签。由于我们的分析中包含了生成模型，我们可以直接可视化共享概念。罗塞塔神经元促进了模型到模型的翻译，实现了各种基于反转的操作，包括跨类对齐、移位、缩放等，而无需专门训练。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09346v2" target="_blank">2306.09346v2</a>
                              </td>
                              <td>Rosetta Neurons: Mining the Common Units in a Model Zoo</td>
                              <td>Amil Dravid</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09346v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09346v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_06588v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DEYO: DETR with YOLO for Step-by-Step Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_06588v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_06588v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_06588v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Object detection is an important topic in computer vision, with post-processing, an essential part of the typical object detection pipeline, posing a significant bottleneck affecting the performance of traditional object detection models. The detection transformer (DETR), as the first end-to-end target detection model, discards the requirement of manual components like the anchor and non-maximum suppression (NMS), significantly simplifying the target detection process. However, compared with most traditional object detection models, DETR converges very slowly, and a query's meaning is obscure. Thus, inspired by the Step-by-Step concept, this paper proposes a new two-stage object detection model, named DETR with YOLO (DEYO), which relies on a progressive inference to solve the above problems. DEYO is a two-stage architecture comprising a classic target detection model and a DETR-like model as the first and second stages, respectively. Specifically, the first stage provides high-quality query and anchor feeding into the second stage, improving the performance and efficiency of the second stage compared to the original DETR model. Meanwhile, the second stage compensates for the performance degradation caused by the first stage detector's limitations. Extensive experiments demonstrate that DEYO attains 50.6 AP and 52.1 AP in 12 and 36 epochs, respectively, while utilizing ResNet-50 as the backbone and multi-scale features on the COCO dataset. Compared with DINO, an optimal DETR-like model, the developed DEYO model affords a significant performance improvement of 1.6 AP and 1.2 AP in two epoch settings.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_06588v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>目标检测是计算机视觉中的一个重要课题，后处理是典型目标检测流水线的重要组成部分，对传统目标检测模型的性能造成了严重的瓶颈。检测转换器（DETR）作为第一个端到端目标检测模型，摒弃了锚和非最大值抑制（NMS）等手动组件的要求，大大简化了目标检测过程。然而，与大多数传统的对象检测模型相比，DETR收敛非常慢，并且查询的含义是模糊的。因此，受分步概念的启发，本文提出了一种新的两阶段目标检测模型，称为DETR with YOLO（DEYO），该模型依靠渐进推理来解决上述问题。DEYO是两阶段架构，包括分别作为第一和第二阶段的经典目标检测模型和类DETR模型。具体而言，第一阶段向第二阶段提供高质量的查询和锚馈送，与原始DETR模型相比，提高了第二阶段的性能和效率。同时，第二级补偿由第一级检测器的限制引起的性能下降。大量实验表明，DEYO在12个和36个时期分别达到50.6个AP和52.1个AP，同时利用ResNet-50作为COCO数据集上的主干和多尺度特征。与DINO（一种类似DETR的最佳模型）相比，所开发的DEYO模型在两个历元设置中提供了1.6 AP和1.2 AP的显著性能改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.06588v3" target="_blank">2211.06588v3</a>
                              </td>
                              <td>DEYO: DETR with YOLO for Step-by-Step Object Detection</td>
                              <td>Haodong Ouyang</td>
                              <td>2022-11-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_06588v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.06588v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09345v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Evaluating Data Attribution for Text-to-Image Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09345v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09345v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09345v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While large text-to-image models are able to synthesize "novel" images, these images are necessarily a reflection of the training data. The problem of data attribution in such models -- which of the images in the training set are most responsible for the appearance of a given generated image -- is a difficult yet important one. As an initial step toward this problem, we evaluate attribution through "customization" methods, which tune an existing large-scale model toward a given exemplar object or style. Our key insight is that this allows us to efficiently create synthetic images that are computationally influenced by the exemplar by construction. With our new dataset of such exemplar-influenced images, we are able to evaluate various data attribution algorithms and different possible feature spaces. Furthermore, by training on our dataset, we can tune standard models, such as DINO, CLIP, and ViT, toward the attribution problem. Even though the procedure is tuned towards small exemplar sets, we show generalization to larger sets. Finally, by taking into account the inherent uncertainty of the problem, we can assign soft attribution scores over a set of training images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09345v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>虽然大型文本到图像模型能够合成“新颖”的图像，但这些图像必然是训练数据的反映。这种模型中的数据归属问题——训练集中的哪些图像对给定生成图像的出现最负责任——是一个困难但重要的问题。作为解决这个问题的第一步，我们通过“定制”方法评估归因，该方法将现有的大规模模型调整为给定的示例对象或风格。我们的关键见解是，这使我们能够有效地创建合成图像，这些图像在计算上受到示例的影响。通过我们的新数据集，我们能够评估各种数据归因算法和不同的可能特征空间。此外，通过在数据集上进行训练，我们可以针对归因问题调整标准模型，如DINO、CLIP和ViT。尽管该过程是针对小样本集进行调整的，但我们显示了对大样本集的泛化。最后，通过考虑问题固有的不确定性，我们可以在一组训练图像上分配软归因分数。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09345v1" target="_blank">2306.09345v1</a>
                              </td>
                              <td>Evaluating Data Attribution for Text-to-Image Models</td>
                              <td>Sheng-Yu Wang</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09345v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09345v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_07483v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Semi-supervised learning made simple with self-supervised clustering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_07483v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_07483v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_07483v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised learning models have been shown to learn rich visual representations without requiring human annotations. However, in many real-world scenarios, labels are partially available, motivating a recent line of work on semi-supervised methods inspired by self-supervised principles. In this paper, we propose a conceptually simple yet empirically powerful approach to turn clustering-based self-supervised methods such as SwAV or DINO into semi-supervised learners. More precisely, we introduce a multi-task framework merging a supervised objective using ground-truth labels and a self-supervised objective relying on clustering assignments with a single cross-entropy loss. This approach may be interpreted as imposing the cluster centroids to be class prototypes. Despite its simplicity, we provide empirical evidence that our approach is highly effective and achieves state-of-the-art performance on CIFAR100 and ImageNet.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_07483v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督学习模型已被证明可以在不需要人工注释的情况下学习丰富的视觉表示。然而，在许多现实世界的场景中，标签是部分可用的，这激发了受自我监督原则启发的半监督方法的最新工作。在本文中，我们提出了一种概念简单但经验强大的方法，将基于聚类的自监督方法（如SwAV或DINO）转变为半监督学习者。更准确地说，我们引入了一个多任务框架，将使用基本事实标签的监督目标和依赖于具有单个交叉熵损失的聚类分配的自监督目标合并在一起。这种方法可以被解释为将集群质心强加为类原型。尽管它很简单，但我们提供的经验证据表明，我们的方法非常有效，并在CIFAR100和ImageNet上实现了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.07483v1" target="_blank">2306.07483v1</a>
                              </td>
                              <td>Semi-supervised learning made simple with self-supervised clustering</td>
                              <td>Enrico Fini</td>
                              <td>2023-06-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_07483v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.07483v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_05382v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Automatic Image Blending Algorithm Based on SAM and DINO</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_05382v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_05382v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_05382v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The field of image blending has gained popularity in recent years for its ability to create visually stunning content. However, the current image blending algorithm has the following problems: 1) The manual creation of the image blending mask requires a lot of manpower and material resources; 2) The image blending algorithm cannot effectively solve the problems of brightness distortion and low resolution. To this end, we propose a new image blending method: it combines semantic object detection and segmentation with corresponding mask generation to automatically blend images, while a two-stage iterative algorithm based on our proposed new saturation loss and PAN algorithm to fix brightness distortion and low resolution issues. Results on publicly available datasets show that our method outperforms many classic image blending algorithms on various performance metrics such as PSNR and SSIM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_05382v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，图像融合领域因其能够创建视觉上令人惊叹的内容而广受欢迎。然而，目前的图像混合算法存在以下问题：1）手动创建图像混合掩模需要大量的人力和物力；2） 图像混合算法不能有效地解决亮度失真和分辨率低的问题。为此，我们提出了一种新的图像混合方法：它将语义对象检测和分割与相应的掩模生成相结合来自动混合图像，而基于我们提出的新的饱和度损失和PAN算法的两阶段迭代算法来解决亮度失真和低分辨率问题。在公开数据集上的结果表明，我们的方法在各种性能指标（如PSNR和SSIM）上优于许多经典的图像混合算法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.05382v2" target="_blank">2306.05382v2</a>
                              </td>
                              <td>Automatic Image Blending Algorithm Based on SAM and DINO</td>
                              <td>Haochen Xue</td>
                              <td>2023-06-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_05382v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.05382v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_06203v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FLSL: Feature-level Self-supervised Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_06203v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_06203v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_06203v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Current self-supervised learning (SSL) methods (e.g., SimCLR, DINO, VICReg, MOCOv3) target primarily on representations at instance level and do not generalize well to dense prediction tasks, such as object detection and segmentation. Towards aligning SSL with dense predictions, this paper demonstrates for the first time the underlying mean-shift clustering process of Vision Transformers (ViT), which aligns well with natural image semantics (e.g., a world of objects and stuffs). By employing transformer for joint embedding and clustering, we propose a two-level feature clustering SSL method, coined Feature-Level Self-supervised Learning (FLSL). We present the formal definition of the FLSL problem and construct the objectives from the mean-shift and k-means perspectives. We show that FLSL promotes remarkable semantic cluster representations and learns an embedding scheme amenable to intra-view and inter-view feature clustering. Experiments show that FLSL yields significant improvements in dense prediction tasks, achieving 44.9 (+2.8)% AP and 46.5% AP in object detection, as well as 40.8 (+2.3)% AP and 42.1% AP in instance segmentation on MS-COCO, using Mask R-CNN with ViT-S/16 and ViT-S/8 as backbone, respectively. FLSL consistently outperforms existing SSL methods across additional benchmarks, including UAV object detection on UAVDT, and video instance segmentation on DAVIS 2017. We conclude by presenting visualization and various ablation studies to better 20 understand the success of FLSL.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_06203v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当前的自监督学习（SSL）方法（例如，SimCLR、DINO、VICReg、MOCOv3）主要针对实例级别的表示，并且不能很好地推广到密集预测任务，例如对象检测和分割。为了使SSL与密集预测相一致，本文首次演示了视觉变换器（ViT）的基本均值偏移聚类过程，该过程与自然图像语义（例如，对象和填充物的世界）非常一致。通过使用transformer进行联合嵌入和聚类，我们提出了一种两级特征聚类SSL方法，称为特征级自监督学习（FLSL）。我们给出了FLSL问题的形式化定义，并从均值偏移和k-均值的角度构建了目标。我们表明，FLSL促进了显著的语义聚类表示，并学习了一种适用于视图内和视图间特征聚类的嵌入方案。实验表明，使用以ViT-S/16和ViT-S/8为骨干的Mask R-CNN，FLSL在密集预测任务中产生了显著的改进，在MS-COCO上分别实现了44.9（+2.8）%AP和46.5%AP，在实例分割中实现了40.8（+2.3）%AP，42.1%AP。FLSL在其他基准测试中始终优于现有的SSL方法，包括UAVDT上的无人机对象检测和DAVIS 2017上的视频实例分割。最后，我们介绍了可视化和各种消融研究，以更好地了解FLSL的成功。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.06203v1" target="_blank">2306.06203v1</a>
                              </td>
                              <td>FLSL: Feature-level Self-supervised Learning</td>
                              <td>Qing Su</td>
                              <td>2023-06-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_06203v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.06203v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2110_15444v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">10 Security and Privacy Problems in Large Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2110_15444v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2110_15444v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2110_15444v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Foundation models--such as GPT, CLIP, and DINO--have achieved revolutionary progress in the past several years and are commonly believed to be a promising approach for general-purpose AI. In particular, self-supervised learning is adopted to pre-train a foundation model using a large amount of unlabeled data. A pre-trained foundation model is like an ``operating system'' of the AI ecosystem. Specifically, a foundation model can be used as a feature extractor for many downstream tasks with little or no labeled training data. Existing studies on foundation models mainly focused on pre-training a better foundation model to improve its performance on downstream tasks in non-adversarial settings, leaving its security and privacy in adversarial settings largely unexplored. A security or privacy issue of a pre-trained foundation model leads to a single point of failure for the AI ecosystem. In this book chapter, we discuss 10 basic security and privacy problems for the pre-trained foundation models, including six confidentiality problems, three integrity problems, and one availability problem. For each problem, we discuss potential opportunities and challenges. We hope our book chapter will inspire future research on the security and privacy of foundation models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2110_15444v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基础模型，如GPT、CLIP和DINO，在过去几年中取得了革命性的进展，通常被认为是通用人工智能的一种很有前途的方法。特别是，采用自我监督学习来使用大量未标记数据预训练基础模型。预先训练的基础模型就像人工智能生态系统的“操作系统”。具体而言，基础模型可以用作许多下游任务的特征提取器，这些任务很少或没有标记的训练数据。现有的基础模型研究主要集中在预训练一个更好的基础模型，以提高其在非对抗性环境中下游任务的性能，而其在对抗性环境下的安全性和隐私性在很大程度上没有得到探索。预先训练的基础模型的安全或隐私问题会导致人工智能生态系统的单点故障。在本书的章节中，我们讨论了预训练的基础模型的10个基本安全和隐私问题，包括6个机密性问题、3个完整性问题和1个可用性问题。对于每个问题，我们都会讨论潜在的机遇和挑战。我们希望本书的章节将启发未来对基金会模型的安全性和隐私性的研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2110.15444v3" target="_blank">2110.15444v3</a>
                              </td>
                              <td>10 Security and Privacy Problems in Large Foundation Models</td>
                              <td>Jinyuan Jia</td>
                              <td>2021-10-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2110_15444v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2110.15444v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04675v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04675v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04675v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04675v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We systematically study a wide variety of image-based generative models spanning semantically-diverse datasets to understand and improve the feature extractors and metrics used to evaluate them. Using best practices in psychophysics, we measure human perception of image realism for generated samples by conducting the largest experiment evaluating generative models to date, and find that no existing metric strongly correlates with human evaluations. Comparing to 16 modern metrics for evaluating the overall performance, fidelity, diversity, and memorization of generative models, we find that the state-of-the-art perceptual realism of diffusion models as judged by humans is not reflected in commonly reported metrics such as FID. This discrepancy is not explained by diversity in generated samples, though one cause is over-reliance on Inception-V3. We address these flaws through a study of alternative self-supervised feature extractors, find that the semantic information encoded by individual networks strongly depends on their training procedure, and show that DINOv2-ViT-L/14 allows for much richer evaluation of generative models. Next, we investigate data memorization, and find that generative models do memorize training examples on simple, smaller datasets like CIFAR10, but not necessarily on more complex datasets like ImageNet. However, our experiments show that current metrics do not properly detect memorization; none in the literature is able to separate memorization from other phenomena such as underfitting or mode shrinkage. To facilitate further development of generative models and their evaluation we release all generated image datasets, human evaluation data, and a modular library to compute 16 common metrics for 8 different encoders at https://github.com/layer6ai-labs/dgm-eval.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04675v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们系统地研究了各种基于图像的生成模型，这些模型跨越语义不同的数据集，以理解和改进用于评估它们的特征提取器和度量。利用心理物理学中的最佳实践，我们通过对生成模型进行迄今为止最大规模的评估实验，测量了人类对生成样本的图像真实感的感知，并发现没有任何现有指标与人类评估密切相关。与用于评估生成模型的整体性能、保真度、多样性和记忆的16个现代指标相比，我们发现，人类判断的扩散模型的最先进的感知真实性没有反映在常见的指标中，如FID。这种差异并不能用生成样本的多样性来解释，尽管其中一个原因是过度依赖Inception-V3。我们通过对替代自监督特征提取器的研究来解决这些缺陷，发现单个网络编码的语义信息在很大程度上取决于它们的训练过程，并表明DINOv2-ViT-L/14允许对生成模型进行更丰富的评估。接下来，我们研究了数据记忆，发现生成模型确实在像CIFAR10这样的简单、较小的数据集上记忆训练示例，但不一定在像ImageNet这样的更复杂数据集上。然而，我们的实验表明，目前的指标并不能正确地检测记忆；文献中没有一个能够将记忆与其他现象（如填充不足或模式收缩）区分开来。为了促进生成模型及其评估的进一步发展，我们发布了所有生成的图像数据集、人类评估数据和模块库，以计算8个不同编码器的16个通用度量https://github.com/layer6ai-labs/dgm-eval.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04675v1" target="_blank">2306.04675v1</a>
                              </td>
                              <td>Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models</td>
                              <td>George Stein</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04675v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04675v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03881v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Emergent Correspondence from Image Diffusion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03881v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03881v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03881v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Finding correspondences between images is a fundamental problem in computer vision. In this paper, we show that correspondence emerges in image diffusion models without any explicit supervision. We propose a simple strategy to extract this implicit knowledge out of diffusion networks as image features, namely DIffusion FeaTures (DIFT), and use them to establish correspondences between real images. Without any additional fine-tuning or supervision on the task-specific data or annotations, DIFT is able to outperform both weakly-supervised methods and competitive off-the-shelf features in identifying semantic, geometric, and temporal correspondences. Particularly for semantic correspondence, DIFT from Stable Diffusion is able to outperform DINO and OpenCLIP by 19 and 14 accuracy points respectively on the challenging SPair-71k benchmark. It even outperforms the state-of-the-art supervised methods on 9 out of 18 categories while remaining on par for the overall performance. Project page: https://diffusionfeatures.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03881v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>寻找图像之间的对应关系是计算机视觉中的一个基本问题。在本文中，我们证明了在没有任何明确监督的情况下，图像扩散模型中会出现对应关系。我们提出了一种简单的策略来从扩散网络中提取这种隐含的知识作为图像特征，即diffusion features（DIFT），并使用它们来建立真实图像之间的对应关系。在没有对特定任务的数据或注释进行任何额外的微调或监督的情况下，DIFT能够在识别语义、几何和时间对应性方面优于弱监督方法和有竞争力的现成特征。特别是在语义对应方面，来自Stable Diffusion的DIFT能够在具有挑战性的SPair 71k基准上分别比DINO和OpenCLIP高出19和14个准确度点。它甚至在18个类别中的9个类别上优于最先进的监督方法，同时在总体性能上保持标准。项目页面：https://diffusionfeatures.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03881v1" target="_blank">2306.03881v1</a>
                              </td>
                              <td>Emergent Correspondence from Image Diffusion</td>
                              <td>Luming Tang</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03881v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03881v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04654v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DenseDINO: Boosting Dense Self-Supervised Learning with Token-Based Point-Level Consistency</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04654v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04654v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04654v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we propose a simple yet effective transformer framework for self-supervised learning called DenseDINO to learn dense visual representations. To exploit the spatial information that the dense prediction tasks require but neglected by the existing self-supervised transformers, we introduce point-level supervision across views in a novel token-based way. Specifically, DenseDINO introduces some extra input tokens called reference tokens to match the point-level features with the position prior. With the reference token, the model could maintain spatial consistency and deal with multi-object complex scene images, thus generalizing better on dense prediction tasks. Compared with the vanilla DINO, our approach obtains competitive performance when evaluated on classification in ImageNet and achieves a large margin (+7.2% mIoU) improvement in semantic segmentation on PascalVOC under the linear probing protocol for segmentation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04654v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了一个简单而有效的自监督学习转换器框架，称为DenseDINO，用于学习密集的视觉表示。为了利用密集预测任务所需但被现有的自监督变换器忽略的空间信息，我们以一种新颖的基于令牌的方式引入了跨视图的点级监督。具体来说，DenseDINO引入了一些称为参考标记的额外输入标记，以将点级特征与位置先验相匹配。有了参考标记，该模型可以保持空间一致性，处理多目标复杂场景图像，从而更好地推广到密集预测任务中。与普通的DINO相比，当在ImageNet中对分类进行评估时，我们的方法获得了有竞争力的性能，并且在用于分割的线性探测协议下，在PascalVOC上的语义分割方面实现了大幅度的改进（+7.2%mIoU）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04654v1" target="_blank">2306.04654v1</a>
                              </td>
                              <td>DenseDINO: Boosting Dense Self-Supervised Learning with Token-Based Point-Level Consistency</td>
                              <td>Yike Yuan</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04654v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04654v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_07598v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RHINO: Rotated DETR with Dynamic Denoising via Hungarian Matching for Oriented Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_07598v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_07598v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_07598v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With the publication of DINO, a variant of the Detection Transformer (DETR), Detection Transformers are breaking the record in the object detection benchmark with the merits of their end-to-end design and scalability. However, the extension of DETR to oriented object detection has not been thoroughly studied although more benefits from its end-to-end architecture are expected such as removing NMS and anchor-related costs. In this paper, we propose a first strong DINO-based baseline for oriented object detection. We found that straightforward employment of DETRs for oriented object detection does not guarantee non-duplicate prediction, and propose a simple cost to mitigate this. Furthermore, we introduce a $\textit{dynamic denoising}$ strategy that uses Hungarian matching to filter redundant noised queries and $\textit{query alignment}$ to preserve matching consistency between Transformer decoder layers. Our proposed model outperforms previous rotated DETRs and other counterparts, achieving state-of-the-art performance in DOTA-v1.0/v1.5/v2.0, and DIOR-R benchmarks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_07598v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着检测变压器（DETR）的变体DINO的发布，检测变压器凭借其端到端设计和可扩展性的优点打破了对象检测基准的记录。然而，DETR向面向对象检测的扩展尚未得到彻底研究，尽管预计其端到端架构会带来更多好处，例如消除NMS和锚相关成本。在本文中，我们提出了第一个基于强DINO的面向对象检测基线。我们发现，直接使用DETR进行定向对象检测并不能保证不重复预测，并提出了一种简单的成本来减轻这种情况。此外，我们引入了$\textit｛动态去噪｝$策略，该策略使用匈牙利匹配来过滤冗余的带噪查询，并使用$\textit{查询对齐｝$来保持Transformer解码器层之间的匹配一致性。我们提出的模型优于以前的旋转DETR和其他同类模型，在DOTA-1.0/v.5/v.20和DIOR-R基准测试中实现了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.07598v3" target="_blank">2305.07598v3</a>
                              </td>
                              <td>RHINO: Rotated DETR with Dynamic Denoising via Hungarian Matching for Oriented Object Detection</td>
                              <td>Hakjin Lee</td>
                              <td>2023-05-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_07598v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.07598v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2206_09959v5_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Global Context Vision Transformers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2206_09959v5_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2206_09959v5_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2206_09959v5_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose global context vision transformer (GC ViT), a novel architecture that enhances parameter and compute utilization for computer vision. Our method leverages global context self-attention modules, joint with standard local self-attention, to effectively and efficiently model both long and short-range spatial interactions, without the need for expensive operations such as computing attention masks or shifting local windows. In addition, we address the lack of the inductive bias in ViTs, and propose to leverage a modified fused inverted residual blocks in our architecture. Our proposed GC ViT achieves state-of-the-art results across image classification, object detection and semantic segmentation tasks. On ImageNet-1K dataset for classification, the variants of GC ViT with 51M, 90M and 201M parameters achieve 84.3%, 85.0% and 85.7% Top-1 accuracy, respectively, at 224 image resolution and without any pre-training, hence surpassing comparably-sized prior art such as CNN-based ConvNeXt and ViT-based MaxViT and Swin Transformer by a large margin. Pre-trained GC ViT backbones in downstream tasks of object detection, instance segmentation, and semantic segmentation using MS COCO and ADE20K datasets outperform prior work consistently. Specifically, GC ViT with a 4-scale DINO detection head achieves a box AP of 58.3 on MS COCO dataset.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2206_09959v5_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了全局上下文视觉转换器（GC-ViT），这是一种提高计算机视觉参数和计算利用率的新架构。我们的方法利用全局上下文自注意模块与标准的局部自注意相结合，有效地对长距离和短距离空间交互进行建模，而不需要计算注意力掩码或移动局部窗口等昂贵的操作。此外，我们解决了ViTs中缺乏电感偏置的问题，并建议在我们的架构中利用改进的融合反向残差块。我们提出的GC-ViT在图像分类、对象检测和语义分割任务中实现了最先进的结果。在用于分类的ImageNet-1K数据集上，具有51M、90M和201M参数的GC ViT变体在224图像分辨率和没有任何预训练的情况下分别达到84.3%、85.0%和85.7%的Top-1准确率，因此大大超过了类似大小的现有技术，如基于CNN的ConvNeXt和基于ViT的MaxViT和Swin Transformer。使用MS COCO和ADE20K数据集在对象检测、实例分割和语义分割的下游任务中预先训练的GC-ViT骨干始终优于先前的工作。具体而言，具有4级DINO检测头的GC ViT在MS COCO数据集上实现了58.3的框AP。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2206.09959v5" target="_blank">2206.09959v5</a>
                              </td>
                              <td>Global Context Vision Transformers</td>
                              <td>Ali Hatamizadeh</td>
                              <td>2022-06-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2206_09959v5_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2206.09959v5" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01398v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Evaluating The Robustness of Self-Supervised Representations to Background/Foreground Removal</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01398v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01398v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01398v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite impressive empirical advances of SSL in solving various tasks, the problem of understanding and characterizing SSL representations learned from input data remains relatively under-explored. We provide a comparative analysis of how the representations produced by SSL models differ when masking parts of the input. Specifically, we considered state-of-the-art SSL pretrained models, such as DINOv2, MAE, and SwaV, and analyzed changes at the representation levels across 4 Image Classification datasets. First, we generate variations of the datasets by applying foreground and background segmentation. Then, we conduct statistical analysis using Canonical Correlation Analysis (CCA) and Centered Kernel Alignment (CKA) to evaluate the robustness of the representations learned in SSL models. Empirically, we show that not all models lead to representations that separate foreground, background, and complete images. Furthermore, we test different masking strategies by occluding the center regions of the images to address cases where foreground and background are difficult. For example, the DTD dataset that focuses on texture rather specific objects.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01398v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管SSL在解决各种任务方面取得了令人印象深刻的经验进步，但理解和表征从输入数据中学习到的SSL表示的问题仍然相对不足。我们提供了SSL模型生成的表示在屏蔽部分输入时的差异的比较分析。具体而言，我们考虑了最先进的SSL预训练模型，如DINOv2、MAE和SwaV，并分析了4个图像分类数据集在表示级别上的变化。首先，我们通过应用前景和背景分割来生成数据集的变体。然后，我们使用标准相关分析（CCA）和中心核对齐（CKA）进行统计分析，以评估SSL模型中学习的表示的稳健性。从经验上讲，我们表明并非所有模型都能产生分离前景、背景和完整图像的表示。此外，我们通过遮挡图像的中心区域来测试不同的掩蔽策略，以解决前景和背景困难的情况。例如，专注于纹理而非特定对象的DTD数据集。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01398v1" target="_blank">2306.01398v1</a>
                              </td>
                              <td>Evaluating The Robustness of Self-Supervised Representations to Background/Foreground Removal</td>
                              <td>Xavier F. Cadet</td>
                              <td>2023-06-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01398v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01398v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2207_00449v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Dissecting Self-Supervised Learning Methods for Surgical Computer Vision</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2207_00449v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2207_00449v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2207_00449v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The field of surgical computer vision has undergone considerable breakthroughs in recent years with the rising popularity of deep neural network-based methods. However, standard fully-supervised approaches for training such models require vast amounts of annotated data, imposing a prohibitively high cost; especially in the clinical domain. Self-Supervised Learning (SSL) methods, which have begun to gain traction in the general computer vision community, represent a potential solution to these annotation costs, allowing to learn useful representations from only unlabeled data. Still, the effectiveness of SSL methods in more complex and impactful domains, such as medicine and surgery, remains limited and unexplored. In this work, we address this critical need by investigating four state-of-the-art SSL methods (MoCo v2, SimCLR, DINO, SwAV) in the context of surgical computer vision. We present an extensive analysis of the performance of these methods on the Cholec80 dataset for two fundamental and popular tasks in surgical context understanding, phase recognition and tool presence detection. We examine their parameterization, then their behavior with respect to training data quantities in semi-supervised settings. Correct transfer of these methods to surgery, as described and conducted in this work, leads to substantial performance gains over generic uses of SSL - up to 7.4% on phase recognition and 20% on tool presence detection - as well as state-of-the-art semi-supervised phase recognition approaches by up to 14%. Further results obtained on a highly diverse selection of surgical datasets exhibit strong generalization properties. The code is available at https://github.com/CAMMA-public/SelfSupSurg.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2207_00449v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，随着深度神经网络方法的日益普及，外科计算机视觉领域取得了相当大的突破。然而，训练此类模型的标准完全监督方法需要大量的注释数据，成本高得令人望而却步；尤其是在临床领域。自监督学习（SSL）方法已经开始在普通计算机视觉社区中获得吸引力，它代表了这些注释成本的潜在解决方案，允许仅从未标记的数据中学习有用的表示。尽管如此，SSL方法在更复杂和更有影响力的领域（如医学和外科）的有效性仍然有限，尚未探索。在这项工作中，我们通过在外科计算机视觉的背景下研究四种最先进的SSL方法（MoCov2、SimCLR、DINO、SwAV）来解决这一关键需求。我们在Cholec80数据集上对这些方法在外科上下文理解、相位识别和工具存在检测中的两项基本和流行任务的性能进行了广泛的分析。我们检查了它们的参数化，然后检查了它们在半监督设置中相对于训练数据量的行为。正如这项工作中所描述和进行的那样，将这些方法正确地转移到手术中，与SSL的一般用途相比，可以获得显著的性能提升——在相位识别方面高达7.4%，在工具存在检测方面高达20%——以及最先进的半监督相位识别方法，高达14%。在高度多样化的外科数据集选择上获得的进一步结果显示出强大的泛化特性。代码可在https://github.com/CAMMA-public/SelfSupSurg.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2207.00449v3" target="_blank">2207.00449v3</a>
                              </td>
                              <td>Dissecting Self-Supervised Learning Methods for Surgical Computer Vision</td>
                              <td>Sanat Ramesh</td>
                              <td>2022-07-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2207_00449v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2207.00449v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_07044v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SSL4EO-S12: A Large-Scale Multi-Modal, Multi-Temporal Dataset for Self-Supervised Learning in Earth Observation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_07044v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_07044v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_07044v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised pre-training bears potential to generate expressive representations without human annotation. Most pre-training in Earth observation (EO) are based on ImageNet or medium-size, labeled remote sensing (RS) datasets. We share an unlabeled RS dataset SSL4EO-S12 (Self-Supervised Learning for Earth Observation - Sentinel-1/2) to assemble a large-scale, global, multimodal, and multi-seasonal corpus of satellite imagery from the ESA Sentinel-1 \& -2 satellite missions. For EO applications we demonstrate SSL4EO-S12 to succeed in self-supervised pre-training for a set of methods: MoCo-v2, DINO, MAE, and data2vec. Resulting models yield downstream performance close to, or surpassing accuracy measures of supervised learning. In addition, pre-training on SSL4EO-S12 excels compared to existing datasets. We make openly available the dataset, related source code, and pre-trained models at https://github.com/zhu-xlab/SSL4EO-S12.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_07044v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自我监督的预训练有可能在没有人为注释的情况下生成表达表征。地球观测（EO）中的大多数预训练都是基于ImageNet或中型标记遥感（RS）数据集。我们共享一个未标记的RS数据集SSL4EO-S12（地球观测的自我监督学习-哨兵-1/2），以收集来自欧空局哨兵-1/2卫星任务的大规模、全球、多模式和多季节的卫星图像语料库。对于EO应用，我们展示了SSL4EO-S12在一组方法的自我监督预训练中的成功：MoCo-v2、DINO、MAE和data2vec。所得到的模型产生的下游性能接近或超过监督学习的准确性度量。此外，与现有数据集相比，SSL4EO-S12上的预训练表现出色。我们在https://github.com/zhu-xlab/SSL4EO-S12.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.07044v2" target="_blank">2211.07044v2</a>
                              </td>
                              <td>SSL4EO-S12: A Large-Scale Multi-Modal, Multi-Temporal Dataset for Self-Supervised Learning in Earth Observation</td>
                              <td>Yi Wang</td>
                              <td>2022-11-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_07044v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.07044v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_11922v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SupeRGB-D: Zero-shot Instance Segmentation in Cluttered Indoor Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_11922v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_11922v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_11922v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Object instance segmentation is a key challenge for indoor robots navigating cluttered environments with many small objects. Limitations in 3D sensing capabilities often make it difficult to detect every possible object. While deep learning approaches may be effective for this problem, manually annotating 3D data for supervised learning is time-consuming. In this work, we explore zero-shot instance segmentation (ZSIS) from RGB-D data to identify unseen objects in a semantic category-agnostic manner. We introduce a zero-shot split for Tabletop Objects Dataset (TOD-Z) to enable this study and present a method that uses annotated objects to learn the ``objectness'' of pixels and generalize to unseen object categories in cluttered indoor environments. Our method, SupeRGB-D, groups pixels into small patches based on geometric cues and learns to merge the patches in a deep agglomerative clustering fashion. SupeRGB-D outperforms existing baselines on unseen objects while achieving similar performance on seen objects. We further show competitive results on the real dataset OCID. With its lightweight design (0.4 MB memory requirement), our method is extremely suitable for mobile and robotic applications. Additional DINO features can increase performance with a higher memory requirement. The dataset split and code are available at https://github.com/evinpinar/supergb-d.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_11922v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对象实例分割是室内机器人在有许多小对象的杂乱环境中导航的一个关键挑战。3D传感能力的局限性往往使检测每一个可能的物体变得困难。虽然深度学习方法可能对这个问题有效，但手动注释3D数据进行监督学习是耗时的。在这项工作中，我们从RGB-D数据中探索了零样本实例分割（ZSIS），以语义分类的方式识别看不见的对象。我们为桌面对象数据集（TOD-Z）引入了零样本分割，以实现这项研究，并提出了一种方法，该方法使用注释对象来学习像素的“对象性”，并推广到杂乱的室内环境中看不见的对象类别。我们的方法SupeRGB-D基于几何线索将像素分组为小块，并学习以深度聚集聚类的方式合并小块。SupeRGB-D在看不见的对象上优于现有的基线，同时在看到的对象上实现了类似的性能。我们在真实数据集OCID上进一步展示了具有竞争力的结果。凭借其轻量级设计（需要0.4 MB内存），我们的方法非常适合移动和机器人应用。额外的DINO功能可以提高内存需求的性能。数据集拆分和代码可在https://github.com/evinpinar/supergb-d.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.11922v2" target="_blank">2212.11922v2</a>
                              </td>
                              <td>SupeRGB-D: Zero-shot Instance Segmentation in Cluttered Indoor Environments</td>
                              <td>Evin Pınar Örnek</td>
                              <td>2022-12-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_11922v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.11922v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_15347v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_15347v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_15347v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_15347v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Text-to-image diffusion models have made significant advances in generating and editing high-quality images. As a result, numerous approaches have explored the ability of diffusion model features to understand and process single images for downstream tasks, e.g., classification, semantic segmentation, and stylization. However, significantly less is known about what these features reveal across multiple, different images and objects. In this work, we exploit Stable Diffusion (SD) features for semantic and dense correspondence and discover that with simple post-processing, SD features can perform quantitatively similar to SOTA representations. Interestingly, the qualitative analysis reveals that SD features have very different properties compared to existing representation learning features, such as the recently released DINOv2: while DINOv2 provides sparse but accurate matches, SD features provide high-quality spatial information but sometimes inaccurate semantic matches. We demonstrate that a simple fusion of these two features works surprisingly well, and a zero-shot evaluation using nearest neighbors on these fused features provides a significant performance gain over state-of-the-art methods on benchmark datasets, e.g., SPair-71k, PF-Pascal, and TSS. We also show that these correspondences can enable interesting applications such as instance swapping in two images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_15347v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文本到图像的扩散模型在生成和编辑高质量图像方面取得了重大进展。因此，许多方法已经探索了扩散模型特征理解和处理下游任务的单个图像的能力，例如分类、语义分割和风格化。然而，人们对这些特征在多个不同的图像和对象中所揭示的内容知之甚少。在这项工作中，我们利用稳定扩散（SD）特征进行语义和密集对应，并发现通过简单的后处理，SD特征可以在数量上执行类似于SOTA的表示。有趣的是，定性分析表明，与现有的表示学习特征（如最近发布的DINOv2）相比，SD特征具有非常不同的特性：虽然DINOv2提供稀疏但准确的匹配，但SD特征提供了高质量的空间信息，但有时语义匹配不准确。我们证明，这两个特征的简单融合效果令人惊讶地好，并且在这些融合特征上使用最近邻居的零样本评估在基准数据集（例如，SPair-71k、PF-Pascal和TSS）上提供了比现有技术方法显著的性能增益。我们还展示了这些对应关系可以实现有趣的应用程序，例如两个图像中的实例交换。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.15347v1" target="_blank">2305.15347v1</a>
                              </td>
                              <td>A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence</td>
                              <td>Junyi Zhang</td>
                              <td>2023-05-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_15347v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.15347v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_14093v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D Open-vocabulary Segmentation with Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_14093v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_14093v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_14093v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature significantly as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting the open-vocabulary multimodal knowledge and object reasoning capability of pre-trained foundation models CLIP and DINO, without necessitating any fine-tuning. Specifically, we distill open-vocabulary visual and textual knowledge from CLIP into a neural radiance field (NeRF) which effectively lifts 2D features into view-consistent 3D segmentation. Furthermore, we introduce the Relevancy-Distribution Alignment loss and Feature-Distribution Alignment loss to respectively mitigate the ambiguities of CLIP features and distill precise object boundaries from DINO features, eliminating the need for segmentation annotations during training. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_14093v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三维场景的开放式词汇分割是人类感知的基本功能，因此也是计算机视觉研究的重要目标。然而，由于缺乏大规模和多样化的3D开放词汇分割数据集来训练健壮和可推广的模型，这项任务受到了严重阻碍。从预先训练的2D开放词汇分割模型中提取知识是有帮助的，但它严重损害了开放词汇的特征，因为2D模型大多是用紧密的词汇数据集进行微调的。我们通过利用预先训练的基础模型CLIP和DINO的开放词汇多模态知识和对象推理能力来应对3D开放词汇分割中的挑战，而不需要任何微调。具体来说，我们将CLIP中的开放词汇视觉和文本知识提取到神经辐射场（NeRF）中，该场有效地将2D特征提升到视图一致的3D分割中。此外，我们引入了相关性分布对齐损失和特征分布对齐损失，以分别减轻CLIP特征的模糊性，并从DINO特征中提取精确的对象边界，从而消除了训练过程中对分割注释的需要。大量实验表明，我们的方法甚至优于用分割注释训练的完全监督模型，这表明可以从2D图像和文本图像对中有效地学习3D开放词汇分割。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.14093v2" target="_blank">2305.14093v2</a>
                              </td>
                              <td>3D Open-vocabulary Segmentation with Foundation Models</td>
                              <td>Kunhao Liu</td>
                              <td>2023-05-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_14093v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.14093v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_12223v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">What Makes for Good Visual Tokenizers for Large Language Models?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_12223v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_12223v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_12223v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We empirically investigate proper pre-training methods to build good visual tokenizers, making Large Language Models (LLMs) powerful Multimodal Large Language Models (MLLMs). In our benchmark, which is curated to evaluate MLLMs visual semantic understanding and fine-grained perception capabilities, we discussed different visual tokenizers pre-trained with dominant methods (i.e., DeiT, CLIP, MAE, DINO), and observe that: i) Fully/weakly supervised models capture more semantics than self-supervised models, but the gap is narrowed by scaling up the pre-training dataset. ii) Self-supervised models are better at fine-grained perception, where patch-level supervision is particularly effective. iii) Tuning the visual tokenizer leads to the loss of semantics obtained from large-scale pretraining, which is unfavorable with relatively small-scale instruction-tuning dataset. Given the findings, we reviewed methods that attempted to unify semantics and fine-grained visual understanding, e.g., patch-level feature distillation with semantically-rich targets. We obtain an intriguing insight mask-based strategies that were once all the rage may not be applicable for obtaining good visual tokenizers. Based on this critical observation, we obtain a new MLLM equipped with a tailored Good Visual Tokenizer (GVT), which exhibits strong visual comprehension capability at multiple scales. In particular, without introducing extra parameters and task-specific fine-tuning, GVT achieves superior performance on visual question answering, image captioning, and other fine-grained visual understanding tasks such as object counting and multi-class identification.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_12223v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们实证研究了适当的预训练方法来构建良好的视觉标记器，使大型语言模型（LLM）成为强大的多模式大型语言模型。在我们的基准测试中，我们讨论了用主流方法（即DeiT、CLIP、MAE、DINO）预训练的不同视觉标记器，并观察到：i）全/弱监督模型比自监督模型捕获更多的语义，但通过扩大预训练数据集，差距缩小了。ii）自监督模型更擅长细粒度感知，其中补丁级别的监督尤其有效。iii）调整可视化标记器会导致从大规模预训练中获得的语义丢失，这对相对小规模的指令调整数据集是不利的。鉴于这些发现，我们回顾了试图统一语义和细粒度视觉理解的方法，例如，具有语义丰富目标的补丁级特征提取。我们获得了一种有趣的基于面具的洞察策略，这种策略曾经风靡一时，但可能不适用于获得良好的视觉标记器。基于这一关键观察，我们获得了一种新的MLLM，该MLLM配备了定制的良好视觉标记器（GVT），在多个尺度上表现出强大的视觉理解能力。特别是，在不引入额外参数和特定任务微调的情况下，GVT在视觉问答、图像字幕和其他细粒度视觉理解任务（如对象计数和多类识别）上实现了卓越的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.12223v2" target="_blank">2305.12223v2</a>
                              </td>
                              <td>What Makes for Good Visual Tokenizers for Large Language Models?</td>
                              <td>Guangzhi Wang</td>
                              <td>2023-05-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_12223v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.12223v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_13552v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Squared Neural Families: A New Class of Tractable Density Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_13552v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_13552v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_13552v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Flexible models for probability distributions are an essential ingredient in many machine learning tasks. We develop and investigate a new class of probability distributions, which we call a Squared Neural Family (SNEFY), formed by squaring the 2-norm of a neural network and normalising it with respect to a base measure. Following the reasoning similar to the well established connections between infinitely wide neural networks and Gaussian processes, we show that SNEFYs admit a closed form normalising constants in many cases of interest, thereby resulting in flexible yet fully tractable density models. SNEFYs strictly generalise classical exponential families, are closed under conditioning, and have tractable marginal distributions. Their utility is illustrated on a variety of density estimation and conditional density estimation tasks. Software available at https://github.com/RussellTsuchida/snefy.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_13552v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>概率分布的灵活模型是许多机器学习任务的重要组成部分。我们开发并研究了一类新的概率分布，我们称之为平方神经家族（SNEFY），它是通过对神经网络的2-范数进行平方并相对于基测度对其进行归一化而形成的。根据类似于无限宽神经网络和高斯过程之间已建立的良好连接的推理，我们表明，在许多感兴趣的情况下，SNEFY允许闭合形式的归一化常数，从而产生灵活但完全可处理的密度模型。SNEFY严格推广了经典指数族，在条件作用下是封闭的，并且具有可处理的边缘分布。它们在各种密度估计和条件密度估计任务中的效用得到了说明。软件可在https://github.com/RussellTsuchida/snefy.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.13552v1" target="_blank">2305.13552v1</a>
                              </td>
                              <td>Squared Neural Families: A New Class of Tractable Density Models</td>
                              <td>Russell Tsuchida</td>
                              <td>2023-05-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_13552v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.13552v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_13291v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Materialistic: Selecting Similar Materials in Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_13291v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_13291v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_13291v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Separating an image into meaningful underlying components is a crucial first step for both editing and understanding images. We present a method capable of selecting the regions of a photograph exhibiting the same material as an artist-chosen area. Our proposed approach is robust to shading, specular highlights, and cast shadows, enabling selection in real images. As we do not rely on semantic segmentation (different woods or metal should not be selected together), we formulate the problem as a similarity-based grouping problem based on a user-provided image location. In particular, we propose to leverage the unsupervised DINO features coupled with a proposed Cross-Similarity module and an MLP head to extract material similarities in an image. We train our model on a new synthetic image dataset, that we release. We show that our method generalizes well to real-world images. We carefully analyze our model's behavior on varying material properties and lighting. Additionally, we evaluate it against a hand-annotated benchmark of 50 real photographs. We further demonstrate our model on a set of applications, including material editing, in-video selection, and retrieval of object photographs with similar materials.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_13291v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>将图像分离为有意义的底层组件是编辑和理解图像的关键第一步。我们提出了一种方法，能够选择与艺术家选择的区域呈现相同材料的照片区域。我们提出的方法对明暗处理、镜面高光和投射阴影都很稳健，可以在真实图像中进行选择。由于我们不依赖于语义分割（不同的木材或金属不应该一起选择），我们将该问题表述为基于用户提供的图像位置的基于相似性的分组问题。特别地，我们建议利用无监督的DINO特征，结合所提出的交叉相似性模块和MLP头来提取图像中的材料相似性。我们在发布的一个新的合成图像数据集上训练我们的模型。我们证明了我们的方法可以很好地推广到真实世界的图像。我们仔细分析了模型在不同材质特性和照明条件下的行为。此外，我们根据50张真实照片的手绘基准对其进行了评估。我们在一系列应用程序上进一步展示了我们的模型，包括素材编辑、视频选择和检索具有类似素材的对象照片。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.13291v1" target="_blank">2305.13291v1</a>
                              </td>
                              <td>Materialistic: Selecting Similar Materials in Images</td>
                              <td>Prafull Sharma</td>
                              <td>2023-05-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_13291v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.13291v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_11092v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Universal Domain Adaptation from Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_11092v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_11092v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_11092v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Foundation models (e.g., CLIP or DINOv2) have shown their impressive learning and transferring capabilities on a wide range of visual tasks, by training on a large corpus of data and adapting to specific downstream tasks. It is, however, interesting that foundation models have not been fully explored for universal domain adaptation (UniDA), which is to learn models using labeled data in a source domain and unlabeled data in a target one, such that the learned models can successfully adapt to the target data. In this paper, we make comprehensive empirical studies of state-of-the-art UniDA methods using foundation models. We first demonstrate that, while foundation models greatly improve the performance of the baseline methods that train the models on the source data alone, existing UniDA methods generally fail to improve over the baseline. This suggests that new research efforts are very necessary for UniDA using foundation models. To this end, we propose a very simple method of target data distillation on the CLIP model, and achieves consistent improvement over the baseline across all the UniDA benchmarks. Our studies are under a newly proposed evaluation metric of universal classification rate (UCR), which is threshold- and ratio-free and addresses the threshold-sensitive issue encountered when using the existing H-score metric.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_11092v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基础模型（例如CLIP或DINOv2）通过在大量数据库上进行训练并适应特定的下游任务，在广泛的视觉任务中显示出了令人印象深刻的学习和转移能力。然而，有趣的是，基础模型尚未被充分探索用于通用域自适应（UniDA），即使用源域中的标记数据和目标域中的未标记数据来学习模型，以便学习的模型能够成功地适应目标数据。在本文中，我们使用基础模型对最先进的UniDA方法进行了全面的实证研究。我们首先证明，虽然基础模型大大提高了仅在源数据上训练模型的基线方法的性能，但现有的UniDA方法通常无法在基线上改进。这表明，对于使用基础模型的UniDA来说，新的研究工作是非常必要的。为此，我们提出了一种在CLIP模型上提取目标数据的非常简单的方法，并在所有UniDA基准中实现了对基线的一致改进。我们的研究是在一种新提出的通用分类率（UCR）评估指标下进行的，该指标是无阈值和无比率的，并解决了使用现有H-核心指标时遇到的阈值敏感问题。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.11092v1" target="_blank">2305.11092v1</a>
                              </td>
                              <td>Universal Domain Adaptation from Foundation Models</td>
                              <td>Bin Deng</td>
                              <td>2023-05-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_11092v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.11092v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_08014v7_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Self-Supervised Learning from Non-Object Centric Images with a Geometric Transformation Sensitive Architecture</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_08014v7_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_08014v7_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_08014v7_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Most invariance-based self-supervised methods rely on single object-centric images (e.g., ImageNet images) for pretraining, learning features that invariant to geometric transformation. However, when images are not object-centric, the semantics of the image can be significantly altered due to cropping. Furthermore, as the model becomes insensitive to geometric transformations, it may struggle to capture location information. For this reason, we propose a Geometric Transformation Sensitive Architecture designed to be sensitive to geometric transformations, specifically focusing on four-fold rotation, random crop, and multi-crop. Our method encourages the student to be sensitive by predicting rotation and using targets that vary with those transformations through pooling and rotating the teacher feature map. Additionally, we use patch correspondence loss to encourage correspondence between patches with similar features. This approach allows us to capture long-term dependencies in a more appropriate way than capturing long-term dependencies by encouraging local-to-global correspondence, which occurs when learning to be insensitive to multi-crop. Our approach demonstrates improved performance when using non-object-centric images as pretraining data compared to other methods that train the model to be insensitive to geometric transformation. We surpass DINO[Caron et al.[2021b]] baseline in tasks including image classification, semantic segmentation, detection, and instance segmentation with improvements of 4.9 $Top-1 Acc$, 3.3 $mIoU$, 3.4 $AP^b$, and 2.7 $AP^m$. Code and pretrained models are publicly available at: https://github.com/bok3948/GTSA</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_08014v7_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大多数基于不变性的自监督方法依赖于以单个对象为中心的图像（例如，ImageNet图像）进行预训练，学习对几何变换不变的特征。然而，当图像不是以对象为中心时，由于裁剪，图像的语义可能会发生显著变化。此外，随着模型对几何变换变得不敏感，它可能很难捕捉位置信息。因此，我们提出了一种几何变换敏感架构，该架构设计为对几何变换敏感，特别关注四重旋转、随机裁剪和多重裁剪。我们的方法通过预测旋转，并通过合并和旋转教师特征图来使用随这些转换而变化的目标，从而鼓励学生保持敏感。此外，我们使用补丁对应损失来鼓励具有相似特征的补丁之间的对应。这种方法使我们能够以一种比通过鼓励局部到全局的对应关系捕获长期依赖关系更合适的方式捕获长期依赖性，这种对应关系发生在学习对多作物不敏感时。与将模型训练为对几何变换不敏感的其他方法相比，当使用非以对象为中心的图像作为预训练数据时，我们的方法展示了改进的性能。我们在包括图像分类、语义分割、检测和实例分割在内的任务中超过了DINO[Caron等人[2021b]]基线，改进了4.9$Top-1Acc$、3.3$mIoU$、3.4$AP^b$和2.7$AP^m$。代码和预训练模型可在以下网站公开获取：https://github.com/bok3948/GTSA</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.08014v7" target="_blank">2304.08014v7</a>
                              </td>
                              <td>Self-Supervised Learning from Non-Object Centric Images with a Geometric Transformation Sensitive Architecture</td>
                              <td>Taeho Kim</td>
                              <td>2023-04-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_08014v7_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.08014v7" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_06558v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Segment and Track Anything</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_06558v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_06558v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_06558v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This report presents a framework called Segment And Track Anything (SAMTrack) that allows users to precisely and effectively segment and track any object in a video. Additionally, SAM-Track employs multimodal interaction methods that enable users to select multiple objects in videos for tracking, corresponding to their specific requirements. These interaction methods comprise click, stroke, and text, each possessing unique benefits and capable of being employed in combination. As a result, SAM-Track can be used across an array of fields, ranging from drone technology, autonomous driving, medical imaging, augmented reality, to biological analysis. SAM-Track amalgamates Segment Anything Model (SAM), an interactive key-frame segmentation model, with our proposed AOT-based tracking model (DeAOT), which secured 1st place in four tracks of the VOT 2022 challenge, to facilitate object tracking in video. In addition, SAM-Track incorporates Grounding-DINO, which enables the framework to support text-based interaction. We have demonstrated the remarkable capabilities of SAM-Track on DAVIS-2016 Val (92.0%), DAVIS-2017 Test (79.2%)and its practicability in diverse applications. The project page is available at: https://github.com/z-x-yang/Segment-and-Track-Anything.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_06558v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>该报告提供了一个名为Segment And Track Anything（SAMTrack）的框架，该框架允许用户准确有效地对视频中的任何对象进行分割和跟踪。此外，SAM Track采用多模式交互方法，使用户能够根据自己的特定要求选择视频中的多个对象进行跟踪。这些交互方法包括点击、笔划和文本，每种方法都具有独特的优点，并且能够组合使用。因此，SAM Track可用于一系列领域，从无人机技术、自动驾驶、医学成像、增强现实到生物分析。SAM Track将交互式关键帧分割模型Segment Anything Model（SAM）与我们提出的基于AOT的跟踪模型（DeAOT）合并，以促进视频中的对象跟踪，DeAOT在VOT 2022挑战的四个轨道中排名第一。此外，SAM Track结合了Grounding DINO，使框架能够支持基于文本的交互。我们已经在DAVIS-2016 Val（92.0%）、DAVIS-2017 Test（79.2%）上展示了SAM Track的卓越能力及其在各种应用中的实用性。项目页面位于：https://github.com/z-x-yang/Segment-and-Track-Anything.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.06558v1" target="_blank">2305.06558v1</a>
                              </td>
                              <td>Segment and Track Anything</td>
                              <td>Yangming Cheng</td>
                              <td>2023-05-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_06558v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.06558v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_06553v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">WeLayout: WeChat Layout Analysis System for the ICDAR 2023 Competition on Robust Layout Segmentation in Corporate Documents</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_06553v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_06553v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_06553v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we introduce WeLayout, a novel system for segmenting the layout of corporate documents, which stands for WeChat Layout Analysis System. Our approach utilizes a sophisticated ensemble of DINO and YOLO models, specifically developed for the ICDAR 2023 Competition on Robust Layout Segmentation. Our method significantly surpasses the baseline, securing a top position on the leaderboard with a mAP of 70.0. To achieve this performance, we concentrated on enhancing various aspects of the task, such as dataset augmentation, model architecture, bounding box refinement, and model ensemble techniques. Additionally, we trained the data separately for each document category to ensure a higher mean submission score. We also developed an algorithm for cell matching to further improve our performance. To identify the optimal weights and IoU thresholds for our model ensemble, we employed a Bayesian optimization algorithm called the Tree-Structured Parzen Estimator. Our approach effectively demonstrates the benefits of combining query-based and anchor-free models for achieving robust layout segmentation in corporate documents.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_06553v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们介绍了WeLayout，一个新的企业文档布局分割系统，即微信布局分析系统。我们的方法利用了专门为ICDAR 2023鲁棒布局分割竞赛开发的DINO和YOLO模型的复杂集合。我们的方法显著超过了基线，以70.0的mAP稳居排行榜榜首。为了实现这一性能，我们集中精力增强任务的各个方面，如数据集扩充、模型架构、边界框细化和模型集成技术。此外，我们为每个文档类别单独训练数据，以确保更高的平均提交分数。我们还开发了一种细胞匹配算法，以进一步提高我们的性能。为了确定我们模型集成的最佳权重和IoU阈值，我们使用了一种称为树结构Parzen估计器的贝叶斯优化算法。我们的方法有效地展示了将基于查询的模型和无锚模型相结合在公司文档中实现稳健布局分割的好处。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.06553v1" target="_blank">2305.06553v1</a>
                              </td>
                              <td>WeLayout: WeChat Layout Analysis System for the ICDAR 2023 Competition on Robust Layout Segmentation in Corporate Documents</td>
                              <td>Mingliang Zhang</td>
                              <td>2023-05-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_06553v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.06553v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </div>
  <div class="text-muted text-center py-2">
    <div class="container">
      <div class="row">
        <div class="col-md-12 my-4">
          <p class="mb-1">© 2023 Li Yingping - Powered by Arxiv API.</p>
        </div>
      </div>
    </div>
  </div>
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
    integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
    crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
    integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
    crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js"
    integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy"
    crossorigin="anonymous"></script>
  <pingendo onclick="window.open('https://pingendo.com/', '_blank')"
    style="cursor:pointer;position: fixed;bottom: 20px;right:20px;padding:4px;background-color: #00b0eb;border-radius: 8px; width:220px;display:flex;flex-direction:row;align-items:center;justify-content:center;font-size:14px;color:white">
    Made with Pingendo Free&nbsp;&nbsp;<img src="https://pingendo.com/site-assets/Pingendo_logo_big.png" class="d-block"
      alt="Pingendo logo" height="16"></pingendo>
</body>

</html>