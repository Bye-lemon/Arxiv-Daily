<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- PAGE settings -->
  <link rel="icon" href="https://templates.pingendo.com/assets/Pingendo_favicon.ico">
  <title>Arxiv Daily</title>
  <!-- CSS dependencies -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"
    type="text/css">
  <link rel="stylesheet" href="assets/wireframe.css">
</head>

<body class="bg-light">
  <div class="py-5">
    <div class="container">
      <div class="row">
        <div class="text-center col-md-7 mx-auto"> <i class="fa d-block fa-bullseye fa-5x mb-4 text-info"></i>
          <h2><b>Arxiv Daily - 2023-10-25</b></h2>
        </div>
      </div>
    </div>
  </div>
  <div class="py-2">
    <div class="container">
      <div class="row">
        <div class="col-md-12">
          <ul class="nav nav-tabs">
            
            <li class="nav-item"> <a href="" class="nav-link active"
                data-toggle="pill" data-target="#SLAM"><i class="fa fa-home"></i> SLAM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#SFM"><i class="fa fa-home"></i> SFM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#LLM"><i class="fa fa-home"></i> LLM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#SAM"><i class="fa fa-home"></i> SAM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#CLIP"><i class="fa fa-home"></i> CLIP</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#DINO"><i class="fa fa-home"></i> DINO</a>
            </li>
            
          </ul>
          <div class="tab-content mt-2">
            
            <div class="tab-pane fade active show" id="SLAM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2310_15023v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SONIC: Sonar Image Correspondence using Pose Supervised Learning for Imaging Sonars</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15023v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15023v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15023v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we address the challenging problem of data association for underwater SLAM through a novel method for sonar image correspondence using learned features. We introduce SONIC (SONar Image Correspondence), a pose-supervised network designed to yield robust feature correspondence capable of withstanding viewpoint variations. The inherent complexity of the underwater environment stems from the dynamic and frequently limited visibility conditions, restricting vision to a few meters of often featureless expanses. This makes camera-based systems suboptimal in most open water application scenarios. Consequently, multibeam imaging sonars emerge as the preferred choice for perception sensors. However, they too are not without their limitations. While imaging sonars offer superior long-range visibility compared to cameras, their measurements can appear different from varying viewpoints. This inherent variability presents formidable challenges in data association, particularly for feature-based methods. Our method demonstrates significantly better performance in generating correspondences for sonar images which will pave the way for more accurate loop closure constraints and sonar-based place recognition. Code as well as simulated and real-world datasets will be made public to facilitate further development in the field.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15023v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们通过一种利用学习特征进行声纳图像对应的新方法来解决水下SLAM的数据关联这一具有挑战性的问题。我们介绍了SONIC（SONar图像对应），这是一种姿态监督网络，旨在产生能够承受视点变化的鲁棒特征对应。水下环境固有的复杂性源于动态和经常有限的能见度条件，将视野限制在几米的范围内，通常没有特征。这使得基于摄像头的系统在大多数开放水域应用场景中都是次优的。因此，多波束成像声纳成为感知传感器的首选。然而，它们也并非没有局限性。虽然与相机相比，成像声纳提供了卓越的远程可见性，但它们的测量结果在不同的视角下可能会有所不同。这种固有的可变性给数据关联带来了巨大的挑战，尤其是对于基于特征的方法。我们的方法在为声纳图像生成对应关系方面表现出了明显更好的性能，这将为更准确的闭环约束和基于声纳的位置识别铺平道路。代码以及模拟和真实世界的数据集将公开，以促进该领域的进一步发展。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15023v1" target="_blank">2310.15023v1</a>
                              </td>
                              <td>SONIC: Sonar Image Correspondence using Pose Supervised Learning for Imaging Sonars</td>
                              <td>Samiran Gode</td>
                              <td>2023-10-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15023v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15023v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_07241v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ConceptFusion: Open-set Multimodal 3D Mapping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_07241v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_07241v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_07241v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Building 3D maps of the environment is central to robot navigation, planning, and interaction with objects in a scene. Most existing approaches that integrate semantic concepts with 3D maps largely remain confined to the closed-set setting: they can only reason about a finite set of concepts, pre-defined at training time. Further, these maps can only be queried using class labels, or in recent work, using text prompts.   We address both these issues with ConceptFusion, a scene representation that is (1) fundamentally open-set, enabling reasoning beyond a closed set of concepts and (ii) inherently multimodal, enabling a diverse range of possible queries to the 3D map, from language, to images, to audio, to 3D geometry, all working in concert. ConceptFusion leverages the open-set capabilities of today's foundation models pre-trained on internet-scale data to reason about concepts across modalities such as natural language, images, and audio. We demonstrate that pixel-aligned open-set features can be fused into 3D maps via traditional SLAM and multi-view fusion approaches. This enables effective zero-shot spatial reasoning, not needing any additional training or finetuning, and retains long-tailed concepts better than supervised approaches, outperforming them by more than 40% margin on 3D IoU. We extensively evaluate ConceptFusion on a number of real-world datasets, simulated home environments, a real-world tabletop manipulation task, and an autonomous driving platform. We showcase new avenues for blending foundation models with 3D open-set multimodal mapping.   For more information, visit our project page https://concept-fusion.github.io or watch our 5-minute explainer video https://www.youtube.com/watch?v=rkXgws8fiDs</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_07241v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>构建环境的3D地图是机器人导航、规划以及与场景中对象交互的核心。大多数现有的将语义概念与3D地图集成的方法在很大程度上仍然局限于闭集设置：它们只能推理在训练时预定义的有限概念集。此外，只能使用类标签或在最近的工作中使用文本提示来查询这些映射。我们使用ConceptFusion解决了这两个问题，ConceptFusion-一种场景表示，它（1）基本上是开放集，能够超越封闭的概念集进行推理，（2）本质上是多模态的，能够对3D地图进行各种可能的查询，从语言到图像，从音频到3D几何，所有这些都协同工作。ConceptFusion利用当今在互联网规模数据上预先训练的基础模型的开放集功能，对自然语言、图像和音频等模态的概念进行推理。我们证明了像素对齐的开放集特征可以通过传统的SLAM和多视图融合方法融合到3D地图中。这实现了有效的零样本空间推理，不需要任何额外的训练或微调，并比监督方法更好地保留了长尾概念，在3D IoU上比它们高出40%以上。我们在许多真实世界的数据集、模拟家庭环境、真实世界的桌面操作任务和自动驾驶平台上广泛评估了ConceptFusion。我们展示了将基础模型与3D开放集多模式映射相结合的新途径。有关更多信息，请访问我们的项目页面https://concept-fusion.github.io或观看我们的5分钟解说视频https://www.youtube.com/watch?v=rkXgws8fiDs</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.07241v3" target="_blank">2302.07241v3</a>
                              </td>
                              <td>ConceptFusion: Open-set Multimodal 3D Mapping</td>
                              <td>Krishna Murthy Jatavallabhula</td>
                              <td>2023-02-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_07241v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.07241v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_14924v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Converting Depth Images and Point Clouds for Feature-based Pose Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_14924v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_14924v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_14924v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, depth sensors have become more and more affordable and have found their way into a growing amount of robotic systems. However, mono- or multi-modal sensor registration, often a necessary step for further processing, faces many challenges on raw depth images or point clouds. This paper presents a method of converting depth data into images capable of visualizing spatial details that are basically hidden in traditional depth images. After noise removal, a neighborhood of points forms two normal vectors whose difference is encoded into this new conversion. Compared to Bearing Angle images, our method yields brighter, higher-contrast images with more visible contours and more details. We tested feature-based pose estimation of both conversions in a visual odometry task and RGB-D SLAM. For all tested features, AKAZE, ORB, SIFT, and SURF, our new Flexion images yield better results than Bearing Angle images and show great potential to bridge the gap between depth data and classical computer vision. Source code is available here: https://rlsch.github.io/depth-flexion-conversion.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_14924v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，深度传感器变得越来越便宜，并已进入越来越多的机器人系统。然而，单模态或多模态传感器配准通常是进一步处理的必要步骤，在原始深度图像或点云上面临许多挑战。本文提出了一种将深度数据转换为能够可视化传统深度图像中基本隐藏的空间细节的图像的方法。在去除噪声之后，点的邻域形成两个法向量，其差值被编码到这个新的转换中。与方位角图像相比，我们的方法产生了更明亮、对比度更高、轮廓更清晰、细节更多的图像。我们在视觉里程计任务和RGB-D SLAM中测试了两种转换的基于特征的姿态估计。对于所有测试的特征，AKAZE、ORB、SIFT和SURF，我们的新Flexion图像比方位角图像产生更好的结果，并显示出弥合深度数据和经典计算机视觉之间差距的巨大潜力。此处提供源代码：https://rlsch.github.io/depth-flexion-conversion.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.14924v1" target="_blank">2310.14924v1</a>
                              </td>
                              <td>Converting Depth Images and Point Clouds for Feature-based Pose Estimation</td>
                              <td>Robert Lösch</td>
                              <td>2023-10-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_14924v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.14924v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_15268v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ObVi-SLAM: Long-Term Object-Visual SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_15268v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_15268v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_15268v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Robots responsible for tasks over long time scales must be able to localize consistently and scalably amid geometric, viewpoint, and appearance changes. Existing visual SLAM approaches rely on low-level feature descriptors that are not robust to such environmental changes and result in large map sizes that scale poorly over long-term deployments. In contrast, object detections are robust to environmental variations and lead to more compact representations, but most object-based SLAM systems target short-term indoor deployments with close objects. In this paper, we introduce ObVi-SLAM to overcome these challenges by leveraging the best of both approaches. ObVi-SLAM uses low-level visual features for high-quality short-term visual odometry; and to ensure global, long-term consistency, ObVi-SLAM builds an uncertainty-aware long-term map of persistent objects and updates it after every deployment. By evaluating ObVi-SLAM on data from 16 deployment sessions spanning different weather and lighting conditions, we empirically show that ObVi-SLAM generates accurate localization estimates consistent over long-time scales in spite of varying appearance conditions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_15268v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>负责长时间尺度任务的机器人必须能够在几何、视点和外观变化中一致且可缩放地进行定位。现有的视觉SLAM方法依赖于低级别的特征描述符，这些特征描述符对这种环境变化不具有鲁棒性，并导致在长期部署中难以扩展的大地图大小。相比之下，对象检测对环境变化是鲁棒的，并导致更紧凑的表示，但大多数基于对象的SLAM系统针对的是具有近距离对象的短期室内部署。在本文中，我们介绍了ObVi SLAM，通过利用这两种方法中的最佳方法来克服这些挑战。ObVi SLAM使用低水平的视觉特征进行高质量的短期视觉里程计；为了确保全局、长期的一致性，ObVi SLAM构建了一个持久对象的不确定性感知长期映射，并在每次部署后进行更新。通过对跨越不同天气和照明条件的16个部署会话的数据进行ObVi SLAM评估，我们的经验表明，尽管外观条件不同，ObVi SLAM在长时间尺度上产生了一致的准确定位估计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.15268v2" target="_blank">2309.15268v2</a>
                              </td>
                              <td>ObVi-SLAM: Long-Term Object-Visual SLAM</td>
                              <td>Amanda Adkins</td>
                              <td>2023-09-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_15268v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.15268v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_08856v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MV-ROPE: Multi-view Constraints for Robust Category-level Object Pose and Size Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_08856v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_08856v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_08856v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a novel framework for RGB-based category-level 6D object pose and size estimation. Our approach relies on the prediction of normalized object coordinate space (NOCS), which serves as an efficient and effective object canonical representation that can be extracted from RGB images. Unlike previous approaches that heavily relied on additional depth readings as input, our novelty lies in leveraging multi-view information, which is commonly available in practical scenarios where a moving camera continuously observes the environment. By introducing multi-view constraints, we can obtain accurate camera pose and depth estimation from a monocular dense SLAM framework. Additionally, by incorporating constraints on the camera relative pose, we can apply trimming strategies and robust pose averaging on the multi-view object poses, resulting in more accurate and robust estimations of category-level object poses even in the absence of direct depth readings. Furthermore, we introduce a novel NOCS prediction network that significantly improves performance. Our experimental results demonstrate the strong performance of our proposed method, even comparable to state-of-the-art RGB-D methods across public dataset sequences. Additionally, we showcase the generalization ability of our method by evaluating it on self-collected datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_08856v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的基于RGB的类别级6D对象姿态和大小估计框架。我们的方法依赖于归一化对象坐标空间（NOCS）的预测，它是一种有效的对象规范表示，可以从RGB图像中提取。与以前严重依赖额外深度读数作为输入的方法不同，我们的新颖之处在于利用多视图信息，这在移动相机连续观察环境的实际场景中很常见。通过引入多视图约束，我们可以从单目密集SLAM框架中获得准确的相机姿态和深度估计。此外，通过结合对相机相对姿态的约束，我们可以对多视图对象姿态应用修剪策略和稳健的姿态平均，即使在没有直接深度读数的情况下，也可以对类别级对象姿态进行更准确和稳健的估计。此外，我们还介绍了一种新的NOCS预测网络，该网络显著提高了性能。我们的实验结果证明了我们提出的方法的强大性能，甚至可以与公共数据集序列中最先进的RGB-D方法相媲美。此外，我们通过在自己收集的数据集上评估我们的方法，展示了它的泛化能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.08856v2" target="_blank">2308.08856v2</a>
                              </td>
                              <td>MV-ROPE: Multi-view Constraints for Robust Category-level Object Pose and Size Estimation</td>
                              <td>Jiaqi Yang</td>
                              <td>2023-08-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_08856v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.08856v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_13768v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PACE: Human and Camera Motion Estimation from in-the-wild Videos</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_13768v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_13768v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_13768v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present a method to estimate human motion in a global scene from moving cameras. This is a highly challenging task due to the coupling of human and camera motions in the video. To address this problem, we propose a joint optimization framework that disentangles human and camera motions using both foreground human motion priors and background scene features. Unlike existing methods that use SLAM as initialization, we propose to tightly integrate SLAM and human motion priors in an optimization that is inspired by bundle adjustment. Specifically, we optimize human and camera motions to match both the observed human pose and scene features. This design combines the strengths of SLAM and motion priors, which leads to significant improvements in human and camera motion estimation. We additionally introduce a motion prior that is suitable for batch optimization, making our approach significantly more efficient than existing approaches. Finally, we propose a novel synthetic dataset that enables evaluating camera motion in addition to human motion from dynamic videos. Experiments on the synthetic and real-world RICH datasets demonstrate that our approach substantially outperforms prior art in recovering both human and camera motions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_13768v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种从移动摄像机中估计全局场景中人类运动的方法。由于视频中人和摄像机运动的耦合，这是一项极具挑战性的任务。为了解决这个问题，我们提出了一个联合优化框架，该框架使用前景人类运动先验和背景场景特征来解开人类和相机的运动。与使用SLAM作为初始化的现有方法不同，我们建议在受束调整启发的优化中紧密集成SLAM和人体运动先验。具体来说，我们优化人体和相机的运动，以匹配观察到的人体姿势和场景特征。这种设计结合了SLAM和运动先验的优势，显著改进了人体和相机的运动估计。我们还引入了一种适用于批量优化的运动先验，使我们的方法比现有方法更有效。最后，我们提出了一个新的合成数据集，该数据集除了可以从动态视频中评估人体运动外，还可以评估相机运动。在合成和真实世界的RICH数据集上的实验表明，我们的方法在恢复人体和相机运动方面大大优于现有技术。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.13768v1" target="_blank">2310.13768v1</a>
                              </td>
                              <td>PACE: Human and Camera Motion Estimation from in-the-wild Videos</td>
                              <td>Muhammed Kocabas</td>
                              <td>2023-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_13768v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.13768v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_13324v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ColAG: A Collaborative Air-Ground Framework for Perception-Limited UGVs' Navigation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_13324v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_13324v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_13324v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Perception is necessary for autonomous navigation in an unknown area crowded with obstacles. It's challenging for a robot to navigate safely without any sensors that can sense the environment, resulting in a $\textit{blind}$ robot, and becomes more difficult when comes to a group of robots. However, it could be costly to equip all robots with expensive perception or SLAM systems. In this paper, we propose a novel system named $\textbf{ColAG}$, to solve the problem of autonomous navigation for a group of $\textit{blind}$ UGVs by introducing cooperation with one UAV, which is the only robot that has full perception capabilities in the group. The UAV uses SLAM for its odometry and mapping while sharing this information with UGVs via limited relative pose estimation. The UGVs plan their trajectories in the received map and predict possible failures caused by the uncertainty of its wheel odometry and unknown risky areas. The UAV dynamically schedules waypoints to prevent UGVs from collisions, formulated as a Vehicle Routing Problem with Time Windows to optimize the UAV's trajectories and minimize time when UGVs have to wait to guarantee safety. We validate our system through extensive simulation with up to 7 UGVs and real-world experiments with 3 UGVs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_13324v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>感知是在充满障碍物的未知区域进行自主导航所必需的。对于一个机器人来说，在没有任何可以感知环境的传感器的情况下安全导航是一项挑战，这导致了$\textit｛blind｝$机器人，当涉及到一组机器人时，这变得更加困难。然而，为所有机器人配备昂贵的感知或SLAM系统可能成本高昂。在本文中，我们提出了一个名为$\textbf｛ColAG｝$的新系统，通过引入与一个无人机的合作来解决一组$\textit｛blind｝$无人值守地面飞行器的自主导航问题，该无人机是该组无人机中唯一具有完全感知能力的机器人。无人机使用SLAM进行里程测量和测绘，同时通过有限的相对姿态估计与UGV共享这些信息。UGV在收到的地图中规划其轨迹，并预测其车轮里程计的不确定性和未知风险区域可能导致的故障。无人机动态调度航路点，以防止无人值守地面飞行器发生碰撞，该问题被定义为具有时间窗口的车辆路线问题，以优化无人机的轨迹，并在无人值守地面车辆必须等待以确保安全时将时间减至最少。我们通过对多达7辆无人值守地面车辆的广泛模拟和对3辆无人值守地下车辆的真实世界实验来验证我们的系统。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.13324v1" target="_blank">2310.13324v1</a>
                              </td>
                              <td>ColAG: A Collaborative Air-Ground Framework for Perception-Limited UGVs' Navigation</td>
                              <td>Zhehan Li</td>
                              <td>2023-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_13324v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.13324v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_13256v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Higher or Lower: Challenges in Object based SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_13256v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_13256v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_13256v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localization and mapping, as a fundamental task in computer vision, has gained higher demands for performance in recent years due to the rapid development of autonomous driving and unmanned aerial vehicles. Traditional SLAM algorithms highly rely on basic geometry features such as points and lines, which are susceptible to environment. Conversely, higher-level object features offer richer information that is crucial for enhancing the overall performance of the framework. However, the effective utilization of object features necessitates careful consideration of various challenges, including complexity and process velocity. Given the advantages and disadvantages of both high-level object feature and low-level geometry features, it becomes essential to make informed choices within the SLAM framework. Taking these factors into account, this paper provides a thorough comparison between geometry features and object features, analyzes the current mainstream application methods of object features in SLAM frameworks, and presents a comprehensive overview of the main challenges involved in object-based SLAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_13256v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，随着自动驾驶和无人机的快速发展，同时定位和绘制地图作为计算机视觉的一项基础任务，对性能提出了更高的要求。传统的SLAM算法高度依赖于点和线等基本几何特征，这些特征容易受到环境的影响。相反，更高级的对象特征提供了更丰富的信息，这对于提高框架的整体性能至关重要。然而，有效利用对象特征需要仔细考虑各种挑战，包括复杂性和处理速度。考虑到高级对象特征和低级几何特征的优缺点，在SLAM框架内做出明智的选择变得至关重要。考虑到这些因素，本文对几何特征和对象特征进行了深入的比较，分析了SLAM框架中当前主流的对象特征应用方法，并全面概述了基于对象的SLAM所面临的主要挑战。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.13256v1" target="_blank">2310.13256v1</a>
                              </td>
                              <td>Higher or Lower: Challenges in Object based SLAM</td>
                              <td>Zhihe Zhang</td>
                              <td>2023-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_13256v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.13256v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_11645v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Abdominal 3-D Scene Rendering from Laparoscopy Surgical Videos using NeRFs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_11645v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_11645v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_11645v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Given that a conventional laparoscope only provides a two-dimensional (2-D) view, the detection and diagnosis of medical ailments can be challenging. To overcome the visual constraints associated with laparoscopy, the use of laparoscopic images and videos to reconstruct the three-dimensional (3-D) anatomical structure of the abdomen has proven to be a promising approach. Neural Radiance Fields (NeRFs) have recently gained attention thanks to their ability to generate photorealistic images from a 3-D static scene, thus facilitating a more comprehensive exploration of the abdomen through the synthesis of new views. This distinguishes NeRFs from alternative methods such as Simultaneous Localization and Mapping (SLAM) and depth estimation. In this paper, we present a comprehensive examination of NeRFs in the context of laparoscopy surgical videos, with the goal of rendering abdominal scenes in 3-D. Although our experimental results are promising, the proposed approach encounters substantial challenges, which require further exploration in future research.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_11645v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>考虑到传统腹腔镜仅提供二维（2-D）视图，对医疗疾病的检测和诊断可能具有挑战性。为了克服与腹腔镜相关的视觉限制，使用腹腔镜图像和视频重建腹部的三维解剖结构已被证明是一种很有前途的方法。神经辐射场（NeRF）最近因其能够从三维静态场景中生成真实感图像而受到关注，从而通过合成新视图促进对腹部的更全面探索。这将NeRF与诸如同时定位和映射（SLAM）和深度估计之类的替代方法区分开来。在本文中，我们在腹腔镜手术视频的背景下对NeRFs进行了全面的检查，目的是在3D中呈现腹部场景。尽管我们的实验结果很有希望，但所提出的方法遇到了巨大的挑战，需要在未来的研究中进一步探索。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.11645v1" target="_blank">2310.11645v1</a>
                              </td>
                              <td>Towards Abdominal 3-D Scene Rendering from Laparoscopy Surgical Videos using NeRFs</td>
                              <td>Khoa Tuan Nguyen</td>
                              <td>2023-10-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_11645v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.11645v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_11598v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Neural Implicit through Volume Rendering with Attentive Depth Fusion Priors</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_11598v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_11598v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_11598v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Learning neural implicit representations has achieved remarkable performance in 3D reconstruction from multi-view images. Current methods use volume rendering to render implicit representations into either RGB or depth images that are supervised by multi-view ground truth. However, rendering a view each time suffers from incomplete depth at holes and unawareness of occluded structures from the depth supervision, which severely affects the accuracy of geometry inference via volume rendering. To resolve this issue, we propose to learn neural implicit representations from multi-view RGBD images through volume rendering with an attentive depth fusion prior. Our prior allows neural networks to perceive coarse 3D structures from the Truncated Signed Distance Function (TSDF) fused from all depth images available for rendering. The TSDF enables accessing the missing depth at holes on one depth image and the occluded parts that are invisible from the current view. By introducing a novel attention mechanism, we allow neural networks to directly use the depth fusion prior with the inferred occupancy as the learned implicit function. Our attention mechanism works with either a one-time fused TSDF that represents a whole scene or an incrementally fused TSDF that represents a partial scene in the context of Simultaneous Localization and Mapping (SLAM). Our evaluations on widely used benchmarks including synthetic and real-world scans show our superiority over the latest neural implicit methods. Project page: https://machineperceptionlab.github.io/Attentive_DF_Prior/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_11598v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>学习神经隐式表示在多视图图像的三维重建中取得了显著的性能。当前的方法使用体积渲染将隐式表示渲染为RGB或深度图像，这些图像由多视图地面实况监督。然而，每次渲染视图都会遇到孔的深度不完整以及深度监督对遮挡结构的不了解，这严重影响了通过体绘制进行几何推断的准确性。为了解决这个问题，我们建议通过具有注意深度融合先验的体绘制，从多视图RGBD图像中学习神经隐式表示。我们的先验允许神经网络从从可用于渲染的所有深度图像中融合的截断有符号距离函数（TSDF）中感知粗略的3D结构。TSDF能够访问一个深度图像上孔的缺失深度以及当前视图中不可见的遮挡部分。通过引入一种新的注意力机制，我们允许神经网络直接使用具有推断占用率的深度融合先验作为学习的内隐函数。在同步定位和映射（SLAM）的背景下，我们的注意力机制与表示整个场景的一次性融合TSDF或表示部分场景的增量融合TSDF一起工作。我们对广泛使用的基准测试（包括合成扫描和真实世界扫描）的评估表明，我们优于最新的神经隐式方法。项目页面：https://machineperceptionlab.github.io/Attentive_DF_Prior/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.11598v1" target="_blank">2310.11598v1</a>
                              </td>
                              <td>Learning Neural Implicit through Volume Rendering with Attentive Depth Fusion Priors</td>
                              <td>Pengchong Hu</td>
                              <td>2023-10-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_11598v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.11598v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2104_08634v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ARES: Accurate, Autonomous, Near Real-time 3D Reconstruction using Drones</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2104_08634v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2104_08634v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2104_08634v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Drones will revolutionize 3D modeling. A 3D model represents an accurate reconstruction of an object or structure. This paper explores the design and implementation of ARES, which provides near real-time, accurate reconstruction of 3D models using a drone-mounted LiDAR; such a capability can be useful to document construction or check aircraft integrity between flights. Accurate reconstruction requires high drone positioning accuracy, and, because GPS can be in accurate, ARES uses SLAM. However, in doing so it must deal with several competing constraints: drone battery and compute resources, SLAM error accumulation, and LiDAR resolution. ARES uses careful trajectory design to find a sweet spot in this constraint space, a fast reconnaissance flight to narrow the search area for structures, and offloads expensive computations to the cloud by streaming compressed LiDAR data over LTE. ARES reconstructs large structures to within 10s of cms and incurs less than 100 ms compute latency.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2104_08634v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>无人机将彻底改变3D建模。3D模型表示对象或结构的精确重建。本文探讨了ARES的设计和实现，该系统使用无人机安装的激光雷达提供近实时、准确的三维模型重建；这种能力可用于记录结构或检查飞行之间的飞机完整性。精确的重建需要高的无人机定位精度，而且由于GPS可以精确，ARES使用SLAM。然而，在这样做的过程中，它必须处理几个相互竞争的约束：无人机电池和计算资源、SLAM误差积累和激光雷达分辨率。ARES使用仔细的轨迹设计来在这个约束空间中找到一个最佳点，通过快速侦察飞行来缩小结构的搜索区域，并通过LTE上的压缩LiDAR数据流将昂贵的计算转移到云端。ARES将大型结构重建到10厘米以内，计算延迟小于100毫秒。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2104.08634v3" target="_blank">2104.08634v3</a>
                              </td>
                              <td>ARES: Accurate, Autonomous, Near Real-time 3D Reconstruction using Drones</td>
                              <td>Fawad Ahmad</td>
                              <td>2021-04-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2104_08634v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2104.08634v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_10931v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Open-Structure: a Structural Benchmark Dataset for SLAM Algorithms</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_10931v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_10931v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_10931v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper introduces a new benchmark dataset, Open-Structure, for evaluating visual odometry and SLAM methods, which directly equips point and line measurements, correspondences, structural associations, and co-visibility factor graphs instead of providing raw images. Based on the proposed benchmark dataset, these 2D or 3D data can be directly input to different stages of SLAM pipelines to avoid the impact of the data preprocessing modules in ablation experiments. First, we propose a dataset generator for real-world and simulated scenarios. In real-world scenes, it maintains the same observations and occlusions as actual feature extraction results. Those generated simulation sequences enhance the dataset's diversity by introducing various carefully designed trajectories and observations. Second, a SLAM baseline is proposed using our dataset to evaluate widely used modules in camera pose tracking, parametrization, and optimization modules. By evaluating these state-of-the-art algorithms across different scenarios, we discern each module's strengths and weaknesses within the camera tracking and optimization process. Our dataset and baseline are available at \url{https://github.com/yanyan-li/Open-Structure}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_10931v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了一个新的基准数据集Open Structure，用于评估视觉里程计和SLAM方法，该数据集直接配备点和线测量、对应关系、结构关联和共视因子图，而不是提供原始图像。基于所提出的基准数据集，这些2D或3D数据可以直接输入到SLAM管道的不同阶段，以避免消融实验中数据预处理模块的影响。首先，我们提出了一个用于真实世界和模拟场景的数据集生成器。在真实世界的场景中，它保持与实际特征提取结果相同的观察和遮挡。这些生成的模拟序列通过引入各种精心设计的轨迹和观测结果，增强了数据集的多样性。其次，使用我们的数据集提出了SLAM基线，以评估相机姿态跟踪、参数化和优化模块中广泛使用的模块。通过在不同场景中评估这些最先进的算法，我们可以在相机跟踪和优化过程中辨别出每个模块的优势和劣势。我们的数据集和基线位于\url{https://github.com/yanyan-li/Open-Structure}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.10931v1" target="_blank">2310.10931v1</a>
                              </td>
                              <td>Open-Structure: a Structural Benchmark Dataset for SLAM Algorithms</td>
                              <td>Yanyan Li</td>
                              <td>2023-10-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_10931v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.10931v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_10862v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Invisible Map: Visual-Inertial SLAM with Fiducial Markers for Smartphone-based Indoor Navigation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_10862v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_10862v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_10862v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present a system for creating building-scale, easily navigable 3D maps using mainstream smartphones. In our approach, we formulate the 3D-mapping problem as an instance of Graph SLAM and infer the position of both building landmarks (fiducial markers) and navigable paths through the environment (phone poses). Our results demonstrate the system's ability to create accurate 3D maps. Further, we highlight the importance of careful selection of mapping hyperparameters and provide a novel technique for tuning these hyperparameters to adapt our algorithm to new environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_10862v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提供了一个使用主流智能手机创建建筑规模、易于导航的3D地图的系统。在我们的方法中，我们将3D映射问题公式化为Graph SLAM的一个实例，并推断建筑地标（基准标记）和环境中的可导航路径（电话姿势）的位置。我们的结果证明了该系统能够创建准确的3D地图。此外，我们强调了仔细选择映射超参数的重要性，并提供了一种新的技术来调整这些超参数，以使我们的算法适应新的环境。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.10862v1" target="_blank">2310.10862v1</a>
                              </td>
                              <td>The Invisible Map: Visual-Inertial SLAM with Fiducial Markers for Smartphone-based Indoor Navigation</td>
                              <td>Paul Ruvolo</td>
                              <td>2023-10-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_10862v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.10862v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_10290v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Autonomous Mapping and Navigation using Fiducial Markers and Pan-Tilt Camera for Assisting Indoor Mobility of Blind and Visually Impaired People</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_10290v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_10290v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_10290v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large indoor spaces have complex layouts making them difficult to navigate. Indoor spaces in hospitals, universities, shopping complexes, etc., carry multi-modal information in the form of text and symbols. Hence, it is difficult for Blind and Visually Impaired (BVI) people to independently navigate such spaces. Indoor environments are usually GPS-denied; therefore, Bluetooth-based, WiFi-based, or Range-based methods are used for localization. These methods have high setup costs, lesser accuracy, and sometimes need special sensing equipment. We propose a Visual Assist (VA) system for the indoor navigation of BVI individuals using visual Fiducial markers for localization. State-of-the-art (SOTA) approaches for visual localization using Fiducial markers use fixed cameras having a narrow field of view. These approaches stop tracking the markers when they are out of sight. We employ a Pan-Tilt turret-mounted camera which enhances the field of view to 360{\deg} for enhanced marker tracking. We, therefore, need fewer markers for mapping and navigation. The efficacy of the proposed VA system is measured on three metrics, i.e., RMSE (Root Mean Square Error), ADNN (Average Distance to Nearest Neighbours), and ATE (Absolute Trajectory Error). Our system outperforms Hector-SLAM, ORB-SLAM3, and UcoSLAM. The proposed system achieves localization accuracy within $\pm8cm$ compared to $\pm12cm$ and $\pm10cm$ for ORB-SLAM3 and UcoSLAM, respectively.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_10290v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型室内空间布局复杂，难以导航。医院、大学、购物中心等的室内空间以文本和符号的形式承载着多模态信息。因此，盲人和视障人士（BVI）很难独立导航这些空间。室内环境通常被GPS拒绝；因此，使用基于蓝牙、基于WiFi或基于范围的方法进行定位。这些方法的设置成本高，精度低，有时需要特殊的传感设备。我们提出了一种视觉辅助（VA）系统，用于BVI个人的室内导航，使用视觉基准标记进行定位。使用基准标记进行视觉定位的现有技术（SOTA）方法使用具有窄视场的固定相机。这些方法在标记不在视线范围内时停止跟踪标记。我们采用了一种安装在云台上的摄像机，它可以将视野提高到360度，以增强标记跟踪。因此，我们需要更少的标记来绘制地图和导航。所提出的VA系统的功效是在三个度量上测量的，即RMSE（均方根误差）、ADNN（到最近邻居的平均距离）和ATE（绝对轨迹误差）。我们的系统性能优于Hector SLAM、ORB-SLAM3和UcoSLAM。与ORB-SLAM3和UcoSLAM的$\pm12cm$和$\pm10cm$相比，所提出的系统在$\pm8cm$内实现了定位精度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.10290v1" target="_blank">2310.10290v1</a>
                              </td>
                              <td>Autonomous Mapping and Navigation using Fiducial Markers and Pan-Tilt Camera for Assisting Indoor Mobility of Blind and Visually Impaired People</td>
                              <td>Dharmateja Adapa</td>
                              <td>2023-10-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_10290v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.10290v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16585v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SeMLaPS: Real-time Semantic Mapping with Latent Prior Networks and Quasi-Planar Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16585v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16585v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16585v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The availability of real-time semantics greatly improves the core geometric functionality of SLAM systems, enabling numerous robotic and AR/VR applications. We present a new methodology for real-time semantic mapping from RGB-D sequences that combines a 2D neural network and a 3D network based on a SLAM system with 3D occupancy mapping. When segmenting a new frame we perform latent feature re-projection from previous frames based on differentiable rendering. Fusing re-projected feature maps from previous frames with current-frame features greatly improves image segmentation quality, compared to a baseline that processes images independently. For 3D map processing, we propose a novel geometric quasi-planar over-segmentation method that groups 3D map elements likely to belong to the same semantic classes, relying on surface normals. We also describe a novel neural network design for lightweight semantic map post-processing. Our system achieves state-of-the-art semantic mapping quality within 2D-3D networks-based systems and matches the performance of 3D convolutional networks on three real indoor datasets, while working in real-time. Moreover, it shows better cross-sensor generalization abilities compared to 3D CNNs, enabling training and inference with different depth sensors. Code and data will be released on project page: http://jingwenwang95.github.io/SeMLaPS</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16585v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>实时语义的可用性极大地提高了SLAM系统的核心几何功能，实现了许多机器人和AR/VR应用。我们提出了一种从RGB-D序列进行实时语义映射的新方法，该方法将2D神经网络和基于SLAM系统的3D网络与3D占用映射相结合。在分割新帧时，我们基于可微分渲染从先前帧执行潜在特征重新投影。与独立处理图像的基线相比，将来自先前帧的重新投影的特征图与当前帧特征融合可以大大提高图像分割质量。对于3D地图处理，我们提出了一种新的几何准平面过分割方法，该方法根据曲面法线对可能属于相同语义类的3D地图元素进行分组。我们还描述了一种用于轻量级语义图后处理的新型神经网络设计。我们的系统在基于2D-3D网络的系统中实现了最先进的语义映射质量，并在三个真实的室内数据集上匹配3D卷积网络的性能，同时实时工作。此外，与3D细胞神经网络相比，它显示出更好的跨传感器泛化能力，能够使用不同的深度传感器进行训练和推理。代码和数据将在项目页面上发布：http://jingwenwang95.github.io/SeMLaPS</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16585v2" target="_blank">2306.16585v2</a>
                              </td>
                              <td>SeMLaPS: Real-time Semantic Mapping with Latent Prior Networks and Quasi-Planar Segmentation</td>
                              <td>Jingwen Wang</td>
                              <td>2023-06-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16585v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16585v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_06950v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D Active Metric-Semantic SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_06950v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_06950v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_06950v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this letter, we address the problem of exploration and metric-semantic mapping of multi-floor GPS-denied indoor environments using Size Weight and Power (SWaP) constrained aerial robots. Most previous work in exploration assumes that robot localization is solved. However, neglecting the state uncertainty of the agent can ultimately lead to cascading errors both in the resulting map and in the state of the agent itself. Furthermore, actions that reduce localization errors may be at direct odds with the exploration task. We propose a framework that balances the efficiency of exploration with actions that reduce the state uncertainty of the agent. In particular, our algorithmic approach for active metric-semantic SLAM is built upon sparse information abstracted from raw problem data, to make it suitable for SWaP-constrained robots. Furthermore, we integrate this framework within a fully autonomous aerial robotic system that achieves autonomous exploration in cluttered, 3D environments. From extensive real-world experiments, we showed that by including Semantic Loop Closure (SLC), we can reduce the robot pose estimation errors by over 90% in translation and approximately 75% in yaw, and the uncertainties in pose estimates and semantic maps by over 70% and 65%, respectively. Although discussed in the context of indoor multi-floor exploration, our system can be used for various other applications, such as infrastructure inspection and precision agriculture where reliable GPS data may not be available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_06950v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这封信中，我们讨论了使用尺寸、重量和功率（SWaP）约束的空中机器人对多层GPS拒绝的室内环境进行探索和度量语义映射的问题。以前的大多数探索工作都假设机器人定位已经解决。然而，忽略代理的状态不确定性最终会导致在生成的映射和代理本身的状态中出现级联错误。此外，减少定位误差的动作可能与探索任务直接不一致。我们提出了一个框架，该框架平衡了探索的效率和减少代理状态不确定性的行动。特别是，我们的主动度量语义SLAM算法方法建立在从原始问题数据中提取的稀疏信息的基础上，使其适用于SWaP约束的机器人。此外，我们将该框架集成在一个完全自主的空中机器人系统中，该系统可以在杂乱的3D环境中实现自主探索。从大量的真实世界实验中，我们表明，通过包括语义环闭合（SLC），我们可以将机器人姿态估计误差在平移时降低90%以上，在偏航时降低约75%，将姿态估计和语义图的不确定性分别降低70%和65%以上。尽管在室内多层勘探的背景下进行了讨论，但我们的系统可用于各种其他应用，如基础设施检查和精密农业，在这些应用中可能无法获得可靠的GPS数据。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.06950v2" target="_blank">2309.06950v2</a>
                              </td>
                              <td>3D Active Metric-Semantic SLAM</td>
                              <td>Yuezhan Tao</td>
                              <td>2023-09-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_06950v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.06950v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_08082v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Jointly Optimized Global-Local Visual Localization of UAVs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_08082v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_08082v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_08082v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Navigation and localization of UAVs present a challenge when global navigation satellite systems (GNSS) are disrupted and unreliable. Traditional techniques, such as simultaneous localization and mapping (SLAM) and visual odometry (VO), exhibit certain limitations in furnishing absolute coordinates and mitigating error accumulation. Existing visual localization methods achieve autonomous visual localization without error accumulation by matching with ortho satellite images. However, doing so cannot guarantee real-time performance due to the complex matching process. To address these challenges, we propose a novel Global-Local Visual Localization (GLVL) network. Our GLVL network is a two-stage visual localization approach, combining a large-scale retrieval module that finds similar regions with the UAV flight scene, and a fine-grained matching module that localizes the precise UAV coordinate, enabling real-time and precise localization. The training process is jointly optimized in an end-to-end manner to further enhance the model capability. Experiments on six UAV flight scenes encompassing both texture-rich and texture-sparse regions demonstrate the ability of our model to achieve the real-time precise localization requirements of UAVs. Particularly, our method achieves a localization error of only 2.39 meters in 0.48 seconds in a village scene with sparse texture features.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_08082v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当全球导航卫星系统（GNSS）中断且不可靠时，无人机的导航和定位面临挑战。传统技术，如同时定位和映射（SLAM）和视觉里程计（VO），在提供绝对坐标和减少误差积累方面表现出一定的局限性。现有的视觉定位方法通过与卫星正射图像的匹配，实现了无误差积累的自主视觉定位。然而，由于匹配过程复杂，这样做并不能保证实时性能。为了应对这些挑战，我们提出了一种新的全球局部视觉定位（GLVL）网络。我们的GLVL网络是一种两阶段视觉定位方法，结合了一个大型检索模块和一个细粒度匹配模块，前者可以查找与无人机飞行场景相似的区域，后者可以定位精确的无人机坐标，实现实时精确的定位。以端到端的方式对训练过程进行联合优化，以进一步增强模型能力。在包括纹理丰富和纹理稀疏区域的六个无人机飞行场景上的实验证明了我们的模型能够实现无人机的实时精确定位要求。特别地，在具有稀疏纹理特征的村庄场景中，我们的方法在0.48秒内实现了仅2.39米的定位误差。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.08082v1" target="_blank">2310.08082v1</a>
                              </td>
                              <td>Jointly Optimized Global-Local Visual Localization of UAVs</td>
                              <td>Haoling Li</td>
                              <td>2023-10-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_08082v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.08082v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_04466v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GRADE: Generating Realistic Animated Dynamic Environments for Robotics Research</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_04466v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_04466v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_04466v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, computer vision tasks like target tracking and human pose estimation have immensely benefited from synthetic data generation and novel rendering techniques. On the other hand, methods in robotics, especially for robot perception, have been slow to leverage these techniques. This is because state-of-the-art simulation frameworks for robotics lack either complete control, integration with the Robot Operating System (ROS), realistic physics or photorealism. To solve this, we present a fully customizable framework for generating realistic animated dynamic environments (GRADE) for robotics research, focused primarily at robot perception. The framework can be used either to generate ground truth data for robotic vision-related tasks and offline processing, or to experiment with robots online in dynamic environments. We build upon the Nvidia Isaac Sim to allow control of custom robots. We provide methods to include assets, populate and control the simulation, and process the data. Using autonomous robots in GRADE, we generate video datasets of an indoor dynamic environment. First, we use it to demonstrate the framework's visual realism by evaluating the sim-to-real gap through experiments with YOLO and Mask R-CNN. Second, we benchmark dynamic SLAM algorithms with this dataset. This not only shows that GRADE can significantly improve training performance and generalization to real sequences, but also highlights how current dynamic SLAM methods over-rely on known benchmarks, failing to generalize. We also introduce a method to precisely repeat a previously recorded experiment, while allowing changes in the surroundings of the robot. Code and data are provided as open-source at https://grade.is.tue.mpg.de.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_04466v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，目标跟踪和人体姿态估计等计算机视觉任务极大地受益于合成数据生成和新颖的渲染技术。另一方面，机器人技术中的方法，特别是机器人感知的方法，在利用这些技术方面进展缓慢。这是因为最先进的机器人模拟框架缺乏完整的控制、与机器人操作系统（ROS）的集成、逼真的物理或真实感。为了解决这个问题，我们提出了一个完全可定制的框架，用于为机器人研究生成逼真的动画动态环境（GRADE），主要关注机器人感知。该框架可以用于为机器人视觉相关任务和离线处理生成地面实况数据，也可以用于在动态环境中对机器人进行在线实验。我们以英伟达Isaac Sim为基础，实现对定制机器人的控制。我们提供了包括资产、填充和控制模拟以及处理数据的方法。使用GRADE中的自主机器人，我们生成了室内动态环境的视频数据集。首先，我们通过YOLO和Mask R-CNN的实验评估模拟到真实的差距，用它来展示框架的视觉真实性。其次，我们用这个数据集对动态SLAM算法进行了基准测试。这不仅表明GRADE可以显著提高训练性能和对真实序列的泛化能力，还突出了当前动态SLAM方法如何过度依赖已知基准，而无法泛化。我们还介绍了一种方法，可以精确地重复之前记录的实验，同时允许机器人周围环境的变化。代码和数据以开源形式提供，网址为https://grade.is.tue.mpg.de.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.04466v2" target="_blank">2303.04466v2</a>
                              </td>
                              <td>GRADE: Generating Realistic Animated Dynamic Environments for Robotics Research</td>
                              <td>Elia Bonetto</td>
                              <td>2023-03-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_04466v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.04466v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07763v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Tightly-Coupled LiDAR-Visual SLAM Based on Geometric Features for Mobile Agents</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07763v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07763v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07763v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The mobile robot relies on SLAM (Simultaneous Localization and Mapping) to provide autonomous navigation and task execution in complex and unknown environments. However, it is hard to develop a dedicated algorithm for mobile robots due to dynamic and challenging situations, such as poor lighting conditions and motion blur. To tackle this issue, we propose a tightly-coupled LiDAR-visual SLAM based on geometric features, which includes two sub-systems (LiDAR and monocular visual SLAM) and a fusion framework. The fusion framework associates the depth and semantics of the multi-modal geometric features to complement the visual line landmarks and to add direction optimization in Bundle Adjustment (BA). This further constrains visual odometry. On the other hand, the entire line segment detected by the visual subsystem overcomes the limitation of the LiDAR subsystem, which can only perform the local calculation for geometric features. It adjusts the direction of linear feature points and filters out outliers, leading to a higher accurate odometry system. Finally, we employ a module to detect the subsystem's operation, providing the LiDAR subsystem's output as a complementary trajectory to our system while visual subsystem tracking fails. The evaluation results on the public dataset M2DGR, gathered from ground robots across various indoor and outdoor scenarios, show that our system achieves more accurate and robust pose estimation compared to current state-of-the-art multi-modal methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07763v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>移动机器人依靠SLAM（同步定位和映射）在复杂和未知的环境中提供自主导航和任务执行。然而，由于动态和具有挑战性的情况，例如较差的照明条件和运动模糊，很难为移动机器人开发专用算法。为了解决这个问题，我们提出了一种基于几何特征的紧密耦合激光雷达视觉SLAM，它包括两个子系统（激光雷达和单目视觉SLAM）和一个融合框架。融合框架将多模态几何特征的深度和语义相关联，以补充视觉线地标，并在束调整（BA）中添加方向优化。这进一步限制了视觉里程计。另一方面，视觉子系统检测到的整个线段克服了激光雷达子系统只能对几何特征进行局部计算的局限性。它调整线性特征点的方向并过滤掉异常值，从而实现更高精度的里程计系统。最后，我们使用一个模块来检测子系统的操作，在视觉子系统跟踪失败时，将激光雷达子系统的输出作为我们系统的补充轨迹。从各种室内和室外场景中的地面机器人收集的公共数据集M2DGR的评估结果表明，与当前最先进的多模态方法相比，我们的系统实现了更准确、更稳健的姿态估计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07763v2" target="_blank">2307.07763v2</a>
                              </td>
                              <td>Tightly-Coupled LiDAR-Visual SLAM Based on Geometric Features for Mobile Agents</td>
                              <td>Ke Cao</td>
                              <td>2023-07-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07763v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07763v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2209_05167v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LF-VISLAM: A SLAM Framework for Large Field-of-View Cameras with Negative Imaging Plane on Mobile Agents</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2209_05167v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2209_05167v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2209_05167v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous Localization And Mapping (SLAM) has become a crucial aspect in the fields of autonomous driving and robotics. One crucial component of visual SLAM is the Field-of-View (FoV) of the camera, as a larger FoV allows for a wider range of surrounding elements and features to be perceived. However, when the FoV of the camera reaches the negative half-plane, traditional methods for representing image feature points using [u,v,1]^T become ineffective. While the panoramic FoV is advantageous for loop closure, its benefits are not easily realized under large-attitude-angle differences where loop-closure frames cannot be easily matched by existing methods. As loop closure on wide-FoV panoramic data further comes with a large number of outliers, traditional outlier rejection methods are not directly applicable. To address these issues, we propose LF-VISLAM, a Visual Inertial SLAM framework for cameras with extremely Large FoV with loop closure. A three-dimensional vector with unit length is introduced to effectively represent feature points even on the negative half-plane. The attitude information of the SLAM system is leveraged to guide the feature point detection of the loop closure. Additionally, a new outlier rejection method based on the unit length representation is integrated into the loop closure module. We collect the PALVIO dataset using a Panoramic Annular Lens (PAL) system with an entire FoV of 360{\deg}x(40{\deg}~120{\deg}) and an Inertial Measurement Unit (IMU) for Visual Inertial Odometry (VIO) to address the lack of panoramic SLAM datasets. Experiments on the established PALVIO and public datasets show that the proposed LF-VISLAM outperforms state-of-the-art SLAM methods. Our code will be open-sourced at https://github.com/flysoaryun/LF-VISLAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2209_05167v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同步定位与映射（SLAM）已成为自动驾驶和机器人领域的一个重要方面。视觉SLAM的一个关键组成部分是相机的视场（FoV），因为更大的视场可以感知更广泛的周围元素和特征。然而，当相机的FoV到达负半平面时，使用[u，v，1]^T表示图像特征点的传统方法变得无效。虽然全景FoV有利于闭环，但在现有方法无法轻松匹配闭环框架的大姿态角差异下，其优点不容易实现。由于宽视场全景数据的闭环进一步带来了大量的异常值，传统的异常值剔除方法不直接适用。为了解决这些问题，我们提出了LF-VISLAM，这是一种视觉惯性SLAM框架，适用于具有环形闭合的超大视场的相机。引入单位长度的三维矢量，即使在负半平面上也能有效地表示特征点。利用SLAM系统的姿态信息来指导闭环的特征点检测。此外，在闭环模块中集成了一种基于单位长度表示的新的异常值抑制方法。我们使用全景环形透镜（PAL）系统和用于视觉惯性里程计（VIO）的惯性测量单元（IMU）收集PALVIO数据集，全景环形透镜系统的整个FoV为360｛\deg｝x（40｛\dig｝~120｛\deg｝），以解决全景SLAM数据集的缺乏问题。在已建立的PALVIO和公共数据集上的实验表明，所提出的LF-VISLAM优于最先进的SLAM方法。我们的代码将在https://github.com/flysoaryun/LF-VISLAM.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2209.05167v3" target="_blank">2209.05167v3</a>
                              </td>
                              <td>LF-VISLAM: A SLAM Framework for Large Field-of-View Cameras with Negative Imaging Plane on Mobile Agents</td>
                              <td>Ze Wang</td>
                              <td>2022-09-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2209_05167v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2209.05167v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_07844v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Saturation-Aware Angular Velocity Estimation: Extending the Robustness of SLAM to Aggressive Motions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_07844v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_07844v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_07844v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a novel angular velocity estimation method to increase the robustness of Simultaneous Localization And Mapping (SLAM) algorithms against gyroscope saturations induced by aggressive motions. Field robotics expose robots to various hazards, including steep terrains, landslides, and staircases, where substantial accelerations and angular velocities can occur if the robot loses stability and tumbles. These extreme motions can saturate sensor measurements, especially gyroscopes, which are the first sensors to become inoperative. While the structural integrity of the robot is at risk, the resilience of the SLAM framework is oftentimes given little consideration. Consequently, even if the robot is physically capable of continuing the mission, its operation will be compromised due to a corrupted representation of the world. Regarding this problem, we propose a way to estimate the angular velocity using accelerometers during extreme rotations caused by tumbling. We show that our method reduces the median localization error by 71.5 % in translation and 65.5 % in rotation and reduces the number of SLAM failures by 73.3 % on the collected data. We also propose the Tumbling-Induced Gyroscope Saturation (TIGS) dataset, which consists of outdoor experiments recording the motion of a lidar subject to angular velocities four times higher than other available datasets. The dataset is available online at https://github.com/norlab-ulaval/Norlab_wiki/wiki/TIGS-Dataset.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_07844v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的角速度估计方法，以提高同步定位和映射（SLAM）算法对侵略性运动引起的陀螺仪饱和的鲁棒性。野外机器人使机器人面临各种危险，包括陡峭的地形、山体滑坡和楼梯，如果机器人失去稳定性并摔倒，可能会出现大幅加速度和角速度。这些极端运动会使传感器测量饱和，尤其是陀螺仪，因为陀螺仪是第一个不工作的传感器。虽然机器人的结构完整性面临风险，但SLAM框架的弹性往往很少得到考虑。因此，即使机器人在物理上有能力继续执行任务，其操作也会因对世界的破坏而受到影响。关于这个问题，我们提出了一种在翻滚引起的极端旋转过程中使用加速度计估计角速度的方法。我们表明，在收集的数据中，我们的方法在平移和旋转中分别将中值定位误差降低了71.5%和65.5%，并将SLAM故障次数降低了73.3%。我们还提出了翻滚诱导陀螺仪饱和（TIGS）数据集，该数据集由室外实验组成，记录激光雷达在角速度比其他可用数据集高四倍的情况下的运动。数据集可在线获取，网址为https://github.com/norlab-ulaval/Norlab_wiki/wiki/TIGS-Dataset.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.07844v1" target="_blank">2310.07844v1</a>
                              </td>
                              <td>Saturation-Aware Angular Velocity Estimation: Extending the Robustness of SLAM to Aggressive Motions</td>
                              <td>Simon-Pierre Deschênes</td>
                              <td>2023-10-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_07844v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.07844v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09531v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LOG-LIO: A LiDAR-Inertial Odometry with Efficient Local Geometric Information Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09531v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09531v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09531v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Local geometric information, i.e. normal and distribution of points, is crucial for LiDAR-based simultaneous localization and mapping (SLAM) because it provides constraints for data association, which further determines the direction of optimization and ultimately affects the accuracy of localization. However, estimating normal and distribution of points are time-consuming tasks even with the assistance of kdtree or volumetric maps. To achieve fast normal estimation, we look into the structure of LiDAR scan and propose a ring-based fast approximate least squares (Ring FALS) method. With the Ring structural information, estimating the normal requires only the range information of the points when a new scan arrives. To efficiently estimate the distribution of points, we extend the ikd-tree to manage the map in voxels and update the distribution of points in each voxel incrementally while maintaining its consistency with the normal estimation. We further fix the distribution after its convergence to balance the time consumption and the correctness of representation. Based on the extracted and maintained local geometric information, we devise a robust and accurate hierarchical data association scheme where point-to-surfel association is prioritized over point-to-plane. Extensive experiments on diverse public datasets demonstrate the advantages of our system compared to other state-of-the-art methods. Our open source implementation is available at https://github.com/tiev-tongji/LOG-LIO.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09531v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>局部几何信息，即点的法线和分布，对于基于激光雷达的同时定位和映射（SLAM）至关重要，因为它为数据关联提供了约束，从而进一步决定了优化的方向，并最终影响定位的准确性。然而，即使在kdtree或体积图的帮助下，估计点的正态和分布也是耗时的任务。为了实现快速正态估计，我们研究了激光雷达扫描的结构，并提出了一种基于环的快速近似最小二乘法（ring-FALS）。利用环形结构信息，当新的扫描到达时，估计法线只需要点的范围信息。为了有效地估计点的分布，我们扩展了ikd树来管理体素中的映射，并在保持其与正常估计的一致性的同时逐步更新每个体素中点的分布。我们在收敛后进一步固定分布，以平衡时间消耗和表示的正确性。基于提取和维护的局部几何信息，我们设计了一种稳健而准确的分层数据关联方案，其中点到表面的关联优先于点到平面。在不同的公共数据集上进行的大量实验表明，与其他最先进的方法相比，我们的系统具有优势。我们的开源实现可在https://github.com/tiev-tongji/LOG-LIO.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09531v3" target="_blank">2307.09531v3</a>
                              </td>
                              <td>LOG-LIO: A LiDAR-Inertial Odometry with Efficient Local Geometric Information Estimation</td>
                              <td>Kai Huang</td>
                              <td>2023-07-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09531v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09531v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_06765v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Efficient Graduated Non-Convexity for Pose Graph Optimization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_06765v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_06765v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_06765v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a novel approach to Graduated Non-Convexity (GNC) and demonstrate its efficacy through its application in robust pose graph optimization, a key component in SLAM backends. Traditional GNC methods often rely on heuristic methods for GNC schedule, updating control parameter {\mu} for escalating the non-convexity. In contrast, our approach leverages the properties of convex functions and convex optimization to identify the boundary points beyond which convexity is no longer guaranteed, thereby eliminating redundant optimization steps in existing methodologies and enhancing both speed and robustness. We show that our method outperforms the state-of-the-art method in terms of speed and accuracy when used for robust back-end pose graph optimization via GNC. Our work builds upon and enhances the open-source riSAM framework. Our implementation can be accessed from: https://github.com/SNU-DLLAB/EGNC-PGO</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_06765v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的分级非凸性（GNC）方法，并通过将其应用于SLAM后端的关键组件——鲁棒姿态图优化中来证明其有效性。传统的GNC方法通常依赖于GNC调度的启发式方法，更新控制参数以升级非凸性。相反，我们的方法利用凸函数和凸优化的性质来识别超出其凸性不再得到保证的边界点，从而消除了现有方法中的冗余优化步骤，并提高了速度和鲁棒性。我们表明，当通过GNC用于稳健的后端姿态图优化时，我们的方法在速度和精度方面优于最先进的方法。我们的工作建立在开源riSAM框架的基础上并对其进行了增强。我们的实施可以访问：https://github.com/SNU-DLLAB/EGNC-PGO</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.06765v1" target="_blank">2310.06765v1</a>
                              </td>
                              <td>Efficient Graduated Non-Convexity for Pose Graph Optimization</td>
                              <td>Wonseok Kang</td>
                              <td>2023-10-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_06765v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.06765v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_06385v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3DS-SLAM: A 3D Object Detection based Semantic SLAM towards Dynamic Indoor Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_06385v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_06385v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_06385v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The existence of variable factors within the environment can cause a decline in camera localization accuracy, as it violates the fundamental assumption of a static environment in Simultaneous Localization and Mapping (SLAM) algorithms. Recent semantic SLAM systems towards dynamic environments either rely solely on 2D semantic information, or solely on geometric information, or combine their results in a loosely integrated manner. In this research paper, we introduce 3DS-SLAM, 3D Semantic SLAM, tailored for dynamic scenes with visual 3D object detection. The 3DS-SLAM is a tightly-coupled algorithm resolving both semantic and geometric constraints sequentially. We designed a 3D part-aware hybrid transformer for point cloud-based object detection to identify dynamic objects. Subsequently, we propose a dynamic feature filter based on HDBSCAN clustering to extract objects with significant absolute depth differences. When compared against ORB-SLAM2, 3DS-SLAM exhibits an average improvement of 98.01% across the dynamic sequences of the TUM RGB-D dataset. Furthermore, it surpasses the performance of the other four leading SLAM systems designed for dynamic environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_06385v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>环境中可变因素的存在可能会导致相机定位精度的下降，因为这违反了同步定位和映射（SLAM）算法中静态环境的基本假设。最近针对动态环境的语义SLAM系统要么仅依赖于2D语义信息，要么仅依赖几何信息，或者以松散集成的方式组合它们的结果。在这篇研究论文中，我们介绍了3DS-SLAM，3D语义SLAM，它是为具有视觉3D对象检测的动态场景量身定制的。3DS-SLAM是一种严格耦合的算法，可以顺序地解决语义和几何约束。我们设计了一个3D零件感知混合转换器，用于基于点云的对象检测，以识别动态对象。随后，我们提出了一种基于HDBSCAN聚类的动态特征滤波器来提取具有显著绝对深度差异的对象。与ORB-SLAM2相比，3DS-SLAM在TUM RGB-D数据集的动态序列中表现出98.01%的平均改进。此外，它的性能超过了为动态环境设计的其他四个领先的SLAM系统。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.06385v1" target="_blank">2310.06385v1</a>
                              </td>
                              <td>3DS-SLAM: A 3D Object Detection based Semantic SLAM towards Dynamic Indoor Environments</td>
                              <td>Ghanta Sai Krishna</td>
                              <td>2023-10-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_06385v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.06385v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_05249v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Evaluating Visual Odometry Methods for Autonomous Driving in Rain</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_05249v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_05249v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_05249v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The increasing demand for autonomous vehicles has created a need for robust navigation systems that can also operate effectively in adverse weather conditions. Visual odometry is a technique used in these navigation systems, enabling the estimation of vehicle position and motion using input from onboard cameras. However, visual odometry accuracy can be significantly impacted in challenging weather conditions, such as heavy rain, snow, or fog. In this paper, we evaluate a range of visual odometry methods, including our DROID-SLAM based heuristic approach. Specifically, these algorithms are tested on both clear and rainy weather urban driving data to evaluate their robustness. We compiled a dataset comprising of a range of rainy weather conditions from different cities. This includes, the Oxford Robotcar dataset from Oxford, the 4Seasons dataset from Munich and an internal dataset collected in Singapore. We evaluated different visual odometry algorithms for both monocular and stereo camera setups using the Absolute Trajectory Error (ATE). From the range of approaches evaluated, our findings suggest that the Depth and Flow for Visual Odometry (DF-VO) algorithm with monocular setup performed the best for short range distances (< 500m) and our proposed DROID-SLAM based heuristic approach for the stereo setup performed relatively well for long-term localization. Both VO algorithms suggested a need for a more robust sensor fusion based approach for localization in rain.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_05249v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对自动驾驶汽车日益增长的需求产生了对鲁棒导航系统的需求，该系统也可以在恶劣天气条件下有效运行。视觉里程计是这些导航系统中使用的一种技术，能够使用车载摄像头的输入来估计车辆的位置和运动。然而，在大雨、雪或雾等具有挑战性的天气条件下，视觉里程计的准确性可能会受到显著影响。在本文中，我们评估了一系列视觉里程计方法，包括我们基于DROID-SLAM的启发式方法。具体来说，这些算法在晴朗和雨天的城市驾驶数据上进行了测试，以评估其稳健性。我们汇编了一个数据集，其中包括来自不同城市的一系列降雨天气条件。这包括来自牛津的Oxford Robotcar数据集、来自慕尼黑的4Seasons数据集和在新加坡收集的内部数据集。我们使用绝对轨迹误差（ATE）评估了单眼和立体相机设置的不同视觉里程计算法。从评估的方法范围来看，我们的研究结果表明，具有单目设置的视觉Odometry的深度和流量（DF-VO）算法在短距离（<500m）中表现最好，并且我们提出的基于DROID-SLAM的立体设置启发式方法在长期定位中表现相对较好。两种VO算法都表明，需要一种更稳健的基于传感器融合的方法来进行降雨定位。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.05249v2" target="_blank">2309.05249v2</a>
                              </td>
                              <td>Evaluating Visual Odometry Methods for Autonomous Driving in Rain</td>
                              <td>Yu Xiang Tan</td>
                              <td>2023-09-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_05249v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.05249v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_06249v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">l-dyno: framework to learn consistent visual features using robot's motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_06249v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_06249v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_06249v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Historically, feature-based approaches have been used extensively for camera-based robot perception tasks such as localization, mapping, tracking, and others. Several of these approaches also combine other sensors (inertial sensing, for example) to perform combined state estimation. Our work rethinks this approach; we present a representation learning mechanism that identifies visual features that best correspond to robot motion as estimated by an external signal. Specifically, we utilize the robot's transformations through an external signal (inertial sensing, for example) and give attention to image space that is most consistent with the external signal. We use a pairwise consistency metric as a representation to keep the visual features consistent through a sequence with the robot's relative pose transformations. This approach enables us to incorporate information from the robot's perspective instead of solely relying on the image attributes. We evaluate our approach on real-world datasets such as KITTI & EuRoC and compare the refined features with existing feature descriptors. We also evaluate our method using our real robot experiment. We notice an average of 49% reduction in the image search space without compromising the trajectory estimation accuracy. Our method reduces the execution time of visual odometry by 4.3% and also reduces reprojection errors. We demonstrate the need to select only the most important features and show the competitiveness using various feature detection baselines.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_06249v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从历史上看，基于特征的方法已被广泛用于基于相机的机器人感知任务，如定位、映射、跟踪等。这些方法中的一些还结合了其他传感器（例如惯性传感）来执行组合状态估计。我们的工作重新思考了这种方法；我们提出了一种表示学习机制，该机制识别最符合由外部信号估计的机器人运动的视觉特征。具体来说，我们通过外部信号（例如惯性传感）利用机器人的变换，并关注与外部信号最一致的图像空间。我们使用成对一致性度量作为表示，以通过与机器人的相对姿势变换的序列来保持视觉特征的一致性。这种方法使我们能够从机器人的角度整合信息，而不是仅仅依赖于图像属性。我们在KITTI和EuRoC等真实世界的数据集上评估了我们的方法，并将改进后的特征与现有的特征描述符进行了比较。我们还使用真实的机器人实验来评估我们的方法。我们注意到，在不影响轨迹估计精度的情况下，图像搜索空间平均减少了49%。我们的方法将视觉里程计的执行时间减少了4.3%，还减少了重投影误差。我们证明了只选择最重要特征的必要性，并使用各种特征检测基线来显示竞争力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.06249v1" target="_blank">2310.06249v1</a>
                              </td>
                              <td>l-dyno: framework to learn consistent visual features using robot's motion</td>
                              <td>Kartikeya Singh</td>
                              <td>2023-10-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_06249v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.06249v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_06160v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Entropy Based Multi-robot Active SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_06160v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_06160v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_06160v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this article, we present an efficient multi-robot active SLAM framework that involves a frontier-sharing method for maximum exploration of an unknown environment. It encourages the robots to spread into the environment while weighting the goal frontiers with the pose graph SLAM uncertainly and path entropy. Our approach works on a limited number of frontier points and weights the goal frontiers with a utility function that encapsulates both the SLAM and map uncertainties, thus providing an efficient and not computationally expensive solution. Our approach has been tested on publicly available simulation environments and on real robots. An accumulative 31% more coverage than similar state-of-the-art approaches has been obtained, proving the capability of our approach for efficient environment exploration.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_06160v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了一种高效的多机器人主动SLAM框架，该框架涉及一种最大限度地探索未知环境的边界共享方法。它鼓励机器人扩散到环境中，同时用姿态图SLAM不确定性和路径熵对目标边界进行加权。我们的方法在有限数量的边界点上工作，并使用封装SLAM和地图不确定性的效用函数对目标边界进行加权，从而提供了一种高效且计算成本不高的解决方案。我们的方法已经在公开的模拟环境和真实的机器人上进行了测试。已经获得了比类似的最先进方法多31%的累积覆盖率，证明了我们的方法在有效的环境勘探方面的能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.06160v1" target="_blank">2310.06160v1</a>
                              </td>
                              <td>Entropy Based Multi-robot Active SLAM</td>
                              <td>Muhammad Farhan Ahmed</td>
                              <td>2023-10-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_06160v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.06160v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_05766v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FeatSense -- A Feature-based Registration Algorithm with GPU-accelerated TSDF-Mapping Backend for NVIDIA Jetson Boards</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_05766v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_05766v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_05766v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents FeatSense, a feature-based GPU-accelerated SLAM system for high resolution LiDARs, combined with a map generation algorithm for real-time generation of large Truncated Signed Distance Fields (TSDFs) on embedded hardware. FeatSense uses LiDAR point cloud features for odometry estimation and point cloud registration. The registered point clouds are integrated into a global Truncated Signed Distance Field (TSDF) representation. FeatSense is intended to run on embedded systems with integrated GPU-accelerator like NVIDIA Jetson boards. In this paper, we present a real-time capable TSDF-SLAM system specially tailored for close coupled CPU/GPU systems. The implementation is evaluated in various structured and unstructured environments and benchmarked against existing reference datasets. The main contribution of this paper is the ability to register up to 128 scan lines of an Ouster OS1-128 LiDAR at 10Hz on a NVIDIA AGX Xavier while achieving a TSDF map generation speedup by a factor of 100 compared to previous work on the same power budget.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_05766v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种用于高分辨率激光雷达的基于特征的GPU加速SLAM系统FeathSense，并结合了一种在嵌入式硬件上实时生成大型截断符号距离场（TSDF）的地图生成算法。FeathSense使用激光雷达点云特征进行里程计估计和点云配准。注册的点云被集成到全局截断有符号距离场（TSDF）表示中。FeathSense旨在运行在具有集成GPU加速器的嵌入式系统上，如NVIDIA Jetson板。在本文中，我们提出了一个实时TSDF-SLAM系统，专门为紧密耦合的CPU/GPU系统量身定制。该实现在各种结构化和非结构化环境中进行了评估，并根据现有的参考数据集进行了基准测试。本文的主要贡献是能够在NVIDIA AGX Xavier上以10Hz的频率注册Ouster OS1-128 LiDAR的多达128条扫描线，同时与以前在相同功率预算下的工作相比，TSDF地图生成速度提高了100倍。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.05766v1" target="_blank">2310.05766v1</a>
                              </td>
                              <td>FeatSense -- A Feature-based Registration Algorithm with GPU-accelerated TSDF-Mapping Backend for NVIDIA Jetson Boards</td>
                              <td>Julian Gaal</td>
                              <td>2023-10-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_05766v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.05766v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_05600v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Care3D: An Active 3D Object Detection Dataset of Real Robotic-Care Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_05600v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_05600v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_05600v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>As labor shortage increases in the health sector, the demand for assistive robotics grows. However, the needed test data to develop those robots is scarce, especially for the application of active 3D object detection, where no real data exists at all. This short paper counters this by introducing such an annotated dataset of real environments. The captured environments represent areas which are already in use in the field of robotic health care research. We further provide ground truth data within one room, for assessing SLAM algorithms running directly on a health care robot.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_05600v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着卫生部门劳动力短缺的加剧，对辅助机器人的需求也在增长。然而，开发这些机器人所需的测试数据很少，尤其是在完全不存在真实数据的主动三维物体检测应用中。这篇简短的论文通过引入这样一个真实环境的注释数据集来反驳这一点。捕捉到的环境代表了机器人医疗保健研究领域中已经在使用的领域。我们进一步在一个房间内提供地面实况数据，用于评估直接在医疗机器人上运行的SLAM算法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.05600v1" target="_blank">2310.05600v1</a>
                              </td>
                              <td>Care3D: An Active 3D Object Detection Dataset of Real Robotic-Care Environments</td>
                              <td>Michael G. Adam</td>
                              <td>2023-10-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_05600v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.05600v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_16772v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">XVO: Generalized Visual Odometry via Cross-Modal Self-Training</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_16772v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_16772v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_16772v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose XVO, a semi-supervised learning method for training generalized monocular Visual Odometry (VO) models with robust off-the-self operation across diverse datasets and settings. In contrast to standard monocular VO approaches which often study a known calibration within a single dataset, XVO efficiently learns to recover relative pose with real-world scale from visual scene semantics, i.e., without relying on any known camera parameters. We optimize the motion estimation model via self-training from large amounts of unconstrained and heterogeneous dash camera videos available on YouTube. Our key contribution is twofold. First, we empirically demonstrate the benefits of semi-supervised training for learning a general-purpose direct VO regression network. Second, we demonstrate multi-modal supervision, including segmentation, flow, depth, and audio auxiliary prediction tasks, to facilitate generalized representations for the VO task. Specifically, we find audio prediction task to significantly enhance the semi-supervised learning process while alleviating noisy pseudo-labels, particularly in highly dynamic and out-of-domain video data. Our proposed teacher network achieves state-of-the-art performance on the commonly used KITTI benchmark despite no multi-frame optimization or knowledge of camera parameters. Combined with the proposed semi-supervised step, XVO demonstrates off-the-shelf knowledge transfer across diverse conditions on KITTI, nuScenes, and Argoverse without fine-tuning.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_16772v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了XVO，这是一种半监督学习方法，用于在不同的数据集和设置中训练具有鲁棒非自操作的广义单目视觉Odometry（VO）模型。与通常在单个数据集中研究已知校准的标准单目VO方法相比，XVO有效地学习从视觉场景语义中恢复具有真实世界尺度的相对姿态，即不依赖于任何已知的相机参数。我们通过从YouTube上提供的大量无约束和异构的行车记录仪视频中进行自我训练来优化运动估计模型。我们的主要贡献是双重的。首先，我们实证证明了半监督训练对学习通用直接VO回归网络的好处。其次，我们演示了多模态监督，包括分割、流、深度和音频辅助预测任务，以便于VO任务的广义表示。具体来说，我们发现音频预测任务可以显著增强半监督学习过程，同时减轻噪声伪标签，特别是在高度动态和域外视频数据中。尽管没有多帧优化或相机参数知识，但我们提出的教师网络在常用的KITTI基准上实现了最先进的性能。结合所提出的半监督步骤，XVO展示了在KITTI、nuScenes和Argovere上跨不同条件的现成知识转移，而无需微调。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.16772v3" target="_blank">2309.16772v3</a>
                              </td>
                              <td>XVO: Generalized Visual Odometry via Cross-Modal Self-Training</td>
                              <td>Lei Lai</td>
                              <td>2023-09-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_16772v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.16772v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_04802v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Hierarchical Unsupervised Topological SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_04802v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_04802v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_04802v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we present a novel framework for unsupervised topological clustering resulting in improved loop. In this paper we present a novel framework for unsupervised topological clustering resulting in improved loop detection and closure for SLAM. A navigating mobile robot clusters its traversal into visually similar topologies where each cluster (topology) contains a set of similar looking images typically observed from spatially adjacent locations. Each such set of spatially adjacent and visually similar grouping of images constitutes a topology obtained without any supervision. We formulate a hierarchical loop discovery strategy that first detects loops at the level of topologies and subsequently at the level of images between the looped topologies. We show over a number of traversals across different Habitat environments that such a hierarchical pipeline significantly improves SOTA image based loop detection and closure methods. Further, as a consequence of improved loop detection, we enhance the loop closure and backend SLAM performance. Such a rendering of a traversal into topological segments is beneficial for downstream tasks such as navigation that can now build a topological graph where spatially adjacent topological clusters are connected by an edge and navigate over such topological graphs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_04802v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了一种新的无监督拓扑聚类框架，从而改进了循环。在本文中，我们提出了一种新的无监督拓扑聚类框架，从而改进了SLAM的环路检测和闭合。导航移动机器人将其遍历聚类为视觉上相似的拓扑，其中每个聚类（拓扑）包含一组通常从空间相邻位置观察到的相似图像。每一组这样的空间上相邻且视觉上相似的图像分组构成了在没有任何监督的情况下获得的拓扑。我们制定了一种分层循环发现策略，该策略首先在拓扑级别检测循环，然后在循环拓扑之间的图像级别检测循环。我们在不同生境环境中的多次遍历中表明，这种分层管道显著改进了基于SOTA图像的环路检测和闭合方法。此外，由于改进了循环检测，我们增强了循环闭合和后端SLAM性能。将遍历呈现为拓扑段对于诸如导航之类的下游任务是有益的，导航现在可以构建拓扑图，其中空间相邻的拓扑簇通过边连接，并在这样的拓扑图上导航。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.04802v1" target="_blank">2310.04802v1</a>
                              </td>
                              <td>Hierarchical Unsupervised Topological SLAM</td>
                              <td>Ayush Sharma</td>
                              <td>2023-10-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_04802v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.04802v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_04787v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">HI-SLAM: Monocular Real-time Dense Mapping with Hybrid Implicit Fields</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_04787v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_04787v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_04787v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this letter, we present a neural field-based real-time monocular mapping framework for accurate and dense Simultaneous Localization and Mapping (SLAM). Recent neural mapping frameworks show promising results, but rely on RGB-D or pose inputs, or cannot run in real-time. To address these limitations, our approach integrates dense-SLAM with neural implicit fields. Specifically, our dense SLAM approach runs parallel tracking and global optimization, while a neural field-based map is constructed incrementally based on the latest SLAM estimates. For the efficient construction of neural fields, we employ multi-resolution grid encoding and signed distance function (SDF) representation. This allows us to keep the map always up-to-date and adapt instantly to global updates via loop closing. For global consistency, we propose an efficient Sim(3)-based pose graph bundle adjustment (PGBA) approach to run online loop closing and mitigate the pose and scale drift. To enhance depth accuracy further, we incorporate learned monocular depth priors. We propose a novel joint depth and scale adjustment (JDSA) module to solve the scale ambiguity inherent in depth priors. Extensive evaluations across synthetic and real-world datasets validate that our approach outperforms existing methods in accuracy and map completeness while preserving real-time performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_04787v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这封信中，我们提出了一个基于神经场的实时单目映射框架，用于精确和密集的同时定位和映射（SLAM）。最近的神经映射框架显示出有希望的结果，但依赖于RGB-D或姿势输入，或者无法实时运行。为了解决这些局限性，我们的方法将密集SLAM与神经隐式场相结合。具体来说，我们的密集SLAM方法运行并行跟踪和全局优化，而基于神经场的映射是基于最新的SLAM估计逐步构建的。为了有效地构造神经场，我们采用了多分辨率网格编码和符号距离函数（SDF）表示。这使我们能够始终保持地图的最新状态，并通过循环关闭立即适应全球更新。为了全局一致性，我们提出了一种有效的基于Sim（3）的姿态图束调整（PGBA）方法来运行在线闭环并减轻姿态和尺度漂移。为了进一步提高深度精度，我们结合了学习的单目深度先验。我们提出了一种新的深度和尺度联合调整（JDSA）模块来解决深度先验中固有的尺度模糊性。对合成和真实世界数据集的广泛评估验证了我们的方法在准确性和地图完整性方面优于现有方法，同时保持了实时性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.04787v1" target="_blank">2310.04787v1</a>
                              </td>
                              <td>HI-SLAM: Monocular Real-time Dense Mapping with Hybrid Implicit Fields</td>
                              <td>Wei Zhang</td>
                              <td>2023-10-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_04787v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.04787v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_04162v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Light-LOAM: A Lightweight LiDAR Odometry and Mapping based on Graph-Matching</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_04162v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_04162v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_04162v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous Localization and Mapping (SLAM) plays an important role in robot autonomy. Reliability and efficiency are the two most valued features for applying SLAM in robot applications. In this paper, we consider achieving a reliable LiDAR-based SLAM function in computation-limited platforms, such as quadrotor UAVs based on graph-based point cloud association. First, contrary to most works selecting salient features for point cloud registration, we propose a non-conspicuous feature selection strategy for reliability and robustness purposes. Then a two-stage correspondence selection method is used to register the point cloud, which includes a KD-tree-based coarse matching followed by a graph-based matching method that uses geometric consistency to vote out incorrect correspondences. Additionally, we propose an odometry approach where the weight optimizations are guided by vote results from the aforementioned geometric consistency graph. In this way, the optimization of LiDAR odometry rapidly converges and evaluates a fairly accurate transformation resulting in the back-end module efficiently finishing the mapping task. Finally, we evaluate our proposed framework on the KITTI odometry dataset and real-world environments. Experiments show that our SLAM system achieves a comparative level or higher level of accuracy with more balanced computation efficiency compared with the mainstream LiDAR-based SLAM solutions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_04162v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同步定位与映射（SLAM）在机器人自主中发挥着重要作用。可靠性和效率是SLAM在机器人应用中最有价值的两个特征。在本文中，我们考虑在计算有限的平台上实现可靠的基于激光雷达的SLAM功能，例如基于图的点云关联的四旋翼无人机。首先，与大多数为点云配准选择显著特征的工作相反，出于可靠性和鲁棒性的目的，我们提出了一种非显著特征选择策略。然后使用两阶段对应关系选择方法来注册点云，该方法包括基于KD树的粗略匹配，然后是基于图的匹配方法，该方法使用几何一致性来投票排除不正确的对应关系。此外，我们提出了一种里程计方法，其中权重优化由上述几何一致性图的投票结果指导。通过这种方式，激光雷达里程计的优化快速收敛并评估相当准确的变换，从而使后端模块有效地完成映射任务。最后，我们在KITTI里程计数据集和真实世界环境中评估了我们提出的框架。实验表明，与主流的基于激光雷达的SLAM解决方案相比，我们的SLAM系统以更平衡的计算效率实现了相当水平或更高水平的精度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.04162v1" target="_blank">2310.04162v1</a>
                              </td>
                              <td>Light-LOAM: A Lightweight LiDAR Odometry and Mapping based on Graph-Matching</td>
                              <td>Shiquan Yi</td>
                              <td>2023-10-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_04162v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.04162v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_04111v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Dense Random Texture Detection using Beta Distribution Statistics</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_04111v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_04111v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_04111v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This note describes a method for detecting dense random texture using fully connected points sampled on image edges. An edge image is randomly sampled with points, the standard L2 distance is calculated between all connected points in a neighbourhood. For each point, a check is made if the point intersects with an image edge. If this is the case, a unity value is added to the distance, otherwise zero. From this an edge excess index is calculated for the fully connected edge graph in the range [1.0..2.0], where 1.0 indicate no edges. The ratio can be interpreted as a sampled Bernoulli process with unknown probability. The Bayesian posterior estimate of the probability can be associated with its conjugate prior which is a Beta($\alpha$, $\beta$) distribution, with hyper parameters $\alpha$ and $\beta$ related to the number of edge crossings. Low values of $\beta$ indicate a texture rich area, higher values less rich. The method has been applied to real-time SLAM-based moving object detection, where points are confined to tracked boxes (rois).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_04111v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文描述了一种使用在图像边缘采样的全连通点来检测密集随机纹理的方法。边缘图像用点随机采样，计算邻域中所有连接点之间的标准L2距离。对于每个点，都会检查该点是否与图像边缘相交。如果是这种情况，则会向距离添加一个单位值，否则为零。由此计算出在[1.0..2.0]范围内的全连通边图的边过剩指数，其中1.0表示没有边。该比率可以解释为具有未知概率的采样伯努利过程。概率的贝叶斯后验估计可以与其共轭先验相关联，共轭先验是Beta（$\alpha$，$\Beta$）分布，超参数$\alpha$和$\Beta$与边缘交叉的数量有关。$\beta$的值越低表示纹理丰富的区域，值越高表示不太丰富。该方法已应用于基于SLAM的实时运动目标检测，其中点被限制在跟踪框（roi）中。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.04111v1" target="_blank">2310.04111v1</a>
                              </td>
                              <td>Dense Random Texture Detection using Beta Distribution Statistics</td>
                              <td>Soeren Molander</td>
                              <td>2023-10-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_04111v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.04111v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_00406v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LIMOT: A Tightly-Coupled System for LiDAR-Inertial Odometry and Multi-Object Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_00406v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_00406v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_00406v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localization and mapping (SLAM) is critical to the implementation of autonomous driving. Most LiDAR-inertial SLAM algorithms assume a static environment, leading to unreliable localization in dynamic environments. Moreover, the accurate tracking of moving objects is of great significance for the control and planning of autonomous vehicles. This study proposes LIMOT, a tightly-coupled multi-object tracking and LiDAR-inertial odometry system that is capable of accurately estimating the poses of both ego-vehicle and objects. We propose a trajectory-based dynamic feature filtering method, which filters out features belonging to moving objects by leveraging tracking results before scan-matching. Factor graph-based optimization is then conducted to optimize the bias of the IMU and the poses of both the ego-vehicle and surrounding objects in a sliding window. Experiments conducted on the KITTI tracking dataset and self-collected dataset show that our method achieves better pose and tracking accuracy than our previous work DL-SLOT and other baseline methods. Our open-source implementation is available at https://github.com/tiev-tongji/LIMOT.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_00406v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和映射（SLAM）对自动驾驶的实现至关重要。大多数激光雷达惯性SLAM算法假设静态环境，导致在动态环境中定位不可靠。此外，运动物体的精确跟踪对自动驾驶汽车的控制和规划具有重要意义。本研究提出了LIMOT，这是一种紧密耦合的多目标跟踪和激光雷达惯性里程计系统，能够准确估计自我车辆和物体的姿态。我们提出了一种基于轨迹的动态特征滤波方法，该方法通过在扫描匹配之前利用跟踪结果来滤除属于运动对象的特征。然后进行基于因子图的优化，以优化IMU的偏置以及自我车辆和周围物体在滑动窗口中的姿态。在KITTI跟踪数据集和自收集数据集上进行的实验表明，与我们之前的工作DL-SLOT和其他基线方法相比，我们的方法实现了更好的姿态和跟踪精度。我们的开源实现可在https://github.com/tiev-tongji/LIMOT.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.00406v2" target="_blank">2305.00406v2</a>
                              </td>
                              <td>LIMOT: A Tightly-Coupled System for LiDAR-Inertial Odometry and Multi-Object Tracking</td>
                              <td>Zhongyang Zhu</td>
                              <td>2023-04-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_00406v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.00406v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_02650v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Active Visual Localization for Multi-Agent Collaboration: A Data-Driven Approach</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_02650v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_02650v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_02650v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Rather than having each newly deployed robot create its own map of its surroundings, the growing availability of SLAM-enabled devices provides the option of simply localizing in a map of another robot or device. In cases such as multi-robot or human-robot collaboration, localizing all agents in the same map is even necessary. However, localizing e.g. a ground robot in the map of a drone or head-mounted MR headset presents unique challenges due to viewpoint changes. This work investigates how active visual localization can be used to overcome such challenges of viewpoint changes. Specifically, we focus on the problem of selecting the optimal viewpoint at a given location. We compare existing approaches in the literature with additional proposed baselines and propose a novel data-driven approach. The result demonstrates the superior performance of the data-driven approach when compared to existing methods, both in controlled simulation experiments and real-world deployment.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_02650v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>与让每个新部署的机器人创建自己的周围环境地图不同，SLAM设备的可用性不断增加，提供了在另一个机器人或设备的地图中简单定位的选项。在多机器人或人机协作等情况下，甚至有必要将所有代理定位在同一地图中。然而，由于视点的变化，在无人机或头戴式MR耳机的地图中定位例如地面机器人会带来独特的挑战。这项工作研究了如何使用主动视觉定位来克服视点变化的挑战。具体来说，我们关注在给定位置选择最佳视点的问题。我们将文献中现有的方法与额外提出的基线进行了比较，并提出了一种新的数据驱动方法。结果表明，与现有方法相比，无论是在受控模拟实验还是在现实世界部署中，数据驱动方法都具有优越的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.02650v1" target="_blank">2310.02650v1</a>
                              </td>
                              <td>Active Visual Localization for Multi-Agent Collaboration: A Data-Driven Approach</td>
                              <td>Matthew Hanlon</td>
                              <td>2023-10-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_02650v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.02650v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2209_01774v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ElasticROS: An Elastically Collaborative Robot Operation System for Fog and Cloud Robotics</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2209_01774v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2209_01774v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2209_01774v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Robots are integrating more huge-size models to enrich functions and improve accuracy, which leads to out-of-control computing pressure. And thus robots are encountering bottlenecks in computing power and battery capacity. Fog or cloud robotics is one of the most anticipated theories to address these issues. Approaches of cloud robotics have developed from system-level to node-level. However, the present node-level systems are not flexible enough to dynamically adapt to changing conditions. To address this, we present ElasticROS, which evolves the present node-level systems into an algorithm-level one. ElasticROS is based on ROS and ROS2. For fog and cloud robotics, it is the first robot operating system with algorithm-level collaborative computing. ElasticROS develops elastic collaborative computing to achieve adaptability to dynamic conditions. The collaborative computing algorithm is the core and challenge of ElasticROS. We abstract the problem and then propose an algorithm named ElasAction to address. It is a dynamic action decision algorithm based on online learning, which determines how robots and servers cooperate. The algorithm dynamically updates parameters to adapt to changes of conditions where the robot is currently in. It achieves elastically distributing of computing tasks to robots and servers according to configurations. In addition, we prove that the regret upper bound of the ElasAction is sublinear, which guarantees its convergence and thus enables ElasticROS to be stable in its elasticity. Finally, we conducted experiments with ElasticROS on common tasks of robotics, including SLAM, grasping and human-robot dialogue, and then measured its performances in latency, CPU usage and power consumption. The algorithm-level ElasticROS performs significantly better than the present node-level system.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2209_01774v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>机器人正在集成更大尺寸的模型，以丰富功能并提高精度，这导致了失控的计算压力。因此，机器人在计算能力和电池容量方面遇到了瓶颈。雾或云机器人是解决这些问题最受期待的理论之一。云机器人的方法已经从系统级发展到节点级。然而，目前的节点级系统不够灵活，不能动态地适应不断变化的条件。为了解决这个问题，我们提出了ElasticROS，它将现有的节点级系统进化为算法级系统。弹性ROS是基于ROS和ROS2。对于雾和云机器人来说，它是第一个具有算法级协同计算的机器人操作系统。ElasticROS开发了弹性协作计算，以实现对动态条件的适应性。协同计算算法是ElasticROS的核心和挑战。我们抽象了这个问题，然后提出了一个名为ElasAction的算法来解决。它是一种基于在线学习的动态行动决策算法，决定机器人和服务器如何合作。该算法动态更新参数以适应机器人当前所处条件的变化，实现了计算任务根据配置弹性分配给机器人和服务器。此外，我们证明了ElasAction的后悔上界是次线性的，这保证了它的收敛性，从而使ElasticROS在其弹性中是稳定的。最后，我们用ElasticROS对机器人的常见任务进行了实验，包括SLAM、抓取和人机对话，然后测量了它在延迟、CPU使用和功耗方面的性能。算法级ElasticROS的性能明显优于当前节点级系统。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2209.01774v2" target="_blank">2209.01774v2</a>
                              </td>
                              <td>ElasticROS: An Elastically Collaborative Robot Operation System for Fog and Cloud Robotics</td>
                              <td>Boyi Liu</td>
                              <td>2022-09-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2209_01774v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2209.01774v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_02392v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A 3D Mixed Reality Interface for Human-Robot Teaming</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_02392v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_02392v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_02392v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a mixed-reality human-robot teaming system. It allows human operators to see in real-time where robots are located, even if they are not in line of sight. The operator can also visualize the map that the robots create of their environment and can easily send robots to new goal positions. The system mainly consists of a mapping and a control module. The mapping module is a real-time multi-agent visual SLAM system that co-localizes all robots and mixed-reality devices to a common reference frame. Visualizations in the mixed-reality device then allow operators to see a virtual life-sized representation of the cumulative 3D map overlaid onto the real environment. As such, the operator can effectively "see through" walls into other rooms. To control robots and send them to new locations, we propose a drag-and-drop interface. An operator can grab any robot hologram in a 3D mini map and drag it to a new desired goal pose. We validate the proposed system through a user study and real-world deployments. We make the mixed-reality application publicly available at https://github.com/cvg/HoloLens_ros.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_02392v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一个混合现实的人机协同系统。它允许人类操作员实时查看机器人的位置，即使它们不在视线范围内。操作员还可以可视化机器人创建的环境地图，并可以轻松地将机器人发送到新的目标位置。该系统主要由一个映射模块和一个控制模块组成。映射模块是一个实时的多智能体视觉SLAM系统，它将所有机器人和混合现实设备共同定位到一个公共参考系中。混合现实设备中的可视化然后允许操作员看到叠加在真实环境上的累积3D地图的虚拟真人大小的表示。因此，操作员可以有效地“透过”墙壁进入其他房间。为了控制机器人并将其发送到新的位置，我们提出了一个拖放界面。操作员可以在3D迷你地图中抓取任何机器人全息图，并将其拖动到新的目标姿势。我们通过用户研究和实际部署验证了所提出的系统。我们在https://github.com/cvg/HoloLens_ros.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.02392v1" target="_blank">2310.02392v1</a>
                              </td>
                              <td>A 3D Mixed Reality Interface for Human-Robot Teaming</td>
                              <td>Jiaqi Chen</td>
                              <td>2023-10-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_02392v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.02392v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_04797v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Set-Type Belief Propagation with Applications to Poisson Multi-Bernoulli SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_04797v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_04797v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_04797v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Belief propagation (BP) is a useful probabilistic inference algorithm for efficiently computing approximate marginal probability densities of random variables. However, in its standard form, BP is only applicable to the vector-type random variables with a fixed and known number of vector elements, while certain applications rely on RFSs with an unknown number of vector elements. In this paper, we develop BP rules for factor graphs defined on sequences of RFSs where each RFS has an unknown number of elements, with the intention of deriving novel inference methods for RFSs. Furthermore, we show that vector-type BP is a special case of set-type BP, where each RFS follows the Bernoulli process. To demonstrate the validity of developed set-type BP, we apply it to the PMB filter for SLAM, which naturally leads to new set-type BP-mapping, SLAM, multi-target tracking, and simultaneous localization and tracking filters. Finally, we explore the relationships between the vector-type BP and the proposed set-type BP PMB-SLAM implementations and show a performance gain of the proposed set-type BP PMB-SLAM filter in comparison with the vector-type BP-SLAM filter.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_04797v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>置信传播（BP）是一种有效计算随机变量近似边际概率密度的概率推理算法。然而，在其标准形式中，BP仅适用于具有固定且已知数量的向量元素的向量型随机变量，而某些应用依赖于具有未知数量向量元素的RFS。在本文中，我们为定义在RFS序列上的因子图开发了BP规则，其中每个RFS具有未知数量的元素，目的是推导RFS的新推理方法。此外，我们证明了向量型BP是集合型BP的特例，其中每个RFS遵循伯努利过程。为了证明所开发的集合型BP的有效性，我们将其应用于SLAM的PMB滤波器，这自然导致了新的集合型BP映射、SLAM、多目标跟踪以及同时定位和跟踪滤波器。最后，我们探索了向量型BP和所提出的集合型BP PMB-SLAM实现之间的关系，并与向量型BP-SLAM滤波器相比，展示了所提出的集型BP PMB-SLAM滤波器的性能增益。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.04797v2" target="_blank">2305.04797v2</a>
                              </td>
                              <td>Set-Type Belief Propagation with Applications to Poisson Multi-Bernoulli SLAM</td>
                              <td>Hyowon Kim</td>
                              <td>2023-05-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_04797v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.04797v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_01967v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Collaborative Active SLAM: Synchronous and Asynchronous Coordination Among Agents</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_01967v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_01967v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_01967v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In the realm of autonomous robotics, a critical challenge lies in developing robust solutions for Active Collaborative SLAM, wherein multiple robots must collaboratively explore and map an unknown environment while intelligently coordinating their movements and sensor data acquisitions. To this aim, we present two approaches for coordinating a system consisting of multiple robots to perform Active Collaborative SLAM (AC-SLAM) for environmental exploration. Our two coordination approaches, synchronous and asynchronous implement a methodology to prioritize robot goal assignments by the central server. We also present a method to efficiently spread the robots for maximum exploration while keeping SLAM uncertainty low. Both coordination approaches were evaluated through simulation on publicly available datasets, obtaining promising results.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_01967v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在自主机器人领域，一个关键的挑战在于为主动协作SLAM开发强大的解决方案，其中多个机器人必须协同探索和绘制未知环境，同时智能地协调它们的运动和传感器数据采集。为此，我们提出了两种方法来协调由多个机器人组成的系统，以执行用于环境勘探的主动协作SLAM（AC-SLAM）。我们的两种协调方法，同步和异步，实现了一种由中央服务器对机器人目标分配进行优先级排序的方法。我们还提出了一种方法，在保持SLAM不确定性较低的同时，有效地分散机器人进行最大限度的探索。这两种协调方法都通过在公开数据集上进行模拟进行了评估，获得了有希望的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.01967v1" target="_blank">2310.01967v1</a>
                              </td>
                              <td>Collaborative Active SLAM: Synchronous and Asynchronous Coordination Among Agents</td>
                              <td>Matteo Maragliano</td>
                              <td>2023-10-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_01967v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.01967v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_01064v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-Sensor Terrestrial SLAM for Real-Time, Large-Scale, and GNSS-Interrupted Forest Mapping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_01064v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_01064v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_01064v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Forests, as critical components of our ecosystem, demand effective monitoring and management. However, conducting real-time forest inventory in large-scale and GNSS-interrupted forest environments has long been a formidable challenge. In this paper, we present a novel solution that leverages robotics and sensor-fusion technologies to overcome these challenges and enable real-time forest inventory with higher accuracy and efficiency. The proposed solution consists of a new SLAM algorithm to create an accurate 3D map of large-scale forest stands with detailed estimation about the number of trees and the corresponding DBH, solely with the consecutive scans of a 3D lidar and an imu. This method utilized a hierarchical unsupervised clustering algorithm to detect the trees and measure the DBH from the lidar point cloud. The algorithm can run simultaneously as the data is being recorded or afterwards on the recorded dataset. Furthermore, due to the proposed fast feature extraction and transform estimation modules, the recorded data can be fed to the SLAM with higher frequency than common SLAM algorithms. The performance of the proposed solution was tested through filed data collection with hand-held sensor platform as well as a mobile forestry robot. The accuracy of the results was also compared to the state-of-the-art SLAM solutions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_01064v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>森林作为我们生态系统的重要组成部分，需要有效的监测和管理。然而，长期以来，在大规模和全球导航卫星系统中断的森林环境中进行实时森林清查一直是一项艰巨的挑战。在本文中，我们提出了一种新的解决方案，该解决方案利用机器人和传感器融合技术来克服这些挑战，并实现更高精度和效率的实时森林清查。所提出的解决方案包括一种新的SLAM算法，仅通过3D激光雷达和imu的连续扫描，即可创建大规模林分的精确3D地图，并详细估计树木数量和相应的DBH。该方法利用分层无监督聚类算法从激光雷达点云中检测树木并测量DBH。该算法可以在记录数据的同时运行，也可以随后在记录的数据集上运行。此外，由于所提出的快速特征提取和变换估计模块，可以将记录的数据以比常见SLAM算法更高的频率馈送到SLAM。通过手持传感器平台和移动林业机器人的现场数据采集，对所提出的解决方案的性能进行了测试。还将结果的准确性与最先进的SLAM解决方案进行了比较。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.01064v1" target="_blank">2310.01064v1</a>
                              </td>
                              <td>Multi-Sensor Terrestrial SLAM for Real-Time, Large-Scale, and GNSS-Interrupted Forest Mapping</td>
                              <td>Weria Khaksar</td>
                              <td>2023-10-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_01064v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.01064v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_17036v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">UniQuadric: A SLAM Backend for Unknown Rigid Object 3D Tracking and Light-Weight Modeling</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_17036v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_17036v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_17036v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Tracking and modeling unknown rigid objects in the environment play a crucial role in autonomous unmanned systems and virtual-real interactive applications. However, many existing Simultaneous Localization, Mapping and Moving Object Tracking (SLAMMOT) methods focus solely on estimating specific object poses and lack estimation of object scales and are unable to effectively track unknown objects. In this paper, we propose a novel SLAM backend that unifies ego-motion tracking, rigid object motion tracking, and modeling within a joint optimization framework. In the perception part, we designed a pixel-level asynchronous object tracker (AOT) based on the Segment Anything Model (SAM) and DeAOT, enabling the tracker to effectively track target unknown objects guided by various predefined tasks and prompts. In the modeling part, we present a novel object-centric quadric parameterization to unify both static and dynamic object initialization and optimization. Subsequently, in the part of object state estimation, we propose a tightly coupled optimization model for object pose and scale estimation, incorporating hybrids constraints into a novel dual sliding window optimization framework for joint estimation. To our knowledge, we are the first to tightly couple object pose tracking with light-weight modeling of dynamic and static objects using quadric. We conduct qualitative and quantitative experiments on simulation datasets and real-world datasets, demonstrating the state-of-the-art robustness and accuracy in motion estimation and modeling. Our system showcases the potential application of object perception in complex dynamic scenes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_17036v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>环境中未知刚性物体的跟踪和建模在自主无人系统和虚拟现实交互应用中发挥着至关重要的作用。然而，许多现有的同步定位、映射和运动对象跟踪（SLAMMT）方法仅专注于估计特定的对象姿态，缺乏对对象尺度的估计，并且无法有效地跟踪未知对象。在本文中，我们提出了一种新的SLAM后端，它将自我运动跟踪、刚体运动跟踪和建模统一在一个联合优化框架内。在感知部分，我们设计了一个基于分段任意模型（SAM）和DeAOT的像素级异步对象跟踪器（AOT），使跟踪器能够在各种预定义任务和提示的引导下有效跟踪目标未知对象。在建模部分，我们提出了一种新的以对象为中心的二次参数化方法，以统一静态和动态对象的初始化和优化。随后，在物体状态估计部分，我们提出了一个用于物体姿态和尺度估计的紧耦合优化模型，将混合约束纳入一个新的双滑动窗口优化框架中，用于联合估计。据我们所知，我们是第一个将物体姿态跟踪与使用二次曲面的动态和静态物体的轻量级建模紧密结合起来的人。我们在模拟数据集和真实世界数据集上进行了定性和定量实验，证明了运动估计和建模的最先进的稳健性和准确性。我们的系统展示了物体感知在复杂动态场景中的潜在应用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.17036v2" target="_blank">2309.17036v2</a>
                              </td>
                              <td>UniQuadric: A SLAM Backend for Unknown Rigid Object 3D Tracking and Light-Weight Modeling</td>
                              <td>Linghao Yang</td>
                              <td>2023-09-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_17036v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.17036v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_00401v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Better Situational Graphs by Inferring High-level Semantic-Relational Concepts</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_00401v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_00401v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_00401v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent works on SLAM extend their pose graphs with higher-level semantic concepts exploiting relationships between them, to provide, not only a richer representation of the situation/environment but also to improve the accuracy of its estimation. Concretely, our previous work, Situational Graphs (S-Graphs), a pioneer in jointly leveraging semantic relationships in the factor optimization process, relies on semantic entities such as wall surfaces and rooms, whose relationship is mathematically defined. Nevertheless, excerpting these high-level concepts relying exclusively on the lower-level factor-graph remains a challenge and it is currently done with ad-hoc algorithms, which limits its capability to include new semantic-relational concepts. To overcome this limitation, in this work, we propose a Graph Neural Network (GNN) for learning high-level semantic-relational concepts that can be inferred from the low-level factor graph. We have demonstrated that we can infer room entities and their relationship to the mapped wall surfaces, more accurately and more computationally efficient than the baseline algorithm. Additionally, to demonstrate the versatility of our method, we provide a new semantic concept, i.e. wall, and its relationship with its wall surfaces. Our proposed method has been integrated into S-Graphs+, and it has been validated in both simulated and real datasets. A docker container with our software will be made available to the scientific community.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_00401v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近关于SLAM的工作用更高层次的语义概念扩展了他们的姿态图，利用它们之间的关系，不仅提供了对情况/环境的更丰富的表示，而且提高了其估计的准确性。具体来说，我们之前的工作，情境图（S-Graphs），在因子优化过程中联合利用语义关系的先驱，依赖于语义实体，如墙面和房间，其关系是数学定义的。然而，完全依赖于较低级别的因子图来提取这些高级概念仍然是一个挑战，并且目前是用特别算法来完成的，这限制了它包含新的语义关系概念的能力。为了克服这一限制，在这项工作中，我们提出了一种图神经网络（GNN），用于学习可以从低级因子图推断出的高级语义关系概念。我们已经证明，与基线算法相比，我们可以更准确、更高效地推断房间实体及其与映射墙表面的关系。此外，为了证明我们方法的多功能性，我们提供了一个新的语义概念，即墙及其与墙表面的关系。我们提出的方法已经集成到S-Graphs+中，并在模拟和真实数据集中得到了验证。一个装有我们软件的docker容器将提供给科学界。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.00401v1" target="_blank">2310.00401v1</a>
                              </td>
                              <td>Better Situational Graphs by Inferring High-level Semantic-Relational Concepts</td>
                              <td>Jose Andres Millan-Romera</td>
                              <td>2023-09-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_00401v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.00401v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_00242v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Walking = Traversable? : Traversability Prediction via Multiple Human Object Tracking under Occlusion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_00242v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_00242v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_00242v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The emerging ``Floor plan from human trails (PfH)" technique has great potential for improving indoor robot navigation by predicting the traversability of occluded floors. This study presents an innovative approach that replaces first-person-view sensors with a third-person-view monocular camera mounted on the observer robot. This approach can gather measurements from multiple humans, expanding its range of applications. The key idea is to use two types of trackers, SLAM and MOT, to monitor stationary objects and moving humans and assess their interactions. This method achieves stable predictions of traversability even in challenging visual scenarios, such as occlusions, nonlinear perspectives, depth uncertainty, and intersections involving multiple humans. Additionally, we extend map quality metrics to apply to traversability maps, facilitating future research. We validate our proposed method through fusion and comparison with established techniques.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_00242v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>新兴的“人行步道平面图”（PfH）“这项技术通过预测被遮挡地板的可穿越性，在改善室内机器人导航方面具有巨大潜力。这项研究提出了一种创新的方法，用安装在观察机器人上的第三人称单眼相机取代第一人称视角传感器。这种方法可以收集多人的测量结果，扩大其应用范围两种类型的跟踪器，SLAM和MOT，用于监测静止物体和移动的人类，并评估它们的相互作用。即使在具有挑战性的视觉场景中，如遮挡、非线性视角、深度不确定性和涉及多人的交叉点，该方法也能实现对可穿越性的稳定预测。此外，我们将地图质量指标扩展到可穿越性地图，为未来的研究提供便利。我们通过与现有技术的融合和比较来验证我们提出的方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.00242v1" target="_blank">2310.00242v1</a>
                              </td>
                              <td>Walking = Traversable? : Traversability Prediction via Multiple Human Object Tracking under Occlusion</td>
                              <td>Jonathan Tay Yu Liang</td>
                              <td>2023-09-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_00242v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.00242v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_16490v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Active SLAM Utility Function Exploiting Path Entropy</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_16490v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_16490v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_16490v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this article we present a utility function for Active SLAM (A-SLAM) which utilizes map entropy along with D-Optimality criterion metrices for weighting goal frontier candidates. We propose a utility function for frontier goal selection that exploits the occupancy grid map by utilizing the path entropy and favors unknown map locations for maximum area coverage while maintaining a low localization and mapping uncertainties. We quantify the efficiency of our method using various graph connectivity matrices and map efficiency indexes for an environment exploration task. Using simulation and experimental results against similar approaches we achieve an average of 32\% more coverage using publicly available data sets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_16490v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了主动SLAM（a-SLAM）的效用函数，该函数利用映射熵和D-最优性准则度量来加权目标前沿候选者。我们提出了一种用于边界目标选择的效用函数，该函数通过利用路径熵来利用占用网格图，并在保持低定位和映射不确定性的同时，支持未知地图位置以实现最大区域覆盖。我们使用环境探索任务的各种图连通性矩阵和地图效率指数来量化我们的方法的效率。通过对类似方法的模拟和实验结果，我们使用公开可用的数据集实现了平均32%的覆盖率。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.16490v1" target="_blank">2309.16490v1</a>
                              </td>
                              <td>Active SLAM Utility Function Exploiting Path Entropy</td>
                              <td>Muhammad Farhan Ahmed</td>
                              <td>2023-09-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_16490v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.16490v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_11654v4_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Active SLAM: A Review On Last Decade</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_11654v4_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_11654v4_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_11654v4_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This article presents a comprehensive review of the Active Simultaneous Localization and Mapping (A-SLAM) research conducted over the past decade. It explores the formulation, applications, and methodologies employed in A-SLAM, particularly in trajectory generation and control-action selection, drawing on concepts from Information Theory (IT) and the Theory of Optimal Experimental Design (TOED). This review includes both qualitative and quantitative analyses of various approaches, deployment scenarios, configurations, path-planning methods, and utility functions within A-SLAM research. Furthermore, this article introduces a novel analysis of Active Collaborative SLAM (AC-SLAM), focusing on collaborative aspects within SLAM systems. It includes a thorough examination of collaborative parameters and approaches, supported by both qualitative and statistical assessments. This study also identifies limitations in the existing literature and suggests potential avenues for future research. This survey serves as a valuable resource for researchers seeking insights into A-SLAM methods and techniques, offering a current overview of A-SLAM formulation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_11654v4_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文对过去十年中进行的主动同步定位和映射（a-SLAM）研究进行了全面回顾。它借鉴了信息论（It）和最优实验设计理论（TOED）的概念，探讨了A-SLAM中使用的公式、应用和方法，特别是在轨迹生成和控制动作选择方面。这篇综述包括对A-SLAM研究中的各种方法、部署场景、配置、路径规划方法和效用函数的定性和定量分析。此外，本文还介绍了一种新的主动协作SLAM（AC-SLAM）分析，重点分析了SLAM系统中的协作方面。它包括在定性和统计评估的支持下，对合作参数和方法进行彻底审查。这项研究还指出了现有文献的局限性，并为未来的研究提出了潜在的途径。这项调查为寻求a-SLAM方法和技术见解的研究人员提供了宝贵的资源，提供了a-SLAM配方的最新概述。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.11654v4" target="_blank">2212.11654v4</a>
                              </td>
                              <td>Active SLAM: A Review On Last Decade</td>
                              <td>Muhammad Farhan Ahmed</td>
                              <td>2022-12-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_11654v4_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.11654v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_06141v4_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Active Semantic Localization with Graph Neural Embedding</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_06141v4_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_06141v4_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_06141v4_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Semantic localization, i.e., robot self-localization with semantic image modality, is critical in recently emerging embodied AI applications (e.g., point-goal navigation, object-goal navigation, vision language navigation) and topological mapping applications (e.g., graph neural SLAM, ego-centric topological map). However, most existing works on semantic localization focus on passive vision tasks without viewpoint planning, or rely on additional rich modalities (e.g., depth measurements). Thus, the problem is largely unsolved. In this work, we explore a lightweight, entirely CPU-based, domain-adaptive semantic localization framework, called graph neural localizer. Our approach is inspired by two recently emerging technologies: (1) Scene graph, which combines the viewpoint- and appearance- invariance of local and global features; (2) Graph neural network, which enables direct learning/recognition of graph data (i.e., non-vector data). Specifically, a graph convolutional neural network is first trained as a scene graph classifier for passive vision, and then its knowledge is transferred to a reinforcement-learning planner for active vision. Experiments on two scenarios, self-supervised learning and unsupervised domain adaptation, using a photo-realistic Habitat simulator validate the effectiveness of the proposed method.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_06141v4_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语义定位，即具有语义图像模态的机器人自我定位，在最近出现的嵌入式AI应用（例如，点目标导航、对象目标导航、视觉语言导航）和拓扑映射应用（例如图神经SLAM、以自我为中心的拓扑图）中至关重要。然而，大多数现有的语义定位工作都集中在被动视觉任务上，而没有视点规划，或者依赖于额外的丰富模式（例如，深度测量）。因此，这个问题基本上没有得到解决。在这项工作中，我们探索了一个轻量级的、完全基于CPU的、领域自适应的语义定位框架，称为图神经定位器。我们的方法受到了两种最近出现的技术的启发：（1）场景图，它结合了局部和全局特征的视点和外观不变性；（2） 图形神经网络，能够直接学习/识别图形数据（即非矢量数据）。具体来说，首先将图卷积神经网络训练为被动视觉的场景图分类器，然后将其知识转移到主动视觉的强化学习规划器中。在自监督学习和无监督领域自适应两种场景下，使用逼真的Habitat模拟器进行了实验，验证了所提出方法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>46</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.06141v4" target="_blank">2305.06141v4</a>
                              </td>
                              <td>Active Semantic Localization with Graph Neural Embedding</td>
                              <td>Mitsuki Yoshida</td>
                              <td>2023-05-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_06141v4_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.06141v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2210_05129v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-Object Navigation with dynamically learned neural implicit representations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2210_05129v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2210_05129v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2210_05129v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Understanding and mapping a new environment are core abilities of any autonomously navigating agent. While classical robotics usually estimates maps in a stand-alone manner with SLAM variants, which maintain a topological or metric representation, end-to-end learning of navigation keeps some form of memory in a neural network. Networks are typically imbued with inductive biases, which can range from vectorial representations to birds-eye metric tensors or topological structures. In this work, we propose to structure neural networks with two neural implicit representations, which are learned dynamically during each episode and map the content of the scene: (i) the Semantic Finder predicts the position of a previously seen queried object; (ii) the Occupancy and Exploration Implicit Representation encapsulates information about explored area and obstacles, and is queried with a novel global read mechanism which directly maps from function space to a usable embedding space. Both representations are leveraged by an agent trained with Reinforcement Learning (RL) and learned online during each episode. We evaluate the agent on Multi-Object Navigation and show the high impact of using neural implicit representations as a memory source.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2210_05129v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>理解和映射新环境是任何自主导航代理的核心能力。虽然经典机器人通常使用SLAM变体以独立的方式估计地图，SLAM变体保持拓扑或度量表示，但导航的端到端学习在神经网络中保持某种形式的记忆。网络通常充满了归纳偏差，其范围从矢量表示到鸟瞰度量张量或拓扑结构。在这项工作中，我们提出用两个神经隐式表示来构建神经网络，这两个表示在每一集中都是动态学习的，并映射场景的内容：（i）语义查找器预测先前看到的查询对象的位置；（ii）占用和探索隐式表示封装了关于探索区域和障碍物的信息，并使用新的全局读取机制进行查询，该机制直接从函数空间映射到可用的嵌入空间。这两种表示都由接受强化学习（RL）培训的代理人利用，并在每一集中在线学习。我们评估了多对象导航中的代理，并展示了使用神经隐式表示作为记忆源的高度影响。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>47</td>
                              <td>
                                <a href="https://arxiv.org/abs/2210.05129v2" target="_blank">2210.05129v2</a>
                              </td>
                              <td>Multi-Object Navigation with dynamically learned neural implicit representations</td>
                              <td>Pierre Marza</td>
                              <td>2022-10-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2210_05129v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2210.05129v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_15065v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Language-EXtended Indoor SLAM (LEXIS): A Versatile System for Real-time Visual Scene Understanding</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_15065v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_15065v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_15065v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Versatile and adaptive semantic understanding would enable autonomous systems to comprehend and interact with their surroundings. Existing fixed-class models limit the adaptability of indoor mobile and assistive autonomous systems. In this work, we introduce LEXIS, a real-time indoor Simultaneous Localization and Mapping (SLAM) system that harnesses the open-vocabulary nature of Large Language Models (LLMs) to create a unified approach to scene understanding and place recognition. The approach first builds a topological SLAM graph of the environment (using visual-inertial odometry) and embeds Contrastive Language-Image Pretraining (CLIP) features in the graph nodes. We use this representation for flexible room classification and segmentation, serving as a basis for room-centric place recognition. This allows loop closure searches to be directed towards semantically relevant places. Our proposed system is evaluated using both public, simulated data and real-world data, covering office and home environments. It successfully categorizes rooms with varying layouts and dimensions and outperforms the state-of-the-art (SOTA). For place recognition and trajectory estimation tasks we achieve equivalent performance to the SOTA, all also utilizing the same pre-trained model. Lastly, we demonstrate the system's potential for planning.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_15065v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多功能和自适应的语义理解将使自主系统能够理解周围环境并与之互动。现有的固定类模型限制了室内移动和辅助自主系统的适应性。在这项工作中，我们介绍了LEXIS，这是一个实时室内同步定位和映射（SLAM）系统，它利用大型语言模型（LLM）的开放词汇特性，创建了一种统一的场景理解和位置识别方法。该方法首先构建环境的拓扑SLAM图（使用视觉惯性里程计），并在图节点中嵌入对比语言图像预训练（CLIP）特征。我们使用这种表示进行灵活的房间分类和分割，作为以房间为中心的位置识别的基础。这允许循环闭包搜索指向语义相关的地方。我们提出的系统使用公共模拟数据和真实世界数据进行评估，涵盖办公室和家庭环境。它成功地对不同布局和尺寸的房间进行了分类，并优于最先进的（SOTA）。对于位置识别和轨迹估计任务，我们实现了与SOTA等效的性能，所有这些都使用了相同的预训练模型。最后，我们展示了该系统的规划潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>48</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.15065v1" target="_blank">2309.15065v1</a>
                              </td>
                              <td>Language-EXtended Indoor SLAM (LEXIS): A Versatile System for Real-time Visual Scene Understanding</td>
                              <td>Christina Kassab</td>
                              <td>2023-09-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_15065v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.15065v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="SFM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2310_14364v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Quantitative Evaluation of Dense 3D Reconstruction of Sinus Anatomy from Monocular Endoscopic Video</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_14364v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_14364v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_14364v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Generating accurate 3D reconstructions from endoscopic video is a promising avenue for longitudinal radiation-free analysis of sinus anatomy and surgical outcomes. Several methods for monocular reconstruction have been proposed, yielding visually pleasant 3D anatomical structures by retrieving relative camera poses with structure-from-motion-type algorithms and fusion of monocular depth estimates. However, due to the complex properties of the underlying algorithms and endoscopic scenes, the reconstruction pipeline may perform poorly or fail unexpectedly. Further, acquiring medical data conveys additional challenges, presenting difficulties in quantitatively benchmarking these models, understanding failure cases, and identifying critical components that contribute to their precision. In this work, we perform a quantitative analysis of a self-supervised approach for sinus reconstruction using endoscopic sequences paired with optical tracking and high-resolution computed tomography acquired from nine ex-vivo specimens. Our results show that the generated reconstructions are in high agreement with the anatomy, yielding an average point-to-mesh error of 0.91 mm between reconstructions and CT segmentations. However, in a point-to-point matching scenario, relevant for endoscope tracking and navigation, we found average target registration errors of 6.58 mm. We identified that pose and depth estimation inaccuracies contribute equally to this error and that locally consistent sequences with shorter trajectories generate more accurate reconstructions. These results suggest that achieving global consistency between relative camera poses and estimated depths with the anatomy is essential. In doing so, we can ensure proper synergy between all components of the pipeline for improved reconstructions that will facilitate clinical application of this innovative technology.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_14364v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>根据内窥镜视频生成准确的3D重建是对鼻窦解剖结构和手术结果进行纵向无辐射分析的一种很有前途的途径。已经提出了几种单目重建方法，通过从运动类型算法中检索具有结构的相对相机姿态并融合单目深度估计，产生视觉上令人愉快的3D解剖结构。然而，由于底层算法和内窥镜场景的复杂特性，重建管道可能表现不佳或意外失败。此外，获取医疗数据带来了额外的挑战，在定量基准测试这些模型、了解故障案例和确定有助于其准确性的关键组件方面存在困难。在这项工作中，我们对一种自监督的鼻窦重建方法进行了定量分析，该方法使用内窥镜序列与从9个离体标本中采集的光学跟踪和高分辨率计算机断层扫描相结合。我们的结果表明，生成的重建与解剖结构高度一致，在重建和CT分割之间产生0.91mm的平均点到网格误差。然而，在与内窥镜跟踪和导航相关的点对点匹配场景中，我们发现平均目标配准误差为6.58 mm。我们发现，姿态和深度估计的不准确度对该误差的贡献相同，轨迹较短的局部一致序列会产生更准确的重建。这些结果表明，实现相对相机姿态和估计深度与解剖结构之间的全局一致性至关重要。通过这样做，我们可以确保管道的所有组成部分之间的适当协同作用，以改进重建，从而促进这项创新技术的临床应用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.14364v1" target="_blank">2310.14364v1</a>
                              </td>
                              <td>A Quantitative Evaluation of Dense 3D Reconstruction of Sinus Anatomy from Monocular Endoscopic Video</td>
                              <td>Jan Emily Mangulabnan</td>
                              <td>2023-10-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_14364v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.14364v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_13605v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FMRT: Learning Accurate Feature Matching with Reconciliatory Transformer</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_13605v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_13605v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_13605v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Local Feature Matching, an essential component of several computer vision tasks (e.g., structure from motion and visual localization), has been effectively settled by Transformer-based methods. However, these methods only integrate long-range context information among keypoints with a fixed receptive field, which constrains the network from reconciling the importance of features with different receptive fields to realize complete image perception, hence limiting the matching accuracy. In addition, these methods utilize a conventional handcrafted encoding approach to integrate the positional information of keypoints into the visual descriptors, which limits the capability of the network to extract reliable positional encoding message. In this study, we propose Feature Matching with Reconciliatory Transformer (FMRT), a novel Transformer-based detector-free method that reconciles different features with multiple receptive fields adaptively and utilizes parallel networks to realize reliable positional encoding. Specifically, FMRT proposes a dedicated Reconciliatory Transformer (RecFormer) that consists of a Global Perception Attention Layer (GPAL) to extract visual descriptors with different receptive fields and integrate global context information under various scales, Perception Weight Layer (PWL) to measure the importance of various receptive fields adaptively, and Local Perception Feed-forward Network (LPFFN) to extract deep aggregated multi-scale local feature representation. Extensive experiments demonstrate that FMRT yields extraordinary performance on multiple benchmarks, including pose estimation, visual localization, homography estimation, and image matching.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_13605v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>局部特征匹配是一些计算机视觉任务（如运动结构和视觉定位）的重要组成部分，已通过基于Transformer的方法得到有效解决。然而，这些方法只将关键点之间的长程上下文信息与固定的感受野相结合，这限制了网络协调特征与不同感受野的重要性以实现完整的图像感知，从而限制了匹配精度。此外，这些方法利用传统的手工编码方法将关键点的位置信息集成到视觉描述符中，这限制了网络提取可靠位置编码消息的能力。在这项研究中，我们提出了具有协调变换器的特征匹配（FMRT），这是一种新的基于变换器的无检测器方法，它自适应地协调不同特征与多个感受野，并利用并行网络实现可靠的位置编码。具体而言，FMRT提出了一种专用的协调转换器（RecFormer），该转换器由全局感知注意力层（GPAL）组成，用于提取具有不同感受野的视觉描述符并整合各种尺度下的全局上下文信息，感知权重层（PWL）用于自适应地测量各种感受野的重要性，以及局部感知前馈网络（LPFFN）来提取深度聚合的多尺度局部特征表示。大量实验表明，FMRT在多个基准上产生了非凡的性能，包括姿态估计、视觉定位、单应性估计和图像匹配。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.13605v1" target="_blank">2310.13605v1</a>
                              </td>
                              <td>FMRT: Learning Accurate Feature Matching with Reconciliatory Transformer</td>
                              <td>Xinyu Zhang</td>
                              <td>2023-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_13605v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.13605v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09109v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09109v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09109v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09109v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in neural reconstruction enable high-quality 3D object reconstruction from casually captured image collections. Current techniques mostly analyze their progress on relatively simple image collections where Structure-from-Motion (SfM) techniques can provide ground-truth (GT) camera poses. We note that SfM techniques tend to fail on in-the-wild image collections such as image search results with varying backgrounds and illuminations. To enable systematic research progress on 3D reconstruction from casual image captures, we propose NAVI: a new dataset of category-agnostic image collections of objects with high-quality 3D scans along with per-image 2D-3D alignments providing near-perfect GT camera parameters. These 2D-3D alignments allow us to extract accurate derivative annotations such as dense pixel correspondences, depth and segmentation maps. We demonstrate the use of NAVI image collections on different problem settings and show that NAVI enables more thorough evaluations that were not possible with existing datasets. We believe NAVI is beneficial for systematic research progress on 3D reconstruction and correspondence estimation. Project page: https://navidataset.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09109v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经重建的最新进展使得能够从随意捕获的图像集合中重建高质量的3D对象。当前的技术主要分析它们在相对简单的图像采集上的进展，其中运动结构（SfM）技术可以提供地面实况（GT）相机姿态。我们注意到，SfM技术在野生图像集合中往往失败，例如具有不同背景和照明的图像搜索结果。为了实现从偶然图像捕获的3D重建的系统研究进展，我们提出了NAVI：一个具有高质量3D扫描的对象的类别不可知图像集合的新数据集，以及提供近乎完美的GT相机参数的每张图像2D-3D对齐。这些2D-3D比对允许我们提取精确的导数注释，例如密集像素对应、深度和分割图。我们展示了NAVI图像集合在不同问题设置中的使用，并表明NAVI能够实现现有数据集无法实现的更彻底的评估。我们相信NAVI有利于三维重建和对应关系估计的系统研究进展。项目页面：https://navidataset.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09109v2" target="_blank">2306.09109v2</a>
                              </td>
                              <td>NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations</td>
                              <td>Varun Jampani</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09109v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09109v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_05504v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Colmap-PCD: An Open-source Tool for Fine Image-to-point cloud Registration</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_05504v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_05504v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_05504v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>State-of-the-art techniques for monocular camera reconstruction predominantly rely on the Structure from Motion (SfM) pipeline. However, such methods often yield reconstruction outcomes that lack crucial scale information, and over time, accumulation of images leads to inevitable drift issues. In contrast, mapping methods based on LiDAR scans are popular in large-scale urban scene reconstruction due to their precise distance measurements, a capability fundamentally absent in visual-based approaches. Researchers have made attempts to utilize concurrent LiDAR and camera measurements in pursuit of precise scaling and color details within mapping outcomes. However, the outcomes are subject to extrinsic calibration and time synchronization precision. In this paper, we propose a novel cost-effective reconstruction pipeline that utilizes a pre-established LiDAR map as a fixed constraint to effectively address the inherent scale challenges present in monocular camera reconstruction. To our knowledge, our method is the first to register images onto the point cloud map without requiring synchronous capture of camera and LiDAR data, granting us the flexibility to manage reconstruction detail levels across various areas of interest. To facilitate further research in this domain, we have released Colmap-PCD${^{3}}$, an open-source tool leveraging the Colmap algorithm, that enables precise fine-scale registration of images to the point cloud map.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_05504v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>单目相机重建的最先进技术主要依赖于运动结构（SfM）流水线。然而，这种方法通常会产生缺乏关键尺度信息的重建结果，并且随着时间的推移，图像的积累会导致不可避免的漂移问题。相比之下，基于激光雷达扫描的地图绘制方法由于其精确的距离测量而在大规模城市场景重建中很受欢迎，而这在基于视觉的方法中根本不具备。研究人员试图利用激光雷达和相机的同时测量，以追求测绘结果中的精确缩放和颜色细节。然而，结果受到外部校准和时间同步精度的影响。在本文中，我们提出了一种新的具有成本效益的重建管道，该管道利用预先建立的激光雷达图作为固定约束，以有效解决单目相机重建中存在的固有规模挑战。据我们所知，我们的方法是第一个将图像配准到点云图上，而不需要同步捕获相机和激光雷达数据，这使我们能够灵活地管理各个感兴趣区域的重建细节级别。为了促进该领域的进一步研究，我们发布了Colmap PCD$｛^｛3｝｝$，这是一款利用Colmap算法的开源工具，可以将图像精确地精细配准到点云地图上。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.05504v1" target="_blank">2310.05504v1</a>
                              </td>
                              <td>Colmap-PCD: An Open-source Tool for Fine Image-to-point cloud Registration</td>
                              <td>Chunge Bai</td>
                              <td>2023-10-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_05504v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.05504v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_05134v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LocoNeRF: A NeRF-based Approach for Local Structure from Motion for Precise Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_05134v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_05134v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_05134v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual localization is a critical task in mobile robotics, and researchers are continuously developing new approaches to enhance its efficiency. In this article, we propose a novel approach to improve the accuracy of visual localization using Structure from Motion (SfM) techniques. We highlight the limitations of global SfM, which suffers from high latency, and the challenges of local SfM, which requires large image databases for accurate reconstruction. To address these issues, we propose utilizing Neural Radiance Fields (NeRF), as opposed to image databases, to cut down on the space required for storage. We suggest that sampling reference images around the prior query position can lead to further improvements. We evaluate the accuracy of our proposed method against ground truth obtained using LIDAR and Advanced Lidar Odometry and Mapping in Real-time (A-LOAM), and compare its storage usage against local SfM with COLMAP in the conducted experiments. Our proposed method achieves an accuracy of 0.068 meters compared to the ground truth, which is slightly lower than the most advanced method COLMAP, which has an accuracy of 0.022 meters. However, the size of the database required for COLMAP is 400 megabytes, whereas the size of our NeRF model is only 160 megabytes. Finally, we perform an ablation study to assess the impact of using reference images from the NeRF reconstruction.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_05134v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉定位是移动机器人的一项关键任务，研究人员正在不断开发新的方法来提高其效率。在本文中，我们提出了一种利用运动结构（SfM）技术提高视觉定位精度的新方法。我们强调了全局SfM的局限性，它具有高延迟，以及局部SfM所面临的挑战，后者需要大型图像数据库才能进行精确重建。为了解决这些问题，我们建议利用神经辐射场（NeRF），而不是图像数据库，来减少存储所需的空间。我们建议，对先前查询位置周围的参考图像进行采样可以带来进一步的改进。我们评估了我们提出的方法相对于使用激光雷达和高级激光雷达实时测距和测绘（A-LOAM）获得的地面实况的准确性，并在所进行的实验中比较了其相对于局部SfM和COLMAP的存储使用情况。与地面实况相比，我们提出的方法实现了0.068米的精度，这略低于最先进的方法COLMAP，后者的精度为0.022米。然而，COLMAP所需的数据库大小为400兆字节，而我们的NeRF模型的大小仅为160兆字节。最后，我们进行了消融研究，以评估使用NeRF重建的参考图像的影响。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.05134v1" target="_blank">2310.05134v1</a>
                              </td>
                              <td>LocoNeRF: A NeRF-based Approach for Local Structure from Motion for Precise Localization</td>
                              <td>Artem Nenashev</td>
                              <td>2023-10-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_05134v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.05134v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2204_04145v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Constrained Bundle Adjustment for Structure From Motion Using Uncalibrated Multi-Camera Systems</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2204_04145v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2204_04145v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2204_04145v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Structure from motion using uncalibrated multi-camera systems is a challenging task. This paper proposes a bundle adjustment solution that implements a baseline constraint respecting that these cameras are static to each other. We assume these cameras are mounted on a mobile platform, uncalibrated, and coarsely synchronized. To this end, we propose the baseline constraint that is formulated for the scenario in which the cameras have overlapping views. The constraint is incorporated in the bundle adjustment solution to keep the relative motion of different cameras static. Experiments were conducted using video frames of two collocated GoPro cameras mounted on a vehicle with no system calibration. These two cameras were placed capturing overlapping contents. We performed our bundle adjustment using the proposed constraint and then produced 3D dense point clouds. Evaluations were performed by comparing these dense point clouds against LiDAR reference data. We showed that, as compared to traditional bundle adjustment, our proposed method achieved an improvement of 29.38%.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2204_04145v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用未校准的多摄像头系统从运动中构建结构是一项具有挑战性的任务。本文提出了一种束调整解决方案，该解决方案实现了一个基线约束，即这些相机彼此是静态的。我们假设这些相机安装在移动平台上，未校准，并且粗略同步。为此，我们提出了针对相机具有重叠视图的场景制定的基线约束。该约束被纳入束调整解决方案中，以保持不同相机的相对运动静止。实验使用安装在车辆上的两个并置GoPro相机的视频帧进行，没有系统校准。这两台摄像机被放置在捕捉重叠内容的位置。我们使用所提出的约束进行了束调整，然后生成了3D密集点云。通过将这些密集点云与激光雷达参考数据进行比较来进行评估。我们表明，与传统的束平差相比，我们提出的方法实现了29.38%的改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2204.04145v2" target="_blank">2204.04145v2</a>
                              </td>
                              <td>Constrained Bundle Adjustment for Structure From Motion Using Uncalibrated Multi-Camera Systems</td>
                              <td>Debao Huang</td>
                              <td>2022-04-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2204_04145v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2204.04145v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_01092v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Leveraging Cutting Edge Deep Learning Based Image Matching for Reconstructing a Large Scene from Sparse Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_01092v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_01092v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_01092v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present the top ranked solution for the AISG-SLA Visual Localisation Challenge benchmark (IJCAI 2023), where the task is to estimate relative motion between images taken in sequence by a camera mounted on a car driving through an urban scene.   For matching images we use our recent deep learning based matcher RoMa. Matching image pairs sequentially and estimating relative motion from point correspondences sampled by RoMa already gives very competitive results -- third rank on the challenge benchmark.   To improve the estimations we extract keypoints in the images, match them using RoMa, and perform structure from motion reconstruction using COLMAP. We choose our recent DeDoDe keypoints for their high repeatability. Further, we address time jumps in the image sequence by matching specific non-consecutive image pairs based on image retrieval with DINOv2. These improvements yield a solution beating all competitors.   We further present a loose upper bound on the accuracy obtainable by the image retrieval approach by also matching hand-picked non-consecutive pairs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_01092v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们为AISG-SLA视觉定位挑战基准（IJCAI 2023）提供了排名最高的解决方案，其中的任务是估计安装在汽车上的摄像头在城市场景中依次拍摄的图像之间的相对运动。对于匹配图像，我们使用最近的基于深度学习的匹配器RoMa。按顺序匹配图像对，并根据RoMa采样的点对应关系估计相对运动，已经给出了非常有竞争力的结果——在挑战基准上排名第三。为了改进估计，我们提取图像中的关键点，使用RoMa进行匹配，并使用COLMAP从运动重建中执行结构。我们选择最近的DeDoDe关键点是因为它们具有较高的可重复性。此外，我们通过将基于图像检索的特定非连续图像对与DINOv2进行匹配来解决图像序列中的时间跳跃问题。这些改进产生了一个击败所有竞争对手的解决方案。我们进一步提出了图像检索方法通过匹配手工拾取的非连续对所获得的精度的宽松上限。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.01092v1" target="_blank">2310.01092v1</a>
                              </td>
                              <td>Leveraging Cutting Edge Deep Learning Based Image Matching for Reconstructing a Large Scene from Sparse Images</td>
                              <td>Georg Bökman</td>
                              <td>2023-10-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_01092v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.01092v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_00783v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Propagating Semantic Labels in Video Data</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_00783v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_00783v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_00783v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Semantic Segmentation combines two sub-tasks: the identification of pixel-level image masks and the application of semantic labels to those masks. Recently, so-called Foundation Models have been introduced; general models trained on very large datasets which can be specialized and applied to more specific tasks. One such model, the Segment Anything Model (SAM), performs image segmentation. Semantic segmentation systems such as CLIPSeg and MaskRCNN are trained on datasets of paired segments and semantic labels. Manual labeling of custom data, however, is time-consuming. This work presents a method for performing segmentation for objects in video. Once an object has been found in a frame of video, the segment can then be propagated to future frames; thus reducing manual annotation effort. The method works by combining SAM with Structure from Motion (SfM). The video input to the system is first reconstructed into 3D geometry using SfM. A frame of video is then segmented using SAM. Segments identified by SAM are then projected onto the the reconstructed 3D geometry. In subsequent video frames, the labeled 3D geometry is reprojected into the new perspective, allowing SAM to be invoked fewer times. System performance is evaluated, including the contributions of the SAM and SfM components. Performance is evaluated over three main metrics: computation time, mask IOU with manual labels, and the number of tracking losses. Results demonstrate that the system has substantial computation time improvements over human performance for tracking objects over video frames, but suffers in performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_00783v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语义分割结合了两个子任务：像素级图像掩码的识别和对掩码应用语义标签。最近，引入了所谓的基础模型；在非常大的数据集上训练的通用模型，这些数据集可以专门化并应用于更具体的任务。一个这样的模型，分段任意模型（SAM），执行图像分割。CLIPSeg和MaskRCNN等语义分割系统是在成对片段和语义标签的数据集上进行训练的。但是，手动标记自定义数据非常耗时。这项工作提出了一种对视频中的对象进行分割的方法。一旦在视频帧中找到对象，则可以将该片段传播到未来的帧；从而减少了手动注释的工作量。该方法将SAM与运动结构（SfM）相结合。首先使用SfM将输入到系统的视频重构为3D几何结构。然后使用SAM对视频帧进行分割。然后将SAM识别的片段投影到重建的3D几何体上。在随后的视频帧中，标记的3D几何体被重新投影到新的透视图中，从而减少SAM的调用次数。评估系统性能，包括SAM和SfM组件的贡献。性能通过三个主要指标进行评估：计算时间、带有手动标签的掩码IOU和跟踪丢失次数。结果表明，该系统在视频帧上跟踪对象的计算时间大大提高了人类的性能，但性能较差。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.00783v1" target="_blank">2310.00783v1</a>
                              </td>
                              <td>Propagating Semantic Labels in Video Data</td>
                              <td>David Balaban</td>
                              <td>2023-10-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_00783v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.00783v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_16632v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Sparse Submodular Function Minimization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_16632v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_16632v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_16632v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we study the problem of minimizing a submodular function $f : 2^V \rightarrow \mathbb{R}$ that is guaranteed to have a $k$-sparse minimizer. We give a deterministic algorithm that computes an additive $\epsilon$-approximate minimizer of such $f$ in $\widetilde{O}(\mathsf{poly}(k) \log(|f|/\epsilon))$ parallel depth using a polynomial number of queries to an evaluation oracle of $f$, where $|f| = \max_{S \subseteq V} |f(S)|$. Further, we give a randomized algorithm that computes an exact minimizer of $f$ with high probability using $\widetilde{O}(|V| \cdot \mathsf{poly}(k))$ queries and polynomial time. When $k = \widetilde{O}(1)$, our algorithms use either nearly-constant parallel depth or a nearly-linear number of evaluation oracle queries. All previous algorithms for this problem either use $\Omega(|V|)$ parallel depth or $\Omega(|V|^2)$ queries.   In contrast to state-of-the-art weakly-polynomial and strongly-polynomial time algorithms for SFM, our algorithms use first-order optimization methods, e.g., mirror descent and follow the regularized leader. We introduce what we call {\em sparse dual certificates}, which encode information on the structure of sparse minimizers, and both our parallel and sequential algorithms provide new algorithmic tools for allowing first-order optimization methods to efficiently compute them. Correspondingly, our algorithm does not invoke fast matrix multiplication or general linear system solvers and in this sense is more combinatorial than previous state-of-the-art methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_16632v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们研究了子模函数$f:2^V\rightarrow\mathbb｛R｝$的最小化问题，该子模函数保证具有$k$-稀疏极小值。我们给出了一个确定性算法，该算法使用对$f$的评估预言的多项式查询数来计算$\widetilde｛O｝（\mathsf｛poly｝（k）\log（|f|/\epsilon））$并行深度中的$f$近似极小值，其中$|f|=\max_｛S\substeq V｝|f（S）|$。此外，我们给出了一个随机算法，该算法使用$\widetilde{O}（|V|\cdot\mathsf{poly}（k））$查询和多项式时间以高概率计算$f$的精确极小值。当$k=\widetilde｛O｝（1）$时，我们的算法使用几乎恒定的并行深度或几乎线性数量的评估oracle查询。以前针对此问题的所有算法都使用$\Omega（|V|）$并行深度或$\Omega（|V|^2）$查询。与最先进的SFM弱多项式和强多项式时间算法相比，我们的算法使用一阶优化方法，例如镜像下降和遵循正则化前导。我们介绍了我们所称的稀疏双证书，它对稀疏最小化器结构的信息进行编码，我们的并行和顺序算法都提供了新的算法工具，允许一阶优化方法有效地计算它们。相应地，我们的算法不调用快速矩阵乘法或一般线性系统求解器，并且在这个意义上比以前最先进的方法更具组合性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.16632v1" target="_blank">2309.16632v1</a>
                              </td>
                              <td>Sparse Submodular Function Minimization</td>
                              <td>Andrei Graur</td>
                              <td>2023-09-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_16632v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.16632v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_13772v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Motion Segmentation from a Moving Monocular Camera</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_13772v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_13772v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_13772v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Identifying and segmenting moving objects from a moving monocular camera is difficult when there is unknown camera motion, different types of object motions and complex scene structures. To tackle these challenges, we take advantage of two popular branches of monocular motion segmentation approaches: point trajectory based and optical flow based methods, by synergistically fusing these two highly complementary motion cues at object level. By doing this, we are able to model various complex object motions in different scene structures at once, which has not been achieved by existing methods. We first obtain object-specific point trajectories and optical flow mask for each common object in the video, by leveraging the recent foundational models in object recognition, segmentation and tracking. We then construct two robust affinity matrices representing the pairwise object motion affinities throughout the whole video using epipolar geometry and the motion information provided by optical flow. Finally, co-regularized multi-view spectral clustering is used to fuse the two affinity matrices and obtain the final clustering. Our method shows state-of-the-art performance on the KT3DMoSeg dataset, which contains complex motions and scene structures. Being able to identify moving objects allows us to remove them for map building when using visual SLAM or SFM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_13772v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当存在未知的摄像机运动、不同类型的物体运动和复杂的场景结构时，从移动的单目摄像机中识别和分割移动物体是困难的。为了应对这些挑战，我们利用了单目运动分割方法的两个流行分支：基于点轨迹的方法和基于光流的方法，通过在对象级别协同融合这两个高度互补的运动线索。通过这样做，我们能够同时对不同场景结构中的各种复杂物体运动进行建模，这是现有方法无法实现的。我们首先利用对象识别、分割和跟踪方面的最新基础模型，获得视频中每个常见对象的特定对象点轨迹和光流掩模。然后，我们使用极线几何和光流提供的运动信息构建了两个稳健的仿射矩阵，表示整个视频中的成对对象运动仿射。最后，使用共正则化多视图谱聚类来融合两个亲和矩阵，得到最终的聚类。我们的方法在KT3DMoSeg数据集上显示了最先进的性能，该数据集包含复杂的运动和场景结构。能够识别移动物体使我们能够在使用视觉SLAM或SFM时将其移除以用于地图构建。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.13772v1" target="_blank">2309.13772v1</a>
                              </td>
                              <td>Motion Segmentation from a Moving Monocular Camera</td>
                              <td>Yuxiang Huang</td>
                              <td>2023-09-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_13772v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.13772v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_12804v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Scalable Semantic 3D Mapping of Coral Reefs with Deep Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_12804v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_12804v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_12804v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Coral reefs are among the most diverse ecosystems on our planet, and are depended on by hundreds of millions of people. Unfortunately, most coral reefs are existentially threatened by global climate change and local anthropogenic pressures. To better understand the dynamics underlying deterioration of reefs, monitoring at high spatial and temporal resolution is key. However, conventional monitoring methods for quantifying coral cover and species abundance are limited in scale due to the extensive manual labor required. Although computer vision tools have been employed to aid in this process, in particular SfM photogrammetry for 3D mapping and deep neural networks for image segmentation, analysis of the data products creates a bottleneck, effectively limiting their scalability. This paper presents a new paradigm for mapping underwater environments from ego-motion video, unifying 3D mapping systems that use machine learning to adapt to challenging conditions under water, combined with a modern approach for semantic segmentation of images. The method is exemplified on coral reefs in the northern Gulf of Aqaba, Red Sea, demonstrating high-precision 3D semantic mapping at unprecedented scale with significantly reduced required labor costs: a 100 m video transect acquired within 5 minutes of diving with a cheap consumer-grade camera can be fully automatically analyzed within 5 minutes. Our approach significantly scales up coral reef monitoring by taking a leap towards fully automatic analysis of video transects. The method democratizes coral reef transects by reducing the labor, equipment, logistics, and computing cost. This can help to inform conservation policies more efficiently. The underlying computational method of learning-based Structure-from-Motion has broad implications for fast low-cost mapping of underwater environments other than coral reefs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_12804v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>珊瑚礁是地球上最多样化的生态系统之一，数亿人依赖珊瑚礁。不幸的是，大多数珊瑚礁都受到全球气候变化和当地人为压力的威胁。为了更好地了解珊瑚礁退化背后的动力学，以高空间和时间分辨率进行监测是关键。然而，由于需要大量的体力劳动，量化珊瑚覆盖率和物种丰度的传统监测方法在规模上受到限制。尽管计算机视觉工具已被用于帮助这一过程，特别是用于3D地图绘制的SfM摄影测量和用于图像分割的深度神经网络，但对数据产品的分析造成了瓶颈，有效地限制了其可扩展性。本文提出了一种从自我运动视频映射水下环境的新范式，将使用机器学习来适应水下具有挑战性的条件的3D映射系统与图像语义分割的现代方法相结合。该方法以红海亚喀巴湾北部的珊瑚礁为例，展示了前所未有的高精度3D语义映射，大大降低了所需的人力成本：用廉价的消费级相机在潜水5分钟内获得的100米视频样带可以在5分钟内全自动分析。我们的方法通过向视频样带的全自动分析迈出了一大步，大大扩大了珊瑚礁监测的规模。该方法通过减少劳动力、设备、物流和计算成本，使珊瑚礁横断面民主化。这有助于更有效地为保护政策提供信息。基于运动结构学习的基本计算方法对珊瑚礁以外的水下环境的快速低成本测绘具有广泛的意义。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.12804v1" target="_blank">2309.12804v1</a>
                              </td>
                              <td>Scalable Semantic 3D Mapping of Coral Reefs with Deep Learning</td>
                              <td>Jonathan Sauder</td>
                              <td>2023-09-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_12804v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.12804v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_11883v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On-the-Fly SfM: What you capture is What you get</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_11883v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_11883v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_11883v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Over the last decades, ample achievements have been made on Structure from motion (SfM). However, the vast majority of them basically work in an offline manner, i.e., images are firstly captured and then fed together into a SfM pipeline for obtaining poses and sparse point cloud. In this work, on the contrary, we present an on-the-fly SfM: running online SfM while image capturing, the newly taken On-the-Fly image is online estimated with the corresponding pose and points, i.e., what you capture is what you get. Specifically, our approach firstly employs a vocabulary tree that is unsupervised trained using learning-based global features for fast image retrieval of newly fly-in image. Then, a robust feature matching mechanism with least squares (LSM) is presented to improve image registration performance. Finally, via investigating the influence of newly fly-in image's connected neighboring images, an efficient hierarchical weighted local bundle adjustment (BA) is used for optimization. Extensive experimental results demonstrate that on-the-fly SfM can meet the goal of robustly registering the images while capturing in an online way.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_11883v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在过去的几十年里，运动结构（SfM）取得了丰硕的成果。然而，它们中的绝大多数基本上是以离线方式工作的，即首先捕获图像，然后将其一起输入到SfM管道中，以获得姿态和稀疏点云。相反，在这项工作中，我们提出了一个动态SfM：在图像捕获时运行在线SfM，新拍摄的动态图像是用相应的姿势和点在线估计的，即，你捕获的就是你得到的。具体来说，我们的方法首先使用了一个词汇树，该词汇树使用基于学习的全局特征进行无监督训练，用于新飞行图像的快速图像检索。然后，提出了一种鲁棒的最小二乘特征匹配机制来提高图像配准性能。最后，通过研究新飞入图像的连接相邻图像的影响，使用有效的分层加权局部束平差（BA）进行优化。大量的实验结果表明，动态SfM可以在在线拍摄的同时实现稳健配准图像的目标。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.11883v1" target="_blank">2309.11883v1</a>
                              </td>
                              <td>On-the-Fly SfM: What you capture is What you get</td>
                              <td>Zongqian Zhan</td>
                              <td>2023-09-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_11883v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.11883v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_10748v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SHOWMe: Benchmarking Object-agnostic Hand-Object 3D Reconstruction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_10748v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_10748v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_10748v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent hand-object interaction datasets show limited real object variability and rely on fitting the MANO parametric model to obtain groundtruth hand shapes. To go beyond these limitations and spur further research, we introduce the SHOWMe dataset which consists of 96 videos, annotated with real and detailed hand-object 3D textured meshes. Following recent work, we consider a rigid hand-object scenario, in which the pose of the hand with respect to the object remains constant during the whole video sequence. This assumption allows us to register sub-millimetre-precise groundtruth 3D scans to the image sequences in SHOWMe. Although simpler, this hypothesis makes sense in terms of applications where the required accuracy and level of detail is important eg., object hand-over in human-robot collaboration, object scanning, or manipulation and contact point analysis. Importantly, the rigidity of the hand-object systems allows to tackle video-based 3D reconstruction of unknown hand-held objects using a 2-stage pipeline consisting of a rigid registration step followed by a multi-view reconstruction (MVR) part. We carefully evaluate a set of non-trivial baselines for these two stages and show that it is possible to achieve promising object-agnostic 3D hand-object reconstructions employing an SfM toolbox or a hand pose estimator to recover the rigid transforms and off-the-shelf MVR algorithms. However, these methods remain sensitive to the initial camera pose estimates which might be imprecise due to lack of textures on the objects or heavy occlusions of the hands, leaving room for improvements in the reconstruction. Code and dataset are available at https://europe.naverlabs.com/research/showme</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_10748v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近的手-物体交互数据集显示出有限的真实物体可变性，并且依赖于拟合MANO参数模型来获得真实的手形状。为了超越这些限制并推动进一步的研究，我们引入了SHOWMe数据集，该数据集由96个视频组成，用真实和详细的手对象3D纹理网格进行注释。根据最近的工作，我们考虑了一个刚性手对象场景，其中手相对于对象的姿势在整个视频序列中保持不变。这一假设使我们能够将亚毫米精度的地面实况3D扫描注册到SHOWMe中的图像序列中。尽管更简单，但这一假设在所需精度和细节水平很重要的应用中是有意义的，例如，人机协作中的对象移交、对象扫描或操作和接触点分析。重要的是，手对象系统的刚性允许使用由刚性配准步骤和多视图重建（MVR）部分组成的两阶段流水线来处理未知手持对象的基于视频的3D重建。我们仔细评估了这两个阶段的一组非平凡基线，并表明使用SfM工具箱或手部姿态估计器来恢复刚性变换和现成的MVR算法，可以实现有前景的对象不可知的3D手部对象重建。然而，这些方法对最初的相机姿态估计仍然敏感，由于物体上缺乏纹理或手的严重遮挡，这些估计可能不精确，这为重建留下了改进的空间。代码和数据集可在https://europe.naverlabs.com/research/showme</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.10748v1" target="_blank">2309.10748v1</a>
                              </td>
                              <td>SHOWMe: Benchmarking Object-agnostic Hand-Object 3D Reconstruction</td>
                              <td>Anilkumar Swamy</td>
                              <td>2023-09-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_10748v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.10748v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_10269v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Using an Uncrewed Surface Vehicle to Create a Volumetric Model of Non-Navigable Rivers and Other Shallow Bodies of Water</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_10269v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_10269v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_10269v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Non-navigable rivers and retention ponds play important roles in buffering communities from flooding, yet emergency planners often have no data as to the volume of water that they can carry before flooding the surrounding. This paper describes a practical approach for using an uncrewed marine surface vehicle (USV) to collect and merge bathymetric maps with digital surface maps of the banks of shallow bodies of water into a unified volumetric model. The below-waterline mesh is developed by applying the Poisson surface reconstruction algorithm to the sparse sonar depth readings of the underwater surface. Dense above-waterline meshes of the banks are created using commercial structure from motion (SfM) packages. Merging is challenging for many reasons, the most significant is gaps in sensor coverage, i.e., the USV cannot collect sonar depth data or visually see sandy beaches leading to a bank thus the two meshes may not intersect. The approach is demonstrated on a Hydronalix EMILY USV with a Humminbird single beam echosounder and Teledyne FLIR camera at Lake ESTI at the Texas A&M Engineering Extension Service Disaster City complex.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_10269v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>不通航的河流和蓄水池在缓冲社区免受洪水侵袭方面发挥着重要作用，但应急规划者往往没有数据表明它们在淹没周围地区之前可以携带的水量。本文描述了一种实用的方法，即使用未折叠的海洋表面飞行器（USV）收集水深图和浅水河岸的数字表面图，并将其合并为一个统一的体积模型。将泊松曲面重建算法应用于水下表面稀疏声纳深度读数，开发了水线下网格。河岸的密集水线上网格是使用商业运动结构（SfM）包创建的。由于多种原因，合并具有挑战性，其中最重要的是传感器覆盖范围的差距，即USV无法收集声纳深度数据，也无法直观地看到通往河岸的沙滩，因此两个网格可能不会相交。该方法在Hydronalix EMILY USV上进行了演示，该V配备了Humminbird单波束回声测深仪和Teledyne FLIR相机，位于德克萨斯州农工工程扩展服务灾难城市综合体的ESTI湖。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.10269v1" target="_blank">2309.10269v1</a>
                              </td>
                              <td>Using an Uncrewed Surface Vehicle to Create a Volumetric Model of Non-Navigable Rivers and Other Shallow Bodies of Water</td>
                              <td>Jayesh Tripathi</td>
                              <td>2023-09-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_10269v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.10269v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_08927v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DynaMoN: Motion-Aware Fast And Robust Camera Localization for Dynamic NeRF</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_08927v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_08927v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_08927v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Dynamic reconstruction with neural radiance fields (NeRF) requires accurate camera poses. These are often hard to retrieve with existing structure-from-motion (SfM) pipelines as both camera and scene content can change. We propose DynaMoN that leverages simultaneous localization and mapping (SLAM) jointly with motion masking to handle dynamic scene content. Our robust SLAM-based tracking module significantly accelerates the training process of the dynamic NeRF while improving the quality of synthesized views at the same time. Extensive experimental validation on TUM RGB-D, BONN RGB-D Dynamic and the DyCheck's iPhone dataset, three real-world datasets, shows the advantages of DynaMoN both for camera pose estimation and novel view synthesis.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_08927v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>利用神经辐射场（NeRF）进行动态重建需要精确的相机姿态。这些通常很难用现有的运动结构（SfM）管道来检索，因为相机和场景内容都可能发生变化。我们提出了DynaMoN，它利用同步定位和映射（SLAM）与运动掩蔽相结合来处理动态场景内容。我们基于SLAM的稳健跟踪模块显著加快了动态NeRF的训练过程，同时提高了合成视图的质量。对TUM RGB-D、BONN RGB-D Dynamic和DyCheck的iPhone数据集这三个真实世界的数据集进行了广泛的实验验证，显示了DynaMoN在相机姿态估计和新颖视图合成方面的优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.08927v1" target="_blank">2309.08927v1</a>
                              </td>
                              <td>DynaMoN: Motion-Aware Fast And Robust Camera Localization for Dynamic NeRF</td>
                              <td>Mert Asim Karaoglu</td>
                              <td>2023-09-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_08927v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.08927v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_04643v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Parallel Submodular Function Minimization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_04643v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_04643v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_04643v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We consider the parallel complexity of submodular function minimization (SFM). We provide a pair of methods which obtain two new query versus depth trade-offs a submodular function defined on subsets of $n$ elements that has integer values between $-M$ and $M$. The first method has depth $2$ and query complexity $n^{O(M)}$ and the second method has depth $\widetilde{O}(n^{1/3} M^{2/3})$ and query complexity $O(\mathrm{poly}(n, M))$. Despite a line of work on improved parallel lower bounds for SFM, prior to our work the only known algorithms for parallel SFM either followed from more general methods for sequential SFM or highly-parallel minimization of convex $\ell_2$-Lipschitz functions. Interestingly, to obtain our second result we provide the first highly-parallel algorithm for minimizing $\ell_\infty$-Lipschitz function over the hypercube which obtains near-optimal depth for obtaining constant accuracy.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_04643v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们考虑了子模函数最小化（SFM）的并行复杂性。我们提供了一对方法，可以获得两种新的查询与深度的权衡——在整数值介于$-M$和$M$之间的$n$元素子集上定义的子模函数。第一种方法的深度为$2$，查询复杂度为$n^｛O（M）｝$，第二种方法的厚度为$\widetilde｛O｝（n^｛1/3｝M^｛2/3｝）$，查询复杂性为$O（\mathrm｛poly｝（n，M））$。尽管有一系列关于改进SFM的并行下界的工作，但在我们的工作之前，并行SFM的唯一已知算法要么遵循序列SFM的更通用方法，要么遵循凸$\ell_2$-Lipschitz函数的高度并行最小化。有趣的是，为了获得我们的第二个结果，我们提供了在超立方体上最小化$\ell_\infty$-Lipschitz函数的第一个高度并行算法，该算法获得了接近最优的深度，以获得恒定的精度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.04643v1" target="_blank">2309.04643v1</a>
                              </td>
                              <td>Parallel Submodular Function Minimization</td>
                              <td>Deeparnab Chakrabarty</td>
                              <td>2023-09-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_04643v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.04643v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_04147v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Robot Localization and Mapping Final Report -- Sequential Adversarial Learning for Self-Supervised Deep Visual Odometry</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_04147v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_04147v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_04147v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual odometry (VO) and SLAM have been using multi-view geometry via local structure from motion for decades. These methods have a slight disadvantage in challenging scenarios such as low-texture images, dynamic scenarios, etc. Meanwhile, use of deep neural networks to extract high level features is ubiquitous in computer vision. For VO, we can use these deep networks to extract depth and pose estimates using these high level features. The visual odometry task then can be modeled as an image generation task where the pose estimation is the by-product. This can also be achieved in a self-supervised manner, thereby eliminating the data (supervised) intensive nature of training deep neural networks. Although some works tried the similar approach [1], the depth and pose estimation in the previous works are vague sometimes resulting in accumulation of error (drift) along the trajectory. The goal of this work is to tackle these limitations of past approaches and to develop a method that can provide better depths and pose estimates. To address this, a couple of approaches are explored: 1) Modeling: Using optical flow and recurrent neural networks (RNN) in order to exploit spatio-temporal correlations which can provide more information to estimate depth. 2) Loss function: Generative adversarial network (GAN) [2] is deployed to improve the depth estimation (and thereby pose too), as shown in Figure 1. This additional loss term improves the realism in generated images and reduces artifacts.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_04147v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>几十年来，视觉里程计（VO）和SLAM一直通过运动的局部结构使用多视图几何。这些方法在低纹理图像、动态场景等具有挑战性的场景中稍有不足。同时，使用深度神经网络提取高级特征在计算机视觉中无处不在。对于VO，我们可以使用这些深度网络来提取使用这些高级特征的深度和姿态估计。视觉里程计任务然后可以被建模为图像生成任务，其中姿态估计是副产品。这也可以以自监督的方式实现，从而消除训练深度神经网络的数据（监督）密集性质。尽管一些工作尝试了类似的方法[1]，但先前工作中的深度和姿态估计是模糊的，有时会导致沿轨迹的误差（漂移）累积。这项工作的目标是解决过去方法的这些局限性，并开发一种可以提供更好深度和姿态估计的方法。为了解决这一问题，我们探索了几种方法：1）建模：使用光流和递归神经网络（RNN）来利用时空相关性，这可以提供更多信息来估计深度。2） 损失函数：部署生成对抗性网络（GAN）[2]以改进深度估计（从而也提高姿态），如图1所示。这个额外的损失项提高了生成图像的真实性并减少了伪影。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.04147v1" target="_blank">2309.04147v1</a>
                              </td>
                              <td>Robot Localization and Mapping Final Report -- Sequential Adversarial Learning for Self-Supervised Deep Visual Odometry</td>
                              <td>Akankshya Kar</td>
                              <td>2023-09-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_04147v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.04147v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_02420v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Doppelgangers: Learning to Disambiguate Images of Similar Structures</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_02420v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_02420v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_02420v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We consider the visual disambiguation task of determining whether a pair of visually similar images depict the same or distinct 3D surfaces (e.g., the same or opposite sides of a symmetric building). Illusory image matches, where two images observe distinct but visually similar 3D surfaces, can be challenging for humans to differentiate, and can also lead 3D reconstruction algorithms to produce erroneous results. We propose a learning-based approach to visual disambiguation, formulating it as a binary classification task on image pairs. To that end, we introduce a new dataset for this problem, Doppelgangers, which includes image pairs of similar structures with ground truth labels. We also design a network architecture that takes the spatial distribution of local keypoints and matches as input, allowing for better reasoning about both local and global cues. Our evaluation shows that our method can distinguish illusory matches in difficult cases, and can be integrated into SfM pipelines to produce correct, disambiguated 3D reconstructions. See our project page for our code, datasets, and more results: http://doppelgangers-3d.github.io/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_02420v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们考虑的视觉消歧任务是确定一对视觉相似的图像是否描绘了相同或不同的3D表面（例如，对称建筑的相同或相反侧）。两张图像观察到不同但在视觉上相似的3D表面，这对人类来说可能很难区分，也可能导致3D重建算法产生错误的结果。我们提出了一种基于学习的视觉消歧方法，将其表述为图像对的二元分类任务。为此，我们为这个问题引入了一个新的数据集，即Doppelgangers，它包括具有基本事实标签的相似结构的图像对。我们还设计了一种网络架构，将局部关键点和匹配的空间分布作为输入，从而能够更好地对局部和全局线索进行推理。我们的评估表明，我们的方法可以在困难的情况下区分虚幻的匹配，并可以集成到SfM管道中，以产生正确的、消除歧义的3D重建。有关我们的代码、数据集和更多结果，请参阅我们的项目页面：http://doppelgangers-3d.github.io/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.02420v1" target="_blank">2309.02420v1</a>
                              </td>
                              <td>Doppelgangers: Learning to Disambiguate Images of Similar Structures</td>
                              <td>Ruojin Cai</td>
                              <td>2023-09-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_02420v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.02420v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_08479v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DeDoDe: Detect, Don't Describe -- Describe, Don't Detect for Local Feature Matching</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_08479v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_08479v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_08479v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Keypoint detection is a pivotal step in 3D reconstruction, whereby sets of (up to) K points are detected in each view of a scene. Crucially, the detected points need to be consistent between views, i.e., correspond to the same 3D point in the scene. One of the main challenges with keypoint detection is the formulation of the learning objective. Previous learning-based methods typically jointly learn descriptors with keypoints, and treat the keypoint detection as a binary classification task on mutual nearest neighbours. However, basing keypoint detection on descriptor nearest neighbours is a proxy task, which is not guaranteed to produce 3D-consistent keypoints. Furthermore, this ties the keypoints to a specific descriptor, complicating downstream usage. In this work, we instead learn keypoints directly from 3D consistency. To this end, we train the detector to detect tracks from large-scale SfM. As these points are often overly sparse, we derive a semi-supervised two-view detection objective to expand this set to a desired number of detections. To train a descriptor, we maximize the mutual nearest neighbour objective over the keypoints with a separate network. Results show that our approach, DeDoDe, achieves significant gains on multiple geometry benchmarks. Code is provided at https://github.com/Parskatt/DeDoDe</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_08479v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>关键点检测是3D重建中的关键步骤，通过该步骤可以在场景的每个视图中检测到（最多）K个点的集合。至关重要的是，检测到的点需要在视图之间保持一致，即对应于场景中的同一3D点。关键点检测的主要挑战之一是学习目标的制定。以前基于学习的方法通常将描述符与关键点联合学习，并将关键点检测视为对相互最近邻居的二元分类任务。然而，基于描述符最近邻居的关键点检测是一项代理任务，不能保证产生3D一致的关键点。此外，这将关键点与特定描述符联系在一起，使下游使用变得复杂。在这项工作中，我们直接从3D一致性中学习关键点。为此，我们训练检测器来检测大规模SfM中的轨道。由于这些点通常过于稀疏，我们导出了一个半监督的双视图检测目标，以将该集扩展到所需的检测数量。为了训练描述符，我们使用单独的网络在关键点上最大化相互最近邻目标。结果表明，我们的方法DeDoDe在多个几何基准上实现了显著的增益。代码提供于https://github.com/Parskatt/DeDoDe</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.08479v2" target="_blank">2308.08479v2</a>
                              </td>
                              <td>DeDoDe: Detect, Don't Describe -- Describe, Don't Detect for Local Feature Matching</td>
                              <td>Johan Edstedt</td>
                              <td>2023-08-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_08479v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.08479v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_00526v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SQLdepth: Generalizable Self-Supervised Fine-Structured Monocular Depth Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_00526v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_00526v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_00526v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, self-supervised monocular depth estimation has gained popularity with numerous applications in autonomous driving and robotics. However, existing solutions primarily seek to estimate depth from immediate visual features, and struggle to recover fine-grained scene details with limited generalization. In this paper, we introduce SQLdepth, a novel approach that can effectively learn fine-grained scene structures from motion. In SQLdepth, we propose a novel Self Query Layer (SQL) to build a self-cost volume and infer depth from it, rather than inferring depth from feature maps. The self-cost volume implicitly captures the intrinsic geometry of the scene within a single frame. Each individual slice of the volume signifies the relative distances between points and objects within a latent space. Ultimately, this volume is compressed to the depth map via a novel decoding approach. Experimental results on KITTI and Cityscapes show that our method attains remarkable state-of-the-art performance (AbsRel = $0.082$ on KITTI, $0.052$ on KITTI with improved ground-truth and $0.106$ on Cityscapes), achieves $9.9\%$, $5.5\%$ and $4.5\%$ error reduction from the previous best. In addition, our approach showcases reduced training complexity, computational efficiency, improved generalization, and the ability to recover fine-grained scene details. Moreover, the self-supervised pre-trained and metric fine-tuned SQLdepth can surpass existing supervised methods by significant margins (AbsRel = $0.043$, $14\%$ error reduction). self-matching-oriented relative distance querying in SQL improves the robustness and zero-shot generalization capability of SQLdepth. Code and the pre-trained weights will be publicly available. Code is available at \href{https://github.com/hisfog/SQLdepth-Impl}{https://github.com/hisfog/SQLdepth-Impl}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_00526v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，自监督单目深度估计在自动驾驶和机器人技术中得到了广泛应用。然而，现有的解决方案主要寻求从即时视觉特征估计深度，并且难以在有限的泛化能力下恢复细粒度的场景细节。在本文中，我们介绍了SQLdepth，这是一种可以有效地从运动中学习细粒度场景结构的新方法。在SQLdepth中，我们提出了一种新的自查询层（SQL）来构建自成本体积并从中推断深度，而不是从特征图中推断深度。自成本体积隐含地捕捉单个帧内场景的固有几何体。体积的每个单独切片表示潜在空间内的点和对象之间的相对距离。最终，通过一种新颖的解码方法将该体积压缩到深度图中。在KITTI和Cityscapes上的实验结果表明，我们的方法获得了显著的最先进的性能（在KITTI上AbsRel=0.082$，在具有改进的地面实况的KITTI上0.052$，在Cityscape上0.106$），与以前的最佳方法相比，实现了9.9\%$、5.5\%$和4.5\%$的误差降低。此外，我们的方法展示了降低的训练复杂性、计算效率、改进的泛化能力以及恢复细粒度场景细节的能力。此外，自监督预训练和度量微调的SQLdepth可以显著超过现有的监督方法（AbsRel=0.043$，误差减少$14\%$）。SQL中面向自匹配的相对距离查询提高了SQLdepth的鲁棒性和零样本泛化能力。代码和预先训练的重量将公开。代码位于\ href{https://github.com/hisfog/SQLdepth-Impl}{https://github.com/hisfog/SQLdepth-Impl}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.00526v1" target="_blank">2309.00526v1</a>
                              </td>
                              <td>SQLdepth: Generalizable Self-Supervised Fine-Structured Monocular Depth Estimation</td>
                              <td>Youhong Wang</td>
                              <td>2023-09-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_00526v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.00526v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2208_00487v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">One Object at a Time: Accurate and Robust Structure From Motion for Robots</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2208_00487v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2208_00487v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2208_00487v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A gaze-fixating robot perceives distance to the fixated object and relative positions of surrounding objects immediately, accurately, and robustly. We show how fixation, which is the act of looking at one object while moving, exploits regularities in the geometry of 3D space to obtain this information. These regularities introduce rotation-translation couplings that are not commonly used in structure from motion. To validate, we use a Franka Emika Robot with an RGB camera. We a) find that error in distance estimate is less than 5 mm at a distance of 15 cm, and b) show how relative position can be used to find obstacles under challenging scenarios. We combine accurate distance estimates and obstacle information into a reactive robot behavior that is able to pick up objects of unknown size, while impeded by unforeseen obstacles. Project page: https://oxidification.com/p/one-object-at-a-time/ .</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2208_00487v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>注视机器人可以立即、准确、稳健地感知到被注视物体的距离和周围物体的相对位置。我们展示了注视，即在移动时看着一个物体的行为，是如何利用三维空间几何中的规律来获得这些信息的。这些规律引入了旋转-平移耦合，这种耦合在结构中并不常用。为了验证，我们使用了一个带有RGB相机的Franka Emika机器人。我们a）发现，在15厘米的距离上，距离估计的误差小于5毫米，b）展示了在具有挑战性的场景下如何使用相对位置来寻找障碍物。我们将准确的距离估计和障碍物信息结合到反应机器人行为中，该行为能够拾取未知大小的物体，同时受到不可预见的障碍物的阻碍。项目页面：https://oxidification.com/p/one-object-at-a-time/。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2208.00487v3" target="_blank">2208.00487v3</a>
                              </td>
                              <td>One Object at a Time: Accurate and Robust Structure From Motion for Robots</td>
                              <td>Aravind Battaje</td>
                              <td>2022-07-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2208_00487v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2208.00487v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_00385v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Dense Voxel 3D Reconstruction Using a Monocular Event Camera</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_00385v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_00385v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_00385v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Event cameras are sensors inspired by biological systems that specialize in capturing changes in brightness. These emerging cameras offer many advantages over conventional frame-based cameras, including high dynamic range, high frame rates, and extremely low power consumption. Due to these advantages, event cameras have increasingly been adapted in various fields, such as frame interpolation, semantic segmentation, odometry, and SLAM. However, their application in 3D reconstruction for VR applications is underexplored. Previous methods in this field mainly focused on 3D reconstruction through depth map estimation. Methods that produce dense 3D reconstruction generally require multiple cameras, while methods that utilize a single event camera can only produce a semi-dense result. Other single-camera methods that can produce dense 3D reconstruction rely on creating a pipeline that either incorporates the aforementioned methods or other existing Structure from Motion (SfM) or Multi-view Stereo (MVS) methods. In this paper, we propose a novel approach for solving dense 3D reconstruction using only a single event camera. To the best of our knowledge, our work is the first attempt in this regard. Our preliminary results demonstrate that the proposed method can produce visually distinguishable dense 3D reconstructions directly without requiring pipelines like those used by existing methods. Additionally, we have created a synthetic dataset with $39,739$ object scans using an event camera simulator. This dataset will help accelerate other relevant research in this field.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_00385v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>事件摄像机是受生物系统启发，专门捕捉亮度变化的传感器。与传统的基于帧的相机相比，这些新兴相机具有许多优势，包括高动态范围、高帧率和极低功耗。由于这些优势，事件摄像机越来越多地应用于各个领域，如帧插值、语义分割、里程计和SLAM。然而，它们在VR应用的3D重建中的应用还没有得到充分的探索。该领域以前的方法主要集中在通过深度图估计进行三维重建。产生密集3D重建的方法通常需要多个相机，而利用单个事件相机的方法只能产生半密集的结果。可以产生密集3D重建的其他单相机方法依赖于创建管道，该管道结合了上述方法或其他现有的运动结构（SfM）或多视图立体（MVS）方法。在本文中，我们提出了一种仅使用单个事件相机来解决密集三维重建的新方法。据我们所知，我们的工作是这方面的第一次尝试。我们的初步结果表明，所提出的方法可以直接产生视觉上可区分的密集三维重建，而不需要像现有方法那样使用管道。此外，我们还使用事件相机模拟器创建了一个合成数据集，其中包含39739$的对象扫描。该数据集将有助于加速该领域的其他相关研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.00385v1" target="_blank">2309.00385v1</a>
                              </td>
                              <td>Dense Voxel 3D Reconstruction Using a Monocular Event Camera</td>
                              <td>Haodong Chen</td>
                              <td>2023-09-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_00385v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.00385v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_10902v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CamP: Camera Preconditioning for Neural Radiance Fields</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_10902v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_10902v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_10902v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural Radiance Fields (NeRF) can be optimized to obtain high-fidelity 3D scene reconstructions of objects and large-scale scenes. However, NeRFs require accurate camera parameters as input -- inaccurate camera parameters result in blurry renderings. Extrinsic and intrinsic camera parameters are usually estimated using Structure-from-Motion (SfM) methods as a pre-processing step to NeRF, but these techniques rarely yield perfect estimates. Thus, prior works have proposed jointly optimizing camera parameters alongside a NeRF, but these methods are prone to local minima in challenging settings. In this work, we analyze how different camera parameterizations affect this joint optimization problem, and observe that standard parameterizations exhibit large differences in magnitude with respect to small perturbations, which can lead to an ill-conditioned optimization problem. We propose using a proxy problem to compute a whitening transform that eliminates the correlation between camera parameters and normalizes their effects, and we propose to use this transform as a preconditioner for the camera parameters during joint optimization. Our preconditioned camera optimization significantly improves reconstruction quality on scenes from the Mip-NeRF 360 dataset: we reduce error rates (RMSE) by 67% compared to state-of-the-art NeRF approaches that do not optimize for cameras like Zip-NeRF, and by 29% relative to state-of-the-art joint optimization approaches using the camera parameterization of SCNeRF. Our approach is easy to implement, does not significantly increase runtime, can be applied to a wide variety of camera parameterizations, and can straightforwardly be incorporated into other NeRF-like models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_10902v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经辐射场（NeRF）可以被优化以获得物体和大规模场景的高保真3D场景重建。然而，NeRF需要精确的相机参数作为输入——不准确的相机参数会导致渲染模糊。通常使用运动结构（SfM）方法作为NeRF的预处理步骤来估计外部和内部相机参数，但这些技术很少产生完美的估计。因此，先前的工作已经提出与NeRF一起联合优化相机参数，但这些方法在具有挑战性的设置中容易出现局部最小值。在这项工作中，我们分析了不同的相机参数化如何影响这个联合优化问题，并观察到标准参数化相对于小扰动在大小上表现出很大的差异，这可能导致病态优化问题。我们建议使用代理问题来计算白化变换，该变换消除了相机参数之间的相关性并归一化了它们的效果，并且我们建议在联合优化期间使用该变换作为相机参数的预处理器。我们的预处理相机优化显著提高了Mip-NeRF 360数据集场景的重建质量：与不针对Zip-NeRF等相机进行优化的最先进的NeRF方法相比，我们降低了67%的错误率（RMSE），与使用SCNeRF相机参数化的最先进联合优化方法相比，降低了29%。我们的方法易于实现，不会显著增加运行时间，可以应用于各种相机参数化，并且可以直接集成到其他类似NeRF的模型中。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.10902v2" target="_blank">2308.10902v2</a>
                              </td>
                              <td>CamP: Camera Preconditioning for Neural Radiance Fields</td>
                              <td>Keunhong Park</td>
                              <td>2023-08-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_10902v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.10902v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_15984v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Structure-from-Motion with Graph Attention Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_15984v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_15984v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_15984v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we tackle the problem of learning Structure-from-Motion (SfM) through the use of graph attention networks. SfM is a classic computer vision problem that is solved though iterative minimization of reprojection errors, referred to as Bundle Adjustment (BA), starting from a good initialization. In order to obtain a good enough initialization to BA, conventional methods rely on a sequence of sub-problems (such as pairwise pose estimation, pose averaging or triangulation) which provides an initial solution that can then be refined using BA. In this work we replace these sub-problems by learning a model that takes as input the 2D keypoints detected across multiple views, and outputs the corresponding camera poses and 3D keypoint coordinates. Our model takes advantage of graph neural networks to learn SfM-specific primitives, and we show that it can be used for fast inference of the reconstruction for new and unseen sequences. The experimental results show that the proposed model outperforms competing learning-based methods, and challenges COLMAP while having lower runtime.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_15984v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们通过使用图注意力网络来解决从运动中学习结构（SfM）的问题。SfM是一个经典的计算机视觉问题，通过迭代最小化重投影误差来解决，称为束调整（BA），从良好的初始化开始。为了获得对BA足够好的初始化，传统方法依赖于一系列子问题（如成对姿态估计、姿态平均或三角测量），这些子问题提供了一个初始解决方案，然后可以使用BA进行细化。在这项工作中，我们通过学习一个模型来替换这些子问题，该模型将在多个视图中检测到的2D关键点作为输入，并输出相应的相机姿势和3D关键点坐标。我们的模型利用图神经网络来学习SfM特定的基元，并表明它可以用于新的和看不见的序列的重建的快速推理。实验结果表明，所提出的模型优于竞争的基于学习的方法，并在具有较低运行时间的同时挑战了COLMAP。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.15984v1" target="_blank">2308.15984v1</a>
                              </td>
                              <td>Learning Structure-from-Motion with Graph Attention Networks</td>
                              <td>Lucas Brynte</td>
                              <td>2023-08-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_15984v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.15984v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_13903v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Disjoint Pose and Shape for 3D Face Reconstruction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_13903v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_13903v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_13903v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Existing methods for 3D face reconstruction from a few casually captured images employ deep learning based models along with a 3D Morphable Model(3DMM) as face geometry prior. Structure From Motion(SFM), followed by Multi-View Stereo (MVS), on the other hand, uses dozens of high-resolution images to reconstruct accurate 3D faces.However, it produces noisy and stretched-out results with only two views available. In this paper, taking inspiration from both these methods, we propose an end-to-end pipeline that disjointly solves for pose and shape to make the optimization stable and accurate. We use a face shape prior to estimate face pose and use stereo matching followed by a 3DMM to solve for the shape. The proposed method achieves end-to-end topological consistency, enables iterative face pose refinement procedure, and show remarkable improvement on both quantitative and qualitative results over existing state-of-the-art methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_13903v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现有的从一些随意捕捉的图像重建3D人脸的方法采用基于深度学习的模型以及3D变形模型（3DMM）作为人脸几何先验。另一方面，“运动结构”（SFM）和“多视图立体”（MVS）使用数十幅高分辨率图像来重建精确的3D人脸。然而，在只有两个视图可用的情况下，它会产生嘈杂和拉伸的结果。在本文中，从这两种方法中获得灵感，我们提出了一种端到端的流水线，该流水线对姿态和形状进行不相交求解，以使优化稳定准确。我们在估计面部姿势之前使用面部形状，并使用立体匹配，然后使用3DMM来求解形状。所提出的方法实现了端到端的拓扑一致性，实现了迭代人脸姿态精化过程，并在定量和定性结果上都比现有的最先进的方法有了显著的改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.13903v1" target="_blank">2308.13903v1</a>
                              </td>
                              <td>Disjoint Pose and Shape for 3D Face Reconstruction</td>
                              <td>Raja Kumar</td>
                              <td>2023-08-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_13903v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.13903v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_10003v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Efficient Multi-View Inverse Rendering Using a Hybrid Differentiable Rendering Method</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_10003v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_10003v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_10003v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recovering the shape and appearance of real-world objects from natural 2D images is a long-standing and challenging inverse rendering problem. In this paper, we introduce a novel hybrid differentiable rendering method to efficiently reconstruct the 3D geometry and reflectance of a scene from multi-view images captured by conventional hand-held cameras. Our method follows an analysis-by-synthesis approach and consists of two phases. In the initialization phase, we use traditional SfM and MVS methods to reconstruct a virtual scene roughly matching the real scene. Then in the optimization phase, we adopt a hybrid approach to refine the geometry and reflectance, where the geometry is first optimized using an approximate differentiable rendering method, and the reflectance is optimized afterward using a physically-based differentiable rendering method. Our hybrid approach combines the efficiency of approximate methods with the high-quality results of physically-based methods. Extensive experiments on synthetic and real data demonstrate that our method can produce reconstructions with similar or higher quality than state-of-the-art methods while being more efficient.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_10003v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从自然2D图像中恢复真实世界对象的形状和外观是一个长期存在且具有挑战性的反向渲染问题。在本文中，我们介绍了一种新的混合可微分渲染方法，以从传统手持相机拍摄的多视图图像中有效地重建场景的3D几何结构和反射率。我们的方法采用综合分析法，由两个阶段组成。在初始化阶段，我们使用传统的SfM和MVS方法来重建与真实场景大致匹配的虚拟场景。然后在优化阶段，我们采用混合方法来细化几何和反射率，其中首先使用近似可微分渲染方法优化几何，然后使用基于物理的可微分渲染法优化反射率。我们的混合方法将近似方法的效率与基于物理的方法的高质量结果相结合。对合成数据和真实数据的大量实验表明，我们的方法可以产生与最先进方法相似或更高质量的重建，同时更有效。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.10003v1" target="_blank">2308.10003v1</a>
                              </td>
                              <td>Efficient Multi-View Inverse Rendering Using a Hybrid Differentiable Rendering Method</td>
                              <td>Xiangyang Zhu</td>
                              <td>2023-08-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_10003v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.10003v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_15667v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_15667v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_15667v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_15667v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Camera pose estimation is a long-standing computer vision problem that to date often relies on classical methods, such as handcrafted keypoint matching, RANSAC and bundle adjustment. In this paper, we propose to formulate the Structure from Motion (SfM) problem inside a probabilistic diffusion framework, modelling the conditional distribution of camera poses given input images. This novel view of an old problem has several advantages. (i) The nature of the diffusion framework mirrors the iterative procedure of bundle adjustment. (ii) The formulation allows a seamless integration of geometric constraints from epipolar geometry. (iii) It excels in typically difficult scenarios such as sparse views with wide baselines. (iv) The method can predict intrinsics and extrinsics for an arbitrary amount of images. We demonstrate that our method PoseDiffusion significantly improves over the classic SfM pipelines and the learned approaches on two real-world datasets. Finally, it is observed that our method can generalize across datasets without further training. Project page: https://posediffusion.github.io/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_15667v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>相机姿态估计是一个长期存在的计算机视觉问题，迄今为止，它通常依赖于经典的方法，如手工关键点匹配、RANSAC和束调整。在本文中，我们建议在概率扩散框架内公式化运动结构（SfM）问题，对给定输入图像的相机姿态的条件分布进行建模。这种对老问题的新颖看法有几个优点。（i） 扩散框架的性质反映了束调整的迭代过程。（ii）该公式允许来自核极几何的几何约束的无缝集成。（iii）它在典型的困难场景中表现出色，例如具有宽基线的稀疏视图。（iv）该方法可以预测任意数量的图像的内在和外在。我们在两个真实世界的数据集上证明了我们的方法PoseDiffusion比经典的SfM管道和学习的方法有了显著的改进。最后，我们观察到，我们的方法可以在不需要进一步训练的情况下在数据集之间进行推广。项目页面：https://posediffusion.github.io/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.15667v3" target="_blank">2306.15667v3</a>
                              </td>
                              <td>PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment</td>
                              <td>Jianyuan Wang</td>
                              <td>2023-06-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_15667v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.15667v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_10705v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Unsupervised 3D Pose Estimation with Non-Rigid Structure-from-Motion Modeling</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_10705v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_10705v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_10705v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Most of the previous 3D human pose estimation work relied on the powerful memory capability of the network to obtain suitable 2D-3D mappings from the training data. Few works have studied the modeling of human posture deformation in motion. In this paper, we propose a new modeling method for human pose deformations and design an accompanying diffusion-based motion prior. Inspired by the field of non-rigid structure-from-motion, we divide the task of reconstructing 3D human skeletons in motion into the estimation of a 3D reference skeleton, and a frame-by-frame skeleton deformation. A mixed spatial-temporal NRSfMformer is used to simultaneously estimate the 3D reference skeleton and the skeleton deformation of each frame from 2D observations sequence, and then sum them to obtain the pose of each frame. Subsequently, a loss term based on the diffusion model is used to ensure that the pipeline learns the correct prior motion knowledge. Finally, we have evaluated our proposed method on mainstream datasets and obtained superior results outperforming the state-of-the-art.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_10705v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>以前的大多数3D人体姿态估计工作都依赖于网络强大的存储能力来从训练数据中获得合适的2D-3D映射。很少有研究人员对人体在运动中的姿势变形进行建模。在本文中，我们提出了一种新的人体姿态变形建模方法，并设计了一种基于扩散的运动先验。受运动中非刚性结构领域的启发，我们将重建运动中的三维人体骨骼的任务分为三维参考骨骼的估计和逐帧骨骼变形。使用混合时空NRSfMformer从2D观测序列中同时估计3D参考骨架和每个帧的骨架变形，然后将它们相加以获得每个帧的姿态。随后，使用基于扩散模型的损失项来确保管道学习正确的先验运动知识。最后，我们在主流数据集上评估了我们提出的方法，并获得了优于现有技术的优越结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.10705v1" target="_blank">2308.10705v1</a>
                              </td>
                              <td>Unsupervised 3D Pose Estimation with Non-Rigid Structure-from-Motion Modeling</td>
                              <td>Haorui Ji</td>
                              <td>2023-08-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_10705v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.10705v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2210_05020v4_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Spectral Sparsification for Communication-Efficient Collaborative Rotation and Translation Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2210_05020v4_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2210_05020v4_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2210_05020v4_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose fast and communication-efficient optimization algorithms for multi-robot rotation averaging and translation estimation problems that arise from collaborative simultaneous localization and mapping (SLAM), structure-from-motion (SfM), and camera network localization applications. Our methods are based on theoretical relations between the Hessians of the underlying Riemannian optimization problems and the Laplacians of suitably weighted graphs. We leverage these results to design a collaborative solver in which robots coordinate with a central server to perform approximate second-order optimization, by solving a Laplacian system at each iteration. Crucially, our algorithms permit robots to employ spectral sparsification to sparsify intermediate dense matrices before communication, and hence provide a mechanism to trade off accuracy with communication efficiency with provable guarantees. We perform rigorous theoretical analysis of our methods and prove that they enjoy (local) linear rate of convergence. Furthermore, we show that our methods can be combined with graduated non-convexity to achieve outlier-robust estimation. Extensive experiments on real-world SLAM and SfM scenarios demonstrate the superior convergence rate and communication efficiency of our methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2210_05020v4_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们为多机器人旋转平均和平移估计问题提出了快速且高效的优化算法，这些问题源于协同同步定位和映射（SLAM）、运动结构（SfM）和相机网络定位应用。我们的方法基于基本黎曼优化问题的Hessians和适当加权图的Laplacian之间的理论关系。我们利用这些结果设计了一个协作求解器，在该求解器中，机器人与中央服务器协调，通过在每次迭代中求解拉普拉斯系统来执行近似的二阶优化。至关重要的是，我们的算法允许机器人在通信前使用频谱稀疏化来稀疏中间密集矩阵，因此提供了一种在通信效率与精度之间进行权衡的机制，并提供了可证明的保证。我们对我们的方法进行了严格的理论分析，并证明它们具有（局部）线性收敛率。此外，我们证明了我们的方法可以与分级非凸性相结合，以实现异常值的鲁棒估计。在真实世界的SLAM和SfM场景上进行的大量实验证明了我们的方法优越的收敛速度和通信效率。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2210.05020v4" target="_blank">2210.05020v4</a>
                              </td>
                              <td>Spectral Sparsification for Communication-Efficient Collaborative Rotation and Translation Estimation</td>
                              <td>Yulun Tian</td>
                              <td>2022-10-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2210_05020v4_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2210.05020v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01246v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Tirtha -- An Automated Platform to Crowdsource Images and Create 3D Models of Heritage Sites</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01246v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01246v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01246v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Digital preservation of Cultural Heritage (CH) sites is crucial to protect them against damage from natural disasters or human activities. Creating 3D models of CH sites has become a popular method of digital preservation thanks to advancements in computer vision and photogrammetry. However, the process is time-consuming, expensive, and typically requires specialized equipment and expertise, posing challenges in resource-limited developing countries. Additionally, the lack of an open repository for 3D models hinders research and public engagement with their heritage. To address these issues, we propose Tirtha, a web platform for crowdsourcing images of CH sites and creating their 3D models. Tirtha utilizes state-of-the-art Structure from Motion (SfM) and Multi-View Stereo (MVS) techniques. It is modular, extensible and cost-effective, allowing for the incorporation of new techniques as photogrammetry advances. Tirtha is accessible through a web interface at https://tirtha.niser.ac.in and can be deployed on-premise or in a cloud environment. In our case studies, we demonstrate the pipeline's effectiveness by creating 3D models of temples in Odisha, India, using crowdsourced images. These models are available for viewing, interaction, and download on the Tirtha website. Our work aims to provide a dataset of crowdsourced images and 3D reconstructions for research in computer vision, heritage conservation, and related domains. Overall, Tirtha is a step towards democratizing digital preservation, primarily in resource-limited developing countries.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01246v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文化遗产（CH）遗址的数字保护对于保护它们免受自然灾害或人类活动的破坏至关重要。由于计算机视觉和摄影测量的进步，创建CH遗址的3D模型已成为一种流行的数字保存方法。然而，这一过程耗时、昂贵，通常需要专门的设备和专业知识，这对资源有限的发展中国家构成了挑战。此外，缺乏开放的3D模型存储库阻碍了研究和公众对其遗产的参与。为了解决这些问题，我们提出了Tirtha，一个用于众包CH网站图像并创建其3D模型的网络平台。Tirtha采用了最先进的运动结构（SfM）和多视图立体（MVS）技术。它是模块化的、可扩展的和具有成本效益的，允许随着摄影测量的发展而结合新技术。Tirtha可以通过以下网站的web界面访问：https://tirtha.niser.ac.in并且可以在内部部署或在云环境中部署。在我们的案例研究中，我们通过使用众包图像创建印度奥迪沙寺庙的3D模型来证明该管道的有效性。这些模型可在Tirtha网站上查看、交互和下载。我们的工作旨在为计算机视觉、遗产保护和相关领域的研究提供众包图像和3D重建的数据集。总的来说，Tirtha是朝着数字保护民主化迈出的一步，主要是在资源有限的发展中国家。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01246v2" target="_blank">2308.01246v2</a>
                              </td>
                              <td>Tirtha -- An Automated Platform to Crowdsource Images and Create 3D Models of Heritage Sites</td>
                              <td>Jyotirmaya Shivottam</td>
                              <td>2023-08-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01246v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01246v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_06794v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MMF-Track: Multi-modal Multi-level Fusion for 3D Single Object Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_06794v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_06794v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_06794v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>3D single object tracking plays a crucial role in computer vision. Mainstream methods mainly rely on point clouds to achieve geometry matching between target template and search area. However, textureless and incomplete point clouds make it difficult for single-modal trackers to distinguish objects with similar structures. To overcome the limitations of geometry matching, we propose a Multi-modal Multi-level Fusion Tracker (MMF-Track), which exploits the image texture and geometry characteristic of point clouds to track 3D target. Specifically, we first propose a Space Alignment Module (SAM) to align RGB images with point clouds in 3D space, which is the prerequisite for constructing inter-modal associations. Then, in feature interaction level, we design a Feature Interaction Module (FIM) based on dual-stream structure, which enhances intra-modal features in parallel and constructs inter-modal semantic associations. Meanwhile, in order to refine each modal feature, we introduce a Coarse-to-Fine Interaction Module (CFIM) to realize the hierarchical feature interaction at different scales. Finally, in similarity fusion level, we propose a Similarity Fusion Module (SFM) to aggregate geometry and texture clues from the target. Experiments show that our method achieves state-of-the-art performance on KITTI (39% Success and 42% Precision gains against previous multi-modal method) and is also competitive on NuScenes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_06794v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三维单目标跟踪在计算机视觉中起着至关重要的作用。主流的方法主要依靠点云来实现目标模板和搜索区域之间的几何匹配。然而，无纹理和不完整的点云使单模态跟踪器难以区分具有相似结构的对象。为了克服几何匹配的局限性，我们提出了一种多模态多级融合跟踪器（MMF Track），该跟踪器利用点云的图像纹理和几何特征来跟踪三维目标。具体来说，我们首先提出了一个空间对齐模块（SAM）来将RGB图像与3D空间中的点云对齐，这是构建模态间关联的先决条件。然后，在特征交互层面，我们设计了一个基于双流结构的特征交互模块，该模块并行增强模态内特征，构建模态间语义关联。同时，为了细化每个模态特征，我们引入了一个从粗到细的交互模块（CFIM）来实现不同尺度的层次特征交互。最后，在相似性融合层面，我们提出了一个相似性融合模块（SFM）来聚合来自目标的几何和纹理线索。实验表明，我们的方法在KITTI上实现了最先进的性能（与以前的多模态方法相比，成功率为39%，精度提高了42%），在NuScenes上也具有竞争力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.06794v2" target="_blank">2305.06794v2</a>
                              </td>
                              <td>MMF-Track: Multi-modal Multi-level Fusion for 3D Single Object Tracking</td>
                              <td>Zhiheng Li</td>
                              <td>2023-05-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_06794v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.06794v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_06147v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Efficient Large-scale AUV-based Visual Seafloor Mapping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_06147v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_06147v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_06147v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Driven by the increasing number of marine data science applications, there is a growing interest in surveying and exploring the vast, uncharted terrain of the deep sea with robotic platforms. Despite impressive results achieved by many on-land visual mapping algorithms in the past decades, transferring these methods from land to the deep sea remains a challenge due to harsh environmental conditions. Typically, deep-sea exploration involves the use of autonomous underwater vehicles (AUVs) equipped with high-resolution cameras and artificial illumination systems. However, images obtained in this manner often suffer from heterogeneous illumination and quality degradation due to attenuation and scattering, on top of refraction of light rays. All of this together often lets on-land SLAM approaches fail underwater or makes Structure-from-Motion approaches drift or omit difficult images, resulting in gaps, jumps or weakly registered areas. In this work, we present a system that incorporates recent developments in underwater imaging and visual mapping to facilitate automated robotic 3D reconstruction of hectares of seafloor. Our approach is efficient in that it detects and reconsiders difficult, weakly registered areas, to avoid omitting images and to make better use of limited dive time; on the other hand it is computationally efficient; leveraging a hybrid approach combining benefits from SLAM and Structure-from-Motion that runs much faster than incremental reconstructions while achieving at least on-par performance. The proposed system has been extensively tested and evaluated during several research cruises, demonstrating its robustness and practicality in real-world conditions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_06147v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在越来越多的海洋数据科学应用的推动下，人们对使用机器人平台测量和探索广阔、未知的深海地形越来越感兴趣。尽管在过去几十年中，许多陆地视觉地图算法取得了令人印象深刻的成果，但由于恶劣的环境条件，将这些方法从陆地转移到深海仍然是一个挑战。通常，深海探测涉及使用配备高分辨率相机和人工照明系统的自动水下航行器。然而，以这种方式获得的图像除了光线的折射之外，还经常由于衰减和散射而遭受不均匀照明和质量下降。所有这些加在一起通常会使陆上SLAM方法在水下失败，或者使“运动结构”方法漂移或忽略困难的图像，从而导致间隙、跳跃或弱配准区域。在这项工作中，我们提出了一个系统，该系统结合了水下成像和视觉地图的最新发展，以促进机器人对公顷海底的自动3D重建。我们的方法是有效的，因为它检测并重新考虑困难的、弱配准的区域，以避免遗漏图像，并更好地利用有限的潜水时间；另一方面，它在计算上是高效的；利用结合SLAM和Structure from Motion的优点的混合方法，该方法比增量重建运行得快得多，同时至少实现了同等性能。所提出的系统在几次研究巡航中进行了广泛的测试和评估，证明了其在现实世界条件下的稳健性和实用性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.06147v1" target="_blank">2308.06147v1</a>
                              </td>
                              <td>Efficient Large-scale AUV-based Visual Seafloor Mapping</td>
                              <td>Mengkun She</td>
                              <td>2023-08-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_06147v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.06147v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_10544v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">EC-SfM: Efficient Covisibility-based Structure-from-Motion for Both Sequential and Unordered Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_10544v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_10544v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_10544v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Structure-from-Motion is a technology used to obtain scene structure through image collection, which is a fundamental problem in computer vision. For unordered Internet images, SfM is very slow due to the lack of prior knowledge about image overlap. For sequential images, knowing the large overlap between adjacent frames, SfM can adopt a variety of acceleration strategies, which are only applicable to sequential data. To further improve the reconstruction efficiency and break the gap of strategies between these two kinds of data, this paper presents an efficient covisibility-based incremental SfM. Different from previous methods, we exploit covisibility and registration dependency to describe the image connection which is suitable to any kind of data. Based on this general image connection, we propose a unified framework to efficiently reconstruct sequential images, unordered images, and the mixture of these two. Experiments on the unordered images and mixed data verify the effectiveness of the proposed method, which is three times faster than the state of the art on feature matching, and an order of magnitude faster on reconstruction without sacrificing the accuracy. The source code is publicly available at https://github.com/openxrlab/xrsfm</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_10544v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>运动结构是一种通过图像采集获得场景结构的技术，是计算机视觉中的一个基本问题。对于无序的互联网图像，由于缺乏图像重叠的先验知识，SfM非常慢。对于序列图像，由于知道相邻帧之间有很大的重叠，SfM可以采用各种加速策略，这些策略仅适用于序列数据。为了进一步提高重建效率，打破这两种数据之间策略的差距，本文提出了一种有效的基于共视性的增量SfM。与以往的方法不同，我们利用共视性和配准依赖性来描述适用于任何类型数据的图像连接。基于这种通用的图像连接，我们提出了一个统一的框架来有效地重建序列图像、无序图像以及这两者的混合图像。在无序图像和混合数据上的实验验证了所提出方法的有效性，该方法在特征匹配方面比现有技术快三倍，在不牺牲精度的情况下重建速度快一个数量级。源代码可在https://github.com/openxrlab/xrsfm</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.10544v2" target="_blank">2302.10544v2</a>
                              </td>
                              <td>EC-SfM: Efficient Covisibility-based Structure-from-Motion for Both Sequential and Unordered Images</td>
                              <td>Zhichao Ye</td>
                              <td>2023-02-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_10544v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.10544v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_02670v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">EDI: ESKF-based Disjoint Initialization for Visual-Inertial SLAM Systems</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_02670v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_02670v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_02670v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual-inertial initialization can be classified into joint and disjoint approaches. Joint approaches tackle both the visual and the inertial parameters together by aligning observations from feature-bearing points based on IMU integration then use a closed-form solution with visual and acceleration observations to find initial velocity and gravity. In contrast, disjoint approaches independently solve the Structure from Motion (SFM) problem and determine inertial parameters from up-to-scale camera poses obtained from pure monocular SLAM. However, previous disjoint methods have limitations, like assuming negligible acceleration bias impact or accurate rotation estimation by pure monocular SLAM. To address these issues, we propose EDI, a novel approach for fast, accurate, and robust visual-inertial initialization. Our method incorporates an Error-state Kalman Filter (ESKF) to estimate gyroscope bias and correct rotation estimates from monocular SLAM, overcoming dependence on pure monocular SLAM for rotation estimation. To estimate the scale factor without prior information, we offer a closed-form solution for initial velocity, scale, gravity, and acceleration bias estimation. To address gravity and acceleration bias coupling, we introduce weights in the linear least-squares equations, ensuring acceleration bias observability and handling outliers. Extensive evaluation on the EuRoC dataset shows that our method achieves an average scale error of 5.8% in less than 3 seconds, outperforming other state-of-the-art disjoint visual-inertial initialization approaches, even in challenging environments and with artificial noise corruption.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_02670v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉惯性初始化可以分为联合方法和不相交方法。联合方法通过基于IMU积分对齐特征承载点的观测结果，将视觉和惯性参数结合在一起，然后使用视觉和加速度观测的闭合形式解来找到初始速度和重力。相反，不相交的方法独立地解决了运动结构（SFM）问题，并根据从纯单目SLAM获得的高比例相机姿态确定惯性参数。然而，以前的不相交方法有局限性，比如假设加速度偏差影响可以忽略不计，或者通过纯单目SLAM进行精确的旋转估计。为了解决这些问题，我们提出了EDI，这是一种快速、准确和稳健的视觉惯性初始化的新方法。我们的方法结合了误差状态卡尔曼滤波器（ESKF）来估计陀螺仪偏差，并校正单目SLAM的旋转估计，克服了对纯单目SLAM旋转估计的依赖。为了在没有先验信息的情况下估计比例因子，我们为初始速度、比例、重力和加速度偏差估计提供了一个闭合形式的解决方案。为了解决重力和加速度偏差的耦合问题，我们在线性最小二乘方程中引入了权重，以确保加速度偏差的可观察性并处理异常值。对EuRoC数据集的广泛评估表明，我们的方法在不到3秒内实现了5.8%的平均尺度误差，即使在具有挑战性的环境和人工噪声破坏的情况下，也优于其他最先进的不相交视觉惯性初始化方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.02670v1" target="_blank">2308.02670v1</a>
                              </td>
                              <td>EDI: ESKF-based Disjoint Initialization for Visual-Inertial SLAM Systems</td>
                              <td>Weihan Wang</td>
                              <td>2023-08-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_02670v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.02670v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01125v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Stereo Visual Odometry with Deep Learning-Based Point and Line Feature Matching using an Attention Graph Neural Network</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01125v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01125v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01125v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Robust feature matching forms the backbone for most Visual Simultaneous Localization and Mapping (vSLAM), visual odometry, 3D reconstruction, and Structure from Motion (SfM) algorithms. However, recovering feature matches from texture-poor scenes is a major challenge and still remains an open area of research. In this paper, we present a Stereo Visual Odometry (StereoVO) technique based on point and line features which uses a novel feature-matching mechanism based on an Attention Graph Neural Network that is designed to perform well even under adverse weather conditions such as fog, haze, rain, and snow, and dynamic lighting conditions such as nighttime illumination and glare scenarios. We perform experiments on multiple real and synthetic datasets to validate the ability of our method to perform StereoVO under low visibility weather and lighting conditions through robust point and line matches. The results demonstrate that our method achieves more line feature matches than state-of-the-art line matching algorithms, which when complemented with point feature matches perform consistently well in adverse weather and dynamic lighting conditions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01125v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>稳健的特征匹配构成了大多数视觉同步定位和映射（vSLAM）、视觉里程计、3D重建和运动结构（SfM）算法的支柱。然而，从纹理差的场景中恢复特征匹配是一个重大挑战，并且仍然是一个开放的研究领域。在本文中，我们提出了一种基于点和线特征的立体视觉Odometry（StereoVO）技术，该技术使用了一种新的基于注意力图神经网络的特征匹配机制，即使在雾、霾、雨和雪等恶劣天气条件以及夜间照明和眩光等动态照明条件下也能表现良好。我们在多个真实和合成数据集上进行了实验，以验证我们的方法通过稳健的点和线匹配在低能见度天气和照明条件下执行StereoVO的能力。结果表明，与最先进的线匹配算法相比，我们的方法实现了更多的线特征匹配，当与点特征匹配相补充时，线匹配算法在恶劣天气和动态照明条件下始终表现良好。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01125v1" target="_blank">2308.01125v1</a>
                              </td>
                              <td>Stereo Visual Odometry with Deep Learning-Based Point and Line Feature Matching using an Attention Graph Neural Network</td>
                              <td>Shenbagaraj Kannapiran</td>
                              <td>2023-08-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01125v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01125v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_11702v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SACReg: Scene-Agnostic Coordinate Regression for Visual Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_11702v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_11702v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_11702v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Scene coordinates regression (SCR), i.e., predicting 3D coordinates for every pixel of a given image, has recently shown promising potential. However, existing methods remain mostly scene-specific or limited to small scenes and thus hardly scale to realistic datasets. In this paper, we propose a new paradigm where a single generic SCR model is trained once to be then deployed to new test scenes, regardless of their scale and without further finetuning. For a given query image, it collects inputs from off-the-shelf image retrieval techniques and Structure-from-Motion databases: a list of relevant database images with sparse pointwise 2D-3D annotations. The model is based on the transformer architecture and can take a variable number of images and sparse 2D-3D annotations as input. It is trained on a few diverse datasets and significantly outperforms other scene regression approaches on several benchmarks, including scene-specific models, for visual localization. In particular, we set a new state of the art on the Cambridge localization benchmark, even outperforming feature-matching-based approaches.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_11702v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>场景坐标回归（SCR），即预测给定图像的每个像素的3D坐标，最近显示出了很有前途的潜力。然而，现有的方法大多是针对场景的，或者仅限于小场景，因此很难扩展到真实的数据集。在本文中，我们提出了一种新的范式，其中单个通用SCR模型被训练一次，然后部署到新的测试场景中，而不考虑其规模，也不需要进一步的微调。对于给定的查询图像，它从现成的图像检索技术和运动数据库的结构中收集输入：具有稀疏逐点2D-3D注释的相关数据库图像列表。该模型基于转换器架构，并且可以采用可变数量的图像和稀疏的2D-3D注释作为输入。它在几个不同的数据集上进行了训练，在视觉定位方面，它在几个基准测试（包括特定场景的模型）上显著优于其他场景回归方法。特别是，我们在剑桥本地化基准上设定了一个新的技术水平，甚至优于基于特征匹配的方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.11702v2" target="_blank">2307.11702v2</a>
                              </td>
                              <td>SACReg: Scene-Agnostic Coordinate Regression for Visual Localization</td>
                              <td>Jerome Revaud</td>
                              <td>2023-07-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_11702v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.11702v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_15055v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_15055v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_15055v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_15055v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce PointOdyssey, a large-scale synthetic dataset, and data generation framework, for the training and evaluation of long-term fine-grained tracking algorithms. Our goal is to advance the state-of-the-art by placing emphasis on long videos with naturalistic motion. Toward the goal of naturalism, we animate deformable characters using real-world motion capture data, we build 3D scenes to match the motion capture environments, and we render camera viewpoints using trajectories mined via structure-from-motion on real videos. We create combinatorial diversity by randomizing character appearance, motion profiles, materials, lighting, 3D assets, and atmospheric effects. Our dataset currently includes 104 videos, averaging 2,000 frames long, with orders of magnitude more correspondence annotations than prior work. We show that existing methods can be trained from scratch in our dataset and outperform the published variants. Finally, we introduce modifications to the PIPs point tracking method, greatly widening its temporal receptive field, which improves its performance on PointOdyssey as well as on two real-world benchmarks. Our data and code are publicly available at: https://pointodyssey.com</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_15055v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了PointOdyssey，一个大规模的合成数据集和数据生成框架，用于长期细粒度跟踪算法的训练和评估。我们的目标是通过强调自然运动的长视频来推进最先进的技术。为了实现自然主义的目标，我们使用真实世界的运动捕捉数据制作可变形角色的动画，我们构建3D场景以匹配运动捕捉环境，我们使用通过真实视频上的运动结构挖掘的轨迹来渲染相机视点。我们通过随机化角色外观、运动剖面、材质、照明、3D资产和大气效果来创造组合多样性。我们的数据集目前包括104个视频，平均2000帧长，与之前的工作相比，对应注释多了几个数量级。我们表明，现有的方法可以在我们的数据集中从头开始训练，并且优于已发布的变体。最后，我们介绍了对PIP点跟踪方法的修改，极大地拓宽了其时间感受野，这提高了其在PointOdyssey和两个真实世界基准上的性能。我们的数据和代码可在以下网址公开获取：https://pointodyssey.com</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.15055v1" target="_blank">2307.15055v1</a>
                              </td>
                              <td>PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking</td>
                              <td>Yang Zheng</td>
                              <td>2023-07-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_15055v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.15055v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_07250v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fusing Structure from Motion and Simulation-Augmented Pose Regression from Optical Flow for Challenging Indoor Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_07250v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_07250v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_07250v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The localization of objects is a crucial task in various applications such as robotics, virtual and augmented reality, and the transportation of goods in warehouses. Recent advances in deep learning have enabled the localization using monocular visual cameras. While structure from motion (SfM) predicts the absolute pose from a point cloud, absolute pose regression (APR) methods learn a semantic understanding of the environment through neural networks. However, both fields face challenges caused by the environment such as motion blur, lighting changes, repetitive patterns, and feature-less structures. This study aims to address these challenges by incorporating additional information and regularizing the absolute pose using relative pose regression (RPR) methods. RPR methods suffer under different challenges, i.e., motion blur. The optical flow between consecutive images is computed using the Lucas-Kanade algorithm, and the relative pose is predicted using an auxiliary small recurrent convolutional network. The fusion of absolute and relative poses is a complex task due to the mismatch between the global and local coordinate systems. State-of-the-art methods fusing absolute and relative poses use pose graph optimization (PGO) to regularize the absolute pose predictions using relative poses. In this work, we propose recurrent fusion networks to optimally align absolute and relative pose predictions to improve the absolute pose prediction. We evaluate eight different recurrent units and construct a simulation environment to pre-train the APR and RPR networks for better generalized training. Additionally, we record a large database of different scenarios in a challenging large-scale indoor environment that mimics a warehouse with transportation robots. We conduct hyperparameter searches and experiments to show the effectiveness of our recurrent fusion method compared to PGO.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_07250v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>物体的定位在机器人、虚拟和增强现实以及仓库中的货物运输等各种应用中是一项至关重要的任务。深度学习的最新进展使得能够使用单目视觉相机进行定位。虽然运动结构（SfM）从点云预测绝对姿态，但绝对姿态回归（APR）方法通过神经网络学习对环境的语义理解。然而，这两个领域都面临着环境带来的挑战，如运动模糊、照明变化、重复模式和无特征结构。本研究旨在通过结合额外信息和使用相对姿态回归（RPR）方法规范绝对姿态来应对这些挑战。RPR方法面临不同的挑战，即运动模糊。使用Lucas Kanade算法计算连续图像之间的光流，并使用辅助的小递归卷积网络预测相对姿态。由于全局坐标系和局部坐标系之间的不匹配，绝对姿态和相对姿态的融合是一项复杂的任务。融合绝对姿态和相对姿态的现有技术方法使用姿态图优化（PGO）来使用相对姿态正则化绝对姿态预测。在这项工作中，我们提出了递归融合网络来优化绝对和相对姿态预测，以改进绝对姿态预测。我们评估了八个不同的递归单元，并构建了一个模拟环境来预训练APR和RPR网络，以便更好地进行广义训练。此外，我们在一个具有挑战性的大型室内环境中记录了不同场景的大型数据库，该环境模拟了带有运输机器人的仓库。我们进行了超参数搜索和实验，以显示与PGO相比，我们的递归融合方法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.07250v2" target="_blank">2304.07250v2</a>
                              </td>
                              <td>Fusing Structure from Motion and Simulation-Augmented Pose Regression from Optical Flow for Challenging Indoor Environments</td>
                              <td>Felix Ott</td>
                              <td>2023-04-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_07250v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.07250v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09981v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Lazy Visual Localization via Motion Averaging</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09981v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09981v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09981v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual (re)localization is critical for various applications in computer vision and robotics. Its goal is to estimate the 6 degrees of freedom (DoF) camera pose for each query image, based on a set of posed database images. Currently, all leading solutions are structure-based that either explicitly construct 3D metric maps from the database with structure-from-motion, or implicitly encode the 3D information with scene coordinate regression models. On the contrary, visual localization without reconstructing the scene in 3D offers clear benefits. It makes deployment more convenient by reducing database pre-processing time, releasing storage requirements, and remaining unaffected by imperfect reconstruction, etc. In this technical report, we demonstrate that it is possible to achieve high localization accuracy without reconstructing the scene from the database. The key to achieving this owes to a tailored motion averaging over database-query pairs. Experiments show that our visual localization proposal, LazyLoc, achieves comparable performance against state-of-the-art structure-based methods. Furthermore, we showcase the versatility of LazyLoc, which can be easily extended to handle complex configurations such as multi-query co-localization and camera rigs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09981v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉（再）定位对于计算机视觉和机器人的各种应用至关重要。其目标是基于一组摆姿势的数据库图像，估计每个查询图像的6个自由度（DoF）相机姿势。目前，所有领先的解决方案都是基于结构的，它们要么从数据库中显式地构建具有运动结构的3D度量图，要么用场景坐标回归模型隐式地编码3D信息。相反，在不重建3D场景的情况下进行视觉定位提供了明显的好处。它通过减少数据库预处理时间、释放存储需求、不受不完美重建的影响等方式使部署更加方便。在本技术报告中，我们证明了在不从数据库重建场景的情况下实现高定位精度是可能的。实现这一点的关键在于对数据库查询对进行定制的运动平均。实验表明，我们的视觉定位方案LazyLoc与最先进的基于结构的方法相比，具有相当的性能。此外，我们还展示了LazyLoc的多功能性，它可以很容易地扩展到处理复杂的配置，如多查询协同定位和相机钻机。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09981v1" target="_blank">2307.09981v1</a>
                              </td>
                              <td>Lazy Visual Localization via Motion Averaging</td>
                              <td>Siyan Dong</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09981v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09981v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07524v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Reducing Causality to Functions with Structural Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07524v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07524v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07524v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The precise definition of causality is currently an open problem in philosophy and statistics. We believe causality should be defined as functions (in mathematics) that map causes to effects. We propose a reductive definition of causality based on Structural Functional Model (SFM). Using delta compression and contrastive forward inference, SFM can produce causal utterances like "X causes Y" and "X is the cause of Y" that match our intuitions. We compile a dataset of causal scenarios and use SFM in all of them. SFM is compatible with but not reducible to probability theory. We also compare SFM with other theories of causation and apply SFM to downstream problems like free will, causal explanation, and mental causation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07524v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>因果关系的精确定义目前是哲学和统计学中一个悬而未决的问题。我们认为因果关系应该被定义为（在数学中）将原因映射到效果的函数。基于结构函数模型，我们提出了因果关系的简化定义。使用delta压缩和对比前向推理，SFM可以产生与我们的直觉相匹配的因果话语，如“X导致Y”和“X是Y的原因”。我们编译了一个因果场景的数据集，并在所有场景中使用SFM。SFM与概率论是相容的，但不可简化为概率论。我们还将SFM与其他因果关系理论进行了比较，并将SFM应用于自由意志、因果解释和精神因果关系等下游问题。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07524v1" target="_blank">2307.07524v1</a>
                              </td>
                              <td>Reducing Causality to Functions with Structural Models</td>
                              <td>Tianyi Miao</td>
                              <td>2023-07-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07524v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07524v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04520v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Efficient Match Pair Retrieval for Large-scale UAV Images via Graph Indexed Global Descriptor</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04520v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04520v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04520v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>SfM (Structure from Motion) has been extensively used for UAV (Unmanned Aerial Vehicle) image orientation. Its efficiency is directly influenced by feature matching. Although image retrieval has been extensively used for match pair selection, high computational costs are consumed due to a large number of local features and the large size of the used codebook. Thus, this paper proposes an efficient match pair retrieval method and implements an integrated workflow for parallel SfM reconstruction. First, an individual codebook is trained online by considering the redundancy of UAV images and local features, which avoids the ambiguity of training codebooks from other datasets. Second, local features of each image are aggregated into a single high-dimension global descriptor through the VLAD (Vector of Locally Aggregated Descriptors) aggregation by using the trained codebook, which remarkably reduces the number of features and the burden of nearest neighbor searching in image indexing. Third, the global descriptors are indexed via the HNSW (Hierarchical Navigable Small World) based graph structure for the nearest neighbor searching. Match pairs are then retrieved by using an adaptive threshold selection strategy and utilized to create a view graph for divide-and-conquer based parallel SfM reconstruction. Finally, the performance of the proposed solution has been verified using three large-scale UAV datasets. The test results demonstrate that the proposed solution accelerates match pair retrieval with a speedup ratio ranging from 36 to 108 and improves the efficiency of SfM reconstruction with competitive accuracy in both relative and absolute orientation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04520v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>SfM（Structure from Motion）已被广泛用于无人机（UAV）的图像定向。其效率直接受到特征匹配的影响。尽管图像检索已被广泛用于匹配对选择，但由于大量的局部特征和所使用的码本的大尺寸，消耗了高计算成本。因此，本文提出了一种高效的匹配对检索方法，并实现了一个用于并行SfM重建的集成工作流。首先，考虑无人机图像和局部特征的冗余性，在线训练单个码本，避免了其他数据集训练码本的模糊性。其次，通过使用训练后的码本进行VLAD（Vector of Locally aggregated Descriptors）聚合，将每个图像的局部特征聚合为单个高维全局描述符，显著减少了图像索引中特征的数量和最近邻搜索的负担。第三，通过基于HNSW（分层导航小世界）的图结构对全局描述符进行索引，用于最近邻居搜索。然后通过使用自适应阈值选择策略来检索匹配对，并用于创建用于基于分治的并行SfM重建的视图图。最后，使用三个大型无人机数据集验证了所提出的解决方案的性能。测试结果表明，所提出的解决方案以36到108的加速比加速了匹配对检索，并在相对和绝对方向上以具有竞争力的精度提高了SfM重建的效率。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04520v1" target="_blank">2307.04520v1</a>
                              </td>
                              <td>Efficient Match Pair Retrieval for Large-scale UAV Images via Graph Indexed Global Descriptor</td>
                              <td>San Jiang</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04520v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04520v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03404v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RGB-D Mapping and Tracking in a Plenoxel Radiance Field</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03404v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03404v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03404v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Building on the success of Neural Radiance Fields (NeRFs), recent years have seen significant advances in the domain of novel view synthesis. These models capture the scene's volumetric radiance field, creating highly convincing dense photorealistic models through the use of simple, differentiable rendering equations. Despite their popularity, these algorithms suffer from severe ambiguities in visual data inherent to the RGB sensor, which means that although images generated with view synthesis can visually appear very believable, the underlying 3D model will often be wrong. This considerably limits the usefulness of these models in practical applications like Robotics and Extended Reality (XR), where an accurate dense 3D reconstruction otherwise would be of significant value. In this technical report, we present the vital differences between view synthesis models and 3D reconstruction models. We also comment on why a depth sensor is essential for modeling accurate geometry in general outward-facing scenes using the current paradigm of novel view synthesis methods. Focusing on the structure-from-motion task, we practically demonstrate this need by extending the Plenoxel radiance field model: Presenting an analytical differential approach for dense mapping and tracking with radiance fields based on RGB-D data without a neural network. Our method achieves state-of-the-art results in both the mapping and tracking tasks while also being faster than competing neural network-based approaches.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03404v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在神经辐射场（NeRFs）成功的基础上，近年来在新视图合成领域取得了重大进展。这些模型捕捉了场景的体积辐射场，通过使用简单、可微分的渲染方程创建了令人信服的密集真实感模型。尽管这些算法很受欢迎，但RGB传感器固有的视觉数据存在严重的模糊性，这意味着尽管通过视图合成生成的图像在视觉上看起来非常可信，但底层的3D模型往往是错误的。这大大限制了这些模型在机器人和扩展现实（XR）等实际应用中的有用性，否则，精确的密集3D重建将具有重要价值。在本技术报告中，我们介绍了视图合成模型和三维重建模型之间的重要区别。我们还评论了为什么深度传感器对于使用当前新型视图合成方法的范式在一般面向外的场景中建模精确的几何体至关重要。专注于运动任务中的结构，我们通过扩展Plenoxel辐射场模型来实际证明这一需求：在没有神经网络的情况下，基于RGB-D数据，提出了一种用于辐射场密集映射和跟踪的解析微分方法。我们的方法在映射和跟踪任务中都取得了最先进的结果，同时也比竞争对手的基于神经网络的方法更快。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03404v1" target="_blank">2307.03404v1</a>
                              </td>
                              <td>RGB-D Mapping and Tracking in a Plenoxel Radiance Field</td>
                              <td>Andreas L. Teigen</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03404v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03404v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_01817v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Human Trajectory Forecasting with Explainable Behavioral Uncertainty</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_01817v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_01817v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_01817v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Human trajectory forecasting helps to understand and predict human behaviors, enabling applications from social robots to self-driving cars, and therefore has been heavily investigated. Most existing methods can be divided into model-free and model-based methods. Model-free methods offer superior prediction accuracy but lack explainability, while model-based methods provide explainability but cannot predict well. Combining both methodologies, we propose a new Bayesian Neural Stochastic Differential Equation model BNSP-SFM, where a behavior SDE model is combined with Bayesian neural networks (BNNs). While the NNs provide superior predictive power, the SDE offers strong explainability with quantifiable uncertainty in behavior and observation. We show that BNSP-SFM achieves up to a 50% improvement in prediction accuracy, compared with 11 state-of-the-art methods. BNSP-SFM also generalizes better to drastically different scenes with different environments and crowd densities (~ 20 times higher than the testing data). Finally, BNSP-SFM can provide predictions with confidence to better explain potential causes of behaviors. The code will be released upon acceptance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_01817v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人类轨迹预测有助于理解和预测人类行为，实现从社交机器人到自动驾驶汽车的应用，因此受到了大量研究。大多数现有的方法可以分为无模型方法和基于模型的方法。无模型方法提供了优越的预测精度但缺乏可解释性，而基于模型的方法提供了可解释性但不能很好地预测。结合这两种方法，我们提出了一个新的贝叶斯神经随机微分方程模型BNSP-SFM，其中行为SDE模型与贝叶斯神经网络（BNNs）相结合。虽然神经网络提供了卓越的预测能力，但SDE提供了强大的可解释性，在行为和观察方面具有可量化的不确定性。我们表明，与11种最先进的方法相比，BNSP-SFM的预测精度提高了50%。BNSP-SFM还可以更好地推广到具有不同环境和人群密度的截然不同的场景（比测试数据高出约20倍）。最后，BNSP-SFM可以提供有信心的预测，以更好地解释行为的潜在原因。该代码将在验收后发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.01817v1" target="_blank">2307.01817v1</a>
                              </td>
                              <td>Human Trajectory Forecasting with Explainable Behavioral Uncertainty</td>
                              <td>Jiangbei Yue</td>
                              <td>2023-07-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_01817v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.01817v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16917v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Drunkard's Odometry: Estimating Camera Motion in Deforming Scenes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16917v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16917v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16917v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Estimating camera motion in deformable scenes poses a complex and open research challenge. Most existing non-rigid structure from motion techniques assume to observe also static scene parts besides deforming scene parts in order to establish an anchoring reference. However, this assumption does not hold true in certain relevant application cases such as endoscopies. Deformable odometry and SLAM pipelines, which tackle the most challenging scenario of exploratory trajectories, suffer from a lack of robustness and proper quantitative evaluation methodologies. To tackle this issue with a common benchmark, we introduce the Drunkard's Dataset, a challenging collection of synthetic data targeting visual navigation and reconstruction in deformable environments. This dataset is the first large set of exploratory camera trajectories with ground truth inside 3D scenes where every surface exhibits non-rigid deformations over time. Simulations in realistic 3D buildings lets us obtain a vast amount of data and ground truth labels, including camera poses, RGB images and depth, optical flow and normal maps at high resolution and quality. We further present a novel deformable odometry method, dubbed the Drunkard's Odometry, which decomposes optical flow estimates into rigid-body camera motion and non-rigid scene deformations. In order to validate our data, our work contains an evaluation of several baselines as well as a novel tracking error metric which does not require ground truth data. Dataset and code: https://davidrecasens.github.io/TheDrunkard'sOdometry/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16917v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在可变形场景中估计相机运动是一个复杂而开放的研究挑战。大多数现有的非刚性结构运动技术假设除了变形场景部分之外，还观察静态场景部分，以建立锚定参考。然而，这一假设在某些相关应用案例中并不成立，例如内镜。可变形里程计和SLAM管道解决了最具挑战性的探索轨迹场景，但缺乏稳健性和适当的定量评估方法。为了用一个通用的基准来解决这个问题，我们引入了Drunkard的数据集，这是一个具有挑战性的合成数据集，旨在在可变形环境中进行视觉导航和重建。该数据集是第一个在3D场景中具有地面实况的大型探索相机轨迹集，其中每个表面随着时间的推移都表现出非刚性变形。在逼真的3D建筑中进行模拟可以让我们获得大量的数据和地面实况标签，包括相机姿态、RGB图像和深度、光流和高分辨率和高质量的法线图。我们进一步提出了一种新的可变形里程计方法，称为Drunkard里程计，该方法将光流估计分解为刚体相机运动和非刚体场景变形。为了验证我们的数据，我们的工作包括对几个基线的评估，以及一种新的跟踪误差度量，该度量不需要地面实况数据。数据集和代码：https://davidrecasens.github.io/TheDrunkard'国内/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16917v1" target="_blank">2306.16917v1</a>
                              </td>
                              <td>The Drunkard's Odometry: Estimating Camera Motion in Deforming Scenes</td>
                              <td>David Recasens</td>
                              <td>2023-06-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16917v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16917v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_15669v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Detector-Free Structure from Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_15669v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_15669v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_15669v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a new structure-from-motion framework to recover accurate camera poses and point clouds from unordered images. Traditional SfM systems typically rely on the successful detection of repeatable keypoints across multiple views as the first step, which is difficult for texture-poor scenes, and poor keypoint detection may break down the whole SfM system. We propose a new detector-free SfM framework to draw benefits from the recent success of detector-free matchers to avoid the early determination of keypoints, while solving the multi-view inconsistency issue of detector-free matchers. Specifically, our framework first reconstructs a coarse SfM model from quantized detector-free matches. Then, it refines the model by a novel iterative refinement pipeline, which iterates between an attention-based multi-view matching module to refine feature tracks and a geometry refinement module to improve the reconstruction accuracy. Experiments demonstrate that the proposed framework outperforms existing detector-based SfM systems on common benchmark datasets. We also collect a texture-poor SfM dataset to demonstrate the capability of our framework to reconstruct texture-poor scenes. Based on this framework, we take $\textit{first place}$ in Image Matching Challenge 2023.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_15669v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们从运动框架中提出了一种新的结构，以从无序图像中恢复准确的相机姿态和点云。传统的SfM系统通常依赖于跨多个视图的可重复关键点的成功检测作为第一步，这对于纹理较差的场景来说是困难的，并且较差的关键点检测可能会破坏整个SfM体系。我们提出了一种新的无检测器SfM框架，以从无检测器匹配器最近的成功中获益，避免早期确定关键点，同时解决无检测器匹配的多视图不一致问题。具体来说，我们的框架首先从量化的无检测器匹配中重建粗略的SfM模型。然后，它通过一种新的迭代精化流水线对模型进行精化，该流水线在基于注意力的多视图匹配模块和几何精化模块之间迭代以精化特征轨迹，从而提高重建精度。实验表明，该框架在通用基准数据集上优于现有的基于检测器的SfM系统。我们还收集了一个纹理较差的SfM数据集，以证明我们的框架重建纹理较差场景的能力。基于这个框架，我们在2023年的图像匹配挑战中获得$\textit｛first place｝$。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.15669v1" target="_blank">2306.15669v1</a>
                              </td>
                              <td>Detector-Free Structure from Motion</td>
                              <td>Xingyi He</td>
                              <td>2023-06-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_15669v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.15669v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="LLM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2310_16049v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_16049v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_16049v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_16049v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While large language models (LLMs) equipped with techniques like chain-of-thought prompting have demonstrated impressive capabilities, they still fall short in their ability to reason robustly in complex settings. However, evaluating LLM reasoning is challenging because system capabilities continue to grow while benchmark datasets for tasks like logical deduction have remained static. We introduce MuSR, a dataset for evaluating language models on multistep soft reasoning tasks specified in a natural language narrative. This dataset has two crucial features. First, it is created through a novel neurosymbolic synthetic-to-natural generation algorithm, enabling the construction of complex reasoning instances that challenge GPT-4 (e.g., murder mysteries roughly 1000 words in length) and which can be scaled further as more capable LLMs are released. Second, our dataset instances are free text narratives corresponding to real-world domains of reasoning; this makes it simultaneously much more challenging than other synthetically-crafted benchmarks while remaining realistic and tractable for human annotators to solve with high accuracy. We evaluate a range of LLMs and prompting techniques on this dataset and characterize the gaps that remain for techniques like chain-of-thought to perform robust reasoning.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_16049v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管配备了思维链提示等技术的大型语言模型（LLM）已经显示出了令人印象深刻的能力，但它们在复杂环境中稳健推理的能力仍然不足。然而，评估LLM推理具有挑战性，因为系统能力不断增长，而逻辑推理等任务的基准数据集保持不变。我们介绍了MuSR，这是一个用于评估自然语言叙述中指定的多步骤软推理任务的语言模型的数据集。该数据集具有两个关键特性。首先，它是通过一种新的神经符号合成到自然生成算法创建的，能够构建挑战GPT-4的复杂推理实例（例如，长度约为1000字的谋杀谜团），并且随着更强大的LLM的发布，这些推理实例可以进一步扩展。其次，我们的数据集实例是与现实世界推理领域相对应的自由文本叙述；这使得它同时比其他综合制作的基准更具挑战性，同时保持现实性和可操作性，供人类注释者以高精度解决。我们在这个数据集上评估了一系列LLM和提示技术，并描述了思想链等技术执行稳健推理所存在的差距。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.16049v1" target="_blank">2310.16049v1</a>
                              </td>
                              <td>MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning</td>
                              <td>Zayne Sprague</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_16049v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.16049v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_16048v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AI Alignment and Social Choice: Fundamental Limitations and Policy Implications</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_16048v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_16048v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_16048v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Aligning AI agents to human intentions and values is a key bottleneck in building safe and deployable AI applications. But whose values should AI agents be aligned with? Reinforcement learning with human feedback (RLHF) has emerged as the key framework for AI alignment. RLHF uses feedback from human reinforcers to fine-tune outputs; all widely deployed large language models (LLMs) use RLHF to align their outputs to human values. It is critical to understand the limitations of RLHF and consider policy challenges arising from these limitations. In this paper, we investigate a specific challenge in building RLHF systems that respect democratic norms. Building on impossibility results in social choice theory, we show that, under fairly broad assumptions, there is no unique voting protocol to universally align AI systems using RLHF through democratic processes. Further, we show that aligning AI agents with the values of all individuals will always violate certain private ethical preferences of an individual user i.e., universal AI alignment using RLHF is impossible. We discuss policy implications for the governance of AI systems built using RLHF: first, the need for mandating transparent voting rules to hold model builders accountable. Second, the need for model builders to focus on developing AI agents that are narrowly aligned to specific user groups.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_16048v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>将人工智能代理与人类意图和价值观相结合是构建安全和可部署的人工智能应用程序的关键瓶颈。但人工智能代理应该与谁的价值观保持一致？人类反馈强化学习（RLHF）已成为人工智能调整的关键框架。RLHF使用来自人类强化者的反馈来微调输出；所有广泛部署的大型语言模型（LLM）都使用RLHF来将其输出与人类价值观相一致。了解RLHF的局限性并考虑这些局限性带来的政策挑战至关重要。在本文中，我们调查了在建立尊重民主规范的RLHF系统方面的一个具体挑战。基于社会选择理论中的不可能结果，我们表明，在相当广泛的假设下，没有唯一的投票协议可以通过民主程序使用RLHF来普遍调整人工智能系统。此外，我们表明，将人工智能代理与所有个人的价值观相一致总是会违反个人用户的某些私人道德偏好，即，使用RLHF的通用人工智能对齐是不可能的。我们讨论了对使用RLHF构建的人工智能系统的治理的政策影响：首先，需要强制执行透明的投票规则，以追究模型构建者的责任。其次，模型构建者需要专注于开发与特定用户群体狭义一致的人工智能代理。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.16048v1" target="_blank">2310.16048v1</a>
                              </td>
                              <td>AI Alignment and Social Choice: Fundamental Limitations and Policy Implications</td>
                              <td>Abhilash Mishra</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_16048v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.16048v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_02019v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Baby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_02019v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_02019v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_02019v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present our submission to the BabyLM challenge, whose goal was to improve the sample efficiency of language models. We trained an ensemble consisting of a GPT-2 and small LLaMA models on the developmentally-plausible, 10M-word BabyLM dataset, then distilled it into a small, 58M-parameter LLaMA model, which exceeds in performance both of its teachers as well as a similar model trained without distillation. This suggests that distillation can not only retain the full performance of the teacher model when the latter is trained on a sufficiently small dataset; it can exceed it, and lead to significantly better performance than direct training.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_02019v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提交给BabyLM挑战，其目标是提高语言模型的样本效率。我们在发育合理的10M单词BabyLM数据集上训练了一个由GPT-2和小型LLaMA模型组成的集合，然后将其提炼成一个58M参数的小型LLaMA模型，该模型的性能超过了其教师以及未经提炼训练的类似模型。这表明，当教师模型在足够小的数据集上训练时，蒸馏不仅可以保持教师模型的全部性能；它可以超越它，并带来比直接训练更好的表现。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.02019v2" target="_blank">2308.02019v2</a>
                              </td>
                              <td>Baby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty</td>
                              <td>Inar Timiryasov</td>
                              <td>2023-08-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_02019v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.02019v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_16045v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Woodpecker: Hallucination Correction for Multimodal Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_16045v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_16045v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_16045v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Hallucination is a big shadow hanging over the rapidly evolving Multimodal Large Language Models (MLLMs), referring to the phenomenon that the generated text is inconsistent with the image content. In order to mitigate hallucinations, existing studies mainly resort to an instruction-tuning manner that requires retraining the models with specific data. In this paper, we pave a different way, introducing a training-free method named Woodpecker. Like a woodpecker heals trees, it picks out and corrects hallucinations from the generated text. Concretely, Woodpecker consists of five stages: key concept extraction, question formulation, visual knowledge validation, visual claim generation, and hallucination correction. Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages. We evaluate Woodpecker both quantitatively and qualitatively and show the huge potential of this new paradigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement in accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released at https://github.com/BradyFU/Woodpecker.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_16045v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>幻觉是笼罩在快速发展的多模式大型语言模型（MLLMs）上的一大阴影，指的是生成的文本与图像内容不一致的现象。为了减轻幻觉，现有的研究主要采用指令调整的方式，需要用特定的数据重新训练模型。在本文中，我们开辟了一条不同的道路，介绍了一种名为啄木鸟的无训练方法。就像啄木鸟治愈树木一样，它从生成的文本中挑选并纠正幻觉。具体来说，啄木鸟包括五个阶段：关键概念提取、问题制定、视觉知识验证、视觉主张生成和幻觉矫正。以补救后的方式实现，啄木鸟可以很容易地服务于不同的MLLM，同时可以通过访问五个阶段的中间输出进行解释。我们对啄木鸟进行了定量和定性评估，并展示了这一新范式的巨大潜力。在POPE基准上，我们的方法比基线MiniGPT-4/mPLUG Owl的准确度提高了30.66%/24.33%。源代码发布于https://github.com/BradyFU/Woodpecker.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.16045v1" target="_blank">2310.16045v1</a>
                              </td>
                              <td>Woodpecker: Hallucination Correction for Multimodal Large Language Models</td>
                              <td>Shukang Yin</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_16045v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.16045v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_16042v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">WebWISE: Web Interface Control and Sequential Exploration with Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_16042v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_16042v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_16042v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The paper investigates using a Large Language Model (LLM) to automatically perform web software tasks using click, scroll, and text input operations. Previous approaches, such as reinforcement learning (RL) or imitation learning, are inefficient to train and task-specific. Our method uses filtered Document Object Model (DOM) elements as observations and performs tasks step-by-step, sequentially generating small programs based on the current observations. We use in-context learning, either benefiting from a single manually provided example, or an automatically generated example based on a successful zero-shot trial. We evaluate the proposed method on the MiniWob++ benchmark. With only one in-context example, our WebWISE method achieves similar or better performance than other methods that require many demonstrations or trials.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_16042v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文研究了使用大型语言模型（LLM）通过点击、滚动和文本输入操作自动执行web软件任务。以前的方法，如强化学习或模仿学习，在训练和特定任务方面效率低下。我们的方法使用过滤后的文档对象模型（DOM）元素作为观察结果，并逐步执行任务，根据当前观察结果顺序生成小程序。我们使用上下文学习，或者受益于单个手动提供的示例，或者基于成功的零样本试验自动生成的示例。我们在MiniWob++基准上评估了所提出的方法。只有一个上下文示例，我们的WebWISE方法比其他需要多次演示或试验的方法实现了类似或更好的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.16042v1" target="_blank">2310.16042v1</a>
                              </td>
                              <td>WebWISE: Web Interface Control and Sequential Exploration with Large Language Models</td>
                              <td>Heyi Tao</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_16042v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.16042v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_16040v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Instruct and Extract: Instruction Tuning for On-Demand Information Extraction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_16040v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_16040v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_16040v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models with instruction-following capabilities open the door to a wider group of users. However, when it comes to information extraction - a classic task in natural language processing - most task-specific systems cannot align well with long-tail ad hoc extraction use cases for non-expert users. To address this, we propose a novel paradigm, termed On-Demand Information Extraction, to fulfill the personalized demands of real-world users. Our task aims to follow the instructions to extract the desired content from the associated text and present it in a structured tabular format. The table headers can either be user-specified or inferred contextually by the model. To facilitate research in this emerging area, we present a benchmark named InstructIE, inclusive of both automatically generated training data, as well as the human-annotated test set. Building on InstructIE, we further develop an On-Demand Information Extractor, ODIE. Comprehensive evaluations on our benchmark reveal that ODIE substantially outperforms the existing open-source models of similar size. Our code and dataset are released on https://github.com/yzjiao/On-Demand-IE.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_16040v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>具有指令跟随功能的大型语言模型为更广泛的用户群体打开了大门。然而，当涉及到信息提取（自然语言处理中的一项经典任务）时，大多数特定于任务的系统无法很好地与非专家用户的长尾特别提取用例保持一致。为了解决这一问题，我们提出了一种新的范式，称为按需信息提取，以满足现实世界用户的个性化需求。我们的任务旨在按照说明从相关文本中提取所需内容，并以结构化表格格式呈现。表头可以是用户指定的，也可以由模型根据上下文推断。为了促进这一新兴领域的研究，我们提出了一个名为InstructionIE的基准，包括自动生成的训练数据和人工注释的测试集。在InstructionIE的基础上，我们进一步开发了一个按需信息提取器ODIE。对我们基准的全面评估表明，ODIE大大优于现有的类似规模的开源模型。我们的代码和数据集发布于https://github.com/yzjiao/On-Demand-IE.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.16040v1" target="_blank">2310.16040v1</a>
                              </td>
                              <td>Instruct and Extract: Instruction Tuning for On-Demand Information Extraction</td>
                              <td>Yizhu Jiao</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_16040v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.16040v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_16035v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">What's Left? Concept Grounding with Logic-Enhanced Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_16035v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_16035v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_16035v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent works such as VisProg and ViperGPT have smartly composed foundation models for visual reasoning-using large language models (LLMs) to produce programs that can be executed by pre-trained vision-language models. However, they operate in limited domains, such as 2D images, not fully exploiting the generalization of language: abstract concepts like "left" can also be grounded in 3D, temporal, and action data, as in moving to your left. This limited generalization stems from these inference-only methods' inability to learn or adapt pre-trained models to a new domain. We propose the Logic-Enhanced Foundation Model (LEFT), a unified framework that learns to ground and reason with concepts across domains with a differentiable, domain-independent, first-order logic-based program executor. LEFT has an LLM interpreter that outputs a program represented in a general, logic-based reasoning language, which is shared across all domains and tasks. LEFT's executor then executes the program with trainable domain-specific grounding modules. We show that LEFT flexibly learns concepts in four domains: 2D images, 3D scenes, human motions, and robotic manipulation. It exhibits strong reasoning ability in a wide variety of tasks, including those that are complex and not seen during training, and can be easily applied to new domains.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_16035v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近的工作，如VisProg和ViperGPT，已经巧妙地构建了视觉推理的基础模型，使用大型语言模型（LLM）来生成可以由预先训练的视觉语言模型执行的程序。然而，它们在有限的领域中运作，例如2D图像，并没有充分利用语言的泛化：像“左”这样的抽象概念也可以基于3D、时间和动作数据，比如向左移动。这种有限的泛化源于这些仅推理的方法无法学习或将预先训练的模型适应新的领域。我们提出了逻辑增强基础模型（LEFT），这是一个统一的框架，通过一个可微的、独立于域的、基于一阶逻辑的程序执行器，学习跨域概念的基础和推理。LEFT有一个LLM解释器，它输出一个用通用的、基于逻辑的推理语言表示的程序，该语言在所有域和任务中共享。LEFT的执行器随后使用可训练的领域特定接地模块执行程序。我们展示了LEFT在四个领域灵活学习概念：2D图像、3D场景、人体运动和机器人操作。它在各种各样的任务中表现出强大的推理能力，包括那些在训练中看不到的复杂任务，并且可以很容易地应用于新的领域。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.16035v1" target="_blank">2310.16035v1</a>
                              </td>
                              <td>What's Left? Concept Grounding with Logic-Enhanced Foundation Models</td>
                              <td>Joy Hsu</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_16035v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.16035v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_16033v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Visual Cropping Improves Zero-Shot Question Answering of Multimodal Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_16033v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_16033v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_16033v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multimodal Large Language Models (LLMs) have recently achieved promising zero-shot accuracy on visual question answering (VQA) -- a fundamental task affecting various downstream applications and domains. Given the great potential for the broad use of these models, it is important to investigate their limitations in dealing with different image and question properties. In this work, we investigate whether multimodal LLMs can perceive small details as well as large details in images. In particular, we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject of the question, declining up to $46\%$ with size. Furthermore, we show that this effect is causal by observing that human visual cropping can significantly mitigate their sensitivity to size. Inspired by the usefulness of human cropping, we then propose three automatic visual cropping methods as inference time mechanisms to improve the zero-shot performance of multimodal LLMs. We study their effectiveness on four popular VQA datasets, and a subset of the VQAv2 dataset tailored towards fine visual details. Our findings suggest that multimodal LLMs should be used with caution in detail-sensitive VQA applications, and that visual cropping is a promising direction to improve their zero-shot performance. Our code and data are publicly available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_16033v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多模式大型语言模型（LLM）最近在视觉问答（VQA）上实现了有希望的零样本准确性，这是影响各种下游应用程序和领域的基本任务。考虑到这些模型广泛使用的巨大潜力，研究它们在处理不同图像和问题属性方面的局限性是很重要的。在这项工作中，我们研究了多模式LLM是否可以感知图像中的小细节和大细节。特别是，我们发现他们在回答视觉问题时的零样本准确度对问题的视觉主题的大小非常敏感，随着大小的增加而下降到46\%$。此外，我们观察到人类视觉裁剪可以显著降低其对尺寸的敏感性，从而表明这种影响是因果关系。受人工裁剪有用性的启发，我们提出了三种自动视觉裁剪方法作为推理时间机制，以提高多模式LLM的零样本性能。我们在四个流行的VQA数据集和针对精细视觉细节定制的VQAv2数据集的子集上研究了它们的有效性。我们的研究结果表明，在细节敏感的VQA应用中应谨慎使用多模式LLM，视觉裁剪是改善其零样本性能的一个有前途的方向。我们的代码和数据是公开的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.16033v1" target="_blank">2310.16033v1</a>
                              </td>
                              <td>Visual Cropping Improves Zero-Shot Question Answering of Multimodal Large Language Models</td>
                              <td>Jiarui Zhang</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_16033v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.16033v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_16028v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">What Algorithms can Transformers Learn? A Study in Length Generalization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_16028v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_16028v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_16028v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models exhibit surprising emergent generalization properties, yet also struggle on many simple reasoning tasks such as arithmetic and parity. This raises the question of if and when Transformer models can learn the true algorithm for solving a task. We study the scope of Transformers' abilities in the specific setting of length generalization on algorithmic tasks. Here, we propose a unifying framework to understand when and how Transformers can exhibit strong length generalization on a given task. Specifically, we leverage RASP (Weiss et al., 2021) -- a programming language designed for the computational model of a Transformer -- and introduce the RASP-Generalization Conjecture: Transformers tend to length generalize on a task if the task can be solved by a short RASP program which works for all input lengths. This simple conjecture remarkably captures most known instances of length generalization on algorithmic tasks. Moreover, we leverage our insights to drastically improve generalization performance on traditionally hard tasks (such as parity and addition). On the theoretical side, we give a simple example where the "min-degree-interpolator" model of learning from Abbe et al. (2023) does not correctly predict Transformers' out-of-distribution behavior, but our conjecture does. Overall, our work provides a novel perspective on the mechanisms of compositional generalization and the algorithmic capabilities of Transformers.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_16028v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型表现出令人惊讶的突发泛化特性，但在许多简单的推理任务（如算术和奇偶校验）上也很吃力。这就提出了一个问题，即Transformer模型是否以及何时可以学习解决任务的真正算法。我们研究了Transformers在算法任务长度泛化的特定设置中的能力范围。在这里，我们提出了一个统一的框架来理解变形金刚何时以及如何在给定的任务上表现出强大的长度泛化能力。具体来说，我们利用了RASP（Weiss et al.，2021）——一种为Transformer的计算模型设计的编程语言——并引入了RASP泛化猜想：如果任务可以通过适用于所有输入长度的短RASP程序来解决，则Transformer倾向于对任务进行长度泛化。这个简单的猜想显著地捕捉到了算法任务中长度泛化的大多数已知实例。此外，我们利用我们的见解大幅提高了传统硬任务（如奇偶校验和加法）的泛化性能。在理论方面，我们给出了一个简单的例子，其中Abbe等人（2023）的“最小阶插值器”学习模型不能正确预测变压器的分布外行为，但我们的猜想确实预测了。总的来说，我们的工作为Transformers的合成泛化机制和算法能力提供了一个新的视角。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.16028v1" target="_blank">2310.16028v1</a>
                              </td>
                              <td>What Algorithms can Transformers Learn? A Study in Length Generalization</td>
                              <td>Hattie Zhou</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_16028v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.16028v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_13829v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning from Mistakes via Cooperative Study Assistant for Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_13829v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_13829v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_13829v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have demonstrated their potential to refine their generation based on their own feedback. However, the feedback from LLM itself is often inaccurate, thereby limiting its benefits. In this paper, we propose Study Assistant for Large LAnguage Model (SALAM), a novel framework with an auxiliary agent to assist the main LLM in learning from mistakes through interactive cooperation. In the gathering phase, the student assistant agent probes the main LLM, analyzes its errors, and collects the interaction in a mistake memory. During the examination phase, the study assistant provides guidelines by retrieving relevant cases to help the main LLM anticipate and avoid similar errors. We first investigate the effectiveness of a general study assistant and then customize it to provide LLM-specific guidance through imitation learning from successful guidance experiences. Our experiments on three LLMs using two challenging frameworks demonstrate that SALAM can significantly boost LLMs by an accuracy margin of up to 6.6 on BBH and 12.6 on BBQ.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_13829v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）已经证明了它们根据自己的反馈改进生成的潜力。然而，LLM本身的反馈往往是不准确的，从而限制了其优势。在本文中，我们提出了大型语言模型的学习助手（SALAM），这是一个新的框架，带有一个辅助代理，通过互动合作帮助主要LLM从错误中学习。在收集阶段，学生助理代理探究主要LLM，分析其错误，并收集错误记忆中的交互。在检查阶段，研究助理通过检索相关案例提供指导，以帮助主要LLM预测和避免类似错误。我们首先调查了一般学习助理的有效性，然后通过模仿学习成功的指导经验，对其进行定制，以提供LLM特定的指导。我们使用两个具有挑战性的框架对三个LLM进行的实验表明，SALAM可以在BBH和BBQ上分别显著提高LLM 6.6和12.6的准确率。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.13829v3" target="_blank">2305.13829v3</a>
                              </td>
                              <td>Learning from Mistakes via Cooperative Study Assistant for Large Language Models</td>
                              <td>Danqing Wang</td>
                              <td>2023-05-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_13829v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.13829v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15991v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">White-box Compiler Fuzzing Empowered by Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15991v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15991v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15991v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Compiler correctness is crucial, as miscompilation falsifying the program behaviors can lead to serious consequences. In the literature, fuzzing has been extensively studied to uncover compiler defects. However, compiler fuzzing remains challenging: Existing arts focus on black- and grey-box fuzzing, which generates tests without sufficient understanding of internal compiler behaviors. As such, they often fail to construct programs to exercise conditions of intricate optimizations. Meanwhile, traditional white-box techniques are computationally inapplicable to the giant codebase of compilers. Recent advances demonstrate that Large Language Models (LLMs) excel in code generation/understanding tasks and have achieved state-of-the-art performance in black-box fuzzing. Nonetheless, prompting LLMs with compiler source-code information remains a missing piece of research in compiler testing.   To this end, we propose WhiteFox, the first white-box compiler fuzzer using LLMs with source-code information to test compiler optimization. WhiteFox adopts a dual-model framework: (i) an analysis LLM examines the low-level optimization source code and produces requirements on the high-level test programs that can trigger the optimization; (ii) a generation LLM produces test programs based on the summarized requirements. Additionally, optimization-triggering tests are used as feedback to further enhance the test generation on the fly. Our evaluation on four popular compilers shows that WhiteFox can generate high-quality tests to exercise deep optimizations requiring intricate conditions, practicing up to 80 more optimizations than state-of-the-art fuzzers. To date, WhiteFox has found in total 96 bugs, with 80 confirmed as previously unknown and 51 already fixed. Beyond compiler testing, WhiteFox can also be adapted for white-box fuzzing of other complex, real-world software systems in general.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15991v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>编译器的正确性至关重要，因为错误编译伪造程序行为会导致严重后果。在文献中，模糊化已经被广泛研究以发现编译器的缺陷。然而，编译器模糊化仍然具有挑战性：现有的技术专注于黑盒和灰盒模糊化，它在对内部编译器行为没有足够了解的情况下生成测试。因此，他们经常无法构建程序来执行复杂的优化条件。同时，传统的白盒技术在计算上不适用于编译器的庞大代码库。最近的进展表明，大型语言模型（LLM）在代码生成/理解任务方面表现出色，并在黑箱模糊处理方面取得了最先进的性能。尽管如此，在编译器测试中，用编译器源代码信息提示LLM仍然是一项缺失的研究。为此，我们提出了WhiteFox，这是第一个使用带有源代码信息的LLM来测试编译器优化的白盒编译器模糊器。WhiteFox采用了双重模型框架：（i）分析LLM检查低级优化源代码，并对可以触发优化的高级测试程序产生需求；（ii）一代LLM基于概括的需求产生测试程序。此外，优化触发测试被用作反馈，以进一步增强动态测试生成。我们对四个流行编译器的评估表明，WhiteFox可以生成高质量的测试来执行需要复杂条件的深度优化，比最先进的模糊器多练习80个优化。到目前为止，WhiteFox总共发现了96个错误，其中80个被确认为以前未知，51个已经修复。除了编译器测试之外，WhiteFox还可以适用于其他复杂的真实世界软件系统的白盒模糊处理。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15991v1" target="_blank">2310.15991v1</a>
                              </td>
                              <td>White-box Compiler Fuzzing Empowered by Large Language Models</td>
                              <td>Chenyuan Yang</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15991v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15991v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15987v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Dissecting In-Context Learning of Translations in GPTs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15987v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15987v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15987v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Most of the recent work in leveraging Large Language Models (LLMs) such as GPT-3 for Machine Translation (MT) has focused on selecting the few-shot samples for prompting. In this work, we try to better understand the role of demonstration attributes for the in-context learning of translations through perturbations of high-quality, in-domain demonstrations. We find that asymmetric perturbation of the source-target mappings yield vastly different results. We show that the perturbation of the source side has surprisingly little impact, while target perturbation can drastically reduce translation quality, suggesting that it is the output text distribution that provides the most important learning signal during in-context learning of translations. We propose a method named Zero-Shot-Context to add this signal automatically in Zero-Shot prompting. We demonstrate that it improves upon the zero-shot translation performance of GPT-3, even making it competitive with few-shot prompted translations.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15987v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近利用大型语言模型（LLM）（如用于机器翻译（MT）的GPT-3）的大多数工作都集中在选择少数镜头样本进行提示上。在这项工作中，我们试图通过扰动高质量的领域内演示，更好地理解演示属性在翻译的上下文学习中的作用。我们发现源-目标映射的非对称扰动产生了截然不同的结果。我们发现，源端的扰动几乎没有影响，而目标端的扰动会大大降低翻译质量，这表明在翻译的上下文学习过程中，输出文本分布提供了最重要的学习信号。我们提出了一种称为Zero-ShotContext的方法来在Zero-Shotneneneea提示中自动添加该信号。我们证明，它改进了GPT-3的零样本翻译性能，甚至使其与提示较少的翻译具有竞争力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15987v1" target="_blank">2310.15987v1</a>
                              </td>
                              <td>Dissecting In-Context Learning of Translations in GPTs</td>
                              <td>Vikas Raunak</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15987v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15987v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03838v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RADAR: Robust AI-Text Detection via Adversarial Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03838v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03838v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03838v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusations of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a robust AI-text detector via adversarial learning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic content to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly 2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets, experimental results show that RADAR significantly outperforms existing AI-text detection methods, especially when paraphrasing is in place. We also identify the strong transferability of RADAR from instruction-tuned LLMs to other LLMs, and evaluate the improved capability of RADAR via GPT-3.5-Turbo.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03838v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）的最新进展和类似ChatGPT的应用程序的日益流行，模糊了人类和机器之间高质量文本生成的边界。然而，除了我们的技术和社会预期的革命性变化之外，区分LLM生成的文本（AI文本）和人类生成的文本的困难也带来了滥用和公平的新挑战，例如虚假内容生成、抄袭和对无辜作者的虚假指控。虽然现有工作表明，当前的人工智能文本检测器对基于LLM的转述不具有鲁棒性，但本文旨在通过提出一个名为RADAR的新框架来弥合这一差距，该框架通过对抗性学习联合训练一个鲁棒的人工智能文本检测器。RADAR是基于转述者和检测器的对抗性训练。转述者的目标是生成真实的内容来躲避人工智能文本检测。RADAR使用来自检测器的反馈来更新转述器，反之亦然。在4个数据集中使用8种不同的LLM（Pythia、Dolly 2.0、Palmyra、Camel、GPT-J、Dolly 1.0、LLaMA和Vicuna）进行评估，实验结果表明，RADAR显著优于现有的人工智能文本检测方法，尤其是在转述到位的情况下。我们还确定了RADAR从指令调谐LLM到其他LLM的强大可转移性，并通过GPT-3.5-Turbo评估了RADAR的改进能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03838v2" target="_blank">2307.03838v2</a>
                              </td>
                              <td>RADAR: Robust AI-Text Detection via Adversarial Learning</td>
                              <td>Xiaomeng Hu</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03838v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03838v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_13469v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Ask Language Model to Clean Your Noisy Translation Data</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_13469v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_13469v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_13469v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Transformer models have demonstrated remarkable performance in neural machine translation (NMT). However, their vulnerability to noisy input poses a significant challenge in practical implementation, where generating clean output from noisy input is crucial. The MTNT dataset is widely used as a benchmark for evaluating the robustness of NMT models against noisy input. Nevertheless, its utility is limited due to the presence of noise in both the source and target sentences. To address this limitation, we focus on cleaning the noise from the target sentences in MTNT, making it more suitable as a benchmark for noise evaluation. Leveraging the capabilities of large language models (LLMs), we observe their impressive abilities in noise removal. For example, they can remove emojis while considering their semantic meaning. Additionally, we show that LLM can effectively rephrase slang, jargon, and profanities. The resulting datasets, called C-MTNT, exhibit significantly less noise in the target sentences while preserving the semantic integrity of the original sentences. Our human and GPT-4 evaluations also lead to a consistent conclusion that LLM performs well on this task. Lastly, experiments on C-MTNT showcased its effectiveness in evaluating the robustness of NMT models, highlighting the potential of advanced language models for data cleaning and emphasizing C-MTNT as a valuable resource.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_13469v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>变换器模型在神经机器翻译（NMT）中表现出了显著的性能。然而，它们对噪声输入的脆弱性在实际实现中提出了重大挑战，从噪声输入中生成干净的输出至关重要。MTNT数据集被广泛用作评估NMT模型对噪声输入的鲁棒性的基准。然而，由于源句和目标句中都存在噪声，它的效用是有限的。为了解决这一限制，我们专注于清除MTNT中目标句子中的噪声，使其更适合作为噪声评估的基准。利用大型语言模型（LLM）的功能，我们观察到它们在去除噪声方面令人印象深刻的能力。例如，他们可以删除表情符号，同时考虑其语义。此外，我们还表明LLM可以有效地改写俚语、行话和脏话。由此产生的数据集称为C-MTNT，在保持原始句子的语义完整性的同时，目标句子中的噪声显著减少。我们的人类和GPT-4评估也得出了一个一致的结论，即LLM在这项任务中表现良好。最后，C-MTNT的实验展示了它在评估NMT模型稳健性方面的有效性，突出了高级语言模型在数据清理方面的潜力，并强调C-MTNT是一种有价值的资源。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.13469v3" target="_blank">2310.13469v3</a>
                              </td>
                              <td>Ask Language Model to Clean Your Noisy Translation Data</td>
                              <td>Quinten Bolding</td>
                              <td>2023-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_13469v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.13469v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15961v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Mixture of Tokens: Efficient LLMs through Cross-Example Aggregation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15961v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15961v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15961v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite the promise of Mixture of Experts (MoE) models in increasing parameter counts of Transformer models while maintaining training and inference costs, their application carries notable drawbacks. The key strategy of these models is to, for each processed token, activate at most a few experts - subsets of an extensive feed-forward layer. But this approach is not without its challenges. The operation of matching experts and tokens is discrete, which makes MoE models prone to issues like training instability and uneven expert utilization. Existing techniques designed to address these concerns, such as auxiliary losses or balance-aware matching, result either in lower model performance or are more difficult to train. In response to these issues, we propose Mixture of Tokens, a fully-differentiable model that retains the benefits of MoE architectures while avoiding the aforementioned difficulties. Rather than routing tokens to experts, this approach mixes tokens from different examples prior to feeding them to experts, enabling the model to learn from all token-expert combinations. Importantly, this mixing can be disabled to avoid mixing of different sequences during inference. Crucially, this method is fully compatible with both masked and causal Large Language Model training and inference.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15961v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管混合专家（MoE）模型有望在保持训练和推理成本的同时增加变压器模型的参数计数，但其应用存在显著缺陷。这些模型的关键策略是，对于每个处理过的令牌，最多激活几个专家——一个广泛的前馈层的子集。但这种方法并非没有挑战。匹配专家和令牌的操作是离散的，这使得MoE模型容易出现训练不稳定和专家利用率不均衡等问题。为解决这些问题而设计的现有技术，如辅助损失或平衡感知匹配，要么导致模型性能降低，要么更难训练。针对这些问题，我们提出了令牌混合，这是一个完全可微的模型，它保留了MoE架构的优势，同时避免了上述困难。这种方法不是将令牌路由到专家，而是在将不同示例的令牌提供给专家之前将其混合，从而使模型能够从所有令牌专家组合中学习。重要的是，可以禁用这种混合，以避免在推理过程中混合不同的序列。至关重要的是，这种方法与掩蔽和因果大语言模型的训练和推理完全兼容。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15961v1" target="_blank">2310.15961v1</a>
                              </td>
                              <td>Mixture of Tokens: Efficient LLMs through Cross-Example Aggregation</td>
                              <td>Szymon Antoniak</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15961v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15961v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15959v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NoteChat: A Dataset of Synthetic Doctor-Patient Conversations Conditioned on Clinical Notes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15959v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15959v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15959v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The detailed clinical records drafted by doctors after each patient's visit are crucial for medical practitioners and researchers. Automating the creation of these notes with language models can reduce the workload of doctors. However, training such models can be difficult due to the limited public availability of conversations between patients and doctors. In this paper, we introduce NoteChat, a cooperative multi-agent framework leveraging Large Language Models (LLMs) for generating synthetic doctor-patient conversations conditioned on clinical notes. NoteChat consists of Planning, Roleplay, and Polish modules. We provide a comprehensive automatic and human evaluation of NoteChat, comparing it with state-of-the-art models, including OpenAI's ChatGPT and GPT-4. Results demonstrate that NoteChat facilitates high-quality synthetic doctor-patient conversations, underscoring the untapped potential of LLMs in healthcare. This work represents the first instance of multiple LLMs cooperating to complete a doctor-patient conversation conditioned on clinical notes, offering promising avenues for the intersection of AI and healthcare</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15959v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>医生在每位患者就诊后起草的详细临床记录对医生和研究人员至关重要。使用语言模型自动创建这些笔记可以减少医生的工作量。然而，由于患者和医生之间对话的公开性有限，培训此类模型可能很困难。在本文中，我们介绍了NoteChat，这是一个利用大型语言模型（LLM）生成基于临床笔记的合成医患对话的多智能体合作框架。NoteChat由计划、角色扮演和波兰语模块组成。我们对NoteChat进行了全面的自动和人工评估，并将其与最先进的模型进行了比较，包括OpenAI的ChatGPT和GPT-4。结果表明，NoteChat促进了高质量的综合医患对话，突显了LLM在医疗保健领域尚未开发的潜力。这项工作代表了多个LLM合作完成以临床笔记为条件的医患对话的第一个例子，为人工智能和医疗保健的交叉提供了有希望的途径</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15959v1" target="_blank">2310.15959v1</a>
                              </td>
                              <td>NoteChat: A Dataset of Synthetic Doctor-Patient Conversations Conditioned on Clinical Notes</td>
                              <td>Junda Wang</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15959v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15959v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15950v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Representation Learning with Large Language Models for Recommendation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15950v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15950v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15950v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recommender systems have seen significant advancements with the influence of deep learning and graph neural networks, particularly in capturing complex user-item relationships. However, these graph-based recommenders heavily depend on ID-based data, potentially disregarding valuable textual information associated with users and items, resulting in less informative learned representations. Moreover, the utilization of implicit feedback data introduces potential noise and bias, posing challenges for the effectiveness of user preference learning. While the integration of large language models (LLMs) into traditional ID-based recommenders has gained attention, challenges such as scalability issues, limitations in text-only reliance, and prompt input constraints need to be addressed for effective implementation in practical recommender systems. To address these challenges, we propose a model-agnostic framework RLMRec that aims to enhance existing recommenders with LLM-empowered representation learning. It proposes a recommendation paradigm that integrates representation learning with LLMs to capture intricate semantic aspects of user behaviors and preferences. RLMRec incorporates auxiliary textual signals, develops a user/item profiling paradigm empowered by LLMs, and aligns the semantic space of LLMs with the representation space of collaborative relational signals through a cross-view alignment framework. This work further establish a theoretical foundation demonstrating that incorporating textual signals through mutual information maximization enhances the quality of representations. In our evaluation, we integrate RLMRec with state-of-the-art recommender models, while also analyzing its efficiency and robustness to noise data. Our implementation codes are available at https://github.com/HKUDS/RLMRec.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15950v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在深度学习和图神经网络的影响下，推荐系统取得了重大进展，特别是在捕捉复杂的用户-项目关系方面。然而，这些基于图的推荐器在很大程度上依赖于基于ID的数据，可能会忽略与用户和项目相关的有价值的文本信息，从而导致信息量较小的学习表示。此外，内隐反馈数据的使用引入了潜在的噪声和偏见，对用户偏好学习的有效性提出了挑战。虽然将大型语言模型（LLM）集成到传统的基于ID的推荐器中已经引起了关注，但为了在实际的推荐系统中有效实现，需要解决诸如可扩展性问题、仅依赖文本的限制和即时输入约束等挑战。为了应对这些挑战，我们提出了一个模型不可知框架RLMRec，旨在通过LLM授权的表示学习来增强现有的推荐者。它提出了一种推荐范式，将表示学习与LLM相结合，以捕捉用户行为和偏好的复杂语义方面。RLMRec结合了辅助文本信号，开发了LLM授权的用户/项目分析范式，并通过跨视图对齐框架将LLM的语义空间与协作关系信号的表示空间对齐。这项工作进一步奠定了理论基础，证明通过相互信息最大化结合文本信号可以提高表征的质量。在我们的评估中，我们将RLMRec与最先进的推荐模型相结合，同时分析其效率和对噪声数据的鲁棒性。我们的实施代码可在https://github.com/HKUDS/RLMRec.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15950v1" target="_blank">2310.15950v1</a>
                              </td>
                              <td>Representation Learning with Large Language Models for Recommendation</td>
                              <td>Xubin Ren</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15950v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15950v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_14321v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Lifelong Robot Learning with Human Assisted Language Planners</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_14321v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_14321v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_14321v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) have been shown to act like planners that can decompose high-level instructions into a sequence of executable instructions. However, current LLM-based planners are only able to operate with a fixed set of skills. We overcome this critical limitation and present a method for using LLM-based planners to query new skills and teach robots these skills in a data and time-efficient manner for rigid object manipulation. Our system can re-use newly acquired skills for future tasks, demonstrating the potential of open world and lifelong learning. We evaluate the proposed framework on multiple tasks in simulation and the real world. Videos are available at: https://sites.google.com/mit.edu/halp-robot-learning.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_14321v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）已经被证明像规划者一样，可以将高级指令分解为一系列可执行指令。然而，目前基于LLM的规划者只能使用一套固定的技能进行操作。我们克服了这一关键限制，并提出了一种使用基于LLM的规划者来查询新技能的方法，并以数据和时间高效的方式向机器人传授这些技能，用于刚性对象操作。我们的系统可以将新获得的技能重新用于未来的任务，展示开放世界和终身学习的潜力。我们在模拟和现实世界中对所提出的多任务框架进行了评估。视频可在以下网址获取：https://sites.google.com/mit.edu/halp-robot-learning.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.14321v2" target="_blank">2309.14321v2</a>
                              </td>
                              <td>Lifelong Robot Learning with Human Assisted Language Planners</td>
                              <td>Meenal Parakh</td>
                              <td>2023-09-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_14321v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.14321v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15941v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">This is not a Dataset: A Large Negation Benchmark to Challenge Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15941v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15941v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15941v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Although large language models (LLMs) have apparently acquired a certain level of grammatical knowledge and the ability to make generalizations, they fail to interpret negation, a crucial step in Natural Language Processing. We try to clarify the reasons for the sub-optimal performance of LLMs understanding negation. We introduce a large semi-automatically generated dataset of circa 400,000 descriptive sentences about commonsense knowledge that can be true or false in which negation is present in about 2/3 of the corpus in different forms. We have used our dataset with the largest available open LLMs in a zero-shot approach to grasp their generalization and inference capability and we have also fine-tuned some of the models to assess whether the understanding of negation can be trained. Our findings show that, while LLMs are proficient at classifying affirmative sentences, they struggle with negative sentences and lack a deep understanding of negation, often relying on superficial cues. Although fine-tuning the models on negative sentences improves their performance, the lack of generalization in handling negation is persistent, highlighting the ongoing challenges of LLMs regarding negation understanding and generalization. The dataset and code are publicly available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15941v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管大型语言模型（LLM）显然已经获得了一定水平的语法知识和概括能力，但它们无法解释否定，这是自然语言处理中的关键一步。我们试图阐明LLM理解否定的次优性能的原因。我们介绍了一个大型半自动生成的数据集，该数据集包含大约400000个关于常识知识的描述性句子，这些句子可以是真的也可以是假的，其中否定以不同的形式存在于大约2/3的语料库中。我们在零样本方法中使用了具有最大可用开放LLM的数据集来掌握它们的泛化和推理能力，我们还对一些模型进行了微调，以评估是否可以训练对否定的理解。我们的研究结果表明，虽然LLM擅长对肯定句进行分类，但他们在与否定句作斗争时缺乏对否定的深刻理解，通常依赖于肤浅的线索。尽管对否定句模型的微调提高了它们的性能，但在处理否定时缺乏概括的现象仍然存在，这突出了LLM在否定理解和概括方面的持续挑战。数据集和代码是公开的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15941v1" target="_blank">2310.15941v1</a>
                              </td>
                              <td>This is not a Dataset: A Large Negation Benchmark to Challenge Large Language Models</td>
                              <td>Iker García-Ferrero</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15941v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15941v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_14229v4_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Zero-Shot Cross-Lingual Summarization via Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_14229v4_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_14229v4_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_14229v4_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Given a document in a source language, cross-lingual summarization (CLS) aims to generate a summary in a different target language. Recently, the emergence of Large Language Models (LLMs), such as GPT-3.5, ChatGPT and GPT-4, has attracted wide attention from the computational linguistics community. However, it is not yet known the performance of LLMs on CLS. In this report, we empirically use various prompts to guide LLMs to perform zero-shot CLS from different paradigms (i.e., end-to-end and pipeline), and provide a preliminary evaluation on the generated summaries. We find that ChatGPT and GPT-4 originally prefer to produce lengthy summaries with detailed information. These two LLMs can further balance informativeness and conciseness with the help of an interactive prompt, significantly improving their CLS performance. Experimental results on three widely-used CLS datasets show that GPT-4 achieves state-of-the-art zero-shot CLS performance, and performs competitively compared with the fine-tuned mBART-50. Moreover, we also find some multi-lingual and bilingual LLMs (i.e., BLOOMZ, ChatGLM-6B, Vicuna-13B and ChatYuan) have limited zero-shot CLS ability. Due to the composite nature of CLS, which requires models to perform summarization and translation simultaneously, accomplishing this task in a zero-shot manner is even a challenge for LLMs. Therefore, we sincerely hope and recommend future LLM research could use CLS as a testbed.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_14229v4_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>给定源语言的文档，跨语言摘要（CLS）旨在生成不同目标语言的摘要。近年来，GPT-3.5、ChatGPT和GPT-4等大型语言模型的出现引起了计算语言学界的广泛关注。然而，目前还不知道LLM在CLS上的性能。在本报告中，我们根据经验使用各种提示来指导LLM从不同的范式（即端到端和管道）执行零样本CLS，并对生成的摘要进行初步评估。我们发现，ChatGPT和GPT-4最初更喜欢生成包含详细信息的冗长摘要。这两个LLM可以在交互式提示的帮助下进一步平衡信息性和简洁性，显著提高其CLS性能。在三个广泛使用的CLS数据集上的实验结果表明，GPT-4实现了最先进的零样本CLS性能，并与微调的mBART-50相比具有竞争力。此外，我们还发现一些多语言和双语LLM（即BLOOMZ、ChatGLM-6B、Vicuna-13B和ChatYuan）的零样本CLS能力有限。由于CLS的复合性质，需要模型同时执行摘要和翻译，因此以零样本方式完成这项任务甚至是LLM的挑战。因此，我们真诚地希望并推荐未来的LLM研究可以将CLS作为试验台。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.14229v4" target="_blank">2302.14229v4</a>
                              </td>
                              <td>Zero-Shot Cross-Lingual Summarization via Large Language Models</td>
                              <td>Jiaan Wang</td>
                              <td>2023-02-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_14229v4_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.14229v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15929v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">E-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M Sparsity</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15929v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15929v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15929v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Traditional pruning methods are known to be challenging to work in Large Language Models (LLMs) for Generative AI because of their unaffordable training process and large computational demands. For the first time, we introduce the information entropy of hidden state features into a pruning metric design, namely E-Sparse, to improve the accuracy of N:M sparsity on LLM. E-Sparse employs the information richness to leverage the channel importance, and further incorporates several novel techniques to put it into effect: (1) it introduces information entropy to enhance the significance of parameter weights and input feature norms as a novel pruning metric, and performs N:M sparsity without modifying the remaining weights. (2) it designs global naive shuffle and local block shuffle to quickly optimize the information distribution and adequately cope with the impact of N:M sparsity on LLMs' accuracy. E-Sparse is implemented as a Sparse-GEMM on FasterTransformer and runs on NVIDIA Ampere GPUs. Extensive experiments on the LLaMA family and OPT models show that E-Sparse can significantly speed up the model inference over the dense model (up to 1.53X) and obtain significant memory saving (up to 43.52%), with acceptable accuracy loss.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15929v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>众所周知，传统的修剪方法在生成型人工智能的大型语言模型（LLM）中很有挑战性，因为它们的训练过程负担不起，计算量大。我们首次将隐藏状态特征的信息熵引入修剪度量设计，即E-Parse，以提高LLM上N:M稀疏性的准确性。E-Parse利用信息丰富性来利用信道重要性，并进一步结合了几种新技术来实现它：（1）引入信息熵来增强参数权重和输入特征范数的重要性，作为一种新的修剪度量，并在不修改剩余权重的情况下执行N:M稀疏性。（2） 它设计了全局naive shuffle和局部块shuffle来快速优化信息分布，并充分应对N:M稀疏性对LLM精度的影响。E-Parse在FasterTransformer上实现为稀疏GEMM，并在NVIDIA Ampere GPU上运行。在LLaMA家族和OPT模型上的大量实验表明，E-Parse可以显著加快密集模型的模型推理速度（高达1.53X），并获得显著的内存节省（高达43.52%），同时具有可接受的精度损失。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15929v1" target="_blank">2310.15929v1</a>
                              </td>
                              <td>E-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M Sparsity</td>
                              <td>Yun Li</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15929v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15929v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_13040v4_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented Dialogue Agents</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_13040v4_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_13040v4_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_13040v4_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Task-oriented dialogue (TOD) models have made significant progress in recent years. However, previous studies primarily focus on datasets written by annotators, which has resulted in a gap between academic research and real-world spoken conversation scenarios. While several small-scale spoken TOD datasets are proposed to address robustness issues such as ASR errors, they ignore the unique challenges in spoken conversation. To tackle the limitations, we introduce SpokenWOZ, a large-scale speech-text dataset for spoken TOD, containing 8 domains, 203k turns, 5.7k dialogues and 249 hours of audios from human-to-human spoken conversations. SpokenWOZ further incorporates common spoken characteristics such as word-by-word processing and reasoning in spoken language. Based on these characteristics, we present cross-turn slot and reasoning slot detection as new challenges. We conduct experiments on various baselines, including text-modal models, newly proposed dual-modal models, and LLMs, e.g., ChatGPT. The results show that the current models still have substantial room for improvement in spoken conversation, where the most advanced dialogue state tracker only achieves 25.65% in joint goal accuracy and the SOTA end-to-end model only correctly completes the user request in 52.1% of dialogues. The dataset, code, and leaderboard are available: https://spokenwoz.github.io/SpokenWOZ-github.io/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_13040v4_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，面向任务的对话模式取得了重大进展。然而，以前的研究主要集中在注释者编写的数据集上，这导致了学术研究与现实世界口语对话场景之间的差距。虽然提出了几个小规模的口语TOD数据集来解决ASR错误等鲁棒性问题，但它们忽略了口语会话中的独特挑战。为了解决这些限制，我们介绍了SpokenWOZ，这是一个用于口语TOD的大型语音文本数据集，包含8个域、203k个转弯、5.7k个对话和249小时的人与人之间口语对话音频。SpokenWOZ进一步融入了常见的口语特征，如口语中的逐字处理和推理。基于这些特点，我们提出了交叉转弯槽和推理槽检测作为新的挑战。我们在各种基线上进行了实验，包括文本模态模型、新提出的双模态模型和LLM，例如ChatGPT。结果表明，目前的模型在口语会话中仍有很大的改进空间，其中最先进的对话状态跟踪器在联合目标准确率上仅达到25.65%，SOTA端到端模型仅在52.1%的对话中正确完成用户请求。数据集、代码和排行榜可用：https://spokenwoz.github.io/SpokenWOZ-github.io/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.13040v4" target="_blank">2305.13040v4</a>
                              </td>
                              <td>SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented Dialogue Agents</td>
                              <td>Shuzheng Si</td>
                              <td>2023-05-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_13040v4_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.13040v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15916v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">In-Context Learning Creates Task Vectors</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15916v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15916v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15916v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In-context learning (ICL) in Large Language Models (LLMs) has emerged as a powerful new learning paradigm. However, its underlying mechanism is still not well understood. In particular, it is challenging to map it to the "standard" machine learning framework, where one uses a training set $S$ to find a best-fitting function $f(x)$ in some hypothesis class. Here we make progress on this problem by showing that the functions learned by ICL often have a very simple structure: they correspond to the transformer LLM whose only inputs are the query $x$ and a single "task vector" calculated from the training set. Thus, ICL can be seen as compressing $S$ into a single task vector $\boldsymbol{\theta}(S)$ and then using this task vector to modulate the transformer to produce the output. We support the above claim via comprehensive experiments across a range of models and tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15916v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大语言模型中的上下文学习（ICL）已经成为一种强大的新学习范式。然而，其潜在机制仍然没有得到很好的理解。特别是，将其映射到“标准”机器学习框架是具有挑战性的，在该框架中，使用训练集$S$在某个假设类中找到最佳拟合函数$f（x）$。在这里，我们通过展示ICL学习的函数通常具有非常简单的结构来解决这个问题：它们对应于transformer LLM，其唯一的输入是查询$x$和从训练集计算的单个“任务向量”。因此，ICL可以被视为将$S$压缩为单个任务向量$\boldsymbol｛theta｝（S）$，然后使用该任务向量来调制转换器以产生输出。我们通过一系列模型和任务的综合实验来支持上述说法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15916v1" target="_blank">2310.15916v1</a>
                              </td>
                              <td>In-Context Learning Creates Task Vectors</td>
                              <td>Roee Hendel</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15916v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15916v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_12871v5_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AnglE-optimized Text Embeddings</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_12871v5_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_12871v5_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_12871v5_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works with LLM-annotated data. Extensive experiments were conducted on various tasks including short-text STS, long-text STS, and domain-specific STS tasks. The results show that AnglE outperforms the state-of-the-art (SOTA) STS models that ignore the cosine saturation zone. These findings demonstrate the ability of AnglE to generate high-quality text embeddings and the usefulness of angle optimization in STS.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_12871v5_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>高质量的文本嵌入是改进语义-文本相似性（STS）任务的关键，而STS是大型语言模型（LLM）应用程序中的关键组件。然而，现有文本嵌入模型面临的一个常见挑战是梯度消失的问题，这主要是因为它们依赖于优化目标中的余弦函数，该函数具有饱和区。为了解决这个问题，本文提出了一种新的角度优化文本嵌入模型angle。AnglE的核心思想是在复杂空间中引入角度优化。这种新方法有效地减轻了余弦函数中饱和区的不利影响，因为饱和区会阻碍梯度并阻碍优化过程。为了建立一个全面的STS评估，我们在现有的短文本STS数据集和GitHub Issues新收集的长文本STS数据集中进行了实验。此外，我们研究了具有有限标记数据的领域特定STS场景，并探讨了AnglE如何使用LLM注释数据。对各种任务进行了广泛的实验，包括短文本STS、长文本STS和特定领域的STS任务。结果表明，AnglE优于忽略余弦饱和区的最先进的（SOTA）STS模型。这些发现证明了AnglE生成高质量文本嵌入的能力以及角度优化在STS中的有用性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.12871v5" target="_blank">2309.12871v5</a>
                              </td>
                              <td>AnglE-optimized Text Embeddings</td>
                              <td>Xianming Li</td>
                              <td>2023-09-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_12871v5_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.12871v5" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15896v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs with Multi-turn Health Conversations Polished by ChatGPT</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15896v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15896v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15896v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have performed well in providing general and extensive health suggestions in single-turn conversations, exemplified by systems such as ChatGPT, ChatGLM, ChatDoctor, DoctorGLM, and etc. However, the limited information provided by users during single turn results in inadequate personalization and targeting of the generated suggestions, which requires users to independently select the useful part. It is mainly caused by the missing ability to engage in multi-turn questioning. In real-world medical consultations, doctors usually employ a series of iterative inquiries to comprehend the patient's condition thoroughly, enabling them to provide effective and personalized suggestions subsequently, which can be defined as chain of questioning (CoQ) for LLMs. To improve the CoQ of LLMs, we propose BianQue, a ChatGLM-based LLM finetuned with the self-constructed health conversation dataset BianQueCorpus that is consist of multiple turns of questioning and health suggestions polished by ChatGPT. Experimental results demonstrate that the proposed BianQue can simultaneously balance the capabilities of both questioning and health suggestions, which will help promote the research and application of LLMs in the field of proactive health.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15896v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）在单回合对话中提供一般和广泛的健康建议方面表现良好，例如ChatGPT、ChatGLM、ChatDoctor、DoctorGLM等系统。然而，用户在单回合会话中提供的有限信息导致生成的建议的个性化和针对性不足，这需要用户独立地选择有用的部分。这主要是由于缺乏进行多回合提问的能力造成的。在现实世界的医疗咨询中，医生通常会采用一系列迭代询问来彻底了解患者的病情，使他们能够随后提供有效和个性化的建议，这可以被定义为LLM的提问链（CoQ）。为了提高LLM的CoQ，我们提出了BianQue，这是一种基于ChatGLM的LLM，使用自建的健康对话数据集BianQueCorpus进行了微调，该数据集由ChatGPT完善的多个提问和健康建议组成。实验结果表明，所提出的BianQue可以同时平衡提问和健康建议的能力，这将有助于促进LLM在主动健康领域的研究和应用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15896v1" target="_blank">2310.15896v1</a>
                              </td>
                              <td>BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs with Multi-turn Health Conversations Polished by ChatGPT</td>
                              <td>Yirong Chen</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15896v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15896v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15047v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Meta- (out-of-context) learning in neural networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15047v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15047v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15047v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Brown et al. (2020) famously introduced the phenomenon of in-context learning in large language models (LLMs). We establish the existence of a phenomenon we call meta-out-of-context learning (meta-OCL) via carefully designed synthetic experiments with LLMs. Our results suggest that meta-OCL leads LLMs to more readily "internalize" the semantic content of text that is, or appears to be, broadly useful (such as true statements, or text from authoritative sources) and use it in appropriate circumstances. We further demonstrate meta-OCL in a synthetic computer vision setting, and propose two hypotheses for the emergence of meta-OCL: one relying on the way models store knowledge in their parameters, and another suggesting that the implicit gradient alignment bias of gradient-descent-based optimizers may be responsible. Finally, we reflect on what our results might imply about capabilities of future AI systems, and discuss potential risks. Our code can be found at https://github.com/krasheninnikov/internalization.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15047v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Brown等人（2020）在大型语言模型（LLM）中引入了著名的上下文学习现象。我们通过精心设计的LLM合成实验，确定了一种现象的存在，我们称之为元上下文外学习（meta-OCL）。我们的研究结果表明，元OCL使LLM更容易“内化”文本的语义内容，这些内容是或似乎是广泛有用的（如真实陈述或权威来源的文本），并在适当的情况下使用。我们在合成计算机视觉环境中进一步证明了元OCL，并为元OCL的出现提出了两个假设：一个依赖于模型在其参数中存储知识的方式，另一个则表明基于梯度下降的优化器的隐式梯度对齐偏差可能是原因。最后，我们反思我们的研究结果可能对未来人工智能系统的能力意味着什么，并讨论潜在的风险。我们的代码可以在https://github.com/krasheninnikov/internalization.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15047v2" target="_blank">2310.15047v2</a>
                              </td>
                              <td>Meta- (out-of-context) learning in neural networks</td>
                              <td>Dmitrii Krasheninnikov</td>
                              <td>2023-10-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15047v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15047v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15851v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Self-Guard: Empower the LLM to Safeguard Itself</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15851v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15851v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15851v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The jailbreak attack can bypass the safety measures of a Large Language Model (LLM), generating harmful content. This misuse of LLM has led to negative societal consequences. Currently, there are two main approaches to address jailbreak attacks: safety training and safeguards. Safety training focuses on further training LLM to enhance its safety. On the other hand, safeguards involve implementing external models or filters to prevent harmful outputs. However, safety training has constraints in its ability to adapt to new attack types and often leads to a drop in model performance. Safeguards have proven to be of limited help. To tackle these issues, we propose a novel approach called Self-Guard, which combines the strengths of both safety methods. Self-Guard includes two stages. In the first stage, we enhance the model's ability to assess harmful content, and in the second stage, we instruct the model to consistently perform harmful content detection on its own responses. The experiment has demonstrated that Self-Guard is robust against jailbreak attacks. In the bad case analysis, we find that LLM occasionally provides harmless responses to harmful queries. Additionally, we evaluated the general capabilities of the LLM before and after safety training, providing evidence that Self-Guard does not result in the LLM's performance degradation. In sensitivity tests, Self-Guard not only avoids inducing over-sensitivity in LLM but also can even mitigate this issue.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15851v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>越狱攻击可以绕过大型语言模型（LLM）的安全措施，生成有害内容。这种对LLM的滥用导致了负面的社会后果。目前，解决越狱袭击的主要方法有两种：安全培训和保障措施。安全培训的重点是进一步培训LLM，以提高其安全性。另一方面，保障措施包括实施外部模型或过滤器，以防止有害输出。然而，安全训练在适应新攻击类型的能力方面受到限制，通常会导致模型性能下降。事实证明，保障措施的帮助有限。为了解决这些问题，我们提出了一种称为“自我保护”的新方法，它结合了两种安全方法的优点。自我保护包括两个阶段。在第一阶段，我们增强了模型评估有害内容的能力，在第二阶段，我们指示模型对自己的响应始终如一地执行有害内容检测。实验表明，“自我保护”对越狱攻击具有强大的抵抗力。在坏情况分析中，我们发现LLM偶尔会对有害查询提供无害的响应。此外，我们在安全培训前后评估了LLM的一般能力，提供了自我保护不会导致LLM性能下降的证据。在灵敏度测试中，Self-Guard不仅避免了LLM中的过度灵敏度，甚至可以缓解这一问题。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15851v1" target="_blank">2310.15851v1</a>
                              </td>
                              <td>Self-Guard: Empower the LLM to Safeguard Itself</td>
                              <td>Zezhong Wang</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15851v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15851v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_02210v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Document-Level Machine Translation with Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_02210v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_02210v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_02210v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) such as ChatGPT can produce coherent, cohesive, relevant, and fluent answers for various natural language processing (NLP) tasks. Taking document-level machine translation (MT) as a testbed, this paper provides an in-depth evaluation of LLMs' ability on discourse modeling. The study focuses on three aspects: 1) Effects of Context-Aware Prompts, where we investigate the impact of different prompts on document-level translation quality and discourse phenomena; 2) Comparison of Translation Models, where we compare the translation performance of ChatGPT with commercial MT systems and advanced document-level MT methods; 3) Analysis of Discourse Modelling Abilities, where we further probe discourse knowledge encoded in LLMs and shed light on impacts of training techniques on discourse modeling. By evaluating on a number of benchmarks, we surprisingly find that LLMs have demonstrated superior performance and show potential to become a new paradigm for document-level translation: 1) leveraging their powerful long-text modeling capabilities, GPT-3.5 and GPT-4 outperform commercial MT systems in terms of human evaluation; 2) GPT-4 demonstrates a stronger ability for probing linguistic knowledge than GPT-3.5. This work highlights the challenges and opportunities of LLMs for MT, which we hope can inspire the future design and evaluation of LLMs.We release our data and annotations at https://github.com/longyuewangdcu/Document-MT-LLM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_02210v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM），如ChatGPT，可以为各种自然语言处理（NLP）任务生成连贯、连贯、相关和流畅的答案。本文以文档级机器翻译（MT）为实验平台，对LLM的语篇建模能力进行了深入的评价。本研究主要从三个方面展开：1）语境感知提示的作用，考察不同提示对文档级翻译质量和语篇现象的影响；2） 翻译模型比较，我们将ChatGPT的翻译性能与商业MT系统和高级文档级MT方法进行比较；3） 话语建模能力分析，我们进一步探讨LLM中编码的话语知识，并揭示训练技术对话语建模的影响。通过对多个基准进行评估，我们惊讶地发现LLM表现出了卓越的性能，并显示出成为文档级翻译新范式的潜力：1）利用其强大的长文本建模能力，GPT-3.5和GPT-4在人类评估方面优于商业MT系统；2） GPT-4表现出比GPT-3.5更强的探究语言知识的能力。这项工作强调了LLM对MT的挑战和机遇，我们希望这能启发LLM的未来设计和评估。我们在https://github.com/longyuewangdcu/Document-MT-LLM.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.02210v2" target="_blank">2304.02210v2</a>
                              </td>
                              <td>Document-Level Machine Translation with Large Language Models</td>
                              <td>Longyue Wang</td>
                              <td>2023-04-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_02210v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.02210v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_12001v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_12001v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_12001v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_12001v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we conduct a thorough investigation into the reasoning capabilities of Large Language Models (LLMs), focusing specifically on the Open Pretrained Transformers (OPT) models as a representative of such models. Our study entails finetuning three different sizes of OPT on a carefully curated reasoning corpus, resulting in two sets of finetuned models: OPT-R, finetuned without explanations, and OPT-RE, finetuned with explanations. We then evaluate all models on 57 out-of-domain tasks drawn from the SUPER-NATURALINSTRUCTIONS benchmark, covering 26 distinct reasoning skills, utilizing three prompting techniques. Through a comprehensive grid of 27 configurations and 6,156 test evaluations, we investigate the dimensions of finetuning, prompting, and scale to understand the role of explanations on different reasoning skills. Our findings reveal that having explanations in the fewshot exemplar has no significant impact on the model's performance when the model is finetuned, while positively affecting the non-finetuned counterpart. Moreover, we observe a slight yet consistent increase in classification accuracy as we incorporate explanations during prompting and finetuning, respectively. Finally, we offer insights on which skills benefit the most from incorporating explanations during finetuning and prompting, such as Numerical (+20.4%) and Analogical (+13.9%) reasoning, as well as skills that exhibit negligible or negative effects.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_12001v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们对大型语言模型（LLM）的推理能力进行了深入的研究，特别关注作为此类模型代表的开放式预训练变换器（OPT）模型。我们的研究需要在精心策划的推理语料库上对三种不同大小的OPT进行微调，从而产生两组微调模型：OPT-R，在没有解释的情况下进行微调，以及OPT-RE，在有解释的情况下作微调。然后，我们利用三种提示技术，对从SUPER-NATURALINSTRUMENTS基准中提取的57个域外任务的所有模型进行了评估，涵盖了26种不同的推理技能。通过27种配置和6156项测试评估的综合网格，我们调查了微调、提示和量表的维度，以了解解释对不同推理技能的作用。我们的研究结果表明，当模型被微调时，在少镜头样本中进行解释对模型的性能没有显著影响，而对未微调的对应物有积极影响。此外，当我们分别在提示和微调过程中加入解释时，我们观察到分类准确性略有但一致的提高。最后，我们提供了关于哪些技能从微调和提示过程中的解释中受益最大的见解，如数值推理（+20.4%）和类比推理（+13.9%），以及表现出可忽略或负面影响的技能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.12001v2" target="_blank">2305.12001v2</a>
                              </td>
                              <td>OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models</td>
                              <td>Badr AlKhamissi</td>
                              <td>2023-05-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_12001v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.12001v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_13570v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Simple Baseline for Knowledge-Based Visual Question Answering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_13570v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_13570v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_13570v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper is on the problem of Knowledge-Based Visual Question Answering (KB-VQA). Recent works have emphasized the significance of incorporating both explicit (through external databases) and implicit (through LLMs) knowledge to answer questions requiring external knowledge effectively. A common limitation of such approaches is that they consist of relatively complicated pipelines and often heavily rely on accessing GPT-3 API. Our main contribution in this paper is to propose a much simpler and readily reproducible pipeline which, in a nutshell, is based on efficient in-context learning by prompting LLaMA (1 and 2) using question-informative captions as contextual information. Contrary to recent approaches, our method is training-free, does not require access to external databases or APIs, and yet achieves state-of-the-art accuracy on the OK-VQA and A-OK-VQA datasets. Finally, we perform several ablation studies to understand important aspects of our method. Our code is publicly available at https://github.com/alexandrosXe/ASimple-Baseline-For-Knowledge-Based-VQA</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_13570v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文主要研究基于知识的可视化问答（KB-VQA）问题。最近的工作强调了结合显性（通过外部数据库）和隐性（通过LLM）知识来有效回答需要外部知识的问题的重要性。这种方法的一个常见限制是，它们由相对复杂的管道组成，并且通常严重依赖于访问GPT-3 API。我们在本文中的主要贡献是提出了一种更简单、易于复制的管道，简而言之，它是基于有效的上下文学习，通过提示LLaMA（1和2）使用问题信息字幕作为上下文信息。与最近的方法相反，我们的方法是免费训练的，不需要访问外部数据库或API，但在OK-VQA和A-OK-VQA数据集上实现了最先进的准确性。最后，我们进行了几项消融研究，以了解我们方法的重要方面。我们的代码可在https://github.com/alexandrosXe/ASimple-Baseline-For-Knowledge-Based-VQA</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.13570v2" target="_blank">2310.13570v2</a>
                              </td>
                              <td>A Simple Baseline for Knowledge-Based Visual Question Answering</td>
                              <td>Alexandros Xenos</td>
                              <td>2023-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_13570v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.13570v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15819v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Generative Language Models Exhibit Social Identity Biases</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15819v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15819v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15819v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The surge in popularity of large language models has given rise to concerns about biases that these models could learn from humans. In this study, we investigate whether ingroup solidarity and outgroup hostility, fundamental social biases known from social science, are present in 51 large language models. We find that almost all foundational language models and some instruction fine-tuned models exhibit clear ingroup-positive and outgroup-negative biases when prompted to complete sentences (e.g., "We are..."). A comparison of LLM-generated sentences with human-written sentences on the internet reveals that these models exhibit similar level, if not greater, levels of bias than human text. To investigate where these biases stem from, we experimentally varied the amount of ingroup-positive or outgroup-negative sentences the model was exposed to during fine-tuning in the context of the United States Democrat-Republican divide. Doing so resulted in the models exhibiting a marked increase in ingroup solidarity and an even greater increase in outgroup hostility. Furthermore, removing either ingroup-positive or outgroup-negative sentences (or both) from the fine-tuning data leads to a significant reduction in both ingroup solidarity and outgroup hostility, suggesting that biases can be reduced by removing biased training data. Our findings suggest that modern language models exhibit fundamental social identity biases and that such biases can be mitigated by curating training data. Our results have practical implications for creating less biased large-language models and further underscore the need for more research into user interactions with LLMs to prevent potential bias reinforcement in humans.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15819v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型的流行引发了人们对这些模型可能向人类学习的偏见的担忧。在这项研究中，我们调查了51个大型语言模型中是否存在群体内团结和群体外敌意，这是社会科学中已知的基本社会偏见。我们发现，几乎所有的基础语言模型和一些教学微调模型在被提示完成句子时（例如，“We are…”）都表现出明显的组内积极和组外消极偏见。LLM生成的句子与互联网上的人类书面句子的比较表明，这些模型表现出与人类文本相似甚至更大的偏见水平。为了调查这些偏见的来源，我们在美国民主党-共和党分歧的背景下，通过实验改变了模型在微调过程中所接触到的组内积极或组外消极句子的数量。这样做导致模型显示出群体内团结的显著增加和群体外敌意的更大增加。此外，从微调数据中删除组内肯定句或组外否定句（或两者）会显著减少组内团结和组外敌意，这表明可以通过删除有偏见的训练数据来减少偏见。我们的研究结果表明，现代语言模型表现出基本的社会身份偏见，这种偏见可以通过管理训练数据来缓解。我们的研究结果对创建偏见较小的大型语言模型具有实际意义，并进一步强调了对用户与LLM的交互进行更多研究的必要性，以防止人类潜在的偏见强化。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15819v1" target="_blank">2310.15819v1</a>
                              </td>
                              <td>Generative Language Models Exhibit Social Identity Biases</td>
                              <td>Tiancheng Hu</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15819v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15819v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_03368v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Evaluating Hallucinations in Chinese Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_03368v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_03368v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_03368v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we establish a benchmark named HalluQA (Chinese Hallucination Question-Answering) to measure the hallucination phenomenon in Chinese large language models. HalluQA contains 450 meticulously designed adversarial questions, spanning multiple domains, and takes into account Chinese historical culture, customs, and social phenomena. During the construction of HalluQA, we consider two types of hallucinations: imitative falsehoods and factual errors, and we construct adversarial samples based on GLM-130B and ChatGPT. For evaluation, we design an automated evaluation method using GPT-4 to judge whether a model output is hallucinated. We conduct extensive experiments on 24 large language models, including ERNIE-Bot, Baichuan2, ChatGLM, Qwen, SparkDesk and etc. Out of the 24 models, 18 achieved non-hallucination rates lower than 50%. This indicates that HalluQA is highly challenging. We analyze the primary types of hallucinations in different types of models and their causes. Additionally, we discuss which types of hallucinations should be prioritized for different types of models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_03368v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们建立了一个名为HalluQA（汉语幻觉问答）的基准来衡量汉语大语言模型中的幻觉现象。HalluQA包含450个精心设计的对抗性问题，跨越多个领域，并考虑到中国的历史文化、习俗和社会现象。在构建HalluQA的过程中，我们考虑了两种类型的幻觉：模仿性虚假和事实性错误，并基于GLM-130B和ChatGPT构建了对抗性样本。对于评估，我们设计了一种使用GPT-4来判断模型输出是否产生幻觉的自动评估方法。我们对ERNIE-Bot、百川2、ChatGLM、Qwen、SparkDesk等24个大型语言模型进行了广泛的实验。在24个模型中，18个模型的非幻觉率低于50%。这表明HalluQA极具挑战性。我们分析了不同类型模型中幻觉的主要类型及其原因。此外，我们还讨论了不同类型的模型应该优先考虑哪些类型的幻觉。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.03368v3" target="_blank">2310.03368v3</a>
                              </td>
                              <td>Evaluating Hallucinations in Chinese Large Language Models</td>
                              <td>Qinyuan Cheng</td>
                              <td>2023-10-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_03368v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.03368v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_01320v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_01320v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_01320v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_01320v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent breakthroughs in large language models (LLMs) have brought remarkable success in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is that the information processed by LLMs is consistently honest, neglecting the pervasive deceptive or misleading information in human society and AI-generated content. This oversight makes LLMs susceptible to malicious manipulations, potentially resulting in detrimental outcomes. This study utilizes the intricate Avalon game as a testbed to explore LLMs' potential in deceptive environments. Avalon, full of misinformation and requiring sophisticated logic, manifests as a "Game-of-Thoughts". Inspired by the efficacy of humans' recursive thinking and perspective-taking in the Avalon game, we introduce a novel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to identify and counteract deceptive information. ReCon combines formulation and refinement contemplation processes; formulation contemplation produces initial thoughts and speech, while refinement contemplation further polishes them. Additionally, we incorporate first-order and second-order perspective transitions into these processes respectively. Specifically, the first-order allows an LLM agent to infer others' mental states, and the second-order involves understanding how others perceive the agent's mental state. After integrating ReCon with different LLMs, extensive experiment results from the Avalon game indicate its efficacy in aiding LLMs to discern and maneuver around deceptive information without extra fine-tuning and data. Finally, we offer a possible explanation for the efficacy of ReCon and explore the current limitations of LLMs in terms of safety, reasoning, speaking style, and format, potentially furnishing insights for subsequent research.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_01320v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近在大型语言模型（LLM）方面的突破在LLM作为Agent领域取得了显著的成功。然而，一个普遍的假设是，LLM处理的信息始终是诚实的，忽略了人类社会中普遍存在的欺骗性或误导性信息以及人工智能生成的内容。这种疏忽使LLM容易受到恶意操作的影响，从而可能导致有害的结果。这项研究利用复杂的Avalon游戏作为试验台，探索LLM在欺骗性环境中的潜力。阿瓦隆充满了错误信息，需要复杂的逻辑，表现为“思想游戏”。受人类在阿瓦隆游戏中递归思维和视角转换的有效性的启发，我们引入了一个新的框架——递归思考（ReCon），以增强LLM识别和抵消欺骗性信息的能力。ReCon结合了制定和完善的思考过程；公式化的沉思产生了最初的思想和言语，而精细化的沉思则进一步完善了它们。此外，我们将一阶和二阶视角转换分别纳入这些过程。具体来说，一阶允许LLM代理推断他人的心理状态，二阶涉及理解他人如何感知代理的心理状态。在将ReCon与不同的LLM集成后，Avalon游戏的大量实验结果表明，它可以在没有额外微调和数据的情况下帮助LLM识别和操纵欺骗性信息。最后，我们对ReCon的疗效提供了一个可能的解释，并探讨了LLM在安全性、推理、说话风格和形式方面的当前局限性，可能为后续研究提供见解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.01320v3" target="_blank">2310.01320v3</a>
                              </td>
                              <td>Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation</td>
                              <td>Shenzhi Wang</td>
                              <td>2023-10-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_01320v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.01320v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15799v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DALE: Generative Data Augmentation for Low-Resource Legal NLP</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15799v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15799v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15799v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present DALE, a novel and effective generative Data Augmentation framework for low-resource LEgal NLP. DALE addresses the challenges existing frameworks pose in generating effective data augmentations of legal documents - legal language, with its specialized vocabulary and complex semantics, morphology, and syntax, does not benefit from data augmentations that merely rephrase the source sentence. To address this, DALE, built on an Encoder-Decoder Language Model, is pre-trained on a novel unsupervised text denoising objective based on selective masking - our masking strategy exploits the domain-specific language characteristics of templatized legal documents to mask collocated spans of text. Denoising these spans helps DALE acquire knowledge about legal concepts, principles, and language usage. Consequently, it develops the ability to generate coherent and diverse augmentations with novel contexts. Finally, DALE performs conditional generation to generate synthetic augmentations for low-resource Legal NLP tasks. We demonstrate the effectiveness of DALE on 13 datasets spanning 6 tasks and 4 low-resource settings. DALE outperforms all our baselines, including LLMs, qualitatively and quantitatively, with improvements of 1%-50%.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15799v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了DALE，一种新的、有效的低资源LEgal NLP生成数据增强框架。DALE解决了现有框架在生成有效的法律文件数据扩充方面带来的挑战——法律语言具有专门的词汇和复杂的语义、形态和语法，不能从仅仅改写源句的数据扩充中受益。为了解决这一问题，基于编码器-解码器语言模型的DALE在一种新的基于选择性掩蔽的无监督文本去噪目标上进行了预训练——我们的掩蔽策略利用模板化法律文件的领域特定语言特征来掩蔽并置的文本跨度。消除这些跨度有助于DALE获得有关法律概念、原则和语言使用的知识。因此，它发展了在新的背景下产生连贯和多样的增强的能力。最后，DALE执行条件生成，为低资源的Legal NLP任务生成合成增强。我们在跨越6个任务和4个低资源设置的13个数据集上证明了DALE的有效性。DALE在质量和数量上都优于我们的所有基线，包括LLM，改善了1%-50%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15799v1" target="_blank">2310.15799v1</a>
                              </td>
                              <td>DALE: Generative Data Augmentation for Low-Resource Legal NLP</td>
                              <td>Sreyan Ghosh</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15799v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15799v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15793v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Improving generalization in large language models by learning prefix subspaces</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15793v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15793v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15793v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This article focuses on large language models (LLMs) fine-tuning in the scarce data regime (also known as the "few-shot" learning setting). We propose a method to increase the generalization capabilities of LLMs based on neural network subspaces. This optimization method, recently introduced in computer vision, aims to improve model generalization by identifying wider local optima through the joint optimization of an entire simplex of models in parameter space. Its adaptation to massive, pretrained transformers, however, poses some challenges. First, their considerable number of parameters makes it difficult to train several models jointly, and second, their deterministic parameter initialization schemes make them unfit for the subspace method as originally proposed. We show in this paper that "Parameter Efficient Fine-Tuning" (PEFT) methods, however, are perfectly compatible with this original approach, and propose to learn entire simplex of continuous prefixes. We test our method on a variant of the GLUE benchmark adapted to the few-shot learning setting, and show that both our contributions jointly lead to a gain in average performances compared to sota methods. The implementation can be found at the following link: https://github.com/Liloulou/prefix_subspace</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15793v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文关注的是在数据稀缺的情况下（也称为“少量”学习环境）对大型语言模型（LLM）进行微调。我们提出了一种基于神经网络子空间的方法来提高LLM的泛化能力。最近在计算机视觉中引入的这种优化方法旨在通过在参数空间中对整个单纯形模型进行联合优化来识别更宽的局部最优值，从而提高模型的泛化能力。然而，它对大规模、经过预训练的变压器的适应带来了一些挑战。首先，它们的大量参数使联合训练几个模型变得困难，其次，它们的确定性参数初始化方案使它们不适合最初提出的子空间方法。然而，我们在本文中表明，“参数有效微调”（PEFT）方法与这种原始方法完全兼容，并建议学习连续前缀的整个单纯形。我们在适用于少镜头学习设置的GLUE基准的变体上测试了我们的方法，并表明与sota方法相比，我们的两个贡献共同导致了平均性能的提高。可以在以下链接中找到实现：https://github.com/Liloulou/prefix_subspace</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15793v1" target="_blank">2310.15793v1</a>
                              </td>
                              <td>Improving generalization in large language models by learning prefix subspaces</td>
                              <td>Louis Falissard</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15793v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15793v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15780v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Make LLM a Testing Expert: Bringing Human-like Interaction to Mobile GUI Testing via Functionality-aware Decisions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15780v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15780v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15780v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Automated Graphical User Interface (GUI) testing plays a crucial role in ensuring app quality, especially as mobile applications have become an integral part of our daily lives. Despite the growing popularity of learning-based techniques in automated GUI testing due to their ability to generate human-like interactions, they still suffer from several limitations, such as low testing coverage, inadequate generalization capabilities, and heavy reliance on training data. Inspired by the success of Large Language Models (LLMs) like ChatGPT in natural language understanding and question answering, we formulate the mobile GUI testing problem as a Q&A task. We propose GPTDroid, asking LLM to chat with the mobile apps by passing the GUI page information to LLM to elicit testing scripts, and executing them to keep passing the app feedback to LLM, iterating the whole process. Within this framework, we have also introduced a functionality-aware memory prompting mechanism that equips the LLM with the ability to retain testing knowledge of the whole process and conduct long-term, functionality-based reasoning to guide exploration. We evaluate it on 93 apps from Google Play and demonstrate that it outperforms the best baseline by 32% in activity coverage, and detects 31% more bugs at a faster rate. Moreover, GPTDroid identify 53 new bugs on Google Play, of which 35 have been confirmed and fixed.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15780v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自动化图形用户界面（GUI）测试在确保应用程序质量方面发挥着至关重要的作用，尤其是在移动应用程序已成为我们日常生活不可或缺的一部分的情况下。尽管基于学习的技术由于其生成类人交互的能力而在自动化GUI测试中越来越受欢迎，但它们仍然存在一些局限性，如测试覆盖率低、泛化能力不足以及严重依赖训练数据。受ChatGPT等大型语言模型（LLM）在自然语言理解和问答方面的成功启发，我们将移动GUI测试问题表述为问答任务。我们提出了GPTDroid，要求LLM通过将GUI页面信息传递给LLM来引发测试脚本，并执行它们来不断将应用反馈传递给LLM，从而迭代整个过程，从而与移动应用程序聊天。在这个框架内，我们还引入了一种功能感知记忆提示机制，使LLM能够保留整个过程的测试知识，并进行长期的基于功能的推理来指导探索。我们在Google Play的93个应用程序上对其进行了评估，并证明它的活动覆盖率比最佳基线高出32%，并以更快的速度检测到31%的错误。此外，GPTDroid在Google Play上发现了53个新错误，其中35个已被确认并修复。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15780v1" target="_blank">2310.15780v1</a>
                              </td>
                              <td>Make LLM a Testing Expert: Bringing Human-like Interaction to Mobile GUI Testing via Functionality-aware Decisions</td>
                              <td>Zhe Liu</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15780v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15780v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15777v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MindLLM: Pre-training Lightweight Large Language Model from Scratch, Evaluations and Domain Applications</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15777v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15777v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15777v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) have demonstrated remarkable performance across various natural language tasks, marking significant strides towards general artificial intelligence. While general artificial intelligence is leveraged by developing increasingly large-scale models, there could be another branch to develop lightweight custom models that better serve certain domains, taking into account the high cost of training and deploying LLMs and the scarcity of resources. In this paper, we present MindLLM, a novel series of bilingual lightweight large language models, trained from scratch, alleviating such burdens by offering models with 1.3 billion and 3 billion parameters. A thorough account of experiences accrued during large model development is given, covering every step of the process, including data construction, model architecture, evaluation, and applications. Such insights are hopefully valuable for fellow academics and developers. MindLLM consistently matches or surpasses the performance of other open-source larger models on some public benchmarks. We also introduce an innovative instruction tuning framework tailored for smaller models to enhance their capabilities efficiently. Moreover, we explore the application of MindLLM in specific vertical domains such as law and finance, underscoring the agility and adaptability of our lightweight models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15777v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）在各种自然语言任务中表现出了非凡的性能，标志着向通用人工智能迈出了重大步伐。虽然通用人工智能通过开发越来越大规模的模型来发挥作用，但考虑到LLM的培训和部署成本高昂以及资源稀缺，可能还有另一个分支可以开发更好地服务于某些领域的轻量级定制模型。在本文中，我们提出了MindLLM，这是一系列新的双语轻量级大型语言模型，从零开始训练，通过提供具有13亿和30亿参数的模型来减轻这些负担。全面介绍了大型模型开发过程中积累的经验，涵盖了过程的每一步，包括数据构建、模型架构、评估和应用。这样的见解有望对其他学者和开发人员有价值。MindLLM在一些公共基准测试上的性能始终与其他开源大型模型相匹配或超越。我们还引入了一个创新的教学调整框架，专门为较小的模型量身定制，以有效地增强其能力。此外，我们探索了MindLLM在法律和金融等特定垂直领域的应用，强调了我们轻量级模型的灵活性和适应性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15777v1" target="_blank">2310.15777v1</a>
                              </td>
                              <td>MindLLM: Pre-training Lightweight Large Language Model from Scratch, Evaluations and Domain Applications</td>
                              <td>Yizhe Yang</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15777v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15777v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15773v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BLESS: Benchmarking Large Language Models on Sentence Simplification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15773v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15773v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15773v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present BLESS, a comprehensive performance benchmark of the most recent state-of-the-art large language models (LLMs) on the task of text simplification (TS). We examine how well off-the-shelf LLMs can solve this challenging task, assessing a total of 44 models, differing in size, architecture, pre-training methods, and accessibility, on three test sets from different domains (Wikipedia, news, and medical) under a few-shot setting. Our analysis considers a suite of automatic metrics as well as a large-scale quantitative investigation into the types of common edit operations performed by the different models. Furthermore, we perform a manual qualitative analysis on a subset of model outputs to better gauge the quality of the generated simplifications. Our evaluation indicates that the best LLMs, despite not being trained on TS, perform comparably with state-of-the-art TS baselines. Additionally, we find that certain LLMs demonstrate a greater range and diversity of edit operations. Our performance benchmark will be available as a resource for the development of future TS methods and evaluation metrics.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15773v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了BLESS，这是最新最先进的大型语言模型（LLM）在文本简化（TS）任务中的综合性能基准。我们研究了现成的LLM在多大程度上能够解决这一具有挑战性的任务，在来自不同领域（维基百科、新闻和医学）的三个测试集上，在几个镜头的设置下，评估了总共44个模型，这些模型在大小、架构、预训练方法和可访问性方面都有所不同。我们的分析考虑了一套自动指标，以及对不同模型执行的常见编辑操作类型的大规模定量调查。此外，我们对模型输出的子集进行手动定性分析，以更好地衡量生成的简化的质量。我们的评估表明，尽管没有接受TS培训，但最好的LLM的性能与最先进的TS基线相当。此外，我们发现某些LLM表现出更大的编辑操作范围和多样性。我们的性能基准将作为开发未来TS方法和评估指标的资源。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15773v1" target="_blank">2310.15773v1</a>
                              </td>
                              <td>BLESS: Benchmarking Large Language Models on Sentence Simplification</td>
                              <td>Tannon Kew</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15773v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15773v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15758v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning From Free-Text Human Feedback -- Collect New Datasets Or Extend Existing Ones?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15758v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15758v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15758v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Learning from free-text human feedback is essential for dialog systems, but annotated data is scarce and usually covers only a small fraction of error types known in conversational AI. Instead of collecting and annotating new datasets from scratch, recent advances in synthetic dialog generation could be used to augment existing dialog datasets with the necessary annotations. However, to assess the feasibility of such an effort, it is important to know the types and frequency of free-text human feedback included in these datasets. In this work, we investigate this question for a variety of commonly used dialog datasets, including MultiWoZ, SGD, BABI, PersonaChat, Wizards-of-Wikipedia, and the human-bot split of the Self-Feeding Chatbot. Using our observations, we derive new taxonomies for the annotation of free-text human feedback in dialogs and investigate the impact of including such data in response generation for three SOTA language generation models, including GPT-2, LLAMA, and Flan-T5. Our findings provide new insights into the composition of the datasets examined, including error types, user response types, and the relations between them.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15758v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从自由文本的人类反馈中学习对对话系统至关重要，但注释数据很少，通常只涵盖对话人工智能中已知的一小部分错误类型。合成对话生成的最新进展可以用来用必要的注释来扩充现有对话数据集，而不是从头开始收集和注释新的数据集。然而，为了评估这种努力的可行性，了解这些数据集中包含的自由文本人类反馈的类型和频率是很重要的。在这项工作中，我们针对各种常用的对话数据集研究了这个问题，包括MultiWoZ、SGD、BABI、PersonaChat、维基百科的巫师，以及自喂食聊天机器人的人类机器人分割。利用我们的观察结果，我们推导了对话框中自由文本人类反馈注释的新分类法，并研究了在三个SOTA语言生成模型（包括GPT-2、LLAMA和Flan-T5）的响应生成中包含这些数据的影响。我们的发现为所检查的数据集的组成提供了新的见解，包括错误类型、用户响应类型以及它们之间的关系。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15758v1" target="_blank">2310.15758v1</a>
                              </td>
                              <td>Learning From Free-Text Human Feedback -- Collect New Datasets Or Extend Existing Ones?</td>
                              <td>Dominic Petrak</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15758v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15758v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15747v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Large Language Models are Temporal and Causal Reasoners for Video Question Answering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15747v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15747v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15747v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) have shown remarkable performances on a wide range of natural language understanding and generation tasks. We observe that the LLMs provide effective priors in exploiting $\textit{linguistic shortcuts}$ for temporal and causal reasoning in Video Question Answering (VideoQA). However, such priors often cause suboptimal results on VideoQA by leading the model to over-rely on questions, $\textit{i.e.}$, $\textit{linguistic bias}$, while ignoring visual content. This is also known as `ungrounded guesses' or `hallucinations'. To address this problem while leveraging LLMs' prior on VideoQA, we propose a novel framework, Flipped-VQA, encouraging the model to predict all the combinations of $\langle$V, Q, A$\rangle$ triplet by flipping the source pair and the target label to understand their complex relationships, $\textit{i.e.}$, predict A, Q, and V given a VQ, VA, and QA pairs, respectively. In this paper, we develop LLaMA-VQA by applying Flipped-VQA to LLaMA, and it outperforms both LLMs-based and non-LLMs-based models on five challenging VideoQA benchmarks. Furthermore, our Flipped-VQA is a general framework that is applicable to various LLMs (OPT and GPT-J) and consistently improves their performances. We empirically demonstrate that Flipped-VQA not only enhances the exploitation of linguistic shortcuts but also mitigates the linguistic bias, which causes incorrect answers over-relying on the question. Code is available at https://github.com/mlvlab/Flipped-VQA.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15747v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）在广泛的自然语言理解和生成任务中表现出了显著的性能。我们观察到，LLM在视频问答（VideoQA）中利用$\textit{语言快捷键}$进行时间和因果推理时提供了有效的先验。然而，这种先验通常会导致模型过度依赖问题$\textit｛即｝$、$\textit{语言偏见｝$，而忽略视觉内容，从而在VideoQA上导致次优结果。这也被称为“没有根据的猜测”或“幻觉”。为了在利用LLM在VideoQA上的优势的同时解决这个问题，我们提出了一个新的框架Flipped VQA，鼓励模型通过翻转源对和目标标签来预测$\langle$V、Q、a$\langle$三元组的所有组合，以理解它们的复杂关系，$\textit｛即｝$，分别在给定VQ、VA和QA对的情况下预测a、Q和V。在本文中，我们通过将Flipped VQA应用于LLaMA来开发LLaMA VQA，并且它在五个具有挑战性的VideoQA基准测试中都优于基于LLM和非LLM的模型。此外，我们的Flipped VQA是一个通用框架，适用于各种LLM（OPT和GPT-J），并不断提高其性能。我们实证证明，Flipped VQA不仅增强了对语言快捷方式的利用，而且减轻了语言偏见，这种偏见会导致错误答案过于依赖问题。代码可在https://github.com/mlvlab/Flipped-VQA.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15747v1" target="_blank">2310.15747v1</a>
                              </td>
                              <td>Large Language Models are Temporal and Causal Reasoners for Video Question Answering</td>
                              <td>Dohwan Ko</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15747v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15747v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15746v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15746v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15746v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15746v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) have showcased impressive performance. However, due to their inability to capture relationships among samples, these frozen LLMs inevitably keep repeating similar mistakes. In this work, we propose our Tuning-free Rule Accumulation (TRAN) framework, which guides LLMs in improving their performance by learning from previous mistakes. Considering data arrives sequentially, LLMs gradually accumulate rules from incorrect cases, forming a rule collection. These rules are then utilized by the LLMs to avoid making similar mistakes when processing subsequent inputs. Moreover, the rules remain independent of the primary prompts, seamlessly complementing prompt design strategies. Experimentally, we show that TRAN improves over recent baselines by a large margin.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15746v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）展示了令人印象深刻的性能。然而，由于无法捕捉样本之间的关系，这些冻结的LLM不可避免地会重复类似的错误。在这项工作中，我们提出了我们的无调整规则积累（TRAN）框架，该框架指导LLM通过从以前的错误中学习来提高其性能。考虑到数据是按顺序到达的，LLM从错误的案例中逐渐积累规则，形成规则集合。LLM利用这些规则来避免在处理后续输入时出现类似错误。此外，这些规则仍然独立于主要提示，无缝地补充了提示设计策略。实验表明，TRAN比最近的基线有很大的改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15746v1" target="_blank">2310.15746v1</a>
                              </td>
                              <td>Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation</td>
                              <td>Zeyuan Yang</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15746v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15746v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_14724v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Survey on LLM-generated Text Detection: Necessity, Methods, and Future Directions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_14724v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_14724v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_14724v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The powerful ability to understand, follow, and generate complex language emerging from large language models (LLMs) makes LLM-generated text flood many areas of our daily lives at an incredible speed and is widely accepted by humans. As LLMs continue to expand, there is an imperative need to develop detectors that can detect LLM-generated text. This is crucial to mitigate potential misuse of LLMs and safeguard realms like artistic expression and social networks from harmful influence of LLM-generated content. The LLM-generated text detection aims to discern if a piece of text was produced by an LLM, which is essentially a binary classification task. The detector techniques have witnessed notable advancements recently, propelled by innovations in watermarking techniques, zero-shot methods, fine-turning LMs methods, adversarial learning methods, LLMs as detectors, and human-assisted methods. In this survey, we collate recent research breakthroughs in this area and underscore the pressing need to bolster detector research. We also delve into prevalent datasets, elucidating their limitations and developmental requirements. Furthermore, we analyze various LLM-generated text detection paradigms, shedding light on challenges like out-of-distribution problems, potential attacks, and data ambiguity. Conclusively, we highlight interesting directions for future research in LLM-generated text detection to advance the implementation of responsible artificial intelligence (AI). Our aim with this survey is to provide a clear and comprehensive introduction for newcomers while also offering seasoned researchers a valuable update in the field of LLM-generated text detection. The useful resources are publicly available at: https://github.com/NLP2CT/LLM-generated-Text-Detection.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_14724v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从大型语言模型（LLM）中产生的理解、遵循和生成复杂语言的强大能力，使LLM生成的文本以惊人的速度充斥着我们日常生活的许多领域，并被人类广泛接受。随着LLM的不断扩展，迫切需要开发能够检测LLM生成的文本的检测器。这对于减少LLM的潜在滥用，保护艺术表达和社交网络等领域免受LLM生成内容的有害影响至关重要。LLM生成的文本检测旨在辨别一段文本是否由LLM生成，这本质上是一项二进制分类任务。最近，在水印技术、零样本方法、微调LMs方法、对抗性学习方法、LLM作为检测器和人工辅助方法的创新推动下，检测器技术取得了显著进步。在这项调查中，我们整理了该领域最近的研究突破，并强调了加强探测器研究的迫切需要。我们还深入研究了流行的数据集，阐明了它们的局限性和开发需求。此外，我们分析了LLM生成的各种文本检测范式，揭示了诸如分发外问题、潜在攻击和数据歧义等挑战。最后，我们强调了LLM生成文本检测的未来研究方向，以推进负责任人工智能（AI）的实现。我们此次调查的目的是为新来者提供清晰全面的介绍，同时也为经验丰富的研究人员提供LLM生成文本检测领域的宝贵更新。有用的资源可在以下网址公开获取：https://github.com/NLP2CT/LLM-generated-Text-Detection.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.14724v2" target="_blank">2310.14724v2</a>
                              </td>
                              <td>A Survey on LLM-generated Text Detection: Necessity, Methods, and Future Directions</td>
                              <td>Junchao Wu</td>
                              <td>2023-10-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_14724v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.14724v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_12962v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Distilling ChatGPT for Explainable Automated Student Answer Assessment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_12962v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_12962v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_12962v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Providing explainable and faithful feedback is crucial for automated student answer assessment. In this paper, we introduce a novel framework that explores using ChatGPT, a cutting-edge large language model, for the concurrent tasks of student answer scoring and rationale generation. We identify the appropriate instructions by prompting ChatGPT with different templates to collect the rationales, where inconsistent rationales are refined to align with marking standards. The refined ChatGPT outputs enable us to fine-tune a smaller language model that simultaneously assesses student answers and provides rationales. Extensive experiments on the benchmark dataset show that the proposed method improves the overall QWK score by 11% compared to ChatGPT. Furthermore, our thorough analysis and human evaluation demonstrate that the rationales generated by our proposed method are comparable to those of ChatGPT. Our approach provides a viable solution to achieve explainable automated assessment in education. Code available at https://github.com/lijiazheng99/aera.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_12962v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>提供可解释和忠实的反馈对于自动化的学生答案评估至关重要。在本文中，我们介绍了一个新的框架，该框架探索使用尖端的大型语言模型ChatGPT来同时执行学生答案评分和基本原理生成的任务。我们通过提示具有不同模板的ChatGPT收集理由来确定适当的说明，其中不一致的理由被细化以符合标记标准。改进的ChatGPT输出使我们能够微调一个较小的语言模型，该模型同时评估学生的答案并提供理由。在基准数据集上的大量实验表明，与ChatGPT相比，所提出的方法将QWK总分提高了11%。此外，我们的全面分析和人类评估表明，我们提出的方法产生的理由与ChatGPT的理由相当。我们的方法为实现教育中可解释的自动化评估提供了一个可行的解决方案。代码可在https://github.com/lijiazheng99/aera.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.12962v2" target="_blank">2305.12962v2</a>
                              </td>
                              <td>Distilling ChatGPT for Explainable Automated Student Answer Assessment</td>
                              <td>Jiazheng Li</td>
                              <td>2023-05-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_12962v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.12962v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_15077v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Contrastive Learning of Sentence Embeddings from Scratch</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_15077v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_15077v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_15077v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Contrastive learning has been the dominant approach to train state-of-the-art sentence embeddings. Previous studies have typically learned sentence embeddings either through the use of human-annotated natural language inference (NLI) data or via large-scale unlabeled sentences in an unsupervised manner. However, even in the case of unlabeled data, their acquisition presents challenges in certain domains due to various reasons. To address these issues, we present SynCSE, a contrastive learning framework that trains sentence embeddings with synthesized data. Specifically, we explore utilizing large language models to synthesize the required data samples for contrastive learning, including (1) producing positive and negative annotations given unlabeled sentences (SynCSE-partial), and (2) generating sentences along with their corresponding annotations from scratch (SynCSE-scratch). Experimental results on sentence similarity and reranking tasks indicate that both SynCSE-partial and SynCSE-scratch greatly outperform unsupervised baselines, and SynCSE-partial even achieves comparable performance to the supervised models in most settings.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_15077v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对比学习一直是训练最先进的句子嵌入的主要方法。先前的研究通常通过使用人类注释的自然语言推理（NLI）数据或以无监督的方式通过大规模未标记的句子来学习句子嵌入。然而，即使在未标记数据的情况下，由于各种原因，它们的获取在某些领域也面临挑战。为了解决这些问题，我们提出了SynCSE，这是一个对比学习框架，可以用合成数据训练句子嵌入。具体而言，我们探索利用大型语言模型来合成对比学习所需的数据样本，包括（1）在给定未标记句子的情况下产生积极和消极的注释（SynCSE部分），以及（2）从头开始产生句子及其相应的注释（SynCSE scratch）。关于句子相似性和重新排序任务的实验结果表明，SynCSE-partial和SynCSE-scratch都大大优于无监督基线，并且SynCSE-partial在大多数情况下甚至达到了与监督模型相当的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.15077v2" target="_blank">2305.15077v2</a>
                              </td>
                              <td>Contrastive Learning of Sentence Embeddings from Scratch</td>
                              <td>Junlei Zhang</td>
                              <td>2023-05-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_15077v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.15077v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15683v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Prevalence and prevention of large language model use in crowd work</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15683v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15683v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15683v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We show that the use of large language models (LLMs) is prevalent among crowd workers, and that targeted mitigation strategies can significantly reduce, but not eliminate, LLM use. On a text summarization task where workers were not directed in any way regarding their LLM use, the estimated prevalence of LLM use was around 30%, but was reduced by about half by asking workers to not use LLMs and by raising the cost of using them, e.g., by disabling copy-pasting. Secondary analyses give further insight into LLM use and its prevention: LLM use yields high-quality but homogeneous responses, which may harm research concerned with human (rather than model) behavior and degrade future models trained with crowdsourced data. At the same time, preventing LLM use may be at odds with obtaining high-quality responses; e.g., when requesting workers not to use LLMs, summaries contained fewer keywords carrying essential information. Our estimates will likely change as LLMs increase in popularity or capabilities, and as norms around their usage change. Yet, understanding the co-evolution of LLM-based tools and users is key to maintaining the validity of research done using crowdsourcing, and we provide a critical baseline before widespread adoption ensues.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15683v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们表明，大型语言模型（LLM）的使用在人群工作者中很普遍，有针对性的缓解策略可以显著减少但不能消除LLM的使用。在文本摘要任务中，没有以任何方式指导工人使用LLM，LLM使用的估计流行率约为30%，但通过要求工人不要使用LLM和提高使用成本（例如禁用复制粘贴），LLM的使用率降低了约一半。二次分析进一步深入了解了LLM的使用及其预防：LLM的应用产生了高质量但同质的反应，这可能会损害与人类（而不是模型）行为有关的研究，并降低用众包数据训练的未来模型。同时，防止LLM的使用可能与获得高质量的响应不一致；例如，当要求员工不要使用LLM时，摘要中包含的携带基本信息的关键词较少。我们的估计可能会随着LLM的受欢迎程度或功能的增加以及其使用规范的变化而变化。然而，理解基于LLM的工具和用户的共同进化是保持众包研究有效性的关键，我们在广泛采用之前提供了一个关键的基线。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15683v1" target="_blank">2310.15683v1</a>
                              </td>
                              <td>Prevalence and prevention of large language model use in crowd work</td>
                              <td>Veniamin Veselovsky</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15683v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15683v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_15023v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_15023v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_15023v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_15023v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, growing interest has been aroused in extending the multimodal capability of large language models (LLMs), e.g., vision-language (VL) learning, which is regarded as the next milestone of artificial general intelligence. However, existing solutions are prohibitively expensive, which not only need to optimize excessive parameters, but also require another large-scale pre-training before VL instruction tuning. In this paper, we propose a novel and affordable solution for the effective VL adaption of LLMs, called Mixture-of-Modality Adaptation (MMA). Instead of using large neural networks to connect the image encoder and LLM, MMA adopts lightweight modules, i.e., adapters, to bridge the gap between LLMs and VL tasks, which also enables the joint optimization of the image and language models. Meanwhile, MMA is also equipped with a routing algorithm to help LLMs achieve an automatic shift between single- and multi-modal instructions without compromising their ability of natural language understanding. To validate MMA, we apply it to a recent LLM called LLaMA and term this formed large vision-language instructed model as LaVIN. To validate MMA and LaVIN, we conduct extensive experiments under two setups, namely multimodal science question answering and multimodal dialogue. The experimental results not only demonstrate the competitive performance and the superior training efficiency of LaVIN than existing multimodal LLMs, but also confirm its great potential as a general-purpose chatbot. More importantly, the actual expenditure of LaVIN is extremely cheap, e.g., only 1.4 training hours with 3.8M trainable parameters, greatly confirming the effectiveness of MMA. Our project is released at https://luogen1996.github.io/lavin.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_15023v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，人们越来越关注扩展大型语言模型（LLM）的多模式能力，例如视觉语言（VL）学习，这被视为通用人工智能的下一个里程碑。然而，现有的解决方案非常昂贵，不仅需要优化过多的参数，还需要在VL指令调优之前进行另一次大规模的预训练。在本文中，我们提出了一种新的、负担得起的LLM有效VL自适应解决方案，称为模态自适应混合（MMA）。MMA没有使用大型神经网络来连接图像编码器和LLM，而是采用了轻量级模块，即适配器，来弥合LLM和VL任务之间的差距，这也实现了图像和语言模型的联合优化。同时，MMA还配备了路由算法，以帮助LLM在不影响其自然语言理解能力的情况下实现单模式和多模式指令之间的自动转换。为了验证MMA，我们将其应用于最近的LLM LLaMA，并将这种形成的大型视觉语言指令模型命名为LaVIN。为了验证MMA和LaVIN，我们在两种设置下进行了广泛的实验，即多模式科学问答和多模式对话。实验结果不仅证明了LaVIN的竞争性能和优于现有多模式LLM的训练效率，还证实了其作为通用聊天机器人的巨大潜力。更重要的是，LaVIN的实际支出非常便宜，例如，只有1.4个训练小时，可训练参数为3.8M，极大地证实了MMA的有效性。我们的项目发布于https://luogen1996.github.io/lavin.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.15023v3" target="_blank">2305.15023v3</a>
                              </td>
                              <td>Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models</td>
                              <td>Gen Luo</td>
                              <td>2023-05-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_15023v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.15023v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15657v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Testing the Limits: Unusual Text Inputs Generation for Mobile App Crash Detection with Large Language Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15657v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15657v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15657v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Mobile applications have become a ubiquitous part of our daily life, providing users with access to various services and utilities. Text input, as an important interaction channel between users and applications, plays an important role in core functionality such as search queries, authentication, messaging, etc. However, certain special text (e.g., -18 for Font Size) can cause the app to crash, and generating diversified unusual inputs for fully testing the app is highly demanded. Nevertheless, this is also challenging due to the combination of explosion dilemma, high context sensitivity, and complex constraint relations. This paper proposes InputBlaster which leverages the LLM to automatically generate unusual text inputs for mobile app crash detection. It formulates the unusual inputs generation problem as a task of producing a set of test generators, each of which can yield a batch of unusual text inputs under the same mutation rule. In detail, InputBlaster leverages LLM to produce the test generators together with the mutation rules serving as the reasoning chain, and utilizes the in-context learning schema to demonstrate the LLM with examples for boosting the performance. InputBlaster is evaluated on 36 text input widgets with cash bugs involving 31 popular Android apps, and results show that it achieves 78% bug detection rate, with 136% higher than the best baseline. Besides, we integrate it with the automated GUI testing tool and detect 37 unseen crashes in real-world apps from Google Play.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15657v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>移动应用程序已成为我们日常生活中无处不在的一部分，为用户提供各种服务和实用程序。文本输入作为用户和应用程序之间的重要交互渠道，在搜索查询、身份验证、消息传递等核心功能中发挥着重要作用。然而，某些特殊的文本（例如，字体大小为-18）可能会导致应用程序崩溃，因此非常需要生成多样化的异常输入来全面测试应用程序。然而，由于爆炸困境、高上下文敏感性和复杂的约束关系的结合，这也是具有挑战性的。本文提出了InputBlaster，它利用LLM自动生成用于移动应用程序崩溃检测的异常文本输入。它将异常输入生成问题公式化为生成一组测试生成器的任务，每个测试生成器可以在相同的突变规则下生成一批异常文本输入。详细地说，InputBlaster利用LLM生成测试生成器，并将突变规则作为推理链，并利用上下文中的学习模式来演示LLM，并举例提高性能。InputBlaster在涉及31款流行安卓应用程序的36款带有现金错误的文本输入小工具上进行了评估，结果显示，它的错误检测率为78%，比最佳基线高出136%。此外，我们将其与自动GUI测试工具集成，并从Google Play中检测到现实世界应用程序中的37个看不见的崩溃。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>46</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15657v1" target="_blank">2310.15657v1</a>
                              </td>
                              <td>Testing the Limits: Unusual Text Inputs Generation for Mobile App Crash Detection with Large Language Model</td>
                              <td>Zhe Liu</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15657v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15657v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15654v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Survey on Detection of LLMs-Generated Content</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15654v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15654v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15654v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The burgeoning capabilities of advanced large language models (LLMs) such as ChatGPT have led to an increase in synthetic content generation with implications across a variety of sectors, including media, cybersecurity, public discourse, and education. As such, the ability to detect LLMs-generated content has become of paramount importance. We aim to provide a detailed overview of existing detection strategies and benchmarks, scrutinizing their differences and identifying key challenges and prospects in the field, advocating for more adaptable and robust models to enhance detection accuracy. We also posit the necessity for a multi-faceted approach to defend against various attacks to counter the rapidly advancing capabilities of LLMs. To the best of our knowledge, this work is the first comprehensive survey on the detection in the era of LLMs. We hope it will provide a broad understanding of the current landscape of LLMs-generated content detection, offering a guiding reference for researchers and practitioners striving to uphold the integrity of digital information in an era increasingly dominated by synthetic content. The relevant papers are summarized and will be consistently updated at https://github.com/Xianjun-Yang/Awesome_papers_on_LLMs_detection.git.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15654v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>ChatGPT等先进的大型语言模型（LLM）的蓬勃发展的能力导致了合成内容生成的增加，影响了媒体、网络安全、公共话语和教育等多个领域。因此，检测LLM生成的内容的能力变得至关重要。我们的目标是详细概述现有的检测策略和基准，仔细审查它们的差异，并确定该领域的关键挑战和前景，倡导更具适应性和鲁棒性的模型来提高检测准确性。我们还认为，有必要采取多方面的方法来防御各种攻击，以对抗LLM快速发展的能力。据我们所知，这项工作是LLMs时代首次对检测进行全面调查。我们希望它能对LLM生成的内容检测的现状有一个广泛的了解，为在合成内容日益主导的时代努力维护数字信息完整性的研究人员和从业者提供指导性参考。相关文件已汇总，并将在https://github.com/Xianjun-Yang/Awesome_papers_on_LLMs_detection.git.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>47</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15654v1" target="_blank">2310.15654v1</a>
                              </td>
                              <td>A Survey on Detection of LLMs-Generated Content</td>
                              <td>Xianjun Yang</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15654v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15654v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15638v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15638v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15638v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15638v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Annotated data plays a critical role in Natural Language Processing (NLP) in training models and evaluating their performance. Given recent developments in Large Language Models (LLMs), models such as ChatGPT demonstrate zero-shot capability on many text-annotation tasks, comparable with or even exceeding human annotators. Such LLMs can serve as alternatives for manual annotation, due to lower costs and higher scalability. However, limited work has leveraged LLMs as complementary annotators, nor explored how annotation work is best allocated among humans and LLMs to achieve both quality and cost objectives. We propose CoAnnotating, a novel paradigm for Human-LLM co-annotation of unstructured texts at scale. Under this framework, we utilize uncertainty to estimate LLMs' annotation capability. Our empirical study shows CoAnnotating to be an effective means to allocate work from results on different datasets, with up to 21% performance improvement over random baseline. For code implementation, see https://github.com/SALT-NLP/CoAnnotating.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15638v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在自然语言处理（NLP）中，注释数据在训练模型和评估模型性能方面发挥着关键作用。鉴于大型语言模型（LLM）的最新发展，像ChatGPT这样的模型在许多文本注释任务上展示了零样本能力，与人类注释器相当，甚至超过了人类注释器。这样的LLM可以作为手动注释的替代方案，因为成本更低，可扩展性更高。然而，有限的工作利用LLM作为补充注释器，也没有探索如何在人类和LLM之间最好地分配注释工作，以实现质量和成本目标。我们提出了CoAnnotating，这是一种在尺度上对非结构化文本进行人类LLM联合注释的新范式。在这个框架下，我们利用不确定性来估计LLM的注释能力。我们的实证研究表明，CoAnnotating是从不同数据集的结果中分配工作的有效手段，与随机基线相比，性能提高了21%。有关代码实现，请参阅https://github.com/SALT-NLP/CoAnnotating.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>48</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15638v1" target="_blank">2310.15638v1</a>
                              </td>
                              <td>CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation</td>
                              <td>Minzhi Li</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15638v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15638v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_13627v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">InstructAlign: High-and-Low Resource Language Alignment via Continual Crosslingual Instruction Tuning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_13627v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_13627v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_13627v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) that are tuned with instructions have demonstrated remarkable capabilities in various tasks and languages. However, their ability to generalize to underrepresented languages is limited due to the scarcity of available data. Additionally, directly adapting new languages to instruction-tuned LLMs can result in catastrophic forgetting, which leads to the loss of multitasking ability. To address this issue, we propose InstructAlign which uses continual crosslingual instruction tuning to enable LLMs to align new unseen languages with previously learned high-resource languages. Our results demonstrate the effectiveness of InstructAlign in enabling the model to understand low-resource languages with limited parallel data while preventing catastrophic forgetting. Our work contributes to the advancement of language adaptation methods, particularly for adapting instruction-tuned LLMs to underrepresented languages. Our code is released on https://github.com/HLTCHKUST/InstructAlign</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_13627v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>经过指令调整的大型语言模型（LLM）在各种任务和语言中都表现出了非凡的能力。然而，由于缺乏可用数据，他们推广到代表性不足的语言的能力有限。此外，直接将新语言适应指令调整的LLM可能会导致灾难性的遗忘，从而导致多任务处理能力的丧失。为了解决这个问题，我们提出了InstructionAlign，它使用持续的跨语言指令调整，使LLM能够将新的看不见的语言与以前学习的高资源语言对齐。我们的结果证明了InstructionAlign的有效性，它使模型能够理解具有有限并行数据的低资源语言，同时防止灾难性遗忘。我们的工作有助于语言适应方法的进步，特别是将调整教学的LLM适应代表性不足的语言。我们的代码发布于https://github.com/HLTCHKUST/InstructAlign</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>49</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.13627v2" target="_blank">2305.13627v2</a>
                              </td>
                              <td>InstructAlign: High-and-Low Resource Language Alignment via Continual Crosslingual Instruction Tuning</td>
                              <td>Samuel Cahyawijaya</td>
                              <td>2023-05-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_13627v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.13627v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="SAM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2310_13548v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Understanding Sycophancy in Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_13548v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_13548v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_13548v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Reinforcement learning from human feedback (RLHF) is a popular technique for training high-quality AI assistants. However, RLHF may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models and whether human preference judgements are responsible. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophantic behavior across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior of RLHF models, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Overall, our results indicate that sycophancy is a general behavior of RLHF models, likely driven in part by human preference judgements favoring sycophantic responses.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_13548v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从人类反馈中强化学习（RLHF）是一种流行的训练高质量人工智能助理的技术。然而，RLHF也可能鼓励模型反应与真实反应相匹配，这种行为被称为阿谀奉承。我们调查了RLHF训练模型中阿谀奉承的普遍性，以及人类偏好判断是否是罪魁祸首。我们首先证明了五个最先进的人工智能助手在四个不同的自由形式文本生成任务中始终表现出谄媚行为。为了了解人类偏好是否驱动RLHF模型的这种广泛观察到的行为，我们分析了现有的人类偏好数据。我们发现，当响应与用户的视图相匹配时，它更有可能是首选。此外，在不可忽略的一小部分时间里，人类和偏好模型都更喜欢写得令人信服的阿谀奉承的回答，而不是正确的回答。针对PM优化模型输出有时也会牺牲真实性来支持阿谀奉承。总的来说，我们的研究结果表明，谄媚是RLHF模型的一种普遍行为，可能部分是由有利于谄媚反应的人类偏好判断驱动的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.13548v2" target="_blank">2310.13548v2</a>
                              </td>
                              <td>Towards Understanding Sycophancy in Language Models</td>
                              <td>Mrinank Sharma</td>
                              <td>2023-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_13548v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.13548v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_13060v5_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Zero-One Laws of Graph Neural Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_13060v5_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_13060v5_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_13060v5_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Graph neural networks (GNNs) are the de facto standard deep learning architectures for machine learning on graphs. This has led to a large body of work analyzing the capabilities and limitations of these models, particularly pertaining to their representation and extrapolation capacity. We offer a novel theoretical perspective on the representation and extrapolation capacity of GNNs, by answering the question: how do GNNs behave as the number of graph nodes become very large? Under mild assumptions, we show that when we draw graphs of increasing size from the Erd\H{o}s-R\'enyi model, the probability that such graphs are mapped to a particular output by a class of GNN classifiers tends to either zero or to one. This class includes the popular graph convolutional network architecture. The result establishes 'zero-one laws' for these GNNs, and analogously to other convergence laws, entails theoretical limitations on their capacity. We empirically verify our results, observing that the theoretical asymptotic limits are evident already on relatively small graphs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_13060v5_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>图神经网络（GNN）是用于图上的机器学习的事实上的标准深度学习架构。这导致了大量的工作来分析这些模型的能力和局限性，特别是与它们的表示和外推能力有关的能力和限制。我们通过回答以下问题，为GNN的表示和外推能力提供了一个新的理论视角：当图节点的数量变得很大时，GNN如何表现？在温和的假设下，我们表明，当我们从Erd\H绘制尺寸增加的图时{o}s-R\在enyi模型中，一类GNN分类器将此类图映射到特定输出的概率往往为零或为一。这一类包括流行的图卷积网络架构。该结果为这些GNN建立了“零一定律”，类似于其他收敛定律，对其能力存在理论限制。我们实证验证了我们的结果，观察到理论渐近极限在相对较小的图上已经很明显了。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.13060v5" target="_blank">2301.13060v5</a>
                              </td>
                              <td>Zero-One Laws of Graph Neural Networks</td>
                              <td>Sam Adam-Day</td>
                              <td>2023-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_13060v5_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.13060v5" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15308v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15308v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15308v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15308v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The landscape of publicly available vision foundation models (VFMs), such as CLIP and Segment Anything Model (SAM), is expanding rapidly. VFMs are endowed with distinct capabilities stemming from their pre-training objectives. For instance, CLIP excels in semantic understanding, while SAM specializes in spatial understanding for segmentation. In this work, we introduce a simple recipe to efficiently merge VFMs into a unified model that assimilates their expertise. Our proposed method integrates multi-task learning, continual learning techniques, and teacher-student distillation. This strategy entails significantly less computational cost compared to traditional multi-task training from scratch. Additionally, it only demands a small fraction of the pre-training datasets that were initially used to train individual models. By applying our method to SAM and CLIP, we derive SAM-CLIP: a unified model that amalgamates the strengths of SAM and CLIP into a single backbone, making it apt for edge device applications. We show that SAM-CLIP learns richer visual representations, equipped with both localization and semantic features, suitable for a broad range of vision tasks. SAM-CLIP obtains improved performance on several head probing tasks when compared with SAM and CLIP. We further show that SAM-CLIP not only retains the foundational strengths of its precursor models but also introduces synergistic functionalities, most notably in zero-shot semantic segmentation, where SAM-CLIP establishes new state-of-the-art results on 5 benchmarks. It outperforms previous models that are specifically designed for this task by a large margin, including +6.8% and +5.9% mean IoU improvement on Pascal-VOC and COCO-Stuff datasets, respectively.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15308v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>公开可用的愿景基础模型（VFM），如CLIP和Segment Anything模型（SAM），正在迅速扩展。VFM由于其训练前的目标而被赋予了独特的能力。例如，CLIP擅长语义理解，而SAM专门用于分割的空间理解。在这项工作中，我们介绍了一个简单的配方，可以有效地将VFM合并到一个统一的模型中，吸收他们的专业知识。我们提出的方法集成了多任务学习、持续学习技术和师生提炼。与传统的从头开始的多任务训练相比，这种策略需要显著降低计算成本。此外，它只需要最初用于训练单个模型的预训练数据集的一小部分。通过将我们的方法应用于SAM和CLIP，我们得出了SAM-CLIP：一个统一的模型，将SAM和CLIP的优势合并为一个骨干，使其适用于边缘设备应用。我们表明，SAM-CLIP学习更丰富的视觉表示，具有定位和语义特征，适用于广泛的视觉任务。与SAM和CLIP相比，SAM-CLIP在几个头部探测任务上获得了改进的性能。我们进一步表明，SAM-CLIP不仅保留了其前体模型的基本优势，而且引入了协同功能，最显著的是在零样本语义分割中，其中SAM-CLIP在5个基准上建立了新的最先进的结果。它在很大程度上优于专门为该任务设计的先前模型，包括Pascal VOC和COCO Stuff数据集的平均IoU改进分别为+6.8%和+5.9%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15308v1" target="_blank">2310.15308v1</a>
                              </td>
                              <td>SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding</td>
                              <td>Haoxiang Wang</td>
                              <td>2023-10-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15308v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15308v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15161v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAM-Med3D</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15161v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15161v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15161v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Although the Segment Anything Model (SAM) has demonstrated impressive performance in 2D natural image segmentation, its application to 3D volumetric medical images reveals significant shortcomings, namely suboptimal performance and unstable prediction, necessitating an excessive number of prompt points to attain the desired outcomes. These issues can hardly be addressed by fine-tuning SAM on medical data because the original 2D structure of SAM neglects 3D spatial information. In this paper, we introduce SAM-Med3D, the most comprehensive study to modify SAM for 3D medical images. Our approach is characterized by its comprehensiveness in two primary aspects: firstly, by comprehensively reformulating SAM to a thorough 3D architecture trained on a comprehensively processed large-scale volumetric medical dataset; and secondly, by providing a comprehensive evaluation of its performance. Specifically, we train SAM-Med3D with over 131K 3D masks and 247 categories. Our SAM-Med3D excels at capturing 3D spatial information, exhibiting competitive performance with significantly fewer prompt points than the top-performing fine-tuned SAM in the medical domain. We then evaluate its capabilities across 15 datasets and analyze it from multiple perspectives, including anatomical structures, modalities, targets, and generalization abilities. Our approach, compared with SAM, showcases pronouncedly enhanced efficiency and broad segmentation capabilities for 3D volumetric medical images. Our code is released at https://github.com/uni-medical/SAM-Med3D.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15161v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管Segment Anything Model（SAM）在2D自然图像分割中表现出了令人印象深刻的性能，但其在3D体积医学图像中的应用揭示了显著的缺点，即性能次优和预测不稳定，需要过多的提示点才能获得所需的结果。这些问题很难通过在医学数据上微调SAM来解决，因为SAM的原始2D结构忽略了3D空间信息。在本文中，我们介绍了SAM-Med3D，这是针对3D医学图像修改SAM的最全面的研究。我们的方法的特点是其在两个主要方面的全面性：首先，通过将SAM全面重新制定为在全面处理的大规模体积医学数据集上训练的彻底的3D架构；第二，对其业绩进行全面评价。具体来说，我们用超过131K个3D掩模和247个类别来训练SAM-Med3D。我们的SAM-Med3D擅长捕捉3D空间信息，与医疗领域表现最好的微调SAM相比，其提示点明显更少，表现出有竞争力的性能。然后，我们在15个数据集中评估其能力，并从多个角度对其进行分析，包括解剖结构、模态、目标和泛化能力。与SAM相比，我们的方法显著提高了3D体积医学图像的效率和广泛的分割能力。我们的代码发布于https://github.com/uni-medical/SAM-Med3D.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15161v1" target="_blank">2310.15161v1</a>
                              </td>
                              <td>SAM-Med3D</td>
                              <td>Haoyu Wang</td>
                              <td>2023-10-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15161v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15161v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_15287v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Crucial Role of Normalization in Sharpness-Aware Minimization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_15287v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_15287v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_15287v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Sharpness-Aware Minimization (SAM) is a recently proposed gradient-based optimizer (Foret et al., ICLR 2021) that greatly improves the prediction performance of deep neural networks. Consequently, there has been a surge of interest in explaining its empirical success. We focus, in particular, on understanding the role played by normalization, a key component of the SAM updates. We theoretically and empirically study the effect of normalization in SAM for both convex and non-convex functions, revealing two key roles played by normalization: i) it helps in stabilizing the algorithm; and ii) it enables the algorithm to drift along a continuum (manifold) of minima -- a property identified by recent theoretical works that is the key to better performance. We further argue that these two properties of normalization make SAM robust against the choice of hyper-parameters, supporting the practicality of SAM. Our conclusions are backed by various experiments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_15287v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Sharpness Aware Minimization（SAM）是最近提出的一种基于梯度的优化器（Foret et al.，ICLR 2021），它大大提高了深度神经网络的预测性能。因此，人们对解释其实证成功的兴趣激增。我们特别关注了解正常化所扮演的角色，这是SAM更新的关键组成部分。我们从理论和经验上研究了归一化在SAM中对凸函数和非凸函数的影响，揭示了归一化所起的两个关键作用：i）它有助于稳定算法；以及ii）它使算法能够沿着最小值的连续体（流形）漂移——最近的理论工作确定了这一特性，这是提高性能的关键。我们进一步论证了归一化的这两个性质使SAM对超参数的选择具有鲁棒性，支持了SAM的实用性。我们的结论得到了各种实验的支持。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.15287v2" target="_blank">2305.15287v2</a>
                              </td>
                              <td>The Crucial Role of Normalization in Sharpness-Aware Minimization</td>
                              <td>Yan Dai</td>
                              <td>2023-05-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_15287v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.15287v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01567v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Segment Anything in High Quality</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01567v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01567v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01567v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The recent Segment Anything Model (SAM) represents a big leap in scaling up segmentation models, allowing for powerful zero-shot capabilities and flexible prompting. Despite being trained with 1.1 billion masks, SAM's mask prediction quality falls short in many cases, particularly when dealing with objects that have intricate structures. We propose HQ-SAM, equipping SAM with the ability to accurately segment any object, while maintaining SAM's original promptable design, efficiency, and zero-shot generalizability. Our careful design reuses and preserves the pre-trained model weights of SAM, while only introducing minimal additional parameters and computation. We design a learnable High-Quality Output Token, which is injected into SAM's mask decoder and is responsible for predicting the high-quality mask. Instead of only applying it on mask-decoder features, we first fuse them with early and final ViT features for improved mask details. To train our introduced learnable parameters, we compose a dataset of 44K fine-grained masks from several sources. HQ-SAM is only trained on the introduced detaset of 44k masks, which takes only 4 hours on 8 GPUs. We show the efficacy of HQ-SAM in a suite of 10 diverse segmentation datasets across different downstream tasks, where 8 out of them are evaluated in a zero-shot transfer protocol. Our code and pretrained models are at https://github.com/SysCV/SAM-HQ.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01567v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近的Segment Anything Model（SAM）代表了在扩展细分模型方面的一个巨大飞跃，允许强大的零样本功能和灵活的提示。尽管SAM使用了11亿个掩模进行训练，但在许多情况下，特别是在处理具有复杂结构的对象时，其掩模预测质量较差。我们提出HQ-SAM，使SAM具备准确分割任何对象的能力，同时保持SAM最初的可提示设计、效率和零样本可推广性。我们的精心设计重用并保留了SAM的预训练模型权重，同时只引入了最小的附加参数和计算。我们设计了一个可学习的高质量输出令牌，它被注入SAM的掩码解码器，并负责预测高质量掩码。我们不是只将其应用于掩模解码器特征，而是首先将其与早期和最终的ViT特征融合，以改进掩模细节。为了训练我们引入的可学习参数，我们从几个来源组成了一个44K细粒度掩码的数据集。HQ-SAM只接受了引入的44k口罩检测的培训，在8个GPU上只需要4个小时。我们在不同下游任务的10个不同分割数据集中展示了HQ-SAM的有效性，其中8个在零样本传输协议中进行了评估。我们的代码和预训练模型位于https://github.com/SysCV/SAM-HQ.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01567v2" target="_blank">2306.01567v2</a>
                              </td>
                              <td>Segment Anything in High Quality</td>
                              <td>Lei Ke</td>
                              <td>2023-06-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01567v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01567v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_14736v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAMCLR: Contrastive pre-training on complex scenes using SAM for view sampling</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_14736v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_14736v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_14736v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In Computer Vision, self-supervised contrastive learning enforces similar representations between different views of the same image. The pre-training is most often performed on image classification datasets, like ImageNet, where images mainly contain a single class of objects. However, when dealing with complex scenes with multiple items, it becomes very unlikely for several views of the same image to represent the same object category. In this setting, we propose SAMCLR, an add-on to SimCLR which uses SAM to segment the image into semantic regions, then sample the two views from the same region. Preliminary results show empirically that when pre-training on Cityscapes and ADE20K, then evaluating on classification on CIFAR-10, STL10 and ImageNette, SAMCLR performs at least on par with, and most often significantly outperforms not only SimCLR, but also DINO and MoCo.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_14736v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在计算机视觉中，自监督对比学习在同一图像的不同视图之间强制执行相似的表示。预训练通常在图像分类数据集上执行，如ImageNet，其中图像主要包含一类对象。然而，当处理具有多个项目的复杂场景时，同一图像的多个视图不太可能表示同一对象类别。在这种设置中，我们提出了SAMCLR，这是SimCLR的一个附加组件，它使用SAM将图像分割成语义区域，然后对同一区域的两个视图进行采样。初步结果实证表明，当在Cityscapes和ADE20K上进行预训练，然后在CIFAR-10、STL10和ImageNette上评估分类时，SAMCLR的表现至少与SimCLR相当，而且通常显著优于SimCLR，还优于DINO和MoCo。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.14736v1" target="_blank">2310.14736v1</a>
                              </td>
                              <td>SAMCLR: Contrastive pre-training on complex scenes using SAM for view sampling</td>
                              <td>Benjamin Missaoui</td>
                              <td>2023-10-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_14736v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.14736v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_14344v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">What's in a Prior? Learned Proximal Networks for Inverse Problems</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_14344v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_14344v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_14344v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Proximal operators are ubiquitous in inverse problems, commonly appearing as part of algorithmic strategies to regularize problems that are otherwise ill-posed. Modern deep learning models have been brought to bear for these tasks too, as in the framework of plug-and-play or deep unrolling, where they loosely resemble proximal operators. Yet, something essential is lost in employing these purely data-driven approaches: there is no guarantee that a general deep network represents the proximal operator of any function, nor is there any characterization of the function for which the network might provide some approximate proximal. This not only makes guaranteeing convergence of iterative schemes challenging but, more fundamentally, complicates the analysis of what has been learned by these networks about their training data. Herein we provide a framework to develop learned proximal networks (LPN), prove that they provide exact proximal operators for a data-driven nonconvex regularizer, and show how a new training strategy, dubbed proximal matching, provably promotes the recovery of the log-prior of the true data distribution. Such LPN provide general, unsupervised, expressive proximal operators that can be used for general inverse problems with convergence guarantees. We illustrate our results in a series of cases of increasing complexity, demonstrating that these models not only result in state-of-the-art performance, but provide a window into the resulting priors learned from data.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_14344v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近似算子在逆问题中普遍存在，通常作为算法策略的一部分出现，以正则化否则不适定的问题。现代深度学习模型也被用来承担这些任务，比如在即插即用或深度展开的框架中，它们松散地类似于近端操作员。然而，在使用这些纯粹的数据驱动方法时，失去了一些重要的东西：不能保证一般的深度网络代表任何函数的近端算子，也不能保证网络可能提供某种近似近端的函数的任何特征。这不仅使保证迭代方案的收敛具有挑战性，而且更根本的是，使这些网络对其训练数据的学习分析变得复杂。在此，我们提供了一个开发学习近端网络（LPN）的框架，证明了它们为数据驱动的非凸正则化子提供了精确的近端算子，并展示了一种新的训练策略，称为近端匹配，如何可证明地促进日志在真实数据分布之前的恢复。这种LPN提供了通用的、无监督的、可表达的近端算子，可用于具有收敛保证的一般逆问题。我们在一系列复杂性增加的案例中说明了我们的结果，证明这些模型不仅带来了最先进的性能，而且提供了一个了解从数据中学习到的先验的窗口。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.14344v1" target="_blank">2310.14344v1</a>
                              </td>
                              <td>What's in a Prior? Learned Proximal Networks for Inverse Problems</td>
                              <td>Zhenghan Fang</td>
                              <td>2023-10-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_14344v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.14344v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2012_02383v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAM: Self-supervised Learning of Pixel-wise Anatomical Embeddings in Radiological Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2012_02383v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2012_02383v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2012_02383v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Radiological images such as computed tomography (CT) and X-rays render anatomy with intrinsic structures. Being able to reliably locate the same anatomical structure across varying images is a fundamental task in medical image analysis. In principle it is possible to use landmark detection or semantic segmentation for this task, but to work well these require large numbers of labeled data for each anatomical structure and sub-structure of interest. A more universal approach would learn the intrinsic structure from unlabeled images. We introduce such an approach, called Self-supervised Anatomical eMbedding (SAM). SAM generates semantic embeddings for each image pixel that describes its anatomical location or body part. To produce such embeddings, we propose a pixel-level contrastive learning framework. A coarse-to-fine strategy ensures both global and local anatomical information are encoded. Negative sample selection strategies are designed to enhance the embedding's discriminability. Using SAM, one can label any point of interest on a template image and then locate the same body part in other images by simple nearest neighbor searching. We demonstrate the effectiveness of SAM in multiple tasks with 2D and 3D image modalities. On a chest CT dataset with 19 landmarks, SAM outperforms widely-used registration algorithms while only taking 0.23 seconds for inference. On two X-ray datasets, SAM, with only one labeled template image, surpasses supervised methods trained on 50 labeled images. We also apply SAM on whole-body follow-up lesion matching in CT and obtain an accuracy of 91%. SAM can also be applied for improving image registration and initializing CNN weights.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2012_02383v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>计算机断层扫描（CT）和X射线等放射学图像呈现具有内在结构的解剖结构。能够在不同的图像中可靠地定位相同的解剖结构是医学图像分析的基本任务。原则上，可以将地标检测或语义分割用于该任务，但为了良好地工作，这些需要针对每个感兴趣的解剖结构和子结构的大量标记数据。一种更通用的方法是从未标记的图像中学习内在结构。我们介绍了这样一种方法，称为自我监督解剖电子床（SAM）。SAM为描述其解剖位置或身体部位的每个图像像素生成语义嵌入。为了产生这样的嵌入，我们提出了一个像素级的对比学习框架。从粗到细的策略确保对全局和局部解剖信息进行编码。设计了负样本选择策略来提高嵌入的可分辨性。使用SAM，可以标记模板图像上的任何兴趣点，然后通过简单的最近邻居搜索在其他图像中定位相同的身体部位。我们展示了SAM在2D和3D图像模式的多任务中的有效性。在具有19个标志的胸部CT数据集上，SAM优于广泛使用的配准算法，而推断仅需0.23秒。在两个X射线数据集上，只有一个标记模板图像的SAM超过了在50个标记图像上训练的监督方法。我们还将SAM应用于CT中的全身随访病变匹配，并获得91%的准确率。SAM还可以用于改进图像配准和初始化CNN权重。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2012.02383v3" target="_blank">2012.02383v3</a>
                              </td>
                              <td>SAM: Self-supervised Learning of Pixel-wise Anatomical Embeddings in Radiological Images</td>
                              <td>Ke Yan</td>
                              <td>2020-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2012_02383v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2012.02383v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_13798v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Specific versus General Principles for Constitutional AI</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_13798v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_13798v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_13798v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Human feedback can prevent overtly harmful utterances in conversational models, but may not automatically mitigate subtle problematic behaviors such as a stated desire for self-preservation or power. Constitutional AI offers an alternative, replacing human feedback with feedback from AI models conditioned only on a list of written principles. We find this approach effectively prevents the expression of such behaviors. The success of simple principles motivates us to ask: can models learn general ethical behaviors from only a single written principle? To test this, we run experiments using a principle roughly stated as "do what's best for humanity". We find that the largest dialogue models can generalize from this short constitution, resulting in harmless assistants with no stated interest in specific motivations like power. A general principle may thus partially avoid the need for a long list of constitutions targeting potentially harmful behaviors. However, more detailed constitutions still improve fine-grained control over specific types of harms. This suggests both general and specific principles have value for steering AI safely.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_13798v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人类的反馈可以防止对话模型中公开的有害言论，但可能不会自动缓解微妙的问题行为，如对自我保护或权力的渴望。宪法人工智能提供了一种替代方案，用仅以书面原则为条件的人工智能模型的反馈取代人类反馈。我们发现这种方法有效地防止了这种行为的表现。简单原则的成功激励我们去问：模型能从一个书面原则中学习一般的道德行为吗？为了验证这一点，我们使用了一个大致被称为“为人类做最好的事”的原则进行实验。我们发现，最大的对话模式可以从这个简短的宪法中概括出来，导致无害的助手对权力等特定动机没有明确的兴趣。因此，一般原则可以部分避免针对潜在有害行为制定一长串宪法的必要性。然而，更详细的构成仍然可以改善对特定类型伤害的细粒度控制。这表明，一般原则和具体原则对安全引导人工智能都有价值。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.13798v1" target="_blank">2310.13798v1</a>
                              </td>
                              <td>Specific versus General Principles for Constitutional AI</td>
                              <td>Sandipan Kundu</td>
                              <td>2023-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_13798v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.13798v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_13774v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Seq2seq is All You Need for Coreference Resolution</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_13774v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_13774v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_13774v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Existing works on coreference resolution suggest that task-specific models are necessary to achieve state-of-the-art performance. In this work, we present compelling evidence that such models are not necessary. We finetune a pretrained seq2seq transformer to map an input document to a tagged sequence encoding the coreference annotation. Despite the extreme simplicity, our model outperforms or closely matches the best coreference systems in the literature on an array of datasets. We also propose an especially simple seq2seq approach that generates only tagged spans rather than the spans interleaved with the original text. Our analysis shows that the model size, the amount of supervision, and the choice of sequence representations are key factors in performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_13774v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>关于共指消解的现有工作表明，特定任务的模型对于实现最先进的性能是必要的。在这项工作中，我们提出了令人信服的证据，证明这种模型是不必要的。我们微调了一个预训练的seq2seq转换器，将输入文档映射到编码共引用注释的标记序列。尽管我们的模型非常简单，但在一系列数据集上，它的性能优于或与文献中最好的共指系统非常匹配。我们还提出了一种特别简单的seq2seq方法，该方法只生成标记的跨度，而不是与原始文本交错的跨度。我们的分析表明，模型大小、监督量和序列表示的选择是性能的关键因素。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.13774v1" target="_blank">2310.13774v1</a>
                              </td>
                              <td>Seq2seq is All You Need for Coreference Resolution</td>
                              <td>Wenzheng Zhang</td>
                              <td>2023-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_13774v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.13774v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_13315v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Zero-Shot Sharpness-Aware Quantization for Pre-trained Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_13315v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_13315v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_13315v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Quantization is a promising approach for reducing memory overhead and accelerating inference, especially in large pre-trained language model (PLM) scenarios. While having no access to original training data due to security and privacy concerns has emerged the demand for zero-shot quantization. Most of the cutting-edge zero-shot quantization methods primarily 1) apply to computer vision tasks, and 2) neglect of overfitting problem in the generative adversarial learning process, leading to sub-optimal performance. Motivated by this, we propose a novel zero-shot sharpness-aware quantization (ZSAQ) framework for the zero-shot quantization of various PLMs. The key algorithm in solving ZSAQ is the SAM-SGA optimization, which aims to improve the quantization accuracy and model generalization via optimizing a minimax problem. We theoretically prove the convergence rate for the minimax optimization problem and this result can be applied to other nonconvex-PL minimax optimization frameworks. Extensive experiments on 11 tasks demonstrate that our method brings consistent and significant performance gains on both discriminative and generative PLMs, i.e., up to +6.98 average score. Furthermore, we empirically validate that our method can effectively improve the model generalization.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_13315v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>量化是减少内存开销和加速推理的一种很有前途的方法，尤其是在大型预训练语言模型（PLM）场景中。虽然由于安全和隐私问题而无法访问原始训练数据，但出现了对零样本量化的需求。大多数前沿的零样本量化方法主要1）适用于计算机视觉任务，2）在生成对抗性学习过程中忽略过拟合问题，导致性能次优。基于此，我们提出了一种新的零样本锐器量化（ZSAQ）框架，用于各种PLM的零样本量化。求解ZSAQ的关键算法是SAM-SGA优化，其目的是通过优化极小极大问题来提高量化精度和模型泛化能力。我们从理论上证明了极小极大优化问题的收敛速度，这一结果可以应用于其他非凸PL极小极大优化框架。对11个任务的大量实验表明，我们的方法在判别性和生成性PLM上都带来了一致且显著的性能提升，即平均得分高达+6.98。此外，我们实证验证了我们的方法可以有效地提高模型的泛化能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.13315v1" target="_blank">2310.13315v1</a>
                              </td>
                              <td>Zero-Shot Sharpness-Aware Quantization for Pre-trained Language Models</td>
                              <td>Miaoxi Zhu</td>
                              <td>2023-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_13315v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.13315v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_13304v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Investigating Emotions, Social Roles, Well-being and Mobile Usage Behaviors During COVID-19 Home Confinement</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_13304v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_13304v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_13304v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Pandemics and epidemics have been an essential part of human history. It has powerfully shaped human civilization, affecting politics, economics, medicine, and everyday human lives. Home confinement, one of the social isolation restrictions, has proven to be an effective way to prevent a pandemic and significantly reduce the spread of the virus. However, these measures have also led to negative psychological effects, which can be more severe than the physical discomfort caused by the pandemic itself. To address this issue, it is crucial to understand how being confined at home affects people's psychological states and their use of digital devices. To this end, we conducted an in-situ study with 32 participants living in states affected by COVID-19 lockdowns for three weeks. We examined human emotions, social roles, well-being, and mobile usage behaviors and explored the predictability of affective states using digital traces. Extensive experimental results show that COVID-19 home confinement is associated with decreased valence and increased work time and that human well-being could be accurately inferred from mobile usage behaviors. We also present a set of interesting findings on how different factors of mobile usage affect human emotions. Overall, the findings of this research have great potential to inform the development of interventions and remote programs to support individuals in similar situations, such as those who are ill or undergoing post-operative home rehabilitation and home work.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_13304v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>流行病和流行病一直是人类历史的重要组成部分。它有力地塑造了人类文明，影响了政治、经济、医学和人类日常生活。居家隔离是社会隔离限制措施之一，已被证明是预防大流行和显著减少病毒传播的有效方法。然而，这些措施也导致了负面的心理影响，可能比疫情本身造成的身体不适更严重。为了解决这个问题，了解被限制在家里如何影响人们的心理状态和对数字设备的使用至关重要。为此，我们对居住在受新冠肺炎封锁影响的州的32名参与者进行了为期三周的现场研究。我们研究了人类的情绪、社会角色、幸福感和移动使用行为，并使用数字痕迹探索了情感状态的可预测性。广泛的实验结果表明，新冠肺炎家庭监禁与化合价降低和工作时间增加有关，可以从移动使用行为中准确推断出人类的幸福感。我们还提出了一组有趣的发现，关于手机使用的不同因素如何影响人类情绪。总的来说，这项研究的发现有很大的潜力为制定干预措施和远程计划提供信息，以支持处于类似情况下的个人，例如生病或正在接受术后家庭康复和家庭工作的人。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.13304v1" target="_blank">2310.13304v1</a>
                              </td>
                              <td>Investigating Emotions, Social Roles, Well-being and Mobile Usage Behaviors During COVID-19 Home Confinement</td>
                              <td>Nan Gao</td>
                              <td>2023-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_13304v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.13304v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_13026v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Weakly-Supervised Semantic Segmentation with Image-Level Labels: from Traditional Models to Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_13026v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_13026v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_13026v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The rapid development of deep learning has driven significant progress in the field of image semantic segmentation - a fundamental task in computer vision. Semantic segmentation algorithms often depend on the availability of pixel-level labels (i.e., masks of objects), which are expensive, time-consuming, and labor-intensive. Weakly-supervised semantic segmentation (WSSS) is an effective solution to avoid such labeling. It utilizes only partial or incomplete annotations and provides a cost-effective alternative to fully-supervised semantic segmentation. In this paper, we focus on the WSSS with image-level labels, which is the most challenging form of WSSS. Our work has two parts. First, we conduct a comprehensive survey on traditional methods, primarily focusing on those presented at premier research conferences. We categorize them into four groups based on where their methods operate: pixel-wise, image-wise, cross-image, and external data. Second, we investigate the applicability of visual foundation models, such as the Segment Anything Model (SAM), in the context of WSSS. We scrutinize SAM in two intriguing scenarios: text prompting and zero-shot learning. We provide insights into the potential and challenges associated with deploying visual foundational models for WSSS, facilitating future developments in this exciting research area.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_13026v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>深度学习的快速发展推动了图像语义分割领域的重大进展，这是计算机视觉的一项基本任务。语义分割算法通常取决于像素级标签（即对象的掩码）的可用性，这是昂贵、耗时和劳动密集型的。弱监督语义分割（WSSS）是避免此类标记的有效解决方案。它只使用部分或不完整的注释，并为完全监督的语义分割提供了一种具有成本效益的替代方案。在本文中，我们重点研究具有图像级标签的WSSS，这是WSSS最具挑战性的形式。我们的工作分为两部分。首先，我们对传统方法进行了全面的调查，主要关注在主要研究会议上提出的方法。我们根据它们的方法的运作方式将它们分为四组：像素、图像、交叉图像和外部数据。其次，我们研究了可视化基础模型（如分段任何东西模型（SAM））在WSSS环境中的适用性。我们在两个有趣的场景中仔细检查SAM：文本提示和零样本学习。我们深入了解了为WSSS部署视觉基础模型的潜力和挑战，促进了这一令人兴奋的研究领域的未来发展。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.13026v1" target="_blank">2310.13026v1</a>
                              </td>
                              <td>Weakly-Supervised Semantic Segmentation with Image-Level Labels: from Traditional Models to Foundation Models</td>
                              <td>Zhaozheng Chen</td>
                              <td>2023-10-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_13026v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.13026v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_12431v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Segment Anything Meets Universal Adversarial Perturbation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_12431v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_12431v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_12431v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>As Segment Anything Model (SAM) becomes a popular foundation model in computer vision, its adversarial robustness has become a concern that cannot be ignored. This works investigates whether it is possible to attack SAM with image-agnostic Universal Adversarial Perturbation (UAP). In other words, we seek a single perturbation that can fool the SAM to predict invalid masks for most (if not all) images. We demonstrate convetional image-centric attack framework is effective for image-independent attacks but fails for universal adversarial attack. To this end, we propose a novel perturbation-centric framework that results in a UAP generation method based on self-supervised contrastive learning (CL), where the UAP is set to the anchor sample and the positive sample is augmented from the UAP. The representations of negative samples are obtained from the image encoder in advance and saved in a memory bank. The effectiveness of our proposed CL-based UAP generation method is validated by both quantitative and qualitative results. On top of the ablation study to understand various components in our proposed method, we shed light on the roles of positive and negative samples in making the generated UAP effective for attacking SAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_12431v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着分段任意模型（SAM）成为计算机视觉中流行的基础模型，其对抗性鲁棒性已成为一个不可忽视的问题。这项工作调查了是否有可能用图像不可知的通用对抗性扰动（UAP）攻击SAM。换句话说，我们寻求一个单一的扰动，它可以欺骗SAM来预测大多数（如果不是全部）图像的无效掩码。我们证明了对流图像中心攻击框架对图像无关攻击有效，但对通用对抗性攻击失败。为此，我们提出了一种新的以扰动为中心的框架，该框架产生了一种基于自监督对比学习（CL）的UAP生成方法，其中UAP被设置为锚样本，正样本从UAP中扩充。负样本的表示预先从图像编码器获得并保存在存储器组中。定量和定性结果都验证了我们提出的基于CL的UAP生成方法的有效性。除了消融研究以了解我们提出的方法中的各种成分外，我们还阐明了阳性和阴性样本在使生成的UAP有效攻击SAM方面的作用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.12431v1" target="_blank">2310.12431v1</a>
                              </td>
                              <td>Segment Anything Meets Universal Adversarial Perturbation</td>
                              <td>Dongshen Han</td>
                              <td>2023-10-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_12431v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.12431v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_06444v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TongueSAM: An Universal Tongue Segmentation Model Based on SAM with Zero-Shot</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_06444v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_06444v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_06444v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Tongue segmentation serves as the primary step in automated TCM tongue diagnosis, which plays a significant role in the diagnostic results. Currently, numerous deep learning based methods have achieved promising results. However, when confronted with tongue images that differ from the training set or possess challenging backgrounds, these methods demonstrate limited performance. To address this issue, this paper proposes a universal tongue segmentation model named TongueSAM based on SAM (Segment Anything Model). SAM is a large-scale pretrained interactive segmentation model known for its powerful zero-shot generalization capability. Applying SAM to tongue segmentation leverages its learned prior knowledge from natural images, enabling the achievement of zero-shot segmentation for various types of tongue images. In this study, a Prompt Generator based on object detection is integrated into SAM to enable an end-to-end automated tongue segmentation method. Experiments demonstrate that TongueSAM achieves exceptional performance across various of tongue segmentation datasets, particularly under zero-shot. Even when dealing with challenging background tongue images, TongueSAM achieves a mIoU of 95.23\% under zero-shot conditions, surpassing other segmentation methods. As far as we know, this is the first application of large-scale pretrained model for tongue segmentation. The project and pretrained model will be made public when the paper is accepted.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_06444v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>舌片分割是中医舌片自动诊断的首要步骤，对诊断结果起着重要作用。目前，许多基于深度学习的方法已经取得了可喜的成果。然而，当面对不同于训练集或具有挑战性背景的舌头图像时，这些方法表现出有限的性能。为了解决这一问题，本文提出了一种基于SAM（Segment Anything model）的通用舌头分割模型TongueSAM。SAM是一种大规模的预训练交互式分割模型，以其强大的零样本泛化能力而闻名。将SAM应用于舌头分割利用了其从自然图像中学习到的先验知识，实现了对各种类型的舌头图像的零样本分割。在这项研究中，基于对象检测的提示生成器被集成到SAM中，以实现端到端的自动舌头分割方法。实验表明，TongueSAM在各种舌头分割数据集上都取得了优异的性能，尤其是在零样本下。即使在处理具有挑战性的背景舌头图像时，TongueSAM在零样本条件下也达到了95.23\%的mIoU，超过了其他分割方法。据我们所知，这是大规模预训练模型在舌头分割中的首次应用。该项目和预训练的模型将在论文被接受时公开。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.06444v2" target="_blank">2308.06444v2</a>
                              </td>
                              <td>TongueSAM: An Universal Tongue Segmentation Model Based on SAM with Zero-Shot</td>
                              <td>Shan Cao</td>
                              <td>2023-08-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_06444v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.06444v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_11649v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Grounding Complex Natural Language Commands for Temporal Tasks in Unseen Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_11649v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_11649v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_11649v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Grounding navigational commands to linear temporal logic (LTL) leverages its unambiguous semantics for reasoning about long-horizon tasks and verifying the satisfaction of temporal constraints. Existing approaches require training data from the specific environment and landmarks that will be used in natural language to understand commands in those environments. We propose Lang2LTL, a modular system and a software package that leverages large language models (LLMs) to ground temporal navigational commands to LTL specifications in environments without prior language data. We comprehensively evaluate Lang2LTL for five well-defined generalization behaviors. Lang2LTL demonstrates the state-of-the-art ability of a single model to ground navigational commands to diverse temporal specifications in 21 city-scaled environments. Finally, we demonstrate a physical robot using Lang2LTL can follow 52 semantically diverse navigational commands in two indoor environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_11649v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于线性时间逻辑（LTL）的导航命令利用其明确的语义来推理长期任务并验证时间约束的满足性。现有的方法需要来自特定环境和地标的训练数据，这些数据将用自然语言来理解这些环境中的命令。我们提出了Lang2LTL，这是一个模块化系统和软件包，它利用大型语言模型（LLM）在没有先前语言数据的环境中将时间导航命令固定为LTL规范。我们综合评估了Lang2LTL的五种定义良好的泛化行为。Lang2LTL展示了单一模型在21个城市规模的环境中根据不同的时间规范对导航命令进行地面定位的最先进能力。最后，我们展示了一个使用Lang2LTL的物理机器人可以在两个室内环境中遵循52个语义不同的导航命令。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.11649v2" target="_blank">2302.11649v2</a>
                              </td>
                              <td>Grounding Complex Natural Language Commands for Temporal Tasks in Unseen Environments</td>
                              <td>Jason Xinyu Liu</td>
                              <td>2023-02-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_11649v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.11649v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_11441v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_11441v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_11441v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_11441v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present Set-of-Mark (SoM), a new visual prompting method, to unleash the visual grounding abilities of large multimodal models (LMMs), such as GPT-4V. As illustrated in Fig. 1 (right), we employ off-the-shelf interactive segmentation models, such as SAM, to partition an image into regions at different levels of granularity, and overlay these regions with a set of marks e.g., alphanumerics, masks, boxes. Using the marked image as input, GPT-4V can answer the questions that require visual grounding. We perform a comprehensive empirical study to validate the effectiveness of SoM on a wide range of fine-grained vision and multimodal tasks. For example, our experiments show that GPT-4V with SoM outperforms the state-of-the-art fully-finetuned referring segmentation model on RefCOCOg in a zero-shot setting.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_11441v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的视觉提示方法——标记集（SoM），以释放大型多模态模型（LMM）的视觉基础能力，如GPT-4V。如图6所示，1（右），我们使用现成的交互式分割模型，如SAM，将图像划分为不同粒度级别的区域，并用一组标记覆盖这些区域，例如字母数字、掩码、框。使用标记的图像作为输入，GPT-4V可以回答需要视觉基础的问题。我们进行了一项全面的实证研究，以验证SoM在各种细粒度视觉和多模式任务上的有效性。例如，我们的实验表明，在零样本设置下，具有SoM的GPT-4V优于RefCOCOg上最先进的完全微调参考分割模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.11441v1" target="_blank">2310.11441v1</a>
                              </td>
                              <td>Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V</td>
                              <td>Jianwei Yang</td>
                              <td>2023-10-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_11441v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.11441v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_13785v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Customized Segment Anything Model for Medical Image Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_13785v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_13785v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_13785v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose SAMed, a general solution for medical image segmentation. Different from the previous methods, SAMed is built upon the large-scale image segmentation model, Segment Anything Model (SAM), to explore the new research paradigm of customizing large-scale models for medical image segmentation. SAMed applies the low-rank-based (LoRA) finetuning strategy to the SAM image encoder and finetunes it together with the prompt encoder and the mask decoder on labeled medical image segmentation datasets. We also observe the warmup finetuning strategy and the AdamW optimizer lead SAMed to successful convergence and lower loss. Different from SAM, SAMed could perform semantic segmentation on medical images. Our trained SAMed model achieves 81.88 DSC and 20.64 HD on the Synapse multi-organ segmentation dataset, which is on par with the state-of-the-art methods. We conduct extensive experiments to validate the effectiveness of our design. Since SAMed only updates a small fraction of the SAM parameters, its deployment cost and storage cost are quite marginal in practical usage. The code of SAMed is available at https://github.com/hitachinsk/SAMed.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_13785v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了SAMed，一种用于医学图像分割的通用解决方案。与以往的方法不同，SAMed建立在大规模图像分割模型Segment Anything model（SAM）的基础上，探索了为医学图像分割定制大规模模型的新研究范式。SAMed将基于低秩（LoRA）的微调策略应用于SAM图像编码器，并在标记的医学图像分割数据集上与提示编码器和掩码解码器一起对其进行微调。我们还观察到预热微调策略和AdamW优化器使SAMed成功收敛并降低了损失。与SAM不同，SAMed可以对医学图像进行语义分割。我们训练的SAMed模型在Synapse多器官分割数据集上实现了81.88 DSC和20.64 HD，与最先进的方法不相上下。我们进行了大量的实验来验证我们设计的有效性。由于SAMed只更新SAM参数的一小部分，因此在实际使用中，其部署成本和存储成本非常低。SAMed的代码可在https://github.com/hitachinsk/SAMed.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.13785v2" target="_blank">2304.13785v2</a>
                              </td>
                              <td>Customized Segment Anything Model for Medical Image Segmentation</td>
                              <td>Kaidong Zhang</td>
                              <td>2023-04-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_13785v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.13785v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_09759v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Prototype-oriented Unsupervised Change Detection for Disaster Management</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_09759v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_09759v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_09759v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Climate change has led to an increased frequency of natural disasters such as floods and cyclones. This emphasizes the importance of effective disaster monitoring. In response, the remote sensing community has explored change detection methods. These methods are primarily categorized into supervised techniques, which yield precise results but come with high labeling costs, and unsupervised techniques, which eliminate the need for labeling but involve intricate hyperparameter tuning. To address these challenges, we propose a novel unsupervised change detection method named Prototype-oriented Unsupervised Change Detection for Disaster Management (PUCD). PUCD captures changes by comparing features from pre-event, post-event, and prototype-oriented change synthesis images via a foundational model, and refines results using the Segment Anything Model (SAM). Although PUCD is an unsupervised change detection, it does not require complex hyperparameter tuning. We evaluate PUCD framework on the LEVIR-Extension dataset and the disaster dataset and it achieves state-of-the-art performance compared to other methods on the LEVIR-Extension dataset.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_09759v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>气候变化导致洪水和旋风等自然灾害的频率增加。这强调了有效的灾害监测的重要性。作为回应，遥感界探索了变化检测方法。这些方法主要分为监督技术和无监督技术，前者产生精确的结果，但标记成本高；后者消除了标记的需要，但涉及复杂的超参数调整。为了应对这些挑战，我们提出了一种新的无监督变化检测方法，称为面向原型的无监督灾害管理变化检测（PUCD）。PUCD通过基础模型比较事件前、事件后和面向原型的变化合成图像的特征来捕捉变化，并使用Segment Anything模型（SAM）细化结果。尽管PUCD是一种无监督的变化检测，但它不需要复杂的超参数调整。我们在LEVIR Extension数据集和灾难数据集上评估了PUCD框架，与在LEVIR Extensions数据集上的其他方法相比，它实现了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.09759v2" target="_blank">2310.09759v2</a>
                              </td>
                              <td>Prototype-oriented Unsupervised Change Detection for Disaster Management</td>
                              <td>Youngtack Oh</td>
                              <td>2023-10-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_09759v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.09759v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_10493v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Evaluation and improvement of Segment Anything Model for interactive histopathology image segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_10493v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_10493v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_10493v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With the emergence of the Segment Anything Model (SAM) as a foundational model for image segmentation, its application has been extensively studied across various domains, including the medical field. However, its potential in the context of histopathology data, specifically in region segmentation, has received relatively limited attention. In this paper, we evaluate SAM's performance in zero-shot and fine-tuned scenarios on histopathology data, with a focus on interactive segmentation. Additionally, we compare SAM with other state-of-the-art interactive models to assess its practical potential and evaluate its generalization capability with domain adaptability. In the experimental results, SAM exhibits a weakness in segmentation performance compared to other models while demonstrating relative strengths in terms of inference time and generalization capability. To improve SAM's limited local refinement ability and to enhance prompt stability while preserving its core strengths, we propose a modification of SAM's decoder. The experimental results suggest that the proposed modification is effective to make SAM useful for interactive histology image segmentation. The code is available at \url{https://github.com/hvcl/SAM_Interactive_Histopathology}</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_10493v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着Segment Anything Model（SAM）作为图像分割的基础模型的出现，其应用在包括医学领域在内的各个领域得到了广泛的研究。然而，它在组织病理学数据方面的潜力，特别是在区域分割方面，受到的关注相对有限。在本文中，我们评估了SAM在组织病理学数据的零样本和微调场景中的性能，重点是交互式分割。此外，我们将SAM与其他最先进的交互模型进行了比较，以评估其实际潜力，并评估其具有领域适应性的泛化能力。在实验结果中，与其他模型相比，SAM在分割性能方面表现出弱点，同时在推理时间和泛化能力方面表现出相对优势。为了提高SAM有限的局部细化能力，在保持其核心优势的同时提高即时稳定性，我们提出了对SAM解码器的改进。实验结果表明，所提出的修改是有效的，使SAM可用于交互式组织学图像分割。代码位于\url{https://github.com/hvcl/SAM_Interactive_Histopathology}</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.10493v1" target="_blank">2310.10493v1</a>
                              </td>
                              <td>Evaluation and improvement of Segment Anything Model for interactive histopathology image segmentation</td>
                              <td>SeungKyu Kim</td>
                              <td>2023-10-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_10493v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.10493v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_10149v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Recursive Segmentation Living Image: An eXplainable AI (XAI) Approach for Computing Structural Beauty of Images or the Livingness of Space</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_10149v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_10149v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_10149v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This study introduces the concept of "structural beauty" as an objective computational approach for evaluating the aesthetic appeal of images. Through the utilization of the Segment anything model (SAM), we propose a method that leverages recursive segmentation to extract finer-grained substructures. Additionally, by reconstructing the hierarchical structure, we obtain a more accurate representation of substructure quantity and hierarchy. This approach reproduces and extends our previous research, allowing for the simultaneous assessment of Livingness in full-color images without the need for grayscale conversion or separate computations for foreground and background Livingness. Furthermore, the application of our method to the Scenic or Not dataset, a repository of subjective scenic ratings, demonstrates a high degree of consistency with subjective ratings in the 0-6 score range. This underscores that structural beauty is not solely a subjective perception, but a quantifiable attribute accessible through objective computation. Through our case studies, we have arrived at three significant conclusions. 1) our method demonstrates the capability to accurately segment meaningful objects, including trees, buildings, and windows, as well as abstract substructures within paintings. 2) we observed that the clarity of an image impacts our computational results; clearer images tend to yield higher Livingness scores. However, for equally blurry images, Livingness does not exhibit a significant reduction, aligning with human visual perception. 3) our approach fundamentally differs from methods employing Convolutional Neural Networks (CNNs) for predicting image scores. Our method not only provides computational results but also offers transparency and interpretability, positioning it as a novel avenue in the realm of Explainable AI (XAI).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_10149v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本研究引入了“结构美”的概念，作为评估图像美感的客观计算方法。通过利用分段任意模型（SAM），我们提出了一种利用递归分段来提取细粒度子结构的方法。此外，通过重构层次结构，我们获得了更准确的子结构数量和层次的表示。这种方法再现并扩展了我们以前的研究，允许在不需要灰度转换或单独计算前景和背景Livingness的情况下同时评估全色图像中的Livingness。此外，将我们的方法应用于风景与否数据集，即主观风景评级的存储库，表明了与0-6分范围内的主观评级的高度一致性。这强调了结构美不仅仅是一种主观感知，而是一种可通过客观计算获得的可量化属性。通过案例研究，我们得出了三个重要结论。1） 我们的方法展示了准确分割有意义物体的能力，包括树木、建筑和窗户，以及绘画中的抽象子结构。2） 我们观察到图像的清晰度会影响我们的计算结果；更清晰的图像往往产生更高的Livingness分数。然而，对于同样模糊的图像，Livingness并没有表现出显著的减少，与人类的视觉感知一致。3） 我们的方法从根本上不同于使用卷积神经网络（CNNs）来预测图像分数的方法。我们的方法不仅提供了计算结果，还提供了透明度和可解释性，将其定位为可解释人工智能（XAI）领域的一条新途径。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.10149v1" target="_blank">2310.10149v1</a>
                              </td>
                              <td>Recursive Segmentation Living Image: An eXplainable AI (XAI) Approach for Computing Structural Beauty of Images or the Livingness of Space</td>
                              <td>Yao Qianxiang</td>
                              <td>2023-10-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_10149v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.10149v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_10010v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Black-box Targeted Adversarial Attack on Segment Anything (SAM)</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_10010v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_10010v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_10010v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Deep recognition models are widely vulnerable to adversarial examples, which change the model output by adding quasi-imperceptible perturbation to the image input. Recently, Segment Anything Model (SAM) has emerged to become a popular foundation model in computer vision due to its impressive generalization to unseen data and tasks. Realizing flexible attacks on SAM is beneficial for understanding the robustness of SAM in the adversarial context. To this end, this work aims to achieve a targeted adversarial attack (TAA) on SAM. Specifically, under a certain prompt, the goal is to make the predicted mask of an adversarial example resemble that of a given target image. The task of TAA on SAM has been realized in a recent arXiv work in the white-box setup by assuming access to prompt and model, which is thus less practical. To address the issue of prompt dependence, we propose a simple yet effective approach by only attacking the image encoder. Moreover, we propose a novel regularization loss to enhance the cross-model transferability by increasing the feature dominance of adversarial images over random natural images. Extensive experiments verify the effectiveness of our proposed simple techniques to conduct a successful black-box TAA on SAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_10010v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>深度识别模型普遍容易受到对抗性示例的影响，这些示例通过在图像输入中添加准不可察觉的扰动来改变模型输出。最近，分段任意模型（SAM）已经成为计算机视觉中流行的基础模型，因为它对看不见的数据和任务具有令人印象深刻的泛化能力。实现对SAM的灵活攻击有利于理解SAM在对抗性环境中的稳健性。为此，这项工作旨在实现对SAM的有针对性的对抗性攻击（TAA）。具体而言，在一定的提示下，目标是使对抗性示例的预测掩码与给定目标图像的掩码相似。在最近的arXiv工作中，通过假设访问提示和模型，在白盒设置中实现了SAM上的TAA任务，因此不太实用。为了解决即时依赖的问题，我们提出了一种简单而有效的方法，只攻击图像编码器。此外，我们提出了一种新的正则化损失，通过增加对抗性图像相对于随机自然图像的特征优势来增强跨模型的可转移性。大量实验验证了我们提出的在SAM上成功进行黑匣子TAA的简单技术的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.10010v1" target="_blank">2310.10010v1</a>
                              </td>
                              <td>Black-box Targeted Adversarial Attack on Segment Anything (SAM)</td>
                              <td>Sheng Zheng</td>
                              <td>2023-10-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_10010v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.10010v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_08820v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAM-guided Unsupervised Domain Adaptation for 3D Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_08820v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_08820v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_08820v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Unsupervised domain adaptation (UDA) in 3D segmentation tasks presents a formidable challenge, primarily stemming from the sparse and unordered nature of point cloud data. Especially for LiDAR point clouds, the domain discrepancy becomes obvious across varying capture scenes, fluctuating weather conditions, and the diverse array of LiDAR devices in use. While previous UDA methodologies have often sought to mitigate this gap by aligning features between source and target domains, this approach falls short when applied to 3D segmentation due to the substantial domain variations. Inspired by the remarkable generalization capabilities exhibited by the vision foundation model, SAM, in the realm of image segmentation, our approach leverages the wealth of general knowledge embedded within SAM to unify feature representations across diverse 3D domains and further solves the 3D domain adaptation problem. Specifically, we harness the corresponding images associated with point clouds to facilitate knowledge transfer and propose an innovative hybrid feature augmentation methodology, which significantly enhances the alignment between the 3D feature space and SAM's feature space, operating at both the scene and instance levels. Our method is evaluated on many widely-recognized datasets and achieves state-of-the-art performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_08820v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>3D分割任务中的无监督域自适应（UDA）提出了一个巨大的挑战，主要源于点云数据的稀疏和无序性质。特别是对于激光雷达点云，在不同的捕获场景、波动的天气条件和使用的各种激光雷达设备中，域差异变得明显。虽然以前的UDA方法经常试图通过对齐源域和目标域之间的特征来缓解这种差距，但由于域的巨大变化，这种方法在应用于3D分割时显得不够。受视觉基础模型SAM在图像分割领域表现出的卓越泛化能力的启发，我们的方法利用SAM中嵌入的丰富的一般知识，在不同的3D域中统一特征表示，并进一步解决了3D域自适应问题。具体而言，我们利用与点云相关的相应图像来促进知识转移，并提出了一种创新的混合特征增强方法，该方法显著增强了3D特征空间和SAM特征空间之间的一致性，在场景和实例级别上操作。我们的方法在许多公认的数据集上进行了评估，并实现了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.08820v2" target="_blank">2310.08820v2</a>
                              </td>
                              <td>SAM-guided Unsupervised Domain Adaptation for 3D Segmentation</td>
                              <td>Xidong Peng</td>
                              <td>2023-10-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_08820v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.08820v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_01429v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Adapting Segment Anything Model for Change Detection in HR Remote Sensing Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_01429v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_01429v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_01429v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision Foundation Models (VFMs) such as the Segment Anything Model (SAM) allow zero-shot or interactive segmentation of visual contents, thus they are quickly applied in a variety of visual scenes. However, their direct use in many Remote Sensing (RS) applications is often unsatisfactory due to the special imaging characteristics of RS images. In this work, we aim to utilize the strong visual recognition capabilities of VFMs to improve the change detection of high-resolution Remote Sensing Images (RSIs). We employ the visual encoder of FastSAM, an efficient variant of the SAM, to extract visual representations in RS scenes. To adapt FastSAM to focus on some specific ground objects in the RS scenes, we propose a convolutional adaptor to aggregate the task-oriented change information. Moreover, to utilize the semantic representations that are inherent to SAM features, we introduce a task-agnostic semantic learning branch to model the semantic latent in bi-temporal RSIs. The resulting method, SAMCD, obtains superior accuracy compared to the SOTA methods and exhibits a sample-efficient learning ability that is comparable to semi-supervised CD methods. To the best of our knowledge, this is the first work that adapts VFMs for the CD of HR RSIs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_01429v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉基础模型（VFM）（如Segment Anything Model（SAM））允许对视觉内容进行零样本或交互式分割，因此它们可以快速应用于各种视觉场景。然而，由于遥感图像的特殊成像特性，它们在许多遥感应用中的直接使用往往不令人满意。在这项工作中，我们的目标是利用VFM强大的视觉识别能力来改进高分辨率遥感图像（RSI）的变化检测。我们使用FastSAM的视觉编码器，一种有效的SAM变体，来提取RS场景中的视觉表示。为了使FastSAM专注于RS场景中的一些特定地面对象，我们提出了一种卷积适配器来聚合面向任务的变化信息。此外，为了利用SAM特征固有的语义表示，我们引入了一个任务不可知的语义学习分支来对双时态RSI中潜在的语义进行建模。与SOTA方法相比，由此产生的方法SAMCD获得了卓越的准确性，并表现出与半监督CD方法相当的样本有效学习能力。据我们所知，这是第一项将VFM应用于HR RSI CD的工作。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.01429v3" target="_blank">2309.01429v3</a>
                              </td>
                              <td>Adapting Segment Anything Model for Change Detection in HR Remote Sensing Images</td>
                              <td>Lei Ding</td>
                              <td>2023-09-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_01429v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.01429v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_02034v4_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAMRS: Scaling-up Remote Sensing Segmentation Dataset with Segment Anything Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_02034v4_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_02034v4_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_02034v4_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The success of the Segment Anything Model (SAM) demonstrates the significance of data-centric machine learning. However, due to the difficulties and high costs associated with annotating Remote Sensing (RS) images, a large amount of valuable RS data remains unlabeled, particularly at the pixel level. In this study, we leverage SAM and existing RS object detection datasets to develop an efficient pipeline for generating a large-scale RS segmentation dataset, dubbed SAMRS. SAMRS totally possesses 105,090 images and 1,668,241 instances, surpassing existing high-resolution RS segmentation datasets in size by several orders of magnitude. It provides object category, location, and instance information that can be used for semantic segmentation, instance segmentation, and object detection, either individually or in combination. We also provide a comprehensive analysis of SAMRS from various aspects. Moreover, preliminary experiments highlight the importance of conducting segmentation pre-training with SAMRS to address task discrepancies and alleviate the limitations posed by limited training data during fine-tuning. The code and dataset will be available at https://github.com/ViTAE-Transformer/SAMRS.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_02034v4_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分段任意模型（SAM）的成功证明了以数据为中心的机器学习的重要性。然而，由于与遥感图像注释相关的困难和高成本，大量有价值的遥感数据仍然没有标记，特别是在像素级别。在这项研究中，我们利用SAM和现有的RS对象检测数据集来开发一个高效的管道，用于生成大规模的RS分割数据集，称为SAMRS。SAMRS总共拥有105090张图像和1668241个实例，在大小上超过了现有的高分辨率RS分割数据集几个数量级。它提供对象类别、位置和实例信息，这些信息可单独或组合用于语义分割、实例分割和对象检测。我们还从各个方面对SAMRS进行了全面的分析。此外，初步实验强调了使用SAMRS进行分割预训练的重要性，以解决任务差异，并缓解微调过程中有限训练数据带来的限制。代码和数据集将在https://github.com/ViTAE-Transformer/SAMRS.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.02034v4" target="_blank">2305.02034v4</a>
                              </td>
                              <td>SAMRS: Scaling-up Remote Sensing Segmentation Dataset with Segment Anything Model</td>
                              <td>Di Wang</td>
                              <td>2023-05-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_02034v4_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.02034v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_08683v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Virtual Augmented Reality for Atari Reinforcement Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_08683v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_08683v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_08683v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Reinforcement Learning (RL) has achieved significant milestones in the gaming domain, most notably Google DeepMind's AlphaGo defeating human Go champion Ken Jie. This victory was also made possible through the Atari Learning Environment (ALE): The ALE has been foundational in RL research, facilitating significant RL algorithm developments such as AlphaGo and others. In current Atari video game RL research, RL agents' perceptions of its environment is based on raw pixel data from the Atari video game screen with minimal image preprocessing. Contrarily, cutting-edge ML research, external to the Atari video game RL research domain, is focusing on enhancing image perception. A notable example is Meta Research's "Segment Anything Model" (SAM), a foundation model capable of segmenting images without prior training (zero-shot). This paper addresses a novel methodical question: Can state-of-the-art image segmentation models such as SAM improve the performance of RL agents playing Atari video games? The results suggest that SAM can serve as a "virtual augmented reality" for the RL agent, boosting its Atari video game playing performance under certain conditions. Comparing RL agent performance results from raw and augmented pixel inputs provides insight into these conditions. Although this paper was limited by computational constraints, the findings show improved RL agent performance for augmented pixel inputs and can inform broader research agendas in the domain of "virtual augmented reality for video game playing RL agents".</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_08683v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>强化学习（RL）在游戏领域取得了重大里程碑，最引人注目的是谷歌DeepMind的AlphaGo击败了人类围棋冠军Ken Jie。这一胜利也是通过雅达利学习环境（ALE）实现的：ALE是RL研究的基础，促进了AlphaGo等重要RL算法的发展。在目前的雅达利电子游戏RL研究中，RL代理对其环境的感知是基于雅达利视频游戏屏幕的原始像素数据，并进行了最小的图像预处理。相反，雅达利电子游戏RL研究领域之外的前沿ML研究正专注于增强图像感知。一个值得注意的例子是Meta Research的“Segment Anything Model”（SAM），这是一个能够在没有事先训练的情况下分割图像的基础模型（零样本）。本文提出了一个新颖而有条理的问题：最先进的图像分割模型（如SAM）能否提高玩雅达利电子游戏的RL代理的性能？结果表明，SAM可以作为RL代理的“虚拟增强现实”，在某些条件下提高其雅达利电子游戏的性能。比较原始和增强像素输入的RL代理性能结果可以深入了解这些情况。尽管本文受到计算约束的限制，但研究结果表明，RL代理在增强像素输入方面的性能有所提高，并可以为“玩视频游戏的RL代理的虚拟增强现实”领域的更广泛研究议程提供信息。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.08683v1" target="_blank">2310.08683v1</a>
                              </td>
                              <td>Virtual Augmented Reality for Atari Reinforcement Learning</td>
                              <td>Christian A. Schiller</td>
                              <td>2023-10-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_08683v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.08683v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_08142v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fine-Grained Annotation for Face Anti-Spoofing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_08142v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_08142v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_08142v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Face anti-spoofing plays a critical role in safeguarding facial recognition systems against presentation attacks. While existing deep learning methods show promising results, they still suffer from the lack of fine-grained annotations, which lead models to learn task-irrelevant or unfaithful features. In this paper, we propose a fine-grained annotation method for face anti-spoofing. Specifically, we first leverage the Segment Anything Model (SAM) to obtain pixel-wise segmentation masks by utilizing face landmarks as point prompts. The face landmarks provide segmentation semantics, which segments the face into regions. We then adopt these regions as masks and assemble them into three separate annotation maps: spoof, living, and background maps. Finally, we combine three separate maps into a three-channel map as annotations for model training. Furthermore, we introduce the Multi-Channel Region Exchange Augmentation (MCREA) to diversify training data and reduce overfitting. Experimental results demonstrate that our method outperforms existing state-of-the-art approaches in both intra-dataset and cross-dataset evaluations.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_08142v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人脸反欺骗在保护面部识别系统免受演示攻击方面发挥着关键作用。虽然现有的深度学习方法显示出了有希望的结果，但它们仍然缺乏细粒度的注释，这导致模型学习与任务无关或不忠实的特征。在本文中，我们提出了一种用于人脸反欺骗的细粒度注释方法。具体来说，我们首先利用Segment Anything Model（SAM），通过使用人脸标志作为点提示来获得逐像素分割掩码。人脸标志提供了分割语义，将人脸分割成多个区域。然后，我们采用这些区域作为遮罩，并将它们组装成三个独立的注释图：恶搞图、生活图和背景图。最后，我们将三个独立的映射组合成一个三通道映射，作为模型训练的注释。此外，我们引入了多通道区域交换增强（MCREA），以使训练数据多样化并减少过拟合。实验结果表明，我们的方法在数据集内和数据集间评估方面都优于现有的最先进方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.08142v1" target="_blank">2310.08142v1</a>
                              </td>
                              <td>Fine-Grained Annotation for Face Anti-Spoofing</td>
                              <td>Xu Chen</td>
                              <td>2023-10-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_08142v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.08142v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_04610v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_04610v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_04610v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_04610v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In the upcoming decade, deep learning may revolutionize the natural sciences, enhancing our capacity to model and predict natural occurrences. This could herald a new era of scientific exploration, bringing significant advancements across sectors from drug development to renewable energy. To answer this call, we present DeepSpeed4Science initiative (deepspeed4science.ai) which aims to build unique capabilities through AI system technology innovations to help domain experts to unlock today's biggest science mysteries. By leveraging DeepSpeed's current technology pillars (training, inference and compression) as base technology enablers, DeepSpeed4Science will create a new set of AI system technologies tailored for accelerating scientific discoveries by addressing their unique complexity beyond the common technical approaches used for accelerating generic large language models (LLMs). In this paper, we showcase the early progress we made with DeepSpeed4Science in addressing two of the critical system challenges in structural biology research.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_04610v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在接下来的十年里，深度学习可能会彻底改变自然科学，增强我们对自然事件建模和预测的能力。这可能预示着科学探索的新时代，带来从药物开发到可再生能源等各个领域的重大进步。为了响应这一号召，我们推出了DeepSpeed4Science倡议（DeepSpeed4Science.ai），旨在通过人工智能系统技术创新建立独特的能力，帮助领域专家解开当今最大的科学谜团。通过利用DeepSpeed当前的技术支柱（训练、推理和压缩）作为基础技术的推动者，DeepSpeed4Science将创建一套新的人工智能系统技术，通过解决其独特的复杂性，超越用于加速通用大型语言模型（LLM）的常见技术方法，来加速科学发现。在这篇论文中，我们展示了DeepSpeed4Science在解决结构生物学研究中的两个关键系统挑战方面取得的早期进展。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.04610v2" target="_blank">2310.04610v2</a>
                              </td>
                              <td>DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies</td>
                              <td>Shuaiwen Leon Song</td>
                              <td>2023-10-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_04610v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.04610v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="CLIP"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2310_09647v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Point-DynRF: Point-based Dynamic Radiance Fields from a Monocular Video</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_09647v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_09647v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_09647v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Dynamic radiance fields have emerged as a promising approach for generating novel views from a monocular video. However, previous methods enforce the geometric consistency to dynamic radiance fields only between adjacent input frames, making it difficult to represent the global scene geometry and degenerates at the viewpoint that is spatio-temporally distant from the input camera trajectory. To solve this problem, we introduce point-based dynamic radiance fields (\textbf{Point-DynRF}), a novel framework where the global geometric information and the volume rendering process are trained by neural point clouds and dynamic radiance fields, respectively. Specifically, we reconstruct neural point clouds directly from geometric proxies and optimize both radiance fields and the geometric proxies using our proposed losses, allowing them to complement each other. We validate the effectiveness of our method with experiments on the NVIDIA Dynamic Scenes Dataset and several causally captured monocular video clips.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_09647v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>动态辐射场已经成为从单目视频生成新视图的一种很有前途的方法。然而，先前的方法仅在相邻输入帧之间强制动态辐射场的几何一致性，使得难以表示全局场景几何，并且在与输入相机轨迹时空距离远的视点退化。为了解决这个问题，我们引入了基于点的动态辐射场（\textbf｛point DynRF｝），这是一种新的框架，其中全局几何信息和体绘制过程分别由神经点云和动态辐射场训练。具体来说，我们直接从几何代理重建神经点云，并使用我们提出的损失优化辐射场和几何代理，使它们相互补充。我们在NVIDIA动态场景数据集和几个因果捕捉的单目视频剪辑上进行了实验，验证了我们方法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.09647v2" target="_blank">2310.09647v2</a>
                              </td>
                              <td>Point-DynRF: Point-based Dynamic Radiance Fields from a Monocular Video</td>
                              <td>Byeongjun Park</td>
                              <td>2023-10-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_09647v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.09647v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_13505v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rewrite Caption Semantics: Bridging Semantic Gaps for Language-Supervised Semantic Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_13505v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_13505v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_13505v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision-Language Pre-training has demonstrated its remarkable zero-shot recognition ability and potential to learn generalizable visual representations from language supervision. Taking a step ahead, language-supervised semantic segmentation enables spatial localization of textual inputs by learning pixel grouping solely from image-text pairs. Nevertheless, the state-of-the-art suffers from clear semantic gaps between visual and textual modality: plenty of visual concepts appeared in images are missing in their paired captions. Such semantic misalignment circulates in pre-training, leading to inferior zero-shot performance in dense predictions due to insufficient visual concepts captured in textual representations. To close such semantic gap, we propose Concept Curation (CoCu), a pipeline that leverages CLIP to compensate for the missing semantics. For each image-text pair, we establish a concept archive that maintains potential visually-matched concepts with our proposed vision-driven expansion and text-to-vision-guided ranking. Relevant concepts can thus be identified via cluster-guided sampling and fed into pre-training, thereby bridging the gap between visual and textual semantics. Extensive experiments over a broad suite of 8 segmentation benchmarks show that CoCu achieves superb zero-shot transfer performance and greatly boosts language-supervised segmentation baseline by a large margin, suggesting the value of bridging semantic gap in pre-training data.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_13505v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉语言预训练已经证明了其显著的零样本识别能力和从语言监督中学习通用视觉表示的潜力。更进一步，语言监督语义分割通过仅从图像-文本对中学习像素分组，实现了文本输入的空间定位。然而，最先进的技术在视觉和文本模态之间存在明显的语义差距：图像中出现的大量视觉概念在其配对字幕中缺失。这种语义错位在预训练中循环，由于文本表示中捕获的视觉概念不足，导致密集预测中的零样本性能较差。为了缩小这种语义差距，我们提出了概念库（CoCu），这是一种利用CLIP来补偿缺失语义的管道。对于每个图像-文本对，我们建立了一个概念档案，该档案通过我们提出的视觉驱动的扩展和文本到视觉引导的排名来维护潜在的视觉匹配概念。因此，可以通过聚类引导的采样来识别相关概念，并将其输入到预训练中，从而弥合视觉语义和文本语义之间的差距。在一套广泛的8个分割基准上进行的大量实验表明，CoCu实现了卓越的零样本传递性能，并在很大程度上大大提高了语言监督的分割基线，这表明了在预训练数据中弥合语义差距的价值。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.13505v3" target="_blank">2309.13505v3</a>
                              </td>
                              <td>Rewrite Caption Semantics: Bridging Semantic Gaps for Language-Supervised Semantic Segmentation</td>
                              <td>Yun Xing</td>
                              <td>2023-09-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_13505v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.13505v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15533v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning with Noisy Labels Using Collaborative Sample Selection and Contrastive Semi-Supervised Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15533v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15533v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15533v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Learning with noisy labels (LNL) has been extensively studied, with existing approaches typically following a framework that alternates between clean sample selection and semi-supervised learning (SSL). However, this approach has a limitation: the clean set selected by the Deep Neural Network (DNN) classifier, trained through self-training, inevitably contains noisy samples. This mixture of clean and noisy samples leads to misguidance in DNN training during SSL, resulting in impaired generalization performance due to confirmation bias caused by error accumulation in sample selection. To address this issue, we propose a method called Collaborative Sample Selection (CSS), which leverages the large-scale pre-trained model CLIP. CSS aims to remove the mixed noisy samples from the identified clean set. We achieve this by training a 2-Dimensional Gaussian Mixture Model (2D-GMM) that combines the probabilities from CLIP with the predictions from the DNN classifier. To further enhance the adaptation of CLIP to LNL, we introduce a co-training mechanism with a contrastive loss in semi-supervised learning. This allows us to jointly train the prompt of CLIP and the DNN classifier, resulting in improved feature representation, boosted classification performance of DNNs, and reciprocal benefits to our Collaborative Sample Selection. By incorporating auxiliary information from CLIP and utilizing prompt fine-tuning, we effectively eliminate noisy samples from the clean set and mitigate confirmation bias during training. Experimental results on multiple benchmark datasets demonstrate the effectiveness of our proposed method in comparison with the state-of-the-art approaches.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15533v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>噪声标签学习（LNL）已被广泛研究，现有方法通常遵循在干净样本选择和半监督学习（SSL）之间交替的框架。然而，这种方法有一个局限性：深度神经网络（DNN）分类器通过自训练选择的干净集不可避免地包含噪声样本。这种干净和噪声样本的混合导致SSL期间DNN训练中的误导，导致由于样本选择中的错误积累导致的确认偏差而导致泛化性能受损。为了解决这个问题，我们提出了一种称为协作样本选择（CSS）的方法，该方法利用了大规模的预训练模型CLIP。CSS的目的是从已识别的干净集中去除混合噪声样本。我们通过训练二维高斯混合模型（2D-GMM）来实现这一点，该模型将CLIP的概率与DNN分类器的预测相结合。为了进一步增强CLIP对LNL的适应性，我们在半监督学习中引入了一种具有对比损失的联合训练机制。这使我们能够联合训练CLIP和DNN分类器的提示，从而改进特征表示，提高DNN的分类性能，并为我们的协作样本选择带来互惠利益。通过结合来自CLIP的辅助信息并利用即时微调，我们有效地从干净集中消除了噪声样本，并减轻了训练过程中的确认偏差。在多个基准数据集上的实验结果表明，与最先进的方法相比，我们提出的方法是有效的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15533v1" target="_blank">2310.15533v1</a>
                              </td>
                              <td>Learning with Noisy Labels Using Collaborative Sample Selection and Contrastive Semi-Supervised Learning</td>
                              <td>Qing Miao</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15533v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15533v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15308v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15308v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15308v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15308v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The landscape of publicly available vision foundation models (VFMs), such as CLIP and Segment Anything Model (SAM), is expanding rapidly. VFMs are endowed with distinct capabilities stemming from their pre-training objectives. For instance, CLIP excels in semantic understanding, while SAM specializes in spatial understanding for segmentation. In this work, we introduce a simple recipe to efficiently merge VFMs into a unified model that assimilates their expertise. Our proposed method integrates multi-task learning, continual learning techniques, and teacher-student distillation. This strategy entails significantly less computational cost compared to traditional multi-task training from scratch. Additionally, it only demands a small fraction of the pre-training datasets that were initially used to train individual models. By applying our method to SAM and CLIP, we derive SAM-CLIP: a unified model that amalgamates the strengths of SAM and CLIP into a single backbone, making it apt for edge device applications. We show that SAM-CLIP learns richer visual representations, equipped with both localization and semantic features, suitable for a broad range of vision tasks. SAM-CLIP obtains improved performance on several head probing tasks when compared with SAM and CLIP. We further show that SAM-CLIP not only retains the foundational strengths of its precursor models but also introduces synergistic functionalities, most notably in zero-shot semantic segmentation, where SAM-CLIP establishes new state-of-the-art results on 5 benchmarks. It outperforms previous models that are specifically designed for this task by a large margin, including +6.8% and +5.9% mean IoU improvement on Pascal-VOC and COCO-Stuff datasets, respectively.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15308v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>公开可用的愿景基础模型（VFM），如CLIP和Segment Anything模型（SAM），正在迅速扩展。VFM由于其训练前的目标而被赋予了独特的能力。例如，CLIP擅长语义理解，而SAM专门用于分割的空间理解。在这项工作中，我们介绍了一个简单的配方，可以有效地将VFM合并到一个统一的模型中，吸收他们的专业知识。我们提出的方法集成了多任务学习、持续学习技术和师生提炼。与传统的从头开始的多任务训练相比，这种策略需要显著降低计算成本。此外，它只需要最初用于训练单个模型的预训练数据集的一小部分。通过将我们的方法应用于SAM和CLIP，我们得出了SAM-CLIP：一个统一的模型，将SAM和CLIP的优势合并为一个骨干，使其适用于边缘设备应用。我们表明，SAM-CLIP学习更丰富的视觉表示，具有定位和语义特征，适用于广泛的视觉任务。与SAM和CLIP相比，SAM-CLIP在几个头部探测任务上获得了改进的性能。我们进一步表明，SAM-CLIP不仅保留了其前体模型的基本优势，而且引入了协同功能，最显著的是在零样本语义分割中，其中SAM-CLIP在5个基准上建立了新的最先进的结果。它在很大程度上优于专门为该任务设计的先前模型，包括Pascal VOC和COCO Stuff数据集的平均IoU改进分别为+6.8%和+5.9%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15308v1" target="_blank">2310.15308v1</a>
                              </td>
                              <td>SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding</td>
                              <td>Haoxiang Wang</td>
                              <td>2023-10-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15308v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15308v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15105v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FD-Align: Feature Discrimination Alignment for Fine-tuning Pre-Trained Models in Few-Shot Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15105v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15105v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15105v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Due to the limited availability of data, existing few-shot learning methods trained from scratch fail to achieve satisfactory performance. In contrast, large-scale pre-trained models such as CLIP demonstrate remarkable few-shot and zero-shot capabilities. To enhance the performance of pre-trained models for downstream tasks, fine-tuning the model on downstream data is frequently necessary. However, fine-tuning the pre-trained model leads to a decrease in its generalizability in the presence of distribution shift, while the limited number of samples in few-shot learning makes the model highly susceptible to overfitting. Consequently, existing methods for fine-tuning few-shot learning primarily focus on fine-tuning the model's classification head or introducing additional structure. In this paper, we introduce a fine-tuning approach termed Feature Discrimination Alignment (FD-Align). Our method aims to bolster the model's generalizability by preserving the consistency of spurious features across the fine-tuning process. Extensive experimental results validate the efficacy of our approach for both ID and OOD tasks. Once fine-tuned, the model can seamlessly integrate with existing methods, leading to performance improvements. Our code can be found in https://github.com/skingorz/FD-Align.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15105v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于数据的可用性有限，现有的少量从头开始训练的射击学习方法无法获得令人满意的性能。相比之下，像CLIP这样的大规模预训练模型表现出显著的少射和零样本能力。为了提高下游任务的预训练模型的性能，经常需要对下游数据的模型进行微调。然而，在存在分布偏移的情况下，对预先训练的模型进行微调会导致其可推广性降低，而少镜头学习中样本数量有限使模型极易受到过拟合的影响。因此，现有的微调少镜头学习的方法主要集中在微调模型的分类头或引入额外的结构上。在本文中，我们介绍了一种称为特征识别对齐（FD Align）的微调方法。我们的方法旨在通过在微调过程中保持虚假特征的一致性来增强模型的可推广性。大量的实验结果验证了我们的方法对ID和OOD任务的有效性。一旦进行了微调，该模型就可以与现有方法无缝集成，从而提高性能。我们的代码可以在https://github.com/skingorz/FD-Align.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15105v1" target="_blank">2310.15105v1</a>
                              </td>
                              <td>FD-Align: Feature Discrimination Alignment for Fine-tuning Pre-Trained Models in Few-Shot Learning</td>
                              <td>Kun Song</td>
                              <td>2023-10-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15105v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15105v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_13683v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CAPIVARA: Cost-Efficient Approach for Improving Multilingual CLIP Performance on Low-Resource Languages</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_13683v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_13683v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_13683v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This work introduces CAPIVARA, a cost-efficient framework designed to enhance the performance of multilingual CLIP models in low-resource languages. While CLIP has excelled in zero-shot vision-language tasks, the resource-intensive nature of model training remains challenging. Many datasets lack linguistic diversity, featuring solely English descriptions for images. CAPIVARA addresses this by augmenting text data using image captioning and machine translation to generate multiple synthetic captions in low-resource languages. We optimize the training pipeline with LiT, LoRA, and gradient checkpointing to alleviate the computational cost. Through extensive experiments, CAPIVARA emerges as state of the art in zero-shot tasks involving images and Portuguese texts. We show the potential for significant improvements in other low-resource languages, achieved by fine-tuning the pre-trained multilingual CLIP using CAPIVARA on a single GPU for 2 hours. Our model and code is available at https://github.com/hiaac-nlp/CAPIVARA.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_13683v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>这项工作介绍了CAPIVARA，这是一个成本效益高的框架，旨在提高低资源语言中多语言CLIP模型的性能。尽管CLIP在零样本视觉语言任务方面表现出色，但模型训练的资源密集性仍然具有挑战性。许多数据集缺乏语言多样性，仅以图像的英文描述为特色。CAPIVARA通过使用图像字幕和机器翻译来增强文本数据，以低资源语言生成多个合成字幕，从而解决了这一问题。我们使用LiT、LoRA和梯度检查点优化训练管道，以降低计算成本。通过广泛的实验，CAPIVARA在涉及图像和葡萄牙语文本的零样本任务中成为最先进的技术。我们展示了在其他低资源语言中显著改进的潜力，通过在单个GPU上使用CAPIVARA对预先训练的多语言CLIP进行2小时的微调来实现。我们的型号和代码可在https://github.com/hiaac-nlp/CAPIVARA.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.13683v2" target="_blank">2310.13683v2</a>
                              </td>
                              <td>CAPIVARA: Cost-Efficient Approach for Improving Multilingual CLIP Performance on Low-Resource Languages</td>
                              <td>Gabriel Oliveira dos Santos</td>
                              <td>2023-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_13683v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.13683v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15061v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The BLA Benchmark: Investigating Basic Language Abilities of Pre-Trained Multimodal Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15061v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15061v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15061v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite the impressive performance achieved by pre-trained language-and-vision models in downstream tasks, it remains an open question whether this reflects a proper understanding of image-text interaction. In this work, we explore to what extent they handle basic linguistic constructions -- active-passive voice, coordination, and relative clauses -- that even preschool children can typically master. We present BLA, a novel, automatically constructed benchmark to evaluate multimodal models on these Basic Language Abilities. We show that different types of Transformer-based systems, such as CLIP, ViLBERT, and BLIP2, generally struggle with BLA in a zero-shot setting, in line with previous findings. Our experiments, in particular, show that most of the tested models only marginally benefit when fine-tuned or prompted with construction-specific samples. Yet, the generative BLIP2 shows promising trends, especially in an in-context learning setting. This opens the door to using BLA not only as an evaluation benchmark but also to improve models' basic language abilities.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15061v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管预先训练的语言和视觉模型在下游任务中取得了令人印象深刻的性能，但这是否反映了对图像-文本交互的正确理解仍然是一个悬而未决的问题。在这项工作中，我们探讨了他们在多大程度上处理基本的语言结构——主动语态、协调和关系从句——即使是学龄前儿童也通常能够掌握这些结构。我们提出了BLA，这是一种新的、自动构建的基准，用于评估这些基本语言能力的多模式模型。我们表明，不同类型的基于变换器的系统，如CLIP、ViLBERT和BLIP2，通常在零样本环境中与BLA发生冲突，这与之前的研究结果一致。特别是，我们的实验表明，当对大多数测试模型进行微调或使用特定于结构的样本进行提示时，它们只会略微受益。然而，生成性BLIP2显示出有希望的趋势，尤其是在情境学习环境中。这为使用BLA打开了大门，不仅可以作为评估基准，还可以提高模型的基本语言能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15061v1" target="_blank">2310.15061v1</a>
                              </td>
                              <td>The BLA Benchmark: Investigating Basic Language Abilities of Pre-Trained Multimodal Models</td>
                              <td>Xinyi Chen</td>
                              <td>2023-10-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15061v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15061v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_08138v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Find What You Want: Learning Demand-conditioned Object Attribute Space for Demand-driven Navigation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_08138v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_08138v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_08138v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The task of Visual Object Navigation (VON) involves an agent's ability to locate a particular object within a given scene. In order to successfully accomplish the VON task, two essential conditions must be fulfilled:1) the user must know the name of the desired object; and 2) the user-specified object must actually be present within the scene. To meet these conditions, a simulator can incorporate pre-defined object names and positions into the metadata of the scene. However, in real-world scenarios, it is often challenging to ensure that these conditions are always met. Human in an unfamiliar environment may not know which objects are present in the scene, or they may mistakenly specify an object that is not actually present. Nevertheless, despite these challenges, human may still have a demand for an object, which could potentially be fulfilled by other objects present within the scene in an equivalent manner. Hence, we propose Demand-driven Navigation (DDN), which leverages the user's demand as the task instruction and prompts the agent to find the object matches the specified demand. DDN aims to relax the stringent conditions of VON by focusing on fulfilling the user's demand rather than relying solely on predefined object categories or names. We propose a method first acquire textual attribute features of objects by extracting common knowledge from a large language model. These textual attribute features are subsequently aligned with visual attribute features using Contrastive Language-Image Pre-training (CLIP). By incorporating the visual attribute features as prior knowledge, we enhance the navigation process. Experiments on AI2Thor with the ProcThor dataset demonstrate the visual attribute features improve the agent's navigation performance and outperform the baseline methods commonly used in VON.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_08138v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉对象导航（VON）的任务涉及代理在给定场景中定位特定对象的能力。为了成功完成VON任务，必须满足两个基本条件：1）用户必须知道所需对象的名称；以及2）用户指定的对象必须实际存在于场景内。为了满足这些条件，模拟器可以将预定义的对象名称和位置合并到场景的元数据中。然而，在现实世界中，确保这些条件始终得到满足往往是一项挑战。处于陌生环境中的人可能不知道场景中存在哪些对象，或者他们可能错误地指定了一个实际上不存在的对象。尽管如此，尽管存在这些挑战，人类可能仍然对一个物体有需求，而场景中的其他物体可能会以同等的方式满足这一需求。因此，我们提出了需求驱动导航（DDN），它利用用户的需求作为任务指令，并提示代理查找与指定需求匹配的对象。DDN旨在通过专注于满足用户的需求而不是仅仅依赖于预定义的对象类别或名称来放宽VON的严格条件。我们提出了一种方法，首先通过从大型语言模型中提取公共知识来获取对象的文本属性特征。随后使用对比语言图像预训练（CLIP）将这些文本属性特征与视觉属性特征对齐。通过将视觉属性特征作为先验知识，我们增强了导航过程。使用ProcThor数据集在AI2Thor上进行的实验表明，视觉属性特征提高了代理的导航性能，并优于VON中常用的基线方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.08138v2" target="_blank">2309.08138v2</a>
                              </td>
                              <td>Find What You Want: Learning Demand-conditioned Object Attribute Space for Demand-driven Navigation</td>
                              <td>Hongcheng Wang</td>
                              <td>2023-09-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_08138v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.08138v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_13856v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning-to-Rank Meets Language: Boosting Language-Driven Ordering Alignment for Ordinal Classification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_13856v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_13856v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_13856v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present a novel language-driven ordering alignment method for ordinal classification. The labels in ordinal classification contain additional ordering relations, making them prone to overfitting when relying solely on training data. Recent developments in pre-trained vision-language models inspire us to leverage the rich ordinal priors in human language by converting the original task into a visionlanguage alignment task. Consequently, we propose L2RCLIP, which fully utilizes the language priors from two perspectives. First, we introduce a complementary prompt tuning technique called RankFormer, designed to enhance the ordering relation of original rank prompts. It employs token-level attention with residual-style prompt blending in the word embedding space. Second, to further incorporate language priors, we revisit the approximate bound optimization of vanilla cross-entropy loss and restructure it within the cross-modal embedding space. Consequently, we propose a cross-modal ordinal pairwise loss to refine the CLIP feature space, where texts and images maintain both semantic alignment and ordering alignment. Extensive experiments on three ordinal classification tasks, including facial age estimation, historical color image (HCI) classification, and aesthetic assessment demonstrate its promising performance. The code is available at https://github.com/raywang335/L2RCLIP.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_13856v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的用于序数分类的语言驱动排序对齐方法。有序分类中的标签包含额外的排序关系，使得它们在仅依赖训练数据时容易过拟合。预先训练的视觉语言模型的最新发展启发我们通过将原始任务转换为视觉语言对齐任务来利用人类语言中丰富的有序先验。因此，我们提出了L2RCLIP，它从两个角度充分利用了语言先验。首先，我们介绍了一种称为RankFormer的互补提示调整技术，旨在增强原始排名提示的排序关系。它在单词嵌入空间中采用了标记级注意力和残余风格提示混合。其次，为了进一步结合语言先验，我们重新审视了vanilla交叉熵损失的近似界优化，并在跨模态嵌入空间中对其进行了重构。因此，我们提出了一种跨模态有序成对损失来精化CLIP特征空间，其中文本和图像保持语义对齐和排序对齐。在面部年龄估计、历史彩色图像（HCI）分类和美学评估三个有序分类任务上的大量实验证明了它的良好性能。代码可在https://github.com/raywang335/L2RCLIP.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.13856v3" target="_blank">2306.13856v3</a>
                              </td>
                              <td>Learning-to-Rank Meets Language: Boosting Language-Driven Ordering Alignment for Ordinal Classification</td>
                              <td>Rui Wang</td>
                              <td>2023-06-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_13856v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.13856v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15200v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Inject Semantic Concepts into Image Tagging for Open-Set Recognition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15200v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15200v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15200v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we introduce the Recognize Anything Plus Model~(RAM++), a fundamental image recognition model with strong open-set recognition capabilities, by injecting semantic concepts into image tagging training framework. Previous approaches are either image tagging models constrained by limited semantics, or vision-language models with shallow interaction for suboptimal performance in multi-tag recognition. In contrast, RAM++ integrates image-text alignment and image-tagging within a unified fine-grained interaction framework based on image-tags-text triplets. This design enables RAM++ not only excel in identifying predefined categories, but also significantly augment the recognition ability in open-set categories. Moreover, RAM++ employs large language models~(LLMs) to generate diverse visual tag descriptions, pioneering the integration of LLM's knowledge into image tagging training. This approach empowers RAM++ to integrate visual description concepts for open-set recognition during inference. Evaluations on comprehensive image recognition benchmarks demonstrate RAM++ exceeds existing state-of-the-art (SOTA) fundamental image recognition models on most aspects. Specifically, for predefined common-used tag categories, RAM++ showcases 10.2 mAP and 15.4 mAP enhancements over CLIP on OpenImages and ImageNet. For open-set categories beyond predefined, RAM++ records improvements of 5 mAP and 6.4 mAP over CLIP and RAM respectively on OpenImages. For diverse human-object interaction phrases, RAM++ achieves 7.8 mAP and 4.7 mAP improvements on the HICO benchmark. Code, datasets and pre-trained models are available at \url{https://github.com/xinyu1205/recognize-anything}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15200v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们通过将语义概念注入到图像标记训练框架中，引入了具有强大的开集识别能力的基本图像识别模型recognition Anything Plus Model~（RAM++）。以前的方法要么是受有限语义约束的图像标记模型，要么是在多标签识别中具有次优性能的浅交互的视觉语言模型。相比之下，RAM++在基于图像-标签-文本三元组的统一细粒度交互框架内集成了图像-文本对齐和图像标记。这种设计使RAM++不仅在识别预定义类别方面表现出色，而且显著增强了开集类别的识别能力。此外，RAM++采用了大型语言模型~（LLM）来生成各种视觉标签描述，开创了将LLM的知识集成到图像标签训练中的先河。这种方法使RAM++能够在推理过程中集成视觉描述概念用于开集识别。对综合图像识别基准的评估表明，RAM++在大多数方面都超过了现有的最先进的基本图像识别模型。具体而言，对于预定义的常用标签类别，RAM++在OpenImages和ImageNet上展示了10.2毫安时和15.4毫安时的CLIP增强功能。对于超出预定义范围的开放集类别，RAM++在OpenImages上分别比CLIP和RAM提高了5mAP和6.4mAP。对于不同的人机交互短语，RAM++在HICO基准上分别实现了7.8毫安时和4.7毫安时的改进。代码、数据集和预先训练的模型可在\url上获得{https://github.com/xinyu1205/recognize-anything}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15200v1" target="_blank">2310.15200v1</a>
                              </td>
                              <td>Inject Semantic Concepts into Image Tagging for Open-Set Recognition</td>
                              <td>Xinyu Huang</td>
                              <td>2023-10-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15200v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15200v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_14581v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Leveraging Image-Text Similarity and Caption Modification for the DataComp Challenge: Filtering Track and BYOD Track</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_14581v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_14581v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_14581v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large web crawl datasets have already played an important role in learning multimodal features with high generalization capabilities. However, there are still very limited studies investigating the details or improvements of data design. Recently, a DataComp challenge has been designed to propose the best training data with the fixed models. This paper presents our solution to both filtering track and BYOD track of the DataComp challenge. Our solution adopts large multimodal models CLIP and BLIP-2 to filter and modify web crawl data, and utilize external datasets along with a bag of tricks to improve the data quality. Experiments show our solution significantly outperforms DataComp baselines (filtering track: 6.6% improvement, BYOD track: 48.5% improvement).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_14581v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型网络爬行数据集已经在学习具有高泛化能力的多模式特征方面发挥了重要作用。然而，研究数据设计细节或改进的研究仍然非常有限。最近，设计了一个DataComp挑战，以提出具有固定模型的最佳训练数据。本文介绍了我们对DataComp挑战的过滤跟踪和BYOD跟踪的解决方案。我们的解决方案采用大型多模式模型CLIP和BLIP-2来过滤和修改网络抓取数据，并利用外部数据集和一系列技巧来提高数据质量。实验表明，我们的解决方案显著优于DataComp基线（过滤轨迹：6.6%的改进，BYOD轨迹：48.5%的改进）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.14581v1" target="_blank">2310.14581v1</a>
                              </td>
                              <td>Leveraging Image-Text Similarity and Caption Modification for the DataComp Challenge: Filtering Track and BYOD Track</td>
                              <td>Shuhei Yokoo</td>
                              <td>2023-10-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_14581v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.14581v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_10434v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning the Visualness of Text Using Large Vision-Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_10434v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_10434v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_10434v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual text evokes an image in a person's mind, while non-visual text fails to do so. A method to automatically detect visualness in text will enable text-to-image retrieval and generation models to augment text with relevant images. This is particularly challenging with long-form text as text-to-image generation and retrieval models are often triggered for text that is designed to be explicitly visual in nature, whereas long-form text could contain many non-visual sentences. To this end, we curate a dataset of 3,620 English sentences and their visualness scores provided by multiple human annotators. We also propose a fine-tuning strategy that adapts large vision-language models like CLIP by modifying the model's contrastive learning objective to map text identified as non-visual to a common NULL image while matching visual text to their corresponding images in the document. We evaluate the proposed approach on its ability to (i) classify visual and non-visual text accurately, and (ii) attend over words that are identified as visual in psycholinguistic studies. Empirical evaluation indicates that our approach performs better than several heuristics and baseline models for the proposed task. Furthermore, to highlight the importance of modeling the visualness of text, we conduct qualitative analyses of text-to-image generation systems like DALL-E. Project webpage: https://gaurav22verma.github.io/text-visualness/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_10434v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉文本唤起了人们脑海中的图像，而非视觉文本则无法做到这一点。一种自动检测文本视觉性的方法将使文本到图像的检索和生成模型能够用相关图像来增强文本。这对长格式文本来说尤其具有挑战性，因为文本到图像的生成和检索模型通常是为本质上明确可见的文本而触发的，而长格式文本可能包含许多非视觉句子。为此，我们策划了一个由3620个英语句子及其视觉评分组成的数据集，这些句子由多个人类注释者提供。我们还提出了一种微调策略，通过修改模型的对比学习目标，将识别为非视觉的文本映射到常见的NULL图像，同时将视觉文本匹配到文档中相应的图像，来适应像CLIP这样的大型视觉语言模型。我们评估了所提出的方法的能力：（i）准确地对视觉和非视觉文本进行分类，以及（ii）处理心理语言学研究中被确定为视觉的单词。经验评估表明，对于所提出的任务，我们的方法比几种启发式和基线模型表现得更好。此外，为了强调文本可视化建模的重要性，我们对DALL-E等文本到图像生成系统进行了定性分析。项目网页：https://gaurav22verma.github.io/text-visualness/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.10434v2" target="_blank">2305.10434v2</a>
                              </td>
                              <td>Learning the Visualness of Text Using Large Vision-Language Models</td>
                              <td>Gaurav Verma</td>
                              <td>2023-05-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_10434v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.10434v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_13137v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_13137v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_13137v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_13137v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have revolutionized natural language processing tasks. However, their practical deployment is hindered by their immense memory and computation requirements. Although recent post-training quantization (PTQ) methods are effective in reducing memory footprint and improving the computational efficiency of LLM, they hand-craft quantization parameters, which leads to low performance and fails to deal with extremely low-bit quantization. To tackle this issue, we introduce an Omnidirectionally calibrated Quantization (OmniQuant) technique for LLMs, which achieves good performance in diverse quantization settings while maintaining the computational efficiency of PTQ by efficiently optimizing various quantization parameters. OmniQuant comprises two innovative components including Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET). LWC modulates the extreme values of weights by optimizing the clipping threshold. Meanwhile, LET tackles activation outliers by shifting the challenge of quantization from activations to weights through a learnable equivalent transformation. Operating within a differentiable framework using block-wise error minimization, OmniQuant can optimize the quantization process efficiently for both weight-only and weight-activation quantization. For instance, the LLaMA-2 model family with the size of 7-70B can be processed with OmniQuant on a single A100-40G GPU within 1-16 hours using 128 samples. Extensive experiments validate OmniQuant's superior performance across diverse quantization configurations such as W4A4, W6A6, W4A16, W3A16, and W2A16. Additionally, OmniQuant demonstrates effectiveness in instruction-tuned models and delivers notable improvements in inference speed and memory reduction on real devices. Codes and models are available at \url{https://github.com/OpenGVLab/OmniQuant}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_13137v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）已经彻底改变了自然语言处理任务。然而，它们的实际部署受到其巨大的内存和计算需求的阻碍。尽管最近的后训练量化（PTQ）方法在减少内存占用和提高LLM的计算效率方面是有效的，但它们手工制作量化参数，导致性能低下，并且无法处理极低位量化。为了解决这个问题，我们为LLM引入了一种全向校准量化（OmniQuant）技术，该技术在不同的量化设置中实现了良好的性能，同时通过有效地优化各种量化参数来保持PTQ的计算效率。OmniQuant包括两个创新组件，包括可学习权重剪裁（LWC）和可学习等价变换（LET）。LWC通过优化削波阈值来调制权重的极值。同时，LET通过可学习的等价变换将量化的挑战从激活转移到权重，从而解决激活异常值。OmniQuant在使用逐块误差最小化的可微框架内操作，可以有效地优化仅权重量化和权重激活量化的量化过程。例如，尺寸为7-70B的LLaMA-2型号系列可以在1-16小时内使用128个样本在单个A100-40G GPU上使用OmniQuant进行处理。大量实验验证了OmniQuant在各种量化配置（如W4A4、W6A6、W4A16、W3A16和W2A16）中的卓越性能。此外，OmniQuant还展示了在指令调优模型中的有效性，并在实际设备上显著提高了推理速度和减少内存。代码和模型位于\url{https://github.com/OpenGVLab/OmniQuant}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.13137v2" target="_blank">2308.13137v2</a>
                              </td>
                              <td>OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models</td>
                              <td>Wenqi Shao</td>
                              <td>2023-08-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_13137v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.13137v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_14222v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">One-for-All: Towards Universal Domain Translation with a Single StyleGAN</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_14222v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_14222v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_14222v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we propose a novel translation model, UniTranslator, for transforming representations between visually distinct domains under conditions of limited training data and significant visual differences. The main idea behind our approach is leveraging the domain-neutral capabilities of CLIP as a bridging mechanism, while utilizing a separate module to extract abstract, domain-agnostic semantics from the embeddings of both the source and target realms. Fusing these abstract semantics with target-specific semantics results in a transformed embedding within the CLIP space. To bridge the gap between the disparate worlds of CLIP and StyleGAN, we introduce a new non-linear mapper, the CLIP2P mapper. Utilizing CLIP embeddings, this module is tailored to approximate the latent distribution in the P space, effectively acting as a connector between these two spaces. The proposed UniTranslator is versatile and capable of performing various tasks, including style mixing, stylization, and translations, even in visually challenging scenarios across different visual domains. Notably, UniTranslator generates high-quality translations that showcase domain relevance, diversity, and improved image quality. UniTranslator surpasses the performance of existing general-purpose models and performs well against specialized models in representative tasks. The source code and trained models will be released to the public.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_14222v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了一种新的翻译模型UniTranslator，用于在有限的训练数据和显著的视觉差异的条件下，在视觉上不同的领域之间转换表示。我们的方法背后的主要思想是利用CLIP的领域中立功能作为桥接机制，同时利用一个单独的模块从源领域和目标领域的嵌入中提取抽象的、与领域无关的语义。将这些抽象语义与特定于目标的语义相融合会导致CLIP空间内的转换嵌入。为了弥合CLIP和StyleGAN的不同世界之间的差距，我们引入了一种新的非线性映射器，即CLIP2P映射器。利用CLIP嵌入，该模块被定制为近似P空间中的潜在分布，有效地充当了这两个空间之间的连接器。所提出的UniTranslator多才多艺，能够执行各种任务，包括风格混合、风格化和翻译，即使在不同视觉领域的视觉挑战场景中也是如此。值得注意的是，UniTranslator生成了高质量的翻译，展示了领域相关性、多样性和改进的图像质量。UniTranslator超越了现有通用模型的性能，在代表性任务中与专用模型相比表现良好。源代码和经过训练的模型将向公众发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.14222v1" target="_blank">2310.14222v1</a>
                              </td>
                              <td>One-for-All: Towards Universal Domain Translation with a Single StyleGAN</td>
                              <td>Yong Du</td>
                              <td>2023-10-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_14222v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.14222v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_14119v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Accelerating Aquatic Soft Robots with Elastic Instability Effects</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_14119v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_14119v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_14119v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Sinusoidal undulation has long been considered the most successful swimming pattern for fish and bionic aquatic robots [1]. However, a swimming pattern generated by the hair clip mechanism (HCM, part iii, Figure 1A) [2]~[5] may challenge this knowledge. HCM is an in-plane prestressed bi-stable mechanism that stores elastic energy and releases the stored energy quickly via its snap-through buckling. When used for fish robots, the HCM functions as the fish body and creates unique swimming patterns that we term HCM undulation. With the same energy consumption [3], HCM fish outperforms the traditionally designed soft fish with a two-fold increase in cruising speed. We reproduce this phenomenon in a single-link simulation with Aquarium [6]. HCM undulation generates an average propulsion of 16.7 N/m, 2-3 times larger than the reference undulation (6.78 N/m), sine pattern (5.34 N/m/s), and cambering sine pattern (6.36 N/m), and achieves an efficiency close to the sine pattern. These results can aid in developing fish robots and faster swimming patterns.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_14119v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>正弦波状一直被认为是鱼类和仿生水上机器人最成功的游泳模式[1]。然而，发夹机制产生的游泳模式（HCM，第三部分，图1A）[2]~[5]可能会挑战这一知识。HCM是一种平面内预应力双稳态机构，它储存弹性能量，并通过其搭扣屈曲快速释放储存的能量。当用于鱼类机器人时，HCM起到鱼体的作用，并创建独特的游泳模式，我们称之为HCM波动。在相同的能耗[3]下，HCM鱼的巡航速度提高了两倍，优于传统设计的软鱼。我们在Aquarium[6]的单链路模拟中重现了这一现象。HCM波动产生16.7N/m的平均推进力，是参考波动（6.78N/m）、正弦模式（5.34N/m/s）和弯曲正弦模式（6.36N/m）的2-3倍，并且实现了接近正弦模式的效率。这些结果可以帮助开发鱼类机器人和更快的游泳模式。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.14119v1" target="_blank">2310.14119v1</a>
                              </td>
                              <td>Accelerating Aquatic Soft Robots with Elastic Instability Effects</td>
                              <td>Zechen Xiong</td>
                              <td>2023-10-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_14119v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.14119v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_14108v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_14108v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_14108v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_14108v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities. This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_14108v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对比语言图像预训练（CLIP）是训练视觉语言模型的一种标准方法。虽然CLIP在图像分类任务中具有可扩展性、可提示性和对分布变化的鲁棒性，但它缺乏对象定位功能。本文研究了以下问题：我们能否用模型动物园的特定任务视觉模型来增强CLIP训练，以改善其视觉表现？为此，我们利用开源的特定任务视觉模型为未分级和有噪声的图像文本数据集生成伪标签。随后，除了对图像和文本对进行对比训练外，我们还在这些伪标签上训练CLIP模型。这种简单的设置在不同的视觉任务中显示出高达16.3%的显著改进，包括分割、检测、深度估计和表面法线估计。重要的是，这些增强是在不损害CLIP现有能力的情况下实现的，包括其在可提示的零样本分类方面的熟练程度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.14108v1" target="_blank">2310.14108v1</a>
                              </td>
                              <td>CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement</td>
                              <td>Mohammadreza Salehi</td>
                              <td>2023-10-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_14108v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.14108v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_14000v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Local Differential Privacy for Number of Paths and Katz Centrality</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_14000v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_14000v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_14000v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we give an algorithm to publish the number of paths and Katz centrality under the local differential privacy (LDP), providing a thorough theoretical analysis. Although various works have already introduced subgraph counting algorithms under LDP, they have primarily concentrated on subgraphs of up to five nodes. The challenge in extending this to larger subgraphs is the cumulative and exponential growth of noise as the subgraph size increases in any publication under LDP. We address this issue by proposing an algorithm to publish the number of paths that start at every node in the graph, leading to an algorithm that publishes the Katz centrality of all nodes. This algorithm employs multiple rounds of communication and the clipping technique. Both our theoretical and experimental assessments indicate that our algorithm exhibits acceptable bias and variance, considerably less than an algorithm that bypasses clipping. Furthermore, our Katz centrality estimation is able to recall up to 90% of the nodes with the highest Katz centrality.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_14000v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文给出了一种在局部差分隐私（LDP）条件下公布路径数和Katz中心性的算法，并进行了深入的理论分析。尽管各种工作已经介绍了LDP下的子图计数算法，但它们主要集中在多达五个节点的子图上。将其扩展到更大的子图的挑战是，在LDP下的任何出版物中，随着子图大小的增加，噪声的累积和指数增长。我们通过提出一种算法来公布从图中每个节点开始的路径数量，从而得出一种公布所有节点的Katz中心性的算法来解决这个问题。该算法采用了多轮通信和裁剪技术。我们的理论和实验评估都表明，我们的算法表现出可接受的偏差和方差，远小于绕过剪裁的算法。此外，我们的Katz中心性估计能够回忆起高达90%的具有最高Katz中心度的节点。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.14000v1" target="_blank">2310.14000v1</a>
                              </td>
                              <td>Local Differential Privacy for Number of Paths and Katz Centrality</td>
                              <td>Louis Betzer</td>
                              <td>2023-10-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_14000v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.14000v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_10672v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Implicit Shape Model Trees: Recognition of 3-D Indoor Scenes and Prediction of Object Poses for Mobile Robots</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_10672v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_10672v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_10672v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present an approach for mobile robots to recognize scenes in object arrangements distributed across cluttered environments. Recognition is enabled by intertwining the robot's search for objects and the assignment of found objects to scenes. Our scene model called "Implicit Shape Model (ISM) trees" allows these two tasks to be solved jointly. This article presents novel algorithms for ISM trees to recognize scenes and predict poses of searched objects. We define scenes as object sets in which some objects are connected via 3-D spatial relations. In previous work, we recognized scenes with single ISMs. However, single ISMs are prone to false positives. As a remedy, we have developed ISM trees, a hierarchical model consisting of multiple ISMs. This article contributes a recognition algorithm that now enables the use of ISM trees for scene recognition. ISM trees should be ideally generated from human demonstrations of object arrangements. As a suitable algorithm was not available, we introduce such a generation algorithm. In line with the active vision paradigm, we combined scene recognition and object search in previous work. However, an efficient algorithm was lacking to make this combination effective. Physical experiments show that this is now overcome with a new algorithm achieving efficient combination through predicted object poses.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_10672v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种移动机器人识别分布在杂乱环境中的物体排列场景的方法。识别是通过将机器人对物体的搜索和将找到的物体分配给场景来实现的。我们的场景模型称为“隐式形状模型（ISM）树”，可以联合解决这两个任务。本文提出了一种新的ISM树算法来识别场景和预测搜索对象的姿态。我们将场景定义为对象集，其中一些对象通过三维空间关系连接。在之前的工作中，我们识别了具有单个ISM的场景。然而，单个ISM容易出现误报。作为补救措施，我们开发了ISM树，这是一个由多个ISM组成的分层模型。本文提供了一种识别算法，现在可以使用ISM树进行场景识别。理想情况下，ISM树应该由对象排列的人类演示生成。由于没有合适的算法，我们引入了这样一种生成算法。根据主动视觉范式，我们在之前的工作中结合了场景识别和物体搜索。然而，缺乏一种有效的算法来使这种组合有效。物理实验表明，现在通过一种新的算法来克服这一问题，该算法通过预测的物体姿态来实现有效的组合。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.10672v3" target="_blank">2301.10672v3</a>
                              </td>
                              <td>Implicit Shape Model Trees: Recognition of 3-D Indoor Scenes and Prediction of Object Poses for Mobile Robots</td>
                              <td>Pascal Meißner</td>
                              <td>2023-01-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_10672v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.10672v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_14926v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Reference-based Restoration of Digitized Analog Videotapes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_14926v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_14926v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_14926v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Analog magnetic tapes have been the main video data storage device for several decades. Videos stored on analog videotapes exhibit unique degradation patterns caused by tape aging and reader device malfunctioning that are different from those observed in film and digital video restoration tasks. In this work, we present a reference-based approach for the resToration of digitized Analog videotaPEs (TAPE). We leverage CLIP for zero-shot artifact detection to identify the cleanest frames of each video through textual prompts describing different artifacts. Then, we select the clean frames most similar to the input ones and employ them as references. We design a transformer-based Swin-UNet network that exploits both neighboring and reference frames via our Multi-Reference Spatial Feature Fusion (MRSFF) blocks. MRSFF blocks rely on cross-attention and attention pooling to take advantage of the most useful parts of each reference frame. To address the absence of ground truth in real-world videos, we create a synthetic dataset of videos exhibiting artifacts that closely resemble those commonly found in analog videotapes. Both quantitative and qualitative experiments show the effectiveness of our approach compared to other state-of-the-art methods. The code, the model, and the synthetic dataset are publicly available at https://github.com/miccunifi/TAPE.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_14926v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>几十年来，模拟磁带一直是主要的视频数据存储设备。存储在模拟录像带上的视频表现出由磁带老化和读取器设备故障引起的独特退化模式，这与在胶片和数字视频恢复任务中观察到的不同。在这项工作中，我们提出了一种基于参考的数字化模拟录像带PE（TAPE）重建方法。我们利用CLIP进行零样本伪影检测，通过描述不同伪影的文本提示来识别每个视频中最干净的帧。然后，我们选择与输入帧最相似的干净帧，并将它们用作参考。我们设计了一个基于转换器的Swin-UNet网络，该网络通过我们的多参考空间特征融合（MRSFF）块利用相邻帧和参考帧。MRSFF块依赖于交叉注意力和注意力池来利用每个参考帧中最有用的部分。为了解决现实世界视频中缺乏基本事实的问题，我们创建了一个视频合成数据集，展示了与模拟录像带中常见的伪像非常相似的伪像。定量和定性实验都表明，与其他最先进的方法相比，我们的方法是有效的。代码、模型和合成数据集可在https://github.com/miccunifi/TAPE.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.14926v1" target="_blank">2310.14926v1</a>
                              </td>
                              <td>Reference-based Restoration of Digitized Analog Videotapes</td>
                              <td>Lorenzo Agnolucci</td>
                              <td>2023-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_14926v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.14926v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_13730v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Localizing and Editing Knowledge in Text-to-Image Generative Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_13730v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_13730v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_13730v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Text-to-Image Diffusion Models such as Stable-Diffusion and Imagen have achieved unprecedented quality of photorealism with state-of-the-art FID scores on MS-COCO and other generation benchmarks. Given a caption, image generation requires fine-grained knowledge about attributes such as object structure, style, and viewpoint amongst others. Where does this information reside in text-to-image generative models? In our paper, we tackle this question and understand how knowledge corresponding to distinct visual attributes is stored in large-scale text-to-image diffusion models. We adapt Causal Mediation Analysis for text-to-image models and trace knowledge about distinct visual attributes to various (causal) components in the (i) UNet and (ii) text-encoder of the diffusion model. In particular, we show that unlike generative large-language models, knowledge about different attributes is not localized in isolated components, but is instead distributed amongst a set of components in the conditional UNet. These sets of components are often distinct for different visual attributes. Remarkably, we find that the CLIP text-encoder in public text-to-image models such as Stable-Diffusion contains only one causal state across different visual attributes, and this is the first self-attention layer corresponding to the last subject token of the attribute in the caption. This is in stark contrast to the causal states in other language models which are often the mid-MLP layers. Based on this observation of only one causal state in the text-encoder, we introduce a fast, data-free model editing method Diff-QuickFix which can effectively edit concepts in text-to-image models. DiffQuickFix can edit (ablate) concepts in under a second with a closed-form update, providing a significant 1000x speedup and comparable editing performance to existing fine-tuning based editing methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_13730v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文本到图像扩散模型，如稳定扩散和Imagen，在MS-COCO和其他生成基准上使用最先进的FID评分，实现了前所未有的真实感质量。给定标题，图像生成需要关于对象结构、样式和视点等属性的细粒度知识。这些信息在文本到图像生成模型中的位置？在我们的论文中，我们解决了这个问题，并了解了与不同视觉属性相对应的知识是如何存储在大规模文本到图像扩散模型中的。我们将因果中介分析应用于文本到图像模型，并将关于不同视觉属性的知识追溯到扩散模型的（i）UNet和（ii）文本编码器中的各种（因果）成分。特别是，我们发现，与生成型大型语言模型不同，关于不同属性的知识不是局限于孤立的组件中，而是分布在条件UNet中的一组组件中。对于不同的视觉属性，这些组件集通常是不同的。值得注意的是，我们发现，在Stable Diffusion等公共文本到图像模型中，CLIP文本编码器在不同的视觉属性中只包含一个因果状态，这是与标题中属性的最后一个主题标记相对应的第一个自关注层。这与其他语言模型中的因果状态形成了鲜明对比，这些模型通常是MLP的中间层。基于对文本编码器中只有一种因果状态的观察，我们介绍了一种快速、无数据的模型编辑方法Diff QuickFix，它可以有效地编辑文本到图像模型中的概念。DiffQuickFix可以通过闭合形式更新在不到一秒钟的时间内编辑（消融）概念，提供显著的1000倍加速和与现有基于微调的编辑方法相当的编辑性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.13730v1" target="_blank">2310.13730v1</a>
                              </td>
                              <td>Localizing and Editing Knowledge in Text-to-Image Generative Models</td>
                              <td>Samyadeep Basu</td>
                              <td>2023-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_13730v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.13730v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_14108v5_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DataComp: In search of the next generation of multimodal datasets</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_14108v5_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_14108v5_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_14108v5_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multimodal datasets are a critical component in recent breakthroughs such as Stable Diffusion and GPT-4, yet their design does not receive the same research attention as model architectures or training algorithms. To address this shortcoming in the ML ecosystem, we introduce DataComp, a testbed for dataset experiments centered around a new candidate pool of 12.8 billion image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing the resulting model on 38 downstream test sets. Our benchmark consists of multiple compute scales spanning four orders of magnitude, which enables the study of scaling trends and makes the benchmark accessible to researchers with varying resources. Our baseline experiments show that the DataComp workflow leads to better training sets. In particular, our best baseline, DataComp-1B, enables training a CLIP ViT-L/14 from scratch to 79.2% zero-shot accuracy on ImageNet, outperforming OpenAI's CLIP ViT-L/14 by 3.7 percentage points while using the same training procedure and compute. We release DataComp and all accompanying code at www.datacomp.ai.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_14108v5_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多模式数据集是最近突破的关键组成部分，如稳定扩散和GPT-4，但它们的设计并没有像模型架构或训练算法那样受到研究关注。为了解决ML生态系统中的这一缺点，我们引入了DataComp，这是一个数据集实验的测试平台，围绕着来自Common Crawl的128亿个图像-文本对的新候选池进行。我们基准测试的参与者设计新的过滤技术或策划新的数据源，然后通过运行我们的标准化CLIP训练代码并在38个下游测试集上测试结果模型来评估他们的新数据集。我们的基准由跨越四个数量级的多个计算量表组成，这使得能够研究缩放趋势，并使具有不同资源的研究人员能够访问该基准。我们的基线实验表明，DataComp工作流可以产生更好的训练集。特别是，我们的最佳基线DataComp-1B能够在ImageNet上将CLIP ViT-L/14从零开始训练到79.2%的零样本精度，在使用相同的训练程序和计算时，比OpenAI的CLIP ViT-L/14高3.7个百分点。我们在www.DataComp.ai上发布DataComp和所有附带代码。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.14108v5" target="_blank">2304.14108v5</a>
                              </td>
                              <td>DataComp: In search of the next generation of multimodal datasets</td>
                              <td>Samir Yitzhak Gadre</td>
                              <td>2023-04-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_14108v5_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.14108v5" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_01819v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Amazing Combinatorial Creation: Acceptable Swap-Sampling for Text-to-Image Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_01819v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_01819v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_01819v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Exploring a machine learning system to generate meaningful combinatorial object images from multiple textual descriptions, emulating human creativity, is a significant challenge as humans are able to construct amazing combinatorial objects, but machines strive to emulate data distribution. In this paper, we develop a straight-forward yet highly effective technique called acceptable swap-sampling to generate a combinatorial object image that exhibits novelty and surprise, utilizing text concepts of different objects. Initially, we propose a swapping mechanism that constructs a novel embedding by exchanging column vectors of two text embeddings for generating a new combinatorial image through a cutting-edge diffusion model. Furthermore, we design an acceptable region by managing suitable CLIP distances between the new image and the original concept generations, increasing the likelihood of accepting the new image with a high-quality combination. This region allows us to efficiently sample a small subset from a new image pool generated by using randomly exchanging column vectors. Lastly, we employ a segmentation method to compare CLIP distances among the segmented components, ultimately selecting the most promising object image from the sampled subset. Our experiments focus on text pairs of objects from ImageNet, and our results demonstrate that our approach outperforms recent methods such as Stable-Diffusion2, DALLE2, ERNIE-ViLG2 and Bing in generating novel and surprising object images, even when the associated concepts appear to be implausible, such as lionfish-abacus. Moreover, during the sampling process, our approach without training and human preference is also comparable to PickScore and HPSv2 trained using human preference datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_01819v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>探索一个机器学习系统，从多个文本描述中生成有意义的组合对象图像，模仿人类的创造力，是一个重大挑战，因为人类能够构建令人惊叹的组合对象，但机器努力模拟数据分布。在本文中，我们开发了一种直接但高效的技术，称为可接受交换采样，利用不同对象的文本概念来生成具有新颖性和惊喜性的组合对象图像。最初，我们提出了一种交换机制，该机制通过交换两个文本嵌入的列向量来构建一个新的嵌入，以通过前沿的扩散模型生成新的组合图像。此外，我们通过管理新图像和原始概念生成之间的适当CLIP距离来设计可接受的区域，增加了接受具有高质量组合的新图像的可能性。该区域允许我们从通过使用随机交换列向量生成的新图像池中有效地采样一个子集。最后，我们使用一种分割方法来比较分割分量之间的CLIP距离，最终从采样子集中选择最有前途的目标图像。我们的实验集中在ImageNet中的对象的文本对上，我们的结果表明，我们的方法在生成新颖而令人惊讶的对象图像方面优于最近的方法，如Stable-Diffusion2、DALLE2、ERNIE-ViLG2和Bing，即使相关概念看起来不可信，如狮子鱼算盘。此外，在采样过程中，我们没有训练和人类偏好的方法也与使用人类偏好数据集训练的PickScore和HPSv2相当。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.01819v2" target="_blank">2310.01819v2</a>
                              </td>
                              <td>Amazing Combinatorial Creation: Acceptable Swap-Sampling for Text-to-Image Generation</td>
                              <td>Jun Li</td>
                              <td>2023-10-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_01819v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.01819v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_13355v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SILC: Improving Vision Language Pretraining with Self-Distillation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_13355v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_13355v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_13355v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Image-Text pretraining on web-scale image caption dataset has become the default recipe for open vocabulary classification and retrieval models thanks to the success of CLIP and its variants. Several works have also used CLIP features for dense prediction tasks and have shown the emergence of open-set abilities. However, the contrastive objective only focuses on image-text alignment and does not incentivise image feature learning for dense prediction tasks. In this work, we propose the simple addition of local-to-global correspondence learning by self-distillation as an additional objective for contrastive pre-training to propose SILC. We show that distilling local image features from an exponential moving average (EMA) teacher model significantly improves model performance on several computer vision tasks including classification, retrieval, and especially segmentation. We further show that SILC scales better with the same training duration compared to the baselines. Our model SILC sets a new state of the art for zero-shot classification, few shot classification, image and text retrieval, zero-shot segmentation, and open vocabulary segmentation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_13355v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于CLIP及其变体的成功，在网络规模的图像字幕数据集上的图像文本预训练已经成为开放词汇分类和检索模型的默认配方。一些工作也将CLIP特征用于密集预测任务，并显示了开集能力的出现。然而，对比目标只关注图像-文本对齐，而没有激励密集预测任务的图像特征学习。在这项工作中，我们提出了通过自蒸馏简单地添加局部到全局的对应学习，作为对比预训练的额外目标，以提出SILC。我们表明，从指数移动平均（EMA）教师模型中提取局部图像特征显著提高了模型在几个计算机视觉任务中的性能，包括分类、检索，尤其是分割。我们进一步表明，与基线相比，在相同的训练持续时间下，SILC的规模更好。我们的模型SILC为零样本分类、少镜头分类、图像和文本检索、零样本分割和开放式词汇分割树立了新的技术水平。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.13355v1" target="_blank">2310.13355v1</a>
                              </td>
                              <td>SILC: Improving Vision Language Pretraining with Self-Distillation</td>
                              <td>Muhammad Ferjad Naeem</td>
                              <td>2023-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_13355v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.13355v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_13292v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_13292v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_13292v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_13292v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A large-scale image-text pair dataset has greatly contributed to the development of vision-language pre-training (VLP) models, which enable zero-shot or few-shot classification without costly annotation. However, in the medical domain, the scarcity of data remains a significant challenge for developing a powerful VLP model. In this paper, we tackle the lack of image-text data in chest X-ray by expanding image-label pair as image-text pair via general prompt and utilizing multiple images and multiple sections in a radiologic report. We also design two contrastive losses, named ICL and TCL, for learning study-level characteristics of medical images and reports, respectively. Our model outperforms the state-of-the-art models trained under the same conditions. Also, enlarged dataset improve the discriminative power of our pre-trained model for classification, while sacrificing marginal retrieval performance. Code is available at https://github.com/kakaobrain/cxr-clip.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_13292v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大规模的图像-文本对数据集极大地促进了视觉语言预训练（VLP）模型的发展，该模型能够实现零样本或少搜索分类，而无需昂贵的注释。然而，在医学领域，数据的稀缺性仍然是开发强大的VLP模型的一个重大挑战。在本文中，我们通过一般提示将图像标签对扩展为图像-文本对，并在放射学报告中利用多个图像和多个切片，来解决胸部X射线中图像-文本数据的缺乏问题。我们还设计了两个对比损失，分别命名为ICL和TCL，用于学习医学图像和报告的研究级特征。我们的模型优于在相同条件下训练的最先进的模型。此外，扩大的数据集提高了我们预先训练的分类模型的鉴别力，同时牺牲了边际检索性能。代码可在https://github.com/kakaobrain/cxr-clip.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.13292v1" target="_blank">2310.13292v1</a>
                              </td>
                              <td>CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training</td>
                              <td>Kihyun You</td>
                              <td>2023-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_13292v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.13292v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_13267v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On the Language Encoder of Contrastive Cross-modal Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_13267v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_13267v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_13267v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Contrastive cross-modal models such as CLIP and CLAP aid various vision-language (VL) and audio-language (AL) tasks. However, there has been limited investigation of and improvement in their language encoder, which is the central component of encoding natural language descriptions of image/audio into vector representations. We extensively evaluate how unsupervised and supervised sentence embedding training affect language encoder quality and cross-modal task performance. In VL pretraining, we found that sentence embedding training language encoder quality and aids in cross-modal tasks, improving contrastive VL models such as CyCLIP. In contrast, AL pretraining benefits less from sentence embedding training, which may result from the limited amount of pretraining data. We analyze the representation spaces to understand the strengths of sentence embedding training, and find that it improves text-space uniformity, at the cost of decreased cross-modal alignment.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_13267v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>CLIP和CLAP等对比跨模态模型有助于各种视觉语言（VL）和音频语言（AL）任务。然而，对其语言编码器的研究和改进有限，该编码器是将图像/音频的自然语言描述编码为矢量表示的核心组件。我们广泛评估了无监督和有监督的句子嵌入训练如何影响语言编码器质量和跨模态任务性能。在VL预训练中，我们发现句子嵌入训练语言编码器的质量，并有助于跨模态任务，改进对比VL模型，如CyCLIP。相反，AL预训练从句子嵌入训练中获益较少，这可能是由于预训练数据量有限。我们分析了表示空间，以了解句子嵌入训练的优势，并发现它以减少跨模态对齐为代价，提高了文本空间的一致性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.13267v1" target="_blank">2310.13267v1</a>
                              </td>
                              <td>On the Language Encoder of Contrastive Cross-modal Models</td>
                              <td>Mengjie Zhao</td>
                              <td>2023-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_13267v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.13267v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_13040v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Robust multimodal models have outlier features and encode more concepts</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_13040v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_13040v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_13040v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>What distinguishes robust models from non-robust ones? This question has gained traction with the appearance of large-scale multimodal models, such as CLIP. These models have demonstrated unprecedented robustness with respect to natural distribution shifts. While it has been shown that such differences in robustness can be traced back to differences in training data, so far it is not known what that translates to in terms of what the model has learned. In this work, we bridge this gap by probing the representation spaces of 12 robust multimodal models with various backbones (ResNets and ViTs) and pretraining sets (OpenAI, LAION-400M, LAION-2B, YFCC15M, CC12M and DataComp). We find two signatures of robustness in the representation spaces of these models: (1) Robust models exhibit outlier features characterized by their activations, with some being several orders of magnitude above average. These outlier features induce privileged directions in the model's representation space. We demonstrate that these privileged directions explain most of the predictive power of the model by pruning up to $80 \%$ of the least important representation space directions without negative impacts on model accuracy and robustness; (2) Robust models encode substantially more concepts in their representation space. While this superposition of concepts allows robust models to store much information, it also results in highly polysemantic features, which makes their interpretation challenging. We discuss how these insights pave the way for future research in various fields, such as model pruning and mechanistic interpretability.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_13040v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>稳健模型和非稳健模型的区别是什么？随着大型多模态模型（如CLIP）的出现，这个问题变得越来越重要。这些模型在自然分布变化方面表现出了前所未有的稳健性。虽然已经表明，这种稳健性的差异可以追溯到训练数据的差异，但到目前为止，还不知道这意味着模型学到了什么。在这项工作中，我们通过探索具有各种骨干（ResNets和ViTs）和预训练集（OpenAI、LAION-400M、LAION-2B、YFCC15M、CC12M和DataComp）的12个鲁棒多模式模型的表示空间来弥合这一差距。我们在这些模型的表示空间中发现了两个鲁棒性特征：（1）鲁棒模型表现出以其激活为特征的异常值特征，其中一些比平均值高出几个数量级。这些异常特征在模型的表示空间中引发了特权方向。我们证明，这些特权方向通过修剪高达$80\%$的最不重要的表示空间方向来解释模型的大部分预测能力，而不会对模型的准确性和稳健性产生负面影响；（2） 稳健模型在其表示空间中编码了更多的概念。虽然这种概念的叠加允许稳健模型存储大量信息，但它也会产生高度多义的特征，这使得它们的解释具有挑战性。我们讨论了这些见解如何为未来在各个领域的研究铺平道路，例如模型修剪和机械可解释性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.13040v1" target="_blank">2310.13040v1</a>
                              </td>
                              <td>Robust multimodal models have outlier features and encode more concepts</td>
                              <td>Jonathan Crabbé</td>
                              <td>2023-10-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_13040v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.13040v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_12921v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_12921v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_12921v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_12921v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Reinforcement learning (RL) requires either manually specifying a reward function, which is often infeasible, or learning a reward model from a large amount of human feedback, which is often very expensive. We study a more sample-efficient alternative: using pretrained vision-language models (VLMs) as zero-shot reward models (RMs) to specify tasks via natural language. We propose a natural and general approach to using VLMs as reward models, which we call VLM-RMs. We use VLM-RMs based on CLIP to train a MuJoCo humanoid to learn complex tasks without a manually specified reward function, such as kneeling, doing the splits, and sitting in a lotus position. For each of these tasks, we only provide a single sentence text prompt describing the desired task with minimal prompt engineering. We provide videos of the trained agents at: https://sites.google.com/view/vlm-rm. We can improve performance by providing a second ``baseline'' prompt and projecting out parts of the CLIP embedding space irrelevant to distinguish between goal and baseline. Further, we find a strong scaling effect for VLM-RMs: larger VLMs trained with more compute and data are better reward models. The failure modes of VLM-RMs we encountered are all related to known capability limitations of current VLMs, such as limited spatial reasoning ability or visually unrealistic environments that are far off-distribution for the VLM. We find that VLM-RMs are remarkably robust as long as the VLM is large enough. This suggests that future VLMs will become more and more useful reward models for a wide range of RL applications.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_12921v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>强化学习（RL）需要手动指定奖励函数，这通常是不可行的，或者从大量的人类反馈中学习奖励模型，这通常非常昂贵。我们研究了一种更具样本效率的替代方法：使用预先训练的视觉语言模型（VLM）作为零样本奖励模型（RM），通过自然语言指定任务。我们提出了一种使用VLM作为奖励模型的自然和通用方法，我们称之为VLM-RM。我们使用基于CLIP的VLM RM来训练MuJoCo人形机器人在没有手动指定奖励功能的情况下学习复杂任务，例如下跪、劈叉和莲花姿势。对于这些任务中的每一个，我们只提供一个句子文本提示，用最少的提示工程来描述所需的任务。我们在以下网站提供训练有素的特工的视频：https://sites.google.com/view/vlm-rm.我们可以通过提供第二个“基线”提示并突出CLIP嵌入空间中与区分目标和基线无关的部分来提高性能。此外，我们发现VLM RM有很强的缩放效应：用更多的计算和数据训练的更大的VLM是更好的奖励模型。我们遇到的VLM RM的故障模式都与当前VLM的已知能力限制有关，例如有限的空间推理能力或视觉上不现实的环境，这些环境对VLM来说是遥远的分布。我们发现，只要VLM足够大，VLM RM就非常健壮。这表明，未来的VLM将成为越来越有用的奖励模型，用于广泛的RL应用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.12921v1" target="_blank">2310.12921v1</a>
                              </td>
                              <td>Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning</td>
                              <td>Juan Rocamonde</td>
                              <td>2023-10-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_12921v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.12921v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_12851v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">EmoDiarize: Speaker Diarization and Emotion Identification from Speech Signals using Convolutional Neural Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_12851v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_12851v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_12851v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In the era of advanced artificial intelligence and human-computer interaction, identifying emotions in spoken language is paramount. This research explores the integration of deep learning techniques in speech emotion recognition, offering a comprehensive solution to the challenges associated with speaker diarization and emotion identification. It introduces a framework that combines a pre-existing speaker diarization pipeline and an emotion identification model built on a Convolutional Neural Network (CNN) to achieve higher precision. The proposed model was trained on data from five speech emotion datasets, namely, RAVDESS, CREMA-D, SAVEE, TESS, and Movie Clips, out of which the latter is a speech emotion dataset created specifically for this research. The features extracted from each sample include Mel Frequency Cepstral Coefficients (MFCC), Zero Crossing Rate (ZCR), Root Mean Square (RMS), and various data augmentation algorithms like pitch, noise, stretch, and shift. This feature extraction approach aims to enhance prediction accuracy while reducing computational complexity. The proposed model yields an unweighted accuracy of 63%, demonstrating remarkable efficiency in accurately identifying emotional states within speech signals.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_12851v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在先进的人工智能和人机交互时代，识别口语中的情绪至关重要。本研究探索了深度学习技术在语音情感识别中的集成，为说话人日记化和情感识别相关的挑战提供了一个全面的解决方案。它引入了一个框架，该框架将预先存在的说话者日记管道和建立在卷积神经网络（CNN）上的情绪识别模型相结合，以实现更高的精度。所提出的模型基于五个语音情感数据集的数据进行训练，即RAVDESS、CREMA-D、SAVEE、TESS和电影剪辑，其中后者是专门为本研究创建的语音情感数据集中。从每个样本中提取的特征包括梅尔频率倒谱系数（MFCC）、零交叉率（ZCR）、均方根（RMS）以及各种数据增强算法，如音调、噪声、拉伸和移位。这种特征提取方法旨在提高预测精度，同时降低计算复杂度。所提出的模型产生了63%的未加权准确率，证明了在准确识别语音信号中的情绪状态方面的显著效率。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.12851v1" target="_blank">2310.12851v1</a>
                              </td>
                              <td>EmoDiarize: Speaker Diarization and Emotion Identification from Speech Signals using Convolutional Neural Networks</td>
                              <td>Hanan Hamza</td>
                              <td>2023-10-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_12851v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.12851v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_12753v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Patch-CLIP: A Patch-Text Pre-Trained Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_12753v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_12753v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_12753v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, patch representation learning has emerged as a necessary research direction for exploiting the capabilities of machine learning in software generation. These representations have driven significant performance enhancements across a variety of tasks involving code changes. While the progress is undeniable, a common limitation among existing models is their specialization: they predominantly excel in either predictive tasks, such as security patch classification, or in generative tasks such as patch description generation. This dichotomy is further exacerbated by a prevalent dependency on potentially noisy data sources. Specifically, many models utilize patches integrated with Abstract Syntax Trees (AST) that, unfortunately, may contain parsing inaccuracies, thus acting as a suboptimal source of supervision. In response to these challenges, we introduce PATCH-CLIP, a novel pre-training framework for patches and natural language text. PATCH-CLIP deploys a triple-loss training strategy for 1) patch-description contrastive learning, which enables to separate patches and descriptions in the embedding space, 2) patch-description matching, which ensures that each patch is associated to its description in the embedding space, and 3) patch-description generation, which ensures that the patch embedding is effective for generation. These losses are implemented for joint learning to achieve good performance in both predictive and generative tasks involving patches. Empirical evaluations focusing on patch description generation, demonstrate that PATCH-CLIP sets new state of the art performance, consistently outperforming the state-of-the-art in metrics like BLEU, ROUGE-L, METEOR, and Recall.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_12753v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，补丁表示学习已成为在软件生成中利用机器学习能力的必要研究方向。这些表示在涉及代码更改的各种任务中推动了显著的性能增强。虽然进展是不可否认的，但现有模型的一个共同局限性是它们的专业化：它们主要擅长预测任务，如安全补丁分类，或生成任务，如补丁描述生成。对潜在噪声数据源的普遍依赖进一步加剧了这种二分法。具体来说，许多模型使用与抽象语法树（AST）集成的补丁，不幸的是，这些补丁可能包含解析不准确，从而成为次优的监督来源。为了应对这些挑战，我们引入了PATCH-CLIP，这是一种用于补丁和自然语言文本的新型预训练框架。PATCH-CLIP部署了一种三重损失训练策略，用于1）补丁描述对比学习，它能够在嵌入空间中分离补丁和描述；2）补丁描述匹配，它确保每个补丁都与其在嵌入空间的描述相关联；3）补丁描述生成，它确保补丁嵌入对生成有效。这些损失是为了联合学习而实现的，以在涉及补丁的预测和生成任务中实现良好的性能。专注于补丁描述生成的经验评估表明，patch-CLIP创造了新的最先进的性能，在BLEU、ROUGE-L、METEOR和Recall等指标上始终优于最先进的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.12753v1" target="_blank">2310.12753v1</a>
                              </td>
                              <td>Patch-CLIP: A Patch-Text Pre-Trained Model</td>
                              <td>Xunzhu Tang</td>
                              <td>2023-10-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_12753v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.12753v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_12709v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Capacity Limitation and Optimization Strategy for Flexible Point-to-Multi-Point Optical Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_12709v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_12709v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_12709v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Point-to-multi-point (PtMP) optical networks become the main solutions for network-edge applications such as passive optical networks and radio access networks. Entropy-loading digital subcarrier multiplexing (DSCM) is the core technology to achieve low latency and approach high capacity for flexible PtMP optical networks. However, the high peak-to-average power ratio of the entropy-loading DSCM signal limits the power budget and restricts the capacity, which can be reduced effectively by clipping operation. In this paper, we derive the theoretical capacity limitation of the flexible PtMP optical networks based on the entropy-loading DSCM signal. Meanwhile, an optimal clipping ratio for the clipping operation is acquired to approach the highest capacity limitation. Based on an accurate clipping-noise model under the optimal clipping ratio, we establish a three-dimensional look-up table for bit-error ratio, spectral efficiency, and link loss. Based on the three-dimensional look-up table, an optimization strategy is proposed to acquire optimal spectral efficiencies for achieving a higher capacity of the flexible PtMP optical networks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_12709v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>点对多点（PtMP）光网络成为无源光网络和无线电接入网络等网络边缘应用的主要解决方案。熵负载数字子载波复用（DSCM）是实现柔性PtMP光网络低延迟、接近高容量的核心技术。然而，熵负载DSCM信号的高峰均功率比限制了功率预算并限制了容量，可以通过削波操作有效地降低容量。在本文中，我们基于熵负载DSCM信号推导了柔性PtMP光网络的理论容量限制。同时，获得用于削波操作的最佳削波比，以接近最高容量限制。基于最优削波比下的精确削波噪声模型，我们建立了一个关于误码率、频谱效率和链路损耗的三维查找表。基于三维查找表，提出了一种优化策略，以获得最佳的光谱效率，从而实现更高容量的柔性PtMP光网络。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.12709v1" target="_blank">2310.12709v1</a>
                              </td>
                              <td>Capacity Limitation and Optimization Strategy for Flexible Point-to-Multi-Point Optical Networks</td>
                              <td>Ji Zhou</td>
                              <td>2023-10-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_12709v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.12709v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_12474v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Enhancing High-Resolution 3D Generation through Pixel-wise Gradient Clipping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_12474v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_12474v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_12474v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>High-resolution 3D object generation remains a challenging task primarily due to the limited availability of comprehensive annotated training data. Recent advancements have aimed to overcome this constraint by harnessing image generative models, pretrained on extensive curated web datasets, using knowledge transfer techniques like Score Distillation Sampling (SDS). Efficiently addressing the requirements of high-resolution rendering often necessitates the adoption of latent representation-based models, such as the Latent Diffusion Model (LDM). In this framework, a significant challenge arises: To compute gradients for individual image pixels, it is necessary to backpropagate gradients from the designated latent space through the frozen components of the image model, such as the VAE encoder used within LDM. However, this gradient propagation pathway has never been optimized, remaining uncontrolled during training. We find that the unregulated gradients adversely affect the 3D model's capacity in acquiring texture-related information from the image generative model, leading to poor quality appearance synthesis. To address this overarching challenge, we propose an innovative operation termed Pixel-wise Gradient Clipping (PGC) designed for seamless integration into existing 3D generative models, thereby enhancing their synthesis quality. Specifically, we control the magnitude of stochastic gradients by clipping the pixel-wise gradients efficiently, while preserving crucial texture-related gradient directions. Despite this simplicity and minimal extra cost, extensive experiments demonstrate the efficacy of our PGC in enhancing the performance of existing 3D generative models for high-resolution object rendering.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_12474v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>高分辨率3D对象生成仍然是一项具有挑战性的任务，主要是由于综合注释训练数据的可用性有限。最近的进展旨在通过利用在广泛策划的网络数据集上预训练的图像生成模型，使用分数蒸馏采样（SDS）等知识转移技术来克服这一限制。有效地满足高分辨率渲染的要求通常需要采用基于潜在表示的模型，例如潜在扩散模型（LDM）。在这个框架中，出现了一个重大挑战：为了计算单个图像像素的梯度，有必要通过图像模型的冻结组件（例如LDM中使用的VAE编码器）从指定的潜在空间反向传播梯度。然而，这种梯度传播途径从未得到优化，在训练过程中仍然不受控制。我们发现，不受调节的梯度对3D模型从图像生成模型获取纹理相关信息的能力产生了不利影响，导致外观合成质量较差。为了应对这一总体挑战，我们提出了一种称为逐像素梯度剪裁（PGC）的创新操作，旨在无缝集成到现有的3D生成模型中，从而提高其合成质量。具体来说，我们通过有效地剪裁逐像素梯度来控制随机梯度的大小，同时保留关键的纹理相关梯度方向。尽管这种简单性和最小的额外成本，但大量实验证明了我们的PGC在提高现有3D生成模型的高分辨率对象渲染性能方面的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.12474v1" target="_blank">2310.12474v1</a>
                              </td>
                              <td>Enhancing High-Resolution 3D Generation through Pixel-wise Gradient Clipping</td>
                              <td>Zijie Pan</td>
                              <td>2023-10-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_12474v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.12474v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_14381v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Connecting Multi-modal Contrastive Representations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_14381v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_14381v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_14381v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multi-modal Contrastive Representation learning aims to encode different modalities into a semantically aligned shared space. This paradigm shows remarkable generalization ability on numerous downstream tasks across various modalities. However, the reliance on massive high-quality data pairs limits its further development on more modalities. This paper proposes a novel training-efficient method for learning MCR without paired data called Connecting Multi-modal Contrastive Representations (C-MCR). Specifically, given two existing MCRs pre-trained on (A, B) and (B, C) modality pairs, we project them to a new space and use the data from the overlapping modality B to aligning the two MCRs in the new space. Meanwhile, since the modality pairs (A, B) and (B, C) are already aligned within each MCR, the connection learned by overlapping modality can also be transferred to non-overlapping modality pair (A, C). To unleash the potential of C-MCR, we further introduce a semantic-enhanced inter- and intra-MCR connection method. We first enhance the semantic consistency and completion of embeddings across different modalities for more robust alignment. Then we utilize the inter-MCR alignment to establish the connection, and employ the intra-MCR alignment to better maintain the connection for inputs from non-overlapping modalities. To demonstrate the effectiveness of C-MCR, we connect CLIP and CLAP via texts to derive audio-visual representations, and integrate CLIP and ULIP via images for 3D-language representations. Remarkably, without using any paired data, C-MCR for audio-visual achieves state-of-the-art performance on audio-image retrieval, audio-visual source localization, and counterfactual audio-image recognition tasks. Furthermore, C-MCR for 3D-language also attains advanced zero-shot 3D point cloud classification accuracy on ModelNet40.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_14381v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多模态对比表示学习旨在将不同的模态编码到语义对齐的共享空间中。这种范式在各种模式下对许多下游任务表现出非凡的泛化能力。然而，对大量高质量数据对的依赖限制了它在更多模式上的进一步发展。本文提出了一种新的无配对数据学习MCR的高效训练方法，称为连接多模态对比表示（C-MCR）。具体而言，给定在（A，B）和（B，C）模态对上预先训练的两个现有MCR，我们将它们投影到新空间，并使用来自重叠模态B的数据来在新空间中对齐两个MCR。同时，由于模态对（A，B）和（B，C）已经在每个MCR内对齐，因此通过重叠模态学习的连接也可以转移到非重叠模态对（A，C）。为了释放C-MCR的潜力，我们进一步引入了一种语义增强的MCR间和MCR内连接方法。我们首先增强了不同模态之间嵌入的语义一致性和完整性，以实现更稳健的对齐。然后，我们使用MCR间比对来建立连接，并使用MCR内比对来更好地维护来自非重叠模态的输入的连接。为了证明C-MCR的有效性，我们通过文本连接CLIP和CLAP以获得视听表示，并通过图像集成CLIP和ULIP以获得3D语言表示。值得注意的是，在不使用任何配对数据的情况下，用于视听的C-MCR在音频图像检索、视听源定位和反事实音频图像识别任务方面实现了最先进的性能。此外，三维语言的C-MCR在ModelNet40上也达到了先进的零样本三维点云分类精度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.14381v2" target="_blank">2305.14381v2</a>
                              </td>
                              <td>Connecting Multi-modal Contrastive Representations</td>
                              <td>Zehan Wang</td>
                              <td>2023-05-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_14381v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.14381v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_11084v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Test-Time Distribution Normalization for Contrastively Learned Vision-language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_11084v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_11084v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_11084v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Advances in the field of vision-language contrastive learning have made it possible for many downstream applications to be carried out efficiently and accurately by simply taking the dot product between image and text representations. One of the most representative approaches proposed recently known as CLIP has garnered widespread adoption due to its effectiveness. CLIP is trained with an InfoNCE loss that takes into account both positive and negative samples to help learn a much more robust representation space. This paper reveals that the common downstream practice of taking a dot product is only a zeroth-order approximation of the optimization goal, resulting in a loss of information during test-time. Intuitively, since the model has been optimized based on the InfoNCE loss, test-time procedures should also be in alignment. The question lies in how one can retrieve any semblance of negative samples information during inference in a computationally efficient way. To this end, we propose Distribution Normalization (DN), where we approximate the mean representation of a batch of test samples and use such a mean to represent what would be analogous to negative samples in the InfoNCE loss. DN requires no retraining or fine-tuning and can be effortlessly applied during inference. Extensive experiments on a wide variety of downstream tasks exhibit a clear advantage of DN over the dot product on top of other existing test-time augmentation methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_11084v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉语言对比学习领域的进步使得许多下游应用可以通过简单地获取图像和文本表示之间的点积来高效准确地进行。最近提出的最具代表性的方法之一，即CLIP，由于其有效性而获得广泛采用。CLIP是在考虑正样本和负样本的InfoNCE损失的情况下进行训练的，以帮助学习更稳健的表示空间。本文揭示了采用点积的常见下游实践只是优化目标的零阶近似，导致在测试期间信息丢失。直观地说，由于模型已根据InfoNCE损失进行了优化，因此测试时间程序也应保持一致。问题在于如何在推理过程中以计算高效的方式检索负样本信息的任何表象。为此，我们提出了分布归一化（DN），其中我们近似一批测试样本的平均值表示，并使用这样的平均值来表示InfoNCE损失中类似于负样本的情况。DN不需要再训练或微调，可以在推理过程中轻松应用。在各种下游任务上进行的大量实验表明，与其他现有的测试时间增加方法相比，DN具有明显的优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.11084v2" target="_blank">2302.11084v2</a>
                              </td>
                              <td>Test-Time Distribution Normalization for Contrastively Learned Vision-language Models</td>
                              <td>Yifei Zhou</td>
                              <td>2023-02-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_11084v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.11084v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_12062v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On the use of Vision-Language models for Visual Sentiment Analysis: a study on CLIP</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_12062v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_12062v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_12062v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This work presents a study on how to exploit the CLIP embedding space to perform Visual Sentiment Analysis. We experiment with two architectures built on top of the CLIP embedding space, which we denote by CLIP-E. We train the CLIP-E models with WEBEmo, the largest publicly available and manually labeled benchmark for Visual Sentiment Analysis, and perform two sets of experiments. First, we test on WEBEmo and compare the CLIP-E architectures with state-of-the-art (SOTA) models and with CLIP Zero-Shot. Second, we perform cross dataset evaluation, and test the CLIP-E architectures trained with WEBEmo on other Visual Sentiment Analysis benchmarks. Our results show that the CLIP-E approaches outperform SOTA models in WEBEmo fine grained categorization, and they also generalize better when tested on datasets that have not been seen during training. Interestingly, we observed that for the FI dataset, CLIP Zero-Shot produces better accuracies than SOTA models and CLIP-E trained on WEBEmo. These results motivate several questions that we discuss in this paper, such as how we should design new benchmarks and evaluate Visual Sentiment Analysis, and whether we should keep designing tailored Deep Learning models for Visual Sentiment Analysis or focus our efforts on better using the knowledge encoded in large vision-language models such as CLIP for this task.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_12062v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文研究了如何利用CLIP嵌入空间进行视觉情感分析。我们对建立在CLIP嵌入空间之上的两种体系结构进行了实验，我们用CLIP-E表示。我们使用WEBEmo训练CLIP-E模型，WEBEmo是最大的公开和手动标记的视觉情感分析基准，并进行了两组实验。首先，我们在WEBEmo上进行测试，并将CLIP-E架构与最先进的（SOTA）模型和CLIP零样本进行比较。其次，我们进行跨数据集评估，并在其他视觉情感分析基准上测试使用WEBEmo训练的CLIP-E架构。我们的结果表明，CLIP-E方法在WEBEmo细粒度分类中优于SOTA模型，并且在训练过程中未发现的数据集上测试时，它们也能更好地泛化。有趣的是，我们观察到，对于FI数据集，CLIP零样本比SOTA模型和在WEBEmo上训练的CLIP-E产生更好的精度。这些结果激发了我们在本文中讨论的几个问题，例如我们应该如何设计新的基准和评估视觉情感分析，以及我们是否应该继续为视觉情感分析设计量身定制的深度学习模型，或者集中精力更好地将CLIP等大型视觉语言模型中编码的知识用于这项任务。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.12062v1" target="_blank">2310.12062v1</a>
                              </td>
                              <td>On the use of Vision-Language models for Visual Sentiment Analysis: a study on CLIP</td>
                              <td>Cristina Bustos</td>
                              <td>2023-10-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_12062v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.12062v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2011_07887v4_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Survey of Requirements for COVID-19 Mitigation Strategies. Part I: Newspaper Clips</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2011_07887v4_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2011_07887v4_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2011_07887v4_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The COVID-19 pandemic has influenced virtually all aspects of our lives. Across the world, countries have applied various mitigation strategies for the epidemic, based on social, political, and technological instruments. We postulate that one should {identify the relevant requirements} before committing to a particular mitigation strategy. One way to achieve it is through an overview of what is considered relevant by the general public, and referred to in the media. To this end, we have collected a number of news clips that mention the possible goals and requirements for a mitigation strategy. The snippets are sorted thematically into several categories, such as health-related goals, social and political impact, civil rights, ethical requirements, and so on.   In a forthcoming companion paper, we will present a digest of the requirements, derived from the news clips, and a preliminary take on their formal specification.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2011_07887v4_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>新冠肺炎大流行几乎影响了我们生活的方方面面。在世界各地，各国根据社会、政治和技术手段，采取了各种缓解疫情的战略。我们假设，在采取特定的缓解策略之前，应该{确定相关需求}。实现这一目标的一种方法是概述公众认为相关的内容，并在媒体上提及。为此，我们收集了一些新闻片段，其中提到了缓解战略的可能目标和要求。这些片段按主题分为几个类别，如与健康相关的目标、社会和政治影响、公民权利、道德要求等。在即将发表的配套论文中，我们将介绍从新闻片段中获得的要求摘要，以及对其正式规范的初步看法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2011.07887v4" target="_blank">2011.07887v4</a>
                              </td>
                              <td>A Survey of Requirements for COVID-19 Mitigation Strategies. Part I: Newspaper Clips</td>
                              <td>Wojciech Jamroga</td>
                              <td>2020-11-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2011_07887v4_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2011.07887v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_11867v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Evaluating the Fairness of Discriminative Foundation Models in Computer Vision</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_11867v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_11867v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_11867v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a novel taxonomy for bias evaluation of discriminative foundation models, such as Contrastive Language-Pretraining (CLIP), that are used for labeling tasks. We then systematically evaluate existing methods for mitigating bias in these models with respect to our taxonomy. Specifically, we evaluate OpenAI's CLIP and OpenCLIP models for key applications, such as zero-shot classification, image retrieval and image captioning. We categorize desired behaviors based around three axes: (i) if the task concerns humans; (ii) how subjective the task is (i.e., how likely it is that people from a diverse range of backgrounds would agree on a labeling); and (iii) the intended purpose of the task and if fairness is better served by impartiality (i.e., making decisions independent of the protected attributes) or representation (i.e., making decisions to maximize diversity). Finally, we provide quantitative fairness evaluations for both binary-valued and multi-valued protected attributes over ten diverse datasets. We find that fair PCA, a post-processing method for fair representations, works very well for debiasing in most of the aforementioned tasks while incurring only minor loss of performance. However, different debiasing approaches vary in their effectiveness depending on the task. Hence, one should choose the debiasing approach depending on the specific use case.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_11867v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的分类法，用于判别基础模型的偏见评估，如用于标记任务的对比语言预训练（CLIP）。然后，我们系统地评估了现有的方法，以减轻这些模型中与我们的分类法有关的偏见。具体而言，我们评估了OpenAI的CLIP和OpenCLIP模型的关键应用，如零样本分类、图像检索和图像字幕。我们根据三个轴对期望的行为进行分类：（i）如果任务涉及人类；（ii）任务的主观程度（即，来自不同背景的人对标签达成一致的可能性有多大）；以及（iii）任务的预期目的，以及公正性（即独立于受保护属性做出决定）或代表性（即做出最大化多样性的决定）是否能更好地服务于公平性。最后，我们在十个不同的数据集上为二值和多值保护属性提供了定量的公平性评估。我们发现，公平PCA是一种用于公平表示的后处理方法，在上述大多数任务中都能很好地进行去偏处理，同时只会导致较小的性能损失。然而，不同的去偏方法的有效性因任务而异。因此，应该根据具体的用例选择去偏方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.11867v1" target="_blank">2310.11867v1</a>
                              </td>
                              <td>Evaluating the Fairness of Discriminative Foundation Models in Computer Vision</td>
                              <td>Junaid Ali</td>
                              <td>2023-10-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_11867v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.11867v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_11739v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Unintended Memorization in Large ASR Models, and How to Mitigate It</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_11739v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_11739v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_11739v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>It is well-known that neural networks can unintentionally memorize their training examples, causing privacy concerns. However, auditing memorization in large non-auto-regressive automatic speech recognition (ASR) models has been challenging due to the high compute cost of existing methods such as hardness calibration. In this work, we design a simple auditing method to measure memorization in large ASR models without the extra compute overhead. Concretely, we speed up randomly-generated utterances to create a mapping between vocal and text information that is difficult to learn from typical training examples. Hence, accurate predictions only for sped-up training examples can serve as clear evidence for memorization, and the corresponding accuracy can be used to measure memorization. Using the proposed method, we showcase memorization in the state-of-the-art ASR models. To mitigate memorization, we tried gradient clipping during training to bound the influence of any individual example on the final model. We empirically show that clipping each example's gradient can mitigate memorization for sped-up training examples with up to 16 repetitions in the training set. Furthermore, we show that in large-scale distributed training, clipping the average gradient on each compute core maintains neutral model quality and compute cost while providing strong privacy protection.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_11739v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>众所周知，神经网络可能会无意中记住它们的训练示例，从而引起隐私问题。然而，由于现有方法（如硬度校准）的计算成本高，在大型非自回归自动语音识别（ASR）模型中审计记忆一直具有挑战性。在这项工作中，我们设计了一种简单的审计方法来测量大型ASR模型中的记忆，而不需要额外的计算开销。具体来说，我们加快随机生成的话语，以在语音和文本信息之间创建映射，这很难从典型的训练示例中学习。因此，仅针对加速训练示例的准确预测可以作为记忆的明确证据，并且相应的准确性可以用于测量记忆。使用所提出的方法，我们展示了最先进的ASR模型中的记忆。为了减少记忆，我们在训练过程中尝试了梯度裁剪，以限制任何单个例子对最终模型的影响。我们的经验表明，对于训练集中最多重复16次的加速训练示例，修剪每个示例的梯度可以减轻记忆。此外，我们还表明，在大规模分布式训练中，对每个计算核心上的平均梯度进行裁剪可以保持中立的模型质量和计算成本，同时提供强大的隐私保护。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.11739v1" target="_blank">2310.11739v1</a>
                              </td>
                              <td>Unintended Memorization in Large ASR Models, and How to Mitigate It</td>
                              <td>Lun Wang</td>
                              <td>2023-10-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_11739v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.11739v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_08825v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_08825v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_08825v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_08825v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multi-modal Large Language Models (MLLMs) have made significant strides in expanding the capabilities of Large Language Models (LLMs) through the incorporation of visual perception interfaces. Despite the emergence of exciting applications and the availability of diverse instruction tuning data, existing approaches often rely on CLIP or its variants as the visual branch, and merely extract features from the deep layers. However, these methods lack a comprehensive analysis of the visual encoders in MLLMs. In this paper, we conduct an extensive investigation into the effectiveness of different vision encoders within MLLMs. Our findings reveal that the shallow layer features of CLIP offer particular advantages for fine-grained tasks such as grounding and region understanding. Surprisingly, the vision-only model DINO, which is not pretrained with text-image alignment, demonstrates promising performance as a visual branch within MLLMs. By simply equipping it with an MLP layer for alignment, DINO surpasses CLIP in fine-grained related perception tasks. Building upon these observations, we propose a simple yet effective feature merging strategy, named COMM, that integrates CLIP and DINO with Multi-level features Merging, to enhance the visual capabilities of MLLMs. We evaluate COMM through comprehensive experiments on a wide range of benchmarks, including image captioning, visual question answering, visual grounding, and object hallucination. Experimental results demonstrate the superior performance of COMM compared to existing methods, showcasing its enhanced visual capabilities within MLLMs. Code will be made available at https://github.com/YuchenLiu98/COMM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_08825v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多模态大语言模型（MLLMs）通过引入视觉感知接口，在扩展大语言模型的能力方面取得了重大进展。尽管出现了令人兴奋的应用程序和各种指令调整数据，但现有的方法往往依赖于CLIP或其变体作为视觉分支，而只是从深层提取特征。然而，这些方法缺乏对MLLM中的视觉编码器的全面分析。在本文中，我们对MLLMs中不同视觉编码器的有效性进行了广泛的研究。我们的研究结果表明，CLIP的浅层特征为细粒度任务（如接地和区域理解）提供了特殊的优势。令人惊讶的是，未经文本图像对齐预训练的仅视觉模型DINO，作为MLLMs中的视觉分支，表现出了良好的性能。通过简单地为其配备MLP层进行对齐，DINO在细粒度相关感知任务中超越了CLIP。基于这些观察结果，我们提出了一种简单而有效的特征合并策略，称为COMM，该策略将CLIP和DINO与多级特征合并相结合，以增强MLLM的视觉能力。我们通过在各种基准上进行综合实验来评估COMM，包括图像字幕、视觉问答、视觉基础和对象幻觉。实验结果表明，与现有方法相比，COMM的性能优越，显示了其在MLLM中增强的视觉能力。代码将在https://github.com/YuchenLiu98/COMM.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.08825v2" target="_blank">2310.08825v2</a>
                              </td>
                              <td>From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models</td>
                              <td>Dongsheng Jiang</td>
                              <td>2023-10-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_08825v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.08825v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_13812v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_13812v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_13812v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_13812v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Contrastively trained vision-language models have achieved remarkable progress in vision and language representation learning, leading to state-of-the-art models for various downstream multimodal tasks. However, recent research has highlighted severe limitations of these models in their ability to perform compositional reasoning over objects, attributes, and relations. Scene graphs have emerged as an effective way to understand images compositionally. These are graph-structured semantic representations of images that contain objects, their attributes, and relations with other objects in a scene. In this work, we consider the scene graph parsed from text as a proxy for the image scene graph and propose a graph decomposition and augmentation framework along with a coarse-to-fine contrastive learning objective between images and text that aligns sentences of various complexities to the same image. Along with this, we propose novel negative mining techniques in the scene graph space for improving attribute binding and relation understanding. Through extensive experiments, we demonstrate the effectiveness of our approach that significantly improves attribute binding, relation understanding, systematic generalization, and productivity on multiple recently proposed benchmarks (For example, improvements upto $18\%$ for systematic generalization, $16.5\%$ for relation understanding over a strong baseline), while achieving similar or better performance than CLIP on various general multimodal tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_13812v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>经过对比训练的视觉语言模型在视觉和语言表征学习方面取得了显著进展，为各种下游多模态任务提供了最先进的模型。然而，最近的研究强调了这些模型在对对象、属性和关系进行组合推理的能力方面的严重局限性。场景图已经成为从合成角度理解图像的一种有效方法。这些是图像的图形结构语义表示，包含对象、对象的属性以及与场景中其他对象的关系。在这项工作中，我们将从文本中解析的场景图视为图像场景图的代理，并提出了一个图分解和扩充框架，以及图像和文本之间从粗到细的对比学习目标，该目标将不同复杂度的句子与同一图像对齐。同时，我们在场景图空间中提出了新的负挖掘技术，以提高属性绑定和关系理解。通过广泛的实验，我们证明了我们的方法的有效性，该方法在最近提出的多个基准上显著提高了属性绑定、关系理解、系统概括和生产力（例如，在强基线上，系统概括的改进高达$18\%$，关系理解的改进高至$16.5\%$），同时在各种通用多模式任务上实现与CLIP相似或更好的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.13812v2" target="_blank">2305.13812v2</a>
                              </td>
                              <td>Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality</td>
                              <td>Harman Singh</td>
                              <td>2023-05-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_13812v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.13812v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_11392v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Automatic Satellite Images Captions Generation Using Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_11392v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_11392v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_11392v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Automatic image captioning is a promising technique for conveying visual information using natural language. It can benefit various tasks in satellite remote sensing, such as environmental monitoring, resource management, disaster management, etc. However, one of the main challenges in this domain is the lack of large-scale image-caption datasets, as they require a lot of human expertise and effort to create. Recent research on large language models (LLMs) has demonstrated their impressive performance in natural language understanding and generation tasks. Nonetheless, most of them cannot handle images (GPT-3.5, Falcon, Claude, etc.), while conventional captioning models pre-trained on general ground-view images often fail to produce detailed and accurate captions for aerial images (BLIP, GIT, CM3, CM3Leon, etc.). To address this problem, we propose a novel approach: Automatic Remote Sensing Image Captioning (ARSIC) to automatically collect captions for remote sensing images by guiding LLMs to describe their object annotations. We also present a benchmark model that adapts the pre-trained generative image2text model (GIT) to generate high-quality captions for remote-sensing images. Our evaluation demonstrates the effectiveness of our approach for collecting captions for remote sensing images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_11392v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自动图像字幕是利用自然语言传递视觉信息的一种很有前途的技术。它可以使卫星遥感的各种任务受益，如环境监测、资源管理、灾害管理等。然而，该领域的主要挑战之一是缺乏大规模的图像字幕数据集，因为它们需要大量的人力专业知识和努力来创建。最近对大型语言模型（LLM）的研究表明，它们在自然语言理解和生成任务中的表现令人印象深刻。尽管如此，它们中的大多数都无法处理图像（GPT-3.5、Falcon、Claude等），而在一般地面视图图像上预先训练的传统字幕模型往往无法为航空图像（BLIP、GIT、CM3、CM3Leon等）生成详细准确的字幕，我们提出了一种新的方法：自动遥感图像字幕（ARSIC），通过引导LLM描述其对象注释来自动收集遥感图像的字幕。我们还提出了一个基准模型，该模型适用于预先训练的生成图像文本模型（GIT），以生成遥感图像的高质量字幕。我们的评估证明了我们收集遥感图像字幕的方法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.11392v1" target="_blank">2310.11392v1</a>
                              </td>
                              <td>Towards Automatic Satellite Images Captions Generation Using Large Language Models</td>
                              <td>Yingxu He</td>
                              <td>2023-10-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_11392v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.11392v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_07184v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NeuroInspect: Interpretable Neuron-based Debugging Framework through Class-conditional Visualizations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_07184v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_07184v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_07184v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite deep learning (DL) has achieved remarkable progress in various domains, the DL models are still prone to making mistakes. This issue necessitates effective debugging tools for DL practitioners to interpret the decision-making process within the networks. However, existing debugging methods often demand extra data or adjustments to the decision process, limiting their applicability. To tackle this problem, we present NeuroInspect, an interpretable neuron-based debugging framework with three key stages: counterfactual explanations, feature visualizations, and false correlation mitigation. Our debugging framework first pinpoints neurons responsible for mistakes in the network and then visualizes features embedded in the neurons to be human-interpretable. To provide these explanations, we introduce CLIP-Illusion, a novel feature visualization method that generates images representing features conditioned on classes to examine the connection between neurons and the decision layer. We alleviate convoluted explanations of the conventional visualization approach by employing class information, thereby isolating mixed properties. This process offers more human-interpretable explanations for model errors without altering the trained network or requiring additional data. Furthermore, our framework mitigates false correlations learned from a dataset under a stochastic perspective, modifying decisions for the neurons considered as the main causes. We validate the effectiveness of our framework by addressing false correlations and improving inferences for classes with the worst performance in real-world settings. Moreover, we demonstrate that NeuroInspect helps debug the mistakes of DL models through evaluation for human understanding. The code is openly available at https://github.com/yeongjoonJu/NeuroInspect.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_07184v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管深度学习（DL）在各个领域都取得了显著的进展，但DL模型仍然容易出错。这个问题需要DL从业者使用有效的调试工具来解释网络中的决策过程。然而，现有的调试方法通常需要额外的数据或对决策过程进行调整，从而限制了它们的适用性。为了解决这个问题，我们提出了NeuroInspect，这是一个可解释的基于神经元的调试框架，有三个关键阶段：反事实解释、特征可视化和虚假相关性缓解。我们的调试框架首先确定网络中错误的神经元，然后可视化嵌入神经元中的特征，使其可由人类解释。为了提供这些解释，我们介绍了CLIP Illusion，这是一种新的特征可视化方法，它生成代表以类为条件的特征的图像，以检查神经元和决策层之间的连接。我们通过使用类信息来缓解传统可视化方法的复杂解释，从而隔离混合属性。这一过程为模型错误提供了更人性化的解释，而无需改变训练的网络或需要额外的数据。此外，我们的框架在随机视角下减轻了从数据集中学习到的错误相关性，修改了被认为是主要原因的神经元的决策。我们通过解决虚假相关性并改进对现实世界中性能最差的类的推断来验证我们框架的有效性。此外，我们证明了NeuroInspect通过评估人类理解来帮助调试DL模型的错误。该代码可在https://github.com/yeongjoonJu/NeuroInspect.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.07184v2" target="_blank">2310.07184v2</a>
                              </td>
                              <td>NeuroInspect: Interpretable Neuron-based Debugging Framework through Class-conditional Visualizations</td>
                              <td>Yeong-Joon Ju</td>
                              <td>2023-10-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_07184v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.07184v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_14342v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_14342v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_14342v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_14342v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Given the massive cost of language model pre-training, a non-trivial improvement of the optimization algorithm would lead to a material reduction on the time and cost of training. Adam and its variants have been state-of-the-art for years, and more sophisticated second-order (Hessian-based) optimizers often incur too much per-step overhead. In this paper, we propose Sophia, Second-order Clipped Stochastic Optimization, a simple scalable second-order optimizer that uses a light-weight estimate of the diagonal Hessian as the pre-conditioner. The update is the moving average of the gradients divided by the moving average of the estimated Hessian, followed by element-wise clipping. The clipping controls the worst-case update size and tames the negative impact of non-convexity and rapid change of Hessian along the trajectory. Sophia only estimates the diagonal Hessian every handful of iterations, which has negligible average per-step time and memory overhead. On language modeling with GPT models of sizes ranging from 125M to 1.5B, Sophia achieves a 2x speed-up compared to Adam in the number of steps, total compute, and wall-clock time, achieving the same perplexity with 50% fewer steps, less total compute, and reduced wall-clock time. Theoretically, we show that Sophia, in a much simplified setting, adapts to the heterogeneous curvatures in different parameter dimensions, and thus has a run-time bound that does not depend on the condition number of the loss.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_14342v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>考虑到语言模型预训练的巨大成本，对优化算法进行不小的改进将大大减少训练的时间和成本。Adam及其变体多年来一直是最先进的，更复杂的二阶（基于Hessian的）优化器通常会产生太多的每步开销。在本文中，我们提出了Sophia，二阶截断随机优化，这是一种简单的可扩展二阶优化器，使用对角Hessian的轻量级估计作为前置条件。更新是梯度的移动平均除以估计的Hessian的移动平均，然后是逐元素剪裁。剪裁控制了最坏情况下的更新大小，并缓和了非凸性和Hessian沿轨迹的快速变化的负面影响。Sophia每几次迭代只估计对角线Hessian，这具有可忽略的平均每步时间和内存开销。在使用大小从125M到1.5B不等的GPT模型进行语言建模时，Sophia在步数、总计算和挂钟时间方面比Adam提高了2倍，实现了同样的困惑，减少了50%的步数、更少的总计算和更少的挂钟时间。从理论上讲，我们证明了Sophia在一个非常简化的设置中，能够适应不同参数维度上的异质曲率，因此具有不依赖于损失条件数的运行时界限。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.14342v3" target="_blank">2305.14342v3</a>
                              </td>
                              <td>Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training</td>
                              <td>Hong Liu</td>
                              <td>2023-05-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_14342v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.14342v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_14014v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLIP4STR: A Simple Baseline for Scene Text Recognition with Pre-trained Vision-Language Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_14014v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_14014v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_14014v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Pre-trained vision-language models~(VLMs) are the de-facto foundation models for various downstream tasks. However, scene text recognition methods still prefer backbones pre-trained on a single modality, namely, the visual modality, despite the potential of VLMs to serve as powerful scene text readers. For example, CLIP can robustly identify regular (horizontal) and irregular (rotated, curved, blurred, or occluded) text in images. With such merits, we transform CLIP into a scene text reader and introduce CLIP4STR, a simple yet effective STR method built upon image and text encoders of CLIP. It has two encoder-decoder branches: a visual branch and a cross-modal branch. The visual branch provides an initial prediction based on the visual feature, and the cross-modal branch refines this prediction by addressing the discrepancy between the visual feature and text semantics. To fully leverage the capabilities of both branches, we design a dual predict-and-refine decoding scheme for inference. CLIP4STR achieves new state-of-the-art performance on 11 STR benchmarks. Additionally, a comprehensive empirical study is provided to enhance the understanding of the adaptation of CLIP to STR. We believe our method establishes a simple but strong baseline for future STR research with VLMs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_14014v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>预先训练的视觉语言模型（VLM）是各种下游任务的事实上的基础模型。然而，尽管VLM有可能成为强大的场景文本阅读器，但场景文本识别方法仍然倾向于在单一模态（即视觉模态）上预先训练的骨干。例如，CLIP可以稳健地识别图像中的规则（水平）和不规则（旋转、弯曲、模糊或遮挡）文本。有了这些优点，我们将CLIP转换为场景文本读取器，并介绍了CLIP4STR，这是一种基于CLIP的图像和文本编码器的简单而有效的STR方法。它有两个编码器-解码器分支：一个视觉分支和一个跨模态分支。视觉分支提供基于视觉特征的初始预测，跨模态分支通过解决视觉特征和文本语义之间的差异来细化该预测。为了充分利用这两个分支的能力，我们设计了一个用于推理的双重预测和细化解码方案。CLIP4STR在11个STR基准测试上实现了最先进的性能。此外，还提供了一项全面的实证研究，以增强对CLIP对STR的适应性的理解。我们相信，我们的方法为未来VLM的STR研究建立了一个简单但有力的基线。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.14014v2" target="_blank">2305.14014v2</a>
                              </td>
                              <td>CLIP4STR: A Simple Baseline for Scene Text Recognition with Pre-trained Vision-Language Model</td>
                              <td>Shuai Zhao</td>
                              <td>2023-05-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_14014v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.14014v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_10533v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Label-efficient Segmentation via Affinity Propagation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_10533v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_10533v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_10533v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Weakly-supervised segmentation with label-efficient sparse annotations has attracted increasing research attention to reduce the cost of laborious pixel-wise labeling process, while the pairwise affinity modeling techniques play an essential role in this task. Most of the existing approaches focus on using the local appearance kernel to model the neighboring pairwise potentials. However, such a local operation fails to capture the long-range dependencies and ignores the topology of objects. In this work, we formulate the affinity modeling as an affinity propagation process, and propose a local and a global pairwise affinity terms to generate accurate soft pseudo labels. An efficient algorithm is also developed to reduce significantly the computational cost. The proposed approach can be conveniently plugged into existing segmentation networks. Experiments on three typical label-efficient segmentation tasks, i.e. box-supervised instance segmentation, point/scribble-supervised semantic segmentation and CLIP-guided semantic segmentation, demonstrate the superior performance of the proposed approach.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_10533v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>具有标签高效稀疏注释的弱监督分割吸引了越来越多的研究关注，以降低费力的逐像素标记过程的成本，而成对亲和建模技术在这项任务中发挥着重要作用。现有的大多数方法都集中在使用局部出现核来对相邻的成对势进行建模。然而，这样的本地操作无法捕获长程依赖关系，并且忽略了对象的拓扑结构。在这项工作中，我们将亲和性建模公式化为亲和性传播过程，并提出了一个局部和全局成对亲和项来生成准确的软伪标签。还开发了一种有效的算法来显著降低计算成本。所提出的方法可以方便地插入到现有的分割网络中。在三个典型的标签高效分割任务上的实验，即盒子监督的实例分割、点/涂鸦监督的语义分割和CLIP引导的语义分割，证明了该方法的优越性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.10533v2" target="_blank">2310.10533v2</a>
                              </td>
                              <td>Label-efficient Segmentation via Affinity Propagation</td>
                              <td>Wentong Li</td>
                              <td>2023-10-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_10533v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.10533v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_13013v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Stable and low-precision training for large-scale vision-language models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_13013v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_13013v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_13013v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce new methods for 1) accelerating and 2) stabilizing training for large language-vision models. 1) For acceleration, we introduce SwitchBack, a linear layer for int8 quantized training which provides a speed-up of 13-25% while matching the performance of bfloat16 training within 0.1 percentage points for the 1B parameter CLIP ViT-Huge -- the largest int8 training to date. Our main focus is int8 as GPU support for float8 is rare, though we also analyze float8 training through simulation. While SwitchBack proves effective for float8, we show that standard techniques are also successful if the network is trained and initialized so that large feature magnitudes are discouraged, which we accomplish via layer-scale initialized with zeros. 2) For stability, we analyze loss spikes and find they consistently occur 1-8 iterations after the squared gradients become under-estimated by their AdamW second moment estimator. As a result, we recommend an AdamW-Adafactor hybrid which avoids loss spikes when training a CLIP ViT-Huge model and outperforms gradient clipping at the scales we test.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_13013v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了用于1）加速和2）稳定大型语言视觉模型训练的新方法。1） 对于加速，我们引入了SwitchBack，这是一个用于int8量化训练的线性层，它提供了13-25%的加速，同时在1B参数CLIP ViT Huge的0.1个百分点内匹配bfloat16训练的性能，这是迄今为止最大的int8训练。我们的主要关注点是int8，因为GPU很少支持float8，尽管我们也通过模拟分析float8训练。虽然SwitchBack被证明对float8是有效的，但我们表明，如果网络被训练和初始化，从而阻止大的特征量，标准技术也是成功的，这是我们通过用零初始化的层尺度来实现的。2） 为了稳定性，我们分析了损失尖峰，发现它们在平方梯度被他们的AdamW二阶矩估计器低估后持续发生1-8次迭代。因此，我们推荐一种AdamW-Adafactor混合，它在训练CLIP-ViT-Huge模型时避免了损失峰值，并且在我们测试的尺度上优于梯度裁剪。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.13013v2" target="_blank">2304.13013v2</a>
                              </td>
                              <td>Stable and low-precision training for large-scale vision-language models</td>
                              <td>Mitchell Wortsman</td>
                              <td>2023-04-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_13013v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.13013v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_10651v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">HairCLIPv2: Unifying Hair Editing via Proxy Feature Blending</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_10651v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_10651v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_10651v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Hair editing has made tremendous progress in recent years. Early hair editing methods use well-drawn sketches or masks to specify the editing conditions. Even though they can enable very fine-grained local control, such interaction modes are inefficient for the editing conditions that can be easily specified by language descriptions or reference images. Thanks to the recent breakthrough of cross-modal models (e.g., CLIP), HairCLIP is the first work that enables hair editing based on text descriptions or reference images. However, such text-driven and reference-driven interaction modes make HairCLIP unable to support fine-grained controls specified by sketch or mask. In this paper, we propose HairCLIPv2, aiming to support all the aforementioned interactions with one unified framework. Simultaneously, it improves upon HairCLIP with better irrelevant attributes (e.g., identity, background) preservation and unseen text descriptions support. The key idea is to convert all the hair editing tasks into hair transfer tasks, with editing conditions converted into different proxies accordingly. The editing effects are added upon the input image by blending the corresponding proxy features within the hairstyle or hair color feature spaces. Besides the unprecedented user interaction mode support, quantitative and qualitative experiments demonstrate the superiority of HairCLIPv2 in terms of editing effects, irrelevant attribute preservation and visual naturalness. Our code is available at \url{https://github.com/wty-ustc/HairCLIPv2}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_10651v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，头发编辑取得了巨大的进步。早期的头发编辑方法使用绘制良好的草图或遮罩来指定编辑条件。尽管它们可以实现非常细粒度的局部控制，但对于可以通过语言描述或参考图像轻松指定的编辑条件来说，这种交互模式是低效的。得益于跨模态模型（例如CLIP）的最新突破，HairCLIP是第一个基于文本描述或参考图像进行头发编辑的作品。然而，这种文本驱动和引用驱动的交互模式使HairCLIP无法支持草图或遮罩指定的细粒度控件。在本文中，我们提出了HairCLIPv2，旨在用一个统一的框架来支持上述所有交互。同时，它改进了HairCLIP，具有更好的无关属性（如身份、背景）保护和不可见文本描述支持。关键思想是将所有头发编辑任务转换为头发传递任务，并相应地将编辑条件转换为不同的代理。通过在发型或头发颜色特征空间内混合相应的代理特征，将编辑效果添加到输入图像上。除了前所未有的用户交互模式支持外，定量和定性实验证明了HairCLIPv2在编辑效果、无关属性保存和视觉自然度方面的优势。我们的代码位于\url{https://github.com/wty-ustc/HairCLIPv2}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.10651v1" target="_blank">2310.10651v1</a>
                              </td>
                              <td>HairCLIPv2: Unifying Hair Editing via Proxy Feature Blending</td>
                              <td>Tianyi Wei</td>
                              <td>2023-10-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_10651v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.10651v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_10591v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Interpreting and Controlling Vision Foundation Models via Text Explanations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_10591v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_10591v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_10591v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large-scale pre-trained vision foundation models, such as CLIP, have become de facto backbones for various vision tasks. However, due to their black-box nature, understanding the underlying rules behind these models' predictions and controlling model behaviors have remained open challenges. We present a framework for interpreting vision transformer's latent tokens with natural language. Given a latent token, our framework retains its semantic information to the final layer using transformer's local operations and retrieves the closest text for explanation. Our approach enables understanding of model visual reasoning procedure without needing additional model training or data collection. Based on the obtained interpretations, our framework allows for model editing that controls model reasoning behaviors and improves model robustness against biases and spurious correlations.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_10591v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大规模预先训练的视觉基础模型，如CLIP，已经成为各种视觉任务的事实骨干。然而，由于它们的黑匣子性质，理解这些模型预测背后的潜在规则和控制模型行为仍然是一个悬而未决的挑战。我们提出了一个用自然语言解释视觉变换器潜在表征的框架。给定一个潜在的标记，我们的框架使用transformer的本地操作将其语义信息保留到最后一层，并检索最接近的文本进行解释。我们的方法能够理解模型视觉推理过程，而无需额外的模型训练或数据收集。基于所获得的解释，我们的框架允许进行模型编辑，以控制模型推理行为，并提高模型对偏差和虚假相关性的鲁棒性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>46</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.10591v1" target="_blank">2310.10591v1</a>
                              </td>
                              <td>Interpreting and Controlling Vision Foundation Models via Text Explanations</td>
                              <td>Haozhe Chen</td>
                              <td>2023-10-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_10591v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.10591v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_10541v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Efficient Dataset Distillation through Alignment with Smooth and High-Quality Expert Trajectories</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_10541v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_10541v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_10541v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Training a large and state-of-the-art machine learning model typically necessitates the use of large-scale datasets, which, in turn, makes the training and parameter-tuning process expensive and time-consuming. Some researchers opt to distil information from real-world datasets into tiny and compact synthetic datasets while maintaining their ability to train a well-performing model, hence proposing a data-efficient method known as Dataset Distillation (DD). Despite recent progress in this field, existing methods still underperform and cannot effectively replace large datasets. In this paper, unlike previous methods that focus solely on improving the efficacy of student distillation, we are the first to recognize the important interplay between expert and student. We argue the significant impact of expert smoothness when employing more potent expert trajectories in subsequent dataset distillation. Based on this, we introduce the integration of clipping loss and gradient penalty to regulate the rate of parameter changes in expert trajectories. Furthermore, in response to the sensitivity exhibited towards randomly initialized variables during distillation, we propose representative initialization for synthetic dataset and balanced inner-loop loss. Finally, we present two enhancement strategies, namely intermediate matching loss and weight perturbation, to mitigate the potential occurrence of cumulative errors. We conduct extensive experiments on datasets of different scales, sizes, and resolutions. The results demonstrate that the proposed method significantly outperforms prior methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_10541v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>训练大型和最先进的机器学习模型通常需要使用大型数据集，这反过来又使训练和参数调整过程变得昂贵和耗时。一些研究人员选择将真实世界数据集中的信息提取到微小而紧凑的合成数据集中，同时保持训练性能良好的模型的能力，因此提出了一种称为数据集提取（DD）的数据高效方法。尽管该领域最近取得了进展，但现有方法仍然表现不佳，无法有效取代大型数据集。在本文中，与以前只关注提高学生提炼效率的方法不同，我们是第一个认识到专家和学生之间重要相互作用的方法。我们认为，当在后续数据集提取中使用更有力的专家轨迹时，专家平滑性会产生重大影响。在此基础上，我们引入了削波损失和梯度惩罚的积分来调节专家轨迹中的参数变化率。此外，为了响应蒸馏过程中对随机初始化变量表现出的敏感性，我们提出了合成数据集和平衡内环损失的代表性初始化。最后，我们提出了两种增强策略，即中间匹配损失和权重扰动，以减轻累积误差的潜在发生。我们在不同规模、大小和分辨率的数据集上进行了广泛的实验。结果表明，该方法明显优于现有方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>47</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.10541v1" target="_blank">2310.10541v1</a>
                              </td>
                              <td>Efficient Dataset Distillation through Alignment with Smooth and High-Quality Expert Trajectories</td>
                              <td>Jiyuan Shen</td>
                              <td>2023-10-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_10541v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.10541v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="DINO"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2310_14736v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAMCLR: Contrastive pre-training on complex scenes using SAM for view sampling</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_14736v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_14736v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_14736v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In Computer Vision, self-supervised contrastive learning enforces similar representations between different views of the same image. The pre-training is most often performed on image classification datasets, like ImageNet, where images mainly contain a single class of objects. However, when dealing with complex scenes with multiple items, it becomes very unlikely for several views of the same image to represent the same object category. In this setting, we propose SAMCLR, an add-on to SimCLR which uses SAM to segment the image into semantic regions, then sample the two views from the same region. Preliminary results show empirically that when pre-training on Cityscapes and ADE20K, then evaluating on classification on CIFAR-10, STL10 and ImageNette, SAMCLR performs at least on par with, and most often significantly outperforms not only SimCLR, but also DINO and MoCo.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_14736v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在计算机视觉中，自监督对比学习在同一图像的不同视图之间强制执行相似的表示。预训练通常在图像分类数据集上执行，如ImageNet，其中图像主要包含一类对象。然而，当处理具有多个项目的复杂场景时，同一图像的多个视图不太可能表示同一对象类别。在这种设置中，我们提出了SAMCLR，这是SimCLR的一个附加组件，它使用SAM将图像分割成语义区域，然后对同一区域的两个视图进行采样。初步结果实证表明，当在Cityscapes和ADE20K上进行预训练，然后在CIFAR-10、STL10和ImageNette上评估分类时，SAMCLR的表现至少与SimCLR相当，而且通常显著优于SimCLR，还优于DINO和MoCo。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.14736v1" target="_blank">2310.14736v1</a>
                              </td>
                              <td>SAMCLR: Contrastive pre-training on complex scenes using SAM for view sampling</td>
                              <td>Benjamin Missaoui</td>
                              <td>2023-10-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_14736v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.14736v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_08854v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rank-DETR for High Quality Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_08854v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_08854v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_08854v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Modern detection transformers (DETRs) use a set of object queries to predict a list of bounding boxes, sort them by their classification confidence scores, and select the top-ranked predictions as the final detection results for the given input image. A highly performant object detector requires accurate ranking for the bounding box predictions. For DETR-based detectors, the top-ranked bounding boxes suffer from less accurate localization quality due to the misalignment between classification scores and localization accuracy, thus impeding the construction of high-quality detectors. In this work, we introduce a simple and highly performant DETR-based object detector by proposing a series of rank-oriented designs, combinedly called Rank-DETR. Our key contributions include: (i) a rank-oriented architecture design that can prompt positive predictions and suppress the negative ones to ensure lower false positive rates, as well as (ii) a rank-oriented loss function and matching cost design that prioritizes predictions of more accurate localization accuracy during ranking to boost the AP under high IoU thresholds. We apply our method to improve the recent SOTA methods (e.g., H-DETR and DINO-DETR) and report strong COCO object detection results when using different backbones such as ResNet-$50$, Swin-T, and Swin-L, demonstrating the effectiveness of our approach. Code is available at \url{https://github.com/LeapLabTHU/Rank-DETR}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_08854v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现代检测变换器（DETR）使用一组对象查询来预测边界框的列表，根据它们的分类置信度得分对它们进行排序，并选择排名靠前的预测作为给定输入图像的最终检测结果。高性能的对象检测器需要边界框预测的精确排序。对于基于DETR的检测器，由于分类分数和定位精度之间的不对准，排名靠前的边界框的定位质量较差，从而阻碍了高质量检测器的构建。在这项工作中，我们通过提出一系列面向秩的设计，结合称为秩DETR，介绍了一种简单且高性能的基于DETR的对象检测器。我们的主要贡献包括：（i）一种面向秩的架构设计，它可以提示阳性预测并抑制阴性预测，以确保更低的假阳性率；以及（ii）一种基于秩的损失函数和匹配成本设计，它在排序过程中优先考虑更准确定位精度的预测，以在高IoU阈值下提高AP。我们将我们的方法应用于改进最近的SOTA方法（例如，H-DETR和DINO-DETR），并在使用不同骨干（如ResNet-$50$、Swin-T和Swin-L）时报告了强大的COCO对象检测结果，证明了我们方法的有效性。代码位于\url{https://github.com/LeapLabTHU/Rank-DETR}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.08854v2" target="_blank">2310.08854v2</a>
                              </td>
                              <td>Rank-DETR for High Quality Object Detection</td>
                              <td>Yifan Pu</td>
                              <td>2023-10-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_08854v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.08854v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_08825v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_08825v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_08825v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_08825v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multi-modal Large Language Models (MLLMs) have made significant strides in expanding the capabilities of Large Language Models (LLMs) through the incorporation of visual perception interfaces. Despite the emergence of exciting applications and the availability of diverse instruction tuning data, existing approaches often rely on CLIP or its variants as the visual branch, and merely extract features from the deep layers. However, these methods lack a comprehensive analysis of the visual encoders in MLLMs. In this paper, we conduct an extensive investigation into the effectiveness of different vision encoders within MLLMs. Our findings reveal that the shallow layer features of CLIP offer particular advantages for fine-grained tasks such as grounding and region understanding. Surprisingly, the vision-only model DINO, which is not pretrained with text-image alignment, demonstrates promising performance as a visual branch within MLLMs. By simply equipping it with an MLP layer for alignment, DINO surpasses CLIP in fine-grained related perception tasks. Building upon these observations, we propose a simple yet effective feature merging strategy, named COMM, that integrates CLIP and DINO with Multi-level features Merging, to enhance the visual capabilities of MLLMs. We evaluate COMM through comprehensive experiments on a wide range of benchmarks, including image captioning, visual question answering, visual grounding, and object hallucination. Experimental results demonstrate the superior performance of COMM compared to existing methods, showcasing its enhanced visual capabilities within MLLMs. Code will be made available at https://github.com/YuchenLiu98/COMM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_08825v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多模态大语言模型（MLLMs）通过引入视觉感知接口，在扩展大语言模型的能力方面取得了重大进展。尽管出现了令人兴奋的应用程序和各种指令调整数据，但现有的方法往往依赖于CLIP或其变体作为视觉分支，而只是从深层提取特征。然而，这些方法缺乏对MLLM中的视觉编码器的全面分析。在本文中，我们对MLLMs中不同视觉编码器的有效性进行了广泛的研究。我们的研究结果表明，CLIP的浅层特征为细粒度任务（如接地和区域理解）提供了特殊的优势。令人惊讶的是，未经文本图像对齐预训练的仅视觉模型DINO，作为MLLMs中的视觉分支，表现出了良好的性能。通过简单地为其配备MLP层进行对齐，DINO在细粒度相关感知任务中超越了CLIP。基于这些观察结果，我们提出了一种简单而有效的特征合并策略，称为COMM，该策略将CLIP和DINO与多级特征合并相结合，以增强MLLM的视觉能力。我们通过在各种基准上进行综合实验来评估COMM，包括图像字幕、视觉问答、视觉基础和对象幻觉。实验结果表明，与现有方法相比，COMM的性能优越，显示了其在MLLM中增强的视觉能力。代码将在https://github.com/YuchenLiu98/COMM.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.08825v2" target="_blank">2310.08825v2</a>
                              </td>
                              <td>From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models</td>
                              <td>Dongsheng Jiang</td>
                              <td>2023-10-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_08825v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.08825v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_10912v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Training-free Open-world Segmentation via Image Prompting Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_10912v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_10912v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_10912v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The realm of computer vision has witnessed a paradigm shift with the advent of foundational models, mirroring the transformative influence of large language models in the domain of natural language processing. This paper delves into the exploration of open-world segmentation, presenting a novel approach called Image Prompt Segmentation (IPSeg) that harnesses the power of vision foundational models. At the heart of IPSeg lies the principle of a training-free paradigm, which capitalizes on image prompting techniques. IPSeg utilizes a single image containing a subjective visual concept as a flexible prompt to query vision foundation models like DINOv2 and Stable Diffusion. Our approach extracts robust features for the prompt image and input image, then matches the input representations to the prompt representations via a novel feature interaction module to generate point prompts highlighting target objects in the input image. The generated point prompts are further utilized to guide the Segment Anything Model to segment the target object in the input image. The proposed method stands out by eliminating the need for exhaustive training sessions, thereby offering a more efficient and scalable solution. Experiments on COCO, PASCAL VOC, and other datasets demonstrate IPSeg's efficacy for flexible open-world segmentation using intuitive image prompts. This work pioneers tapping foundation models for open-world understanding through visual concepts conveyed in images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_10912v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着基础模型的出现，计算机视觉领域发生了范式转变，反映了大型语言模型在自然语言处理领域的变革性影响。本文深入探讨了开放世界分割的探索，提出了一种称为图像提示分割（IPSeg）的新方法，该方法利用了视觉基础模型的力量。IPSeg的核心是无训练范式的原则，它利用了图像提示技术。IPSeg利用包含主观视觉概念的单个图像作为灵活的提示来查询视觉基础模型，如DINOv2和Stable Diffusion。我们的方法为提示图像和输入图像提取鲁棒特征，然后通过一个新颖的特征交互模块将输入表示与提示表示匹配，以生成突出显示输入图像中目标对象的点提示。生成的点提示被进一步用于引导Segment Anything Model对输入图像中的目标对象进行分割。所提出的方法通过消除对详尽培训课程的需求而脱颖而出，从而提供了一个更高效和可扩展的解决方案。在COCO、PASCAL VOC和其他数据集上的实验证明了IPSeg使用直观的图像提示进行灵活的开放世界分割的有效性。这项工作开创了通过图像中传达的视觉概念来挖掘开放世界理解的基础模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.10912v1" target="_blank">2310.10912v1</a>
                              </td>
                              <td>Towards Training-free Open-world Segmentation via Image Prompting Foundation Models</td>
                              <td>Lv Tang</td>
                              <td>2023-10-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_10912v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.10912v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_08873v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Interactive Navigation in Environments with Traversable Obstacles Using Large Language and Vision-Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_08873v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_08873v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_08873v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper proposes an interactive navigation framework by using large language and vision-language models, allowing robots to navigate in environments with traversable obstacles. We utilize the large language model (GPT-3.5) and the open-set Vision-language Model (Grounding DINO) to create an action-aware costmap to perform effective path planning without fine-tuning. With the large models, we can achieve an end-to-end system from textual instructions like "Can you pass through the curtains to deliver medicines to me?", to bounding boxes (e.g., curtains) with action-aware attributes. They can be used to segment LiDAR point clouds into two parts: traversable and untraversable parts, and then an action-aware costmap is constructed for generating a feasible path. The pre-trained large models have great generalization ability and do not require additional annotated data for training, allowing fast deployment in the interactive navigation tasks. We choose to use multiple traversable objects such as curtains and grasses for verification by instructing the robot to traverse them. Besides, traversing curtains in a medical scenario was tested. All experimental results demonstrated the proposed framework's effectiveness and adaptability to diverse environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_08873v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文利用大型语言和视觉语言模型提出了一种交互式导航框架，使机器人能够在有可穿越障碍物的环境中导航。我们利用大型语言模型（GPT-3.5）和开放集视觉语言模型（Grounding DINO）来创建一个动作感知成本图，以在不进行微调的情况下执行有效的路径规划。使用大型模型，我们可以实现一个端到端的系统，从“你能穿过窗帘给我送药吗？”这样的文本说明，到具有动作感知属性的边界框（例如窗帘）。它们可以用于将激光雷达点云划分为两部分：可遍历部分和不可遍历部分，然后构建一个行动感知成本图，以生成可行的路径。预训练的大型模型具有很强的泛化能力，不需要额外的注释数据进行训练，允许在交互式导航任务中快速部署。我们选择使用多个可遍历对象，如窗帘和草，通过指示机器人遍历它们来进行验证。此外，还测试了在医疗场景中穿过窗帘的情况。所有实验结果都证明了所提出的框架的有效性和对不同环境的适应性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.08873v1" target="_blank">2310.08873v1</a>
                              </td>
                              <td>Interactive Navigation in Environments with Traversable Obstacles Using Large Language and Vision-Language Models</td>
                              <td>Zhen Zhang</td>
                              <td>2023-10-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_08873v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.08873v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_07033v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Computational Pathology at Health System Scale -- Self-Supervised Foundation Models from Three Billion Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_07033v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_07033v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_07033v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent breakthroughs in self-supervised learning have enabled the use of large unlabeled datasets to train visual foundation models that can generalize to a variety of downstream tasks. While this training paradigm is well suited for the medical domain where annotations are scarce, large-scale pre-training in the medical domain, and in particular pathology, has not been extensively studied. Previous work in self-supervised learning in pathology has leveraged smaller datasets for both pre-training and evaluating downstream performance. The aim of this project is to train the largest academic foundation model and benchmark the most prominent self-supervised learning algorithms by pre-training and evaluating downstream performance on large clinical pathology datasets. We collected the largest pathology dataset to date, consisting of over 3 billion images from over 423 thousand microscopy slides. We compared pre-training of visual transformer models using the masked autoencoder (MAE) and DINO algorithms. We evaluated performance on six clinically relevant tasks from three anatomic sites and two institutions: breast cancer detection, inflammatory bowel disease detection, breast cancer estrogen receptor prediction, lung adenocarcinoma EGFR mutation prediction, and lung cancer immunotherapy response prediction. Our results demonstrate that pre-training on pathology data is beneficial for downstream performance compared to pre-training on natural images. Additionally, the DINO algorithm achieved better generalization performance across all tasks tested. The presented results signify a phase change in computational pathology research, paving the way into a new era of more performant models based on large-scale, parallel pre-training at the billion-image scale.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_07033v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近在自我监督学习方面取得的突破使得能够使用大型未标记数据集来训练视觉基础模型，该模型可以推广到各种下游任务。虽然这种训练范式非常适合注释稀少的医学领域，但医学领域，特别是病理学领域的大规模预训练尚未得到广泛研究。先前在病理学自我监督学习方面的工作利用较小的数据集进行预训练和评估下游性能。该项目的目的是通过在大型临床病理学数据集上进行预训练和评估下游性能，训练最大的学术基础模型，并对最突出的自监督学习算法进行基准测试。我们收集了迄今为止最大的病理学数据集，包括来自42.3万张显微镜载玻片的30多亿张图像。我们比较了使用掩蔽自动编码器（MAE）和DINO算法的视觉变换器模型的预训练。我们评估了来自三个解剖部位和两个机构的六项临床相关任务的表现：乳腺癌症检测、炎症性肠病检测、乳腺癌症雌激素受体预测、肺腺癌EGFR突变预测和癌症免疫疗法反应预测。我们的结果表明，与对自然图像的预训练相比，对病理学数据的预训练有利于下游性能。此外，DINO算法在所有测试任务中都实现了更好的泛化性能。所提出的结果标志着计算病理学研究的一个阶段性变化，为进入一个基于十亿图像规模的大规模并行预训练的更高性能模型的新时代铺平了道路。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.07033v1" target="_blank">2310.07033v1</a>
                              </td>
                              <td>Computational Pathology at Health System Scale -- Self-Supervised Foundation Models from Three Billion Images</td>
                              <td>Gabriele Campanella</td>
                              <td>2023-10-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_07033v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.07033v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_06836v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">What Does Stable Diffusion Know about the 3D Scene?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_06836v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_06836v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_06836v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in generative models like Stable Diffusion enable the generation of highly photo-realistic images. Our objective in this paper is to probe the diffusion network to determine to what extent it 'understands' different properties of the 3D scene depicted in an image. To this end, we make the following contributions: (i) We introduce a protocol to evaluate whether a network models a number of physical 'properties' of the 3D scene by probing for explicit features that represent these properties. The probes are applied on datasets of real images with annotations for the property. (ii) We apply this protocol to properties covering scene geometry, scene material, support relations, lighting, and view dependent measures. (iii) We find that Stable Diffusion is good at a number of properties including scene geometry, support relations, shadows and depth, but less performant for occlusion. (iv) We also apply the probes to other models trained at large-scale, including DINO and CLIP, and find their performance inferior to that of Stable Diffusion.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_06836v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>生成模型（如稳定扩散）的最新进展使得能够生成高度照片逼真的图像。我们在本文中的目标是探索扩散网络，以确定它在多大程度上“理解”图像中描绘的3D场景的不同特性。为此，我们做出了以下贡献：（i）我们引入了一种协议，通过探测表示3D场景的显式特征来评估网络是否对这些物理“属性”进行建模。探针应用于带有属性注释的真实图像数据集。（ii）我们将此协议应用于涵盖场景几何、场景材质、支持关系、照明和视图相关度量的属性。（iii）我们发现Stable Diffusion在许多属性上都很好，包括场景几何、支持关系、阴影和深度，但在遮挡方面性能较差。（iv）我们还将探针应用于大规模训练的其他模型，包括DINO和CLIP，发现它们的性能不如稳定扩散。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.06836v1" target="_blank">2310.06836v1</a>
                              </td>
                              <td>What Does Stable Diffusion Know about the 3D Scene?</td>
                              <td>Guanqi Zhan</td>
                              <td>2023-10-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_06836v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.06836v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_16118v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">D$^3$Fields: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable Robotic Manipulation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_16118v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_16118v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_16118v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Scene representation has been a crucial design choice in robotic manipulation systems. An ideal representation should be 3D, dynamic, and semantic to meet the demands of diverse manipulation tasks. However, previous works often lack all three properties simultaneously. In this work, we introduce D$^3$Fields - dynamic 3D descriptor fields. These fields capture the dynamics of the underlying 3D environment and encode both semantic features and instance masks. Specifically, we project arbitrary 3D points in the workspace onto multi-view 2D visual observations and interpolate features derived from foundational models. The resulting fused descriptor fields allow for flexible goal specifications using 2D images with varied contexts, styles, and instances. To evaluate the effectiveness of these descriptor fields, we apply our representation to a wide range of robotic manipulation tasks in a zero-shot manner. Through extensive evaluation in both real-world scenarios and simulations, we demonstrate that D$^3$Fields are both generalizable and effective for zero-shot robotic manipulation tasks. In quantitative comparisons with state-of-the-art dense descriptors, such as Dense Object Nets and DINO, D$^3$Fields exhibit significantly better generalization abilities and manipulation accuracy.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_16118v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>场景表示一直是机器人操作系统中至关重要的设计选择。理想的表示应该是3D、动态和语义的，以满足不同操作任务的需求。然而，以前的作品往往同时缺乏这三种特性。在这项工作中，我们介绍了D$^3$Fields-动态三维描述符字段。这些字段捕获底层3D环境的动态，并对语义特征和实例掩码进行编码。具体来说，我们将工作空间中的任意3D点投影到多视图2D视觉观察上，并对从基础模型中导出的特征进行插值。由此产生的融合描述符字段允许使用具有不同上下文、样式和实例的2D图像来实现灵活的目标规范。为了评估这些描述域的有效性，我们以零样本的方式将我们的表示应用于广泛的机器人操作任务。通过在现实世界场景和模拟中的广泛评估，我们证明D$^3$Fields对于零样本机器人操纵任务是可推广和有效的。在与最先进的密集描述符（如密集对象网和DINO）的定量比较中，D$^3$Fields表现出明显更好的泛化能力和操作精度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.16118v2" target="_blank">2309.16118v2</a>
                              </td>
                              <td>D$^3$Fields: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable Robotic Manipulation</td>
                              <td>Yixuan Wang</td>
                              <td>2023-09-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_16118v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.16118v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_03940v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Hard View Selection for Contrastive Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_03940v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_03940v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_03940v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Many Contrastive Learning (CL) methods train their models to be invariant to different "views" of an image input for which a good data augmentation pipeline is crucial. While considerable efforts were directed towards improving pre-text tasks, architectures, or robustness (e.g., Siamese networks or teacher-softmax centering), the majority of these methods remain strongly reliant on the random sampling of operations within the image augmentation pipeline, such as the random resized crop or color distortion operation. In this paper, we argue that the role of the view generation and its effect on performance has so far received insufficient attention. To address this, we propose an easy, learning-free, yet powerful Hard View Selection (HVS) strategy designed to extend the random view generation to expose the pretrained model to harder samples during CL training. It encompasses the following iterative steps: 1) randomly sample multiple views and create pairs of two views, 2) run forward passes for each view pair on the currently trained model, 3) adversarially select the pair yielding the worst loss, and 4) run the backward pass with the selected pair. In our empirical analysis we show that under the hood, HVS increases task difficulty by controlling the Intersection over Union of views during pretraining. With only 300-epoch pretraining, HVS is able to closely rival the 800-epoch DINO baseline which remains very favorable even when factoring in the slowdown induced by the additional forwards of HVS. Additionally, HVS consistently achieves accuracy improvements on ImageNet between 0.55% and 1.9% on linear evaluation and similar improvements on transfer tasks across multiple CL methods, such as DINO, SimSiam, and SimCLR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_03940v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>许多对比学习（CL）方法将其模型训练为对图像输入的不同“视图”保持不变，而良好的数据增强管道对图像输入至关重要。尽管在改进文本前任务、架构或鲁棒性（例如，连体网络或教师softmax居中）方面做出了相当大的努力，但这些方法中的大多数仍然强烈依赖于图像增强管道内操作的随机采样，例如随机调整大小的裁剪或颜色失真操作。在本文中，我们认为，到目前为止，视图生成的作用及其对性能的影响还没有得到足够的关注。为了解决这一问题，我们提出了一种简单、无需学习但功能强大的硬视图选择（HVS）策略，该策略旨在扩展随机视图生成，以在CL训练期间将预训练的模型暴露给更硬的样本。它包括以下迭代步骤：1）随机采样多个视图并创建两个视图对，2）在当前训练的模型上为每个视图对运行前向传递，3）对抗性地选择产生最差损失的对，以及4）使用所选对运行后向传递。在我们的实证分析中，我们发现在引擎盖下，HVS通过在预训练过程中控制视图并集上的交集来增加任务难度。HVS只需要300个历元的预训练，就可以与800个历元DINO基线相媲美，即使考虑到HVS额外前锋导致的速度放缓，这一基线仍然非常有利。此外，HVS在ImageNet上的线性评估精度持续提高0.55%至1.9%，在多种CL方法（如DINO、SimSiam和SimCLR）的传输任务上也实现了类似的改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.03940v1" target="_blank">2310.03940v1</a>
                              </td>
                              <td>Hard View Selection for Contrastive Learning</td>
                              <td>Fabio Ferreira</td>
                              <td>2023-10-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_03940v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.03940v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_03513v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exploring DINO: Emergent Properties and Limitations for Synthetic Aperture Radar Imagery</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_03513v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_03513v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_03513v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised learning (SSL) models have recently demonstrated remarkable performance across various tasks, including image segmentation. This study delves into the emergent characteristics of the Self-Distillation with No Labels (DINO) algorithm and its application to Synthetic Aperture Radar (SAR) imagery. We pre-train a vision transformer (ViT)-based DINO model using unlabeled SAR data, and later fine-tune the model to predict high-resolution land cover maps. We rigorously evaluate the utility of attention maps generated by the ViT backbone, and compare them with the model's token embedding space. We observe a small improvement in model performance with pre-training compared to training from scratch, and discuss the limitations and opportunities of SSL for remote sensing and land cover segmentation. Beyond small performance increases, we show that ViT attention maps hold great intrinsic value for remote sensing, and could provide useful inputs to other algorithms. With this, our work lays the ground-work for bigger and better SSL models for Earth Observation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_03513v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督学习（SSL）模型最近在包括图像分割在内的各种任务中表现出了显著的性能。本研究探讨了无标签自蒸馏（DINO）算法的涌现特性及其在合成孔径雷达（SAR）图像中的应用。我们使用未标记的SAR数据预训练基于视觉变换器（ViT）的DINO模型，然后对该模型进行微调，以预测高分辨率的土地覆盖图。我们严格评估了ViT主干生成的注意力图的效用，并将其与模型的令牌嵌入空间进行了比较。我们观察到，与从头开始的训练相比，预训练在模型性能上略有改进，并讨论了SSL在遥感和土地覆盖分割方面的局限性和机会。除了性能的小幅提高外，我们还表明，ViT注意力图对遥感具有巨大的内在价值，可以为其他算法提供有用的输入。有了这一点，我们的工作为更大更好的地球观测SSL模型奠定了基础。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.03513v1" target="_blank">2310.03513v1</a>
                              </td>
                              <td>Exploring DINO: Emergent Properties and Limitations for Synthetic Aperture Radar Imagery</td>
                              <td>Joseph A. Gallego-Mejia</td>
                              <td>2023-10-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_03513v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.03513v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_02116v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Hierarchical Concept Discovery Models: A Concept Pyramid Scheme</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_02116v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_02116v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_02116v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Deep Learning algorithms have recently gained significant attention due to their impressive performance. However, their high complexity and un-interpretable mode of operation hinders their confident deployment in real-world safety-critical tasks. This work targets ante hoc interpretability, and specifically Concept Bottleneck Models (CBMs). Our goal is to design a framework that admits a highly interpretable decision making process with respect to human understandable concepts, on multiple levels of granularity. To this end, we propose a novel hierarchical concept discovery formulation leveraging: (i) recent advances in image-text models, and (ii) an innovative formulation for multi-level concept selection via data-driven and sparsity inducing Bayesian arguments. Within this framework, concept information does not solely rely on the similarity between the whole image and general unstructured concepts; instead, we introduce the notion of concept hierarchy to uncover and exploit more granular concept information residing in patch-specific regions of the image scene. As we experimentally show, the proposed construction not only outperforms recent CBM approaches, but also yields a principled framework towards interpetability.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_02116v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>深度学习算法最近因其令人印象深刻的性能而受到极大关注。然而，它们的高复杂性和不可解释的操作模式阻碍了它们在现实世界安全关键任务中的自信部署。这项工作的目标是事前可解释性，特别是概念瓶颈模型（CBM）。我们的目标是设计一个框架，允许在多个粒度级别上，就人类可理解的概念而言，进行高度可解释的决策过程。为此，我们提出了一种新的分层概念发现公式，该公式利用：（i）图像-文本模型的最新进展，以及（ii）通过数据驱动和稀疏性诱导的贝叶斯论点进行多级概念选择的创新公式。在这个框架内，概念信息不仅仅依赖于整个图像和一般非结构化概念之间的相似性；相反，我们引入了概念层次的概念，以揭示和利用驻留在图像场景的补丁特定区域中的更细粒度的概念信息。正如我们实验表明的那样，所提出的构造不仅优于最近的CBM方法，而且产生了一个关于互操作性的原则框架。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.02116v1" target="_blank">2310.02116v1</a>
                              </td>
                              <td>Hierarchical Concept Discovery Models: A Concept Pyramid Scheme</td>
                              <td>Konstantinos P. Panousis</td>
                              <td>2023-10-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_02116v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.02116v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_02048v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exploring Generalisability of Self-Distillation with No Labels for SAR-Based Vegetation Prediction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_02048v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_02048v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_02048v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this work we pre-train a DINO-ViT based model using two Synthetic Aperture Radar datasets (S1GRD or GSSIC) across three regions (China, Conus, Europe). We fine-tune the models on smaller labeled datasets to predict vegetation percentage, and empirically study the connection between the embedding space of the models and their ability to generalize across diverse geographic regions and to unseen data. For S1GRD, embedding spaces of different regions are clearly separated, while GSSIC's overlaps. Positional patterns remain during fine-tuning, and greater distances in embeddings often result in higher errors for unfamiliar regions. With this, our work increases our understanding of generalizability for self-supervised models applied to remote sensing.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_02048v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项工作中，我们使用三个地区（中国、美国和欧洲）的两个合成孔径雷达数据集（S1GRD或GSSIC）预训练了一个基于DINO ViT的模型。我们在较小的标记数据集上微调模型，以预测植被百分比，并实证研究模型的嵌入空间与其在不同地理区域和未知数据中推广的能力之间的联系。对于S1GRD，不同区域的嵌入空间明显分离，而GSSIC的嵌入空间重叠。在微调过程中，位置模式仍然存在，嵌入中距离越大，通常会导致不熟悉区域的误差越大。通过这一点，我们的工作增加了我们对应用于遥感的自监督模型的可推广性的理解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.02048v1" target="_blank">2310.02048v1</a>
                              </td>
                              <td>Exploring Generalisability of Self-Distillation with No Labels for SAR-Based Vegetation Prediction</td>
                              <td>Laura Martínez-Ferrer</td>
                              <td>2023-10-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_02048v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.02048v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2210_02808v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Effective Self-supervised Pre-training on Low-compute Networks without Distillation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2210_02808v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2210_02808v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2210_02808v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite the impressive progress of self-supervised learning (SSL), its applicability to low-compute networks has received limited attention. Reported performance has trailed behind standard supervised pre-training by a large margin, barring self-supervised learning from making an impact on models that are deployed on device. Most prior works attribute this poor performance to the capacity bottleneck of the low-compute networks and opt to bypass the problem through the use of knowledge distillation (KD). In this work, we revisit SSL for efficient neural networks, taking a closer at what are the detrimental factors causing the practical limitations, and whether they are intrinsic to the self-supervised low-compute setting. We find that, contrary to accepted knowledge, there is no intrinsic architectural bottleneck, we diagnose that the performance bottleneck is related to the model complexity vs regularization strength trade-off. In particular, we start by empirically observing that the use of local views can have a dramatic impact on the effectiveness of the SSL methods. This hints at view sampling being one of the performance bottlenecks for SSL on low-capacity networks. We hypothesize that the view sampling strategy for large neural networks, which requires matching views in very diverse spatial scales and contexts, is too demanding for low-capacity architectures. We systematize the design of the view sampling mechanism, leading to a new training methodology that consistently improves the performance across different SSL methods (e.g. MoCo-v2, SwAV, DINO), different low-size networks (e.g. MobileNetV2, ResNet18, ResNet34, ViT-Ti), and different tasks (linear probe, object detection, instance segmentation and semi-supervised learning). Our best models establish a new state-of-the-art for SSL methods on low-compute networks despite not using a KD loss term.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2210_02808v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管自监督学习（SSL）取得了令人印象深刻的进展，但其在低计算网络中的适用性却受到了有限的关注。报告的性能在很大程度上落后于标准监督的预训练，禁止自我监督学习对部署在设备上的模型产生影响。大多数先前的工作将这种较差的性能归因于低计算网络的容量瓶颈，并选择通过使用知识蒸馏（KD）来绕过该问题。在这项工作中，我们重新审视了高效神经网络的SSL，深入了解了造成实际限制的有害因素是什么，以及它们是否是自监督低计算设置所固有的。我们发现，与公认的知识相反，不存在固有的体系结构瓶颈，我们诊断出性能瓶颈与模型复杂性与正则化强度的权衡有关。特别是，我们从经验上观察到，局部视图的使用会对SSL方法的有效性产生巨大影响。这暗示视图采样是低容量网络上SSL的性能瓶颈之一。我们假设，大型神经网络的视图采样策略需要在非常不同的空间尺度和上下文中匹配视图，对于低容量架构来说要求太高。我们将视图采样机制的设计系统化，从而产生了一种新的训练方法，该方法在不同的SSL方法（例如MoCo-v2、SwAV、DINO）、不同的低规模网络（例如MobileNetV2、ResNet18、ResNet34、ViT-Ti）和不同的任务（线性探测、对象检测、实例分割和半监督学习）中持续提高性能。我们的最佳模型为低计算网络上的SSL方法建立了新的最先进的技术，尽管没有使用KD损失项。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2210.02808v2" target="_blank">2210.02808v2</a>
                              </td>
                              <td>Effective Self-supervised Pre-training on Low-compute Networks without Distillation</td>
                              <td>Fuwen Tan</td>
                              <td>2022-10-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2210_02808v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2210.02808v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_01092v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Leveraging Cutting Edge Deep Learning Based Image Matching for Reconstructing a Large Scene from Sparse Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_01092v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_01092v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_01092v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present the top ranked solution for the AISG-SLA Visual Localisation Challenge benchmark (IJCAI 2023), where the task is to estimate relative motion between images taken in sequence by a camera mounted on a car driving through an urban scene.   For matching images we use our recent deep learning based matcher RoMa. Matching image pairs sequentially and estimating relative motion from point correspondences sampled by RoMa already gives very competitive results -- third rank on the challenge benchmark.   To improve the estimations we extract keypoints in the images, match them using RoMa, and perform structure from motion reconstruction using COLMAP. We choose our recent DeDoDe keypoints for their high repeatability. Further, we address time jumps in the image sequence by matching specific non-consecutive image pairs based on image retrieval with DINOv2. These improvements yield a solution beating all competitors.   We further present a loose upper bound on the accuracy obtainable by the image retrieval approach by also matching hand-picked non-consecutive pairs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_01092v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们为AISG-SLA视觉定位挑战基准（IJCAI 2023）提供了排名最高的解决方案，其中的任务是估计安装在汽车上的摄像头在城市场景中依次拍摄的图像之间的相对运动。对于匹配图像，我们使用最近的基于深度学习的匹配器RoMa。按顺序匹配图像对，并根据RoMa采样的点对应关系估计相对运动，已经给出了非常有竞争力的结果——在挑战基准上排名第三。为了改进估计，我们提取图像中的关键点，使用RoMa进行匹配，并使用COLMAP从运动重建中执行结构。我们选择最近的DeDoDe关键点是因为它们具有较高的可重复性。此外，我们通过将基于图像检索的特定非连续图像对与DINOv2进行匹配来解决图像序列中的时间跳跃问题。这些改进产生了一个击败所有竞争对手的解决方案。我们进一步提出了图像检索方法通过匹配手工拾取的非连续对所获得的精度的宽松上限。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.01092v1" target="_blank">2310.01092v1</a>
                              </td>
                              <td>Leveraging Cutting Edge Deep Learning Based Image Matching for Reconstructing a Large Scene from Sparse Images</td>
                              <td>Georg Bökman</td>
                              <td>2023-10-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_01092v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.01092v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_14093v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Weakly Supervised 3D Open-vocabulary Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_14093v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_14093v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_14093v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting pre-trained foundation models CLIP and DINO in a weakly supervised manner. Specifically, given only the open-vocabulary text descriptions of the objects in a scene, we distill the open-vocabulary multimodal knowledge and object reasoning capability of CLIP and DINO into a neural radiance field (NeRF), which effectively lifts 2D features into view-consistent 3D segmentation. A notable aspect of our approach is that it does not require any manual segmentation annotations for either the foundation models or the distillation process. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations in certain scenes, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs. Code is available at \url{https://github.com/Kunhao-Liu/3D-OVS}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_14093v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三维场景的开放式词汇分割是人类感知的基本功能，因此也是计算机视觉研究的重要目标。然而，由于缺乏大规模和多样化的3D开放词汇分割数据集来训练健壮和可推广的模型，这项任务受到了严重阻碍。从预先训练的2D开放词汇分割模型中提取知识是有帮助的，但它损害了开放词汇的特征，因为2D模型大多是用紧密的词汇数据集进行微调的。我们通过以弱监督的方式利用预先训练的基础模型CLIP和DINO来解决3D开放词汇分割中的挑战。具体而言，仅给定场景中对象的开放词汇文本描述，我们将CLIP和DINO的开放词汇多模态知识和对象推理能力提取到神经辐射场（NeRF）中，这有效地将2D特征提升到视图一致的3D分割中。我们的方法的一个值得注意的方面是，它不需要对基础模型或蒸馏过程进行任何手动分割注释。大量实验表明，在某些场景中，我们的方法甚至优于用分割注释训练的完全监督模型，这表明可以从2D图像和文本图像对中有效地学习3D开放词汇分割。代码位于\url{https://github.com/Kunhao-Liu/3D-OVS}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.14093v3" target="_blank">2305.14093v3</a>
                              </td>
                              <td>Weakly Supervised 3D Open-vocabulary Segmentation</td>
                              <td>Kunhao Liu</td>
                              <td>2023-05-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_14093v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.14093v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_14265v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Industrial Application of 6D Pose Estimation for Robotic Manipulation in Automotive Internal Logistics</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_14265v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_14265v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_14265v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite the advances in robotics a large proportion of the of parts handling tasks in the automotive industry's internal logistics are not automated but still performed by humans. A key component to competitively automate these processes is a 6D pose estimation that can handle a large number of different parts, is adaptable to new parts with little manual effort, and is sufficiently accurate and robust with respect to industry requirements. In this context, the question arises as to the current status quo with respect to these measures. To address this we built a representative 6D pose estimation pipeline with state-of-the-art components from economically scalable real to synthetic data generation to pose estimators and evaluated it on automotive parts with regards to a realistic sequencing process. We found that using the data generation approaches, the performance of the trained 6D pose estimators are promising, but do not meet industry requirements. We reveal that the reason for this is the inability of the estimators to provide reliable uncertainties for their poses, rather than the ability of to provide sufficiently accurate poses. In this context we further analyzed how RGB- and RGB-D-based approaches compare against this background and show that they are differently vulnerable to the domain gap induced by synthetic data.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_14265v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管机器人技术取得了进步，但汽车行业内部物流中很大一部分零件处理任务并不是自动化的，而是由人类完成的。有竞争力地自动化这些过程的一个关键组件是6D姿态估计，它可以处理大量不同的零件，只需很少的手动操作就可以适应新零件，并且相对于行业要求来说足够准确和稳健。在这方面，出现了关于这些措施目前现状的问题。为了解决这一问题，我们建立了一个具有代表性的6D姿态估计管道，其中包括从经济可扩展的真实到合成数据生成到姿态估计器的最先进组件，并根据现实的测序过程在汽车零件上进行了评估。我们发现，使用数据生成方法，经过训练的6D姿态估计器的性能很有希望，但不符合行业要求。我们发现，造成这种情况的原因是估计器无法为其姿态提供可靠的不确定性，而不是提供足够准确姿态的能力。在这种情况下，我们进一步分析了基于RGB和RGB-D的方法在这种背景下的比较，并表明它们不同地容易受到合成数据引起的域间隙的影响。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.14265v1" target="_blank">2309.14265v1</a>
                              </td>
                              <td>Industrial Application of 6D Pose Estimation for Robotic Manipulation in Automotive Internal Logistics</td>
                              <td>Philipp Quentin</td>
                              <td>2023-09-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_14265v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.14265v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_13578v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A SAM-based Solution for Hierarchical Panoptic Segmentation of Crops and Weeds Competition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_13578v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_13578v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_13578v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Panoptic segmentation in agriculture is an advanced computer vision technique that provides a comprehensive understanding of field composition. It facilitates various tasks such as crop and weed segmentation, plant panoptic segmentation, and leaf instance segmentation, all aimed at addressing challenges in agriculture. Exploring the application of panoptic segmentation in agriculture, the 8th Workshop on Computer Vision in Plant Phenotyping and Agriculture (CVPPA) hosted the challenge of hierarchical panoptic segmentation of crops and weeds using the PhenoBench dataset. To tackle the tasks presented in this competition, we propose an approach that combines the effectiveness of the Segment AnyThing Model (SAM) for instance segmentation with prompt input from object detection models. Specifically, we integrated two notable approaches in object detection, namely DINO and YOLO-v8. Our best-performing model achieved a PQ+ score of 81.33 based on the evaluation metrics of the competition.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_13578v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>农业中的全景分割是一种先进的计算机视觉技术，可以全面了解田地的组成。它促进了各种任务，如作物和杂草分割、植物全景分割和叶片实例分割，所有这些都旨在解决农业中的挑战。探索全景分割在农业中的应用，第八届植物表型与农业计算机视觉研讨会（CVPPA）主办了使用PhenoBench数据集对作物和杂草进行分层全景分割的挑战。为了解决此次竞争中提出的任务，我们提出了一种方法，该方法将分段AnyThing模型（SAM）的有效性（例如，分段）与来自对象检测模型的即时输入相结合。具体来说，我们在物体检测中集成了两种著名的方法，即DINO和YOLO-v8。根据比赛的评估指标，我们表现最好的模型获得了81.33的PQ+分数。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.13578v1" target="_blank">2309.13578v1</a>
                              </td>
                              <td>A SAM-based Solution for Hierarchical Panoptic Segmentation of Crops and Weeds Competition</td>
                              <td>Khoa Dang Nguyen</td>
                              <td>2023-09-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_13578v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.13578v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_12969v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Detect Every Thing with Few Examples</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_12969v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_12969v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_12969v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Open-set object detection aims at detecting arbitrary categories beyond those seen during training. Most recent advancements have adopted the open-vocabulary paradigm, utilizing vision-language backbones to represent categories with language. In this paper, we introduce DE-ViT, an open-set object detector that employs vision-only DINOv2 backbones and learns new categories through example images instead of language. To improve general detection ability, we transform multi-classification tasks into binary classification tasks while bypassing per-class inference, and propose a novel region propagation technique for localization. We evaluate DE-ViT on open-vocabulary, few-shot, and one-shot object detection benchmark with COCO and LVIS. For COCO, DE-ViT outperforms the open-vocabulary SoTA by 6.9 AP50 and achieves 50 AP50 in novel classes. DE-ViT surpasses the few-shot SoTA by 15 mAP on 10-shot and 7.2 mAP on 30-shot and one-shot SoTA by 2.8 AP50. For LVIS, DE-ViT outperforms the open-vocabulary SoTA by 2.2 mask AP and reaches 34.3 mask APr. Code is available at https://github.com/mlzxy/devit.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_12969v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>开集对象检测的目的是检测训练中发现的任意类别之外的任意类别。最近的进步采用了开放词汇范式，利用视觉语言主干来用语言表示类别。在本文中，我们介绍了DE ViT，这是一种开放集对象检测器，它使用仅视觉的DINOv2主干，并通过示例图像而不是语言来学习新的类别。为了提高一般检测能力，我们将多分类任务转换为二进制分类任务，同时绕过每类推理，并提出了一种新的区域传播定位技术。我们使用COCO和LVIS在开放词汇、少镜头和单镜头对象检测基准上评估了DE-ViT。对于COCO，DE ViT比开放词汇SoTA高出6.9 AP50，并在新类别中达到50 AP50。DE ViT在10次发射时以15毫安时的容量超过了少数发射的SoTA，在30次发射时则以7.2毫安时的体积超过了SoTA，而在一次发射时，则以2.8 AP50的容量超过SoTA。对于LVIS，DE ViT比开放词汇表SoTA高2.2掩码AP，达到34.3掩码AP。代码可在https://github.com/mlzxy/devit.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.12969v1" target="_blank">2309.12969v1</a>
                              </td>
                              <td>Detect Every Thing with Few Examples</td>
                              <td>Xinyu Zhang</td>
                              <td>2023-09-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_12969v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.12969v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_12925v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A New Security Threat in MCUs -- SoC-wide timing side channels and how to find them</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_12925v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_12925v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_12925v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Microarchitectural timing side channels have been thoroughly investigated as a security threat in hardware designs featuring shared buffers (e.g., caches) and/or parallelism between attacker and victim task execution. Contradicting common intuitions, recent activities demonstrate, however, that this threat is real also in microcontroller SoCs without such features. In this paper, we describe SoC-wide timing side channels previously neglected by security analysis and present a new formal method to close this gap. In a case study with the RISC-V Pulpissimo SoC platform, our method found a vulnerability to a so far unknown attack variant that allows an attacker to obtain information about a victim's memory access behavior. After implementing a conservative fix, we were able to verify that the SoC is now secure w.r.t. timing side channels.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_12925v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在以共享缓冲区（例如缓存）和/或攻击者和受害者任务执行之间的并行性为特征的硬件设计中，微体系结构定时侧通道已被彻底调查为安全威胁。然而，与普遍的直觉相矛盾的是，最近的活动表明，这种威胁在没有这些功能的微控制器SoC中也是真实存在的。在本文中，我们描述了以前被安全分析忽视的SoC宽定时侧信道，并提出了一种新的形式化方法来弥补这一差距。在RISC-V Pulpissimo SoC平台的案例研究中，我们的方法发现了一个迄今未知的攻击变体的漏洞，该漏洞使攻击者能够获得有关受害者内存访问行为的信息。在实现保守修复后，我们能够验证SoC现在是安全的w.r.t.定时侧信道。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.12925v1" target="_blank">2309.12925v1</a>
                              </td>
                              <td>A New Security Threat in MCUs -- SoC-wide timing side channels and how to find them</td>
                              <td>Johannes Müller</td>
                              <td>2023-09-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_12925v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.12925v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_10972v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SEMPART: Self-supervised Multi-resolution Partitioning of Image Semantics</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_10972v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_10972v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_10972v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Accurately determining salient regions of an image is challenging when labeled data is scarce. DINO-based self-supervised approaches have recently leveraged meaningful image semantics captured by patch-wise features for locating foreground objects. Recent methods have also incorporated intuitive priors and demonstrated value in unsupervised methods for object partitioning. In this paper, we propose SEMPART, which jointly infers coarse and fine bi-partitions over an image's DINO-based semantic graph. Furthermore, SEMPART preserves fine boundary details using graph-driven regularization and successfully distills the coarse mask semantics into the fine mask. Our salient object detection and single object localization findings suggest that SEMPART produces high-quality masks rapidly without additional post-processing and benefits from co-optimizing the coarse and fine branches.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_10972v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当标记数据稀少时，准确确定图像的显著区域是一项挑战。基于DINO的自监督方法最近利用了由逐片特征捕获的有意义的图像语义来定位前景对象。最近的方法也结合了直观的先验，并在无监督的对象划分方法中证明了其价值。在本文中，我们提出了SEMPART，它在图像的基于DINO的语义图上联合推断粗分和细分。此外，SEMPART使用图驱动的正则化来保留精细边界细节，并成功地将粗略掩码语义提取到精细掩码中。我们显著的目标检测和单目标定位结果表明，SEMPART在没有额外后处理的情况下快速生成高质量的掩模，并受益于粗分支和细分支的协同优化。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.10972v1" target="_blank">2309.10972v1</a>
                              </td>
                              <td>SEMPART: Self-supervised Multi-resolution Partitioning of Image Semantics</td>
                              <td>Sriram Ravindran</td>
                              <td>2023-09-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_10972v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.10972v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_10726v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Few-Shot Panoptic Segmentation With Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_10726v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_10726v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_10726v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Current state-of-the-art methods for panoptic segmentation require an immense amount of annotated training data that is both arduous and expensive to obtain posing a significant challenge for their widespread adoption. Concurrently, recent breakthroughs in visual representation learning have sparked a paradigm shift leading to the advent of large foundation models that can be trained with completely unlabeled images. In this work, we propose to leverage such task-agnostic image features to enable few-shot panoptic segmentation by presenting Segmenting Panoptic Information with Nearly 0 labels (SPINO). In detail, our method combines a DINOv2 backbone with lightweight network heads for semantic segmentation and boundary estimation. We show that our approach, albeit being trained with only ten annotated images, predicts high-quality pseudo-labels that can be used with any existing panoptic segmentation method. Notably, we demonstrate that SPINO achieves competitive results compared to fully supervised baselines while using less than 0.3% of the ground truth labels, paving the way for learning complex visual recognition tasks leveraging foundation models. To illustrate its general applicability, we further deploy SPINO on real-world robotic vision systems for both outdoor and indoor environments. To foster future research, we make the code and trained models publicly available at http://spino.cs.uni-freiburg.de.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_10726v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>目前最先进的全景分割方法需要大量的带注释的训练数据，这既困难又昂贵，对其广泛采用构成了重大挑战。与此同时，视觉表征学习的最新突破引发了范式的转变，导致了可以用完全未标记的图像训练的大型基础模型的出现。在这项工作中，我们建议利用这种任务不可知的图像特征，通过呈现具有近0个标签的分割全景信息（SPINO）来实现少镜头全景分割。详细地说，我们的方法将DINOv2主干与轻量级网络头相结合，用于语义分割和边界估计。我们表明，尽管我们的方法仅用10张注释图像进行训练，但它预测了可用于任何现有全景分割方法的高质量伪标签。值得注意的是，我们证明，与完全监督的基线相比，SPINO在使用不到0.3%的基本事实标签的情况下取得了有竞争力的结果，为利用基础模型学习复杂的视觉识别任务铺平了道路。为了说明其普遍适用性，我们进一步将SPINO部署在室外和室内环境的真实世界机器人视觉系统上。为了促进未来的研究，我们在http://spino.cs.uni-freiburg.de.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.10726v1" target="_blank">2309.10726v1</a>
                              </td>
                              <td>Few-Shot Panoptic Segmentation With Foundation Models</td>
                              <td>Markus Käppeler</td>
                              <td>2023-09-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_10726v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.10726v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_07970v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Language Embedded Radiance Fields for Zero-Shot Task-Oriented Grasping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_07970v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_07970v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_07970v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Grasping objects by a specific part is often crucial for safety and for executing downstream tasks. Yet, learning-based grasp planners lack this behavior unless they are trained on specific object part data, making it a significant challenge to scale object diversity. Instead, we propose LERF-TOGO, Language Embedded Radiance Fields for Task-Oriented Grasping of Objects, which uses vision-language models zero-shot to output a grasp distribution over an object given a natural language query. To accomplish this, we first reconstruct a LERF of the scene, which distills CLIP embeddings into a multi-scale 3D language field queryable with text. However, LERF has no sense of objectness, meaning its relevancy outputs often return incomplete activations over an object which are insufficient for subsequent part queries. LERF-TOGO mitigates this lack of spatial grouping by extracting a 3D object mask via DINO features and then conditionally querying LERF on this mask to obtain a semantic distribution over the object with which to rank grasps from an off-the-shelf grasp planner. We evaluate LERF-TOGO's ability to grasp task-oriented object parts on 31 different physical objects, and find it selects grasps on the correct part in 81% of all trials and grasps successfully in 69%. See the project website at: lerftogo.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_07970v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由特定部件抓住物体通常对安全和执行下游任务至关重要。然而，基于学习的抓取规划者缺乏这种行为，除非他们接受特定对象零件数据的训练，这使得缩放对象多样性成为一个重大挑战。相反，我们提出了LERF-TOGO，面向任务的对象抓取的语言嵌入辐射场，它使用视觉语言模型零样本在给定自然语言查询的对象上输出抓取分布。为了实现这一点，我们首先重建场景的LERF，该LERF将CLIP嵌入提取到可通过文本查询的多尺度3D语言字段中。然而，LERF没有对象性，这意味着它的相关性输出通常会返回对象上不完整的激活，这不足以进行后续的零件查询。LER-TOGO通过经由DINO特征提取3D对象掩码，然后有条件地查询该掩码上的LERF，以获得对象上的语义分布，从而从现成的抓取规划器对抓取进行排序，从而缓解了这种空间分组的缺乏。我们评估了LER-TOGO在31个不同实物上抓取面向任务的物体部分的能力，发现它在81%的试验中选择了正确部分的抓取，在69%的试验中成功抓取。参见项目网站：lerftogo.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.07970v2" target="_blank">2309.07970v2</a>
                              </td>
                              <td>Language Embedded Radiance Fields for Zero-Shot Task-Oriented Grasping</td>
                              <td>Adam Rashid</td>
                              <td>2023-09-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_07970v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.07970v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2203_01881v5_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2203_01881v5_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2203_01881v5_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2203_01881v5_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised learning (SSL) has shown impressive results in downstream classification tasks. However, there is limited work in understanding their failure modes and interpreting their learned representations. In this paper, we study the representation space of state-of-the-art self-supervised models including SimCLR, SwaV, MoCo, BYOL, DINO, SimSiam, VICReg and Barlow Twins. Without the use of class label information, we discover discriminative features that correspond to unique physical attributes in images, present mostly in correctly-classified representations. Using these features, we can compress the representation space by up to 40% without significantly affecting linear classification performance. We then propose Self-Supervised Representation Quality Score (or Q-Score), an unsupervised score that can reliably predict if a given sample is likely to be mis-classified during linear evaluation, achieving AUPRC of 91.45 on ImageNet-100 and 78.78 on ImageNet-1K. Q-Score can also be used as a regularization term on pre-trained encoders to remedy low-quality representations. Fine-tuning with Q-Score regularization can boost the linear probing accuracy of SSL models by up to 5.8% on ImageNet-100 and 3.7% on ImageNet-1K compared to their baselines. Finally, using gradient heatmaps and Salient ImageNet masks, we define a metric to quantify the interpretability of each representation. We show that discriminative features are strongly correlated to core attributes and, enhancing these features through Q-score regularization makes SSL representations more interpretable.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2203_01881v5_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督学习（SSL）在下游分类任务中显示出令人印象深刻的结果。然而，在理解他们的失败模式和解释他们的习得表征方面的工作有限。在本文中，我们研究了最先进的自监督模型的表示空间，包括SimCLR、SwaV、MoCo、BYOL、DINO、SimSiam、VICReg和Barlow Twins。在不使用类标签信息的情况下，我们发现了与图像中的独特物理属性相对应的判别特征，这些特征主要存在于正确分类的表示中。使用这些特征，我们可以将表示空间压缩40%，而不会显著影响线性分类性能。然后，我们提出了自监督表示质量分数（或Q-Score），这是一种无监督的分数，可以可靠地预测给定样本在线性评估过程中是否可能被错误分类，在ImageNet-100上实现了91.45的AUPRC，在ImageNet-1K上实现了78.78的AUPRC。Q-Score也可以用作预训练编码器上的正则化术语，以补救低质量表示。与基线相比，使用Q-Score正则化进行微调可以在ImageNet-100上将SSL模型的线性探测精度提高5.8%，在ImageNet-1K上提高3.7%。最后，使用梯度热图和突出的ImageNet掩码，我们定义了一个度量来量化每个表示的可解释性。我们表明，判别特征与核心属性密切相关，通过Q分数正则化增强这些特征使SSL表示更具可解释性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2203.01881v5" target="_blank">2203.01881v5</a>
                              </td>
                              <td>Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features</td>
                              <td>Neha Kalibhat</td>
                              <td>2022-03-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2203_01881v5_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2203.01881v5" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_04200v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Cyclic Operator Precedence Grammars for Parallel Parsing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_04200v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_04200v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_04200v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Operator precedence languages (OPL) enjoy the local parsability property, which essentially means that a code fragment enclosed within a pair of markers -- playing the role of parentheses -- can be compiled with no knowledge of its external context. Such a property has been exploited to build parallel compilers for languages formalized as OPLs. It has been observed, however, that when the syntax trees of the sentences have a linear substructure, its parsing must necessarily proceed sequentially making it impossible to split such a subtree into chunks to be processed in parallel. Such an inconvenience is due to the fact that so far much literature on OPLs has assumed the hypothesis that equality precedence relation cannot be cyclic. This hypothesis was motivated by the need to keep the mathematical notation as simple as possible.   We present an enriched version of operator precedence grammars, called cyclic, that allows to use a simplified version of regular expressions in the right hand sides of grammar's rules; for this class of operator precedence grammars the acyclicity hypothesis of the equality precedence relation is no more needed to guarantee the algebraic properties of the generated languages. The expressive power of the cyclic grammars is now fully equivalent to that of other formalisms defining OPLs such as operator precedence automata, monadic second order logic and operator precedence expressions. As a result cyclic operator precedence grammars now produce also unranked syntax trees and sentences with flat unbounded substructures that can be naturally partitioned into chunks suitable for parallel parsing.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_04200v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>运算符优先语言（OPL）具有局部可解析性，这本质上意味着一个包含在一对标记中的代码片段（扮演括号的角色）可以在不了解其外部上下文的情况下进行编译。这种特性已被用来为形式化为OPL的语言构建并行编译器。然而，已经观察到，当句子的语法树具有线性子结构时，其解析必须按顺序进行，这使得不可能将这样的子树分割成要并行处理的块。这种不便是由于到目前为止，许多关于OPL的文献都假设平等优先关系不可能是循环的。这一假设的动机是需要保持数学符号尽可能简单。我们提出了运算符优先语法的丰富版本，称为循环语法，它允许在语法规则的右侧使用正则表达式的简化版本；对于这类算子优先语法，不再需要等式优先关系的非循环性假设来保证生成语言的代数性质。循环语法的表达能力现在完全等同于定义OPL的其他形式主义，如算子优先自动机、一元二阶逻辑和算子优先表达式。因此，循环运算符优先语法现在也产生了具有平坦无界子结构的未排序语法树和句子，这些子结构可以自然地划分为适合并行解析的块。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.04200v1" target="_blank">2309.04200v1</a>
                              </td>
                              <td>Cyclic Operator Precedence Grammars for Parallel Parsing</td>
                              <td>Michele Chiari</td>
                              <td>2023-09-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_04200v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.04200v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_03999v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Adapting Self-Supervised Representations to Multi-Domain Setups</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_03999v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_03999v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_03999v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Current state-of-the-art self-supervised approaches, are effective when trained on individual domains but show limited generalization on unseen domains. We observe that these models poorly generalize even when trained on a mixture of domains, making them unsuitable to be deployed under diverse real-world setups. We therefore propose a general-purpose, lightweight Domain Disentanglement Module (DDM) that can be plugged into any self-supervised encoder to effectively perform representation learning on multiple, diverse domains with or without shared classes. During pre-training according to a self-supervised loss, DDM enforces a disentanglement in the representation space by splitting it into a domain-variant and a domain-invariant portion. When domain labels are not available, DDM uses a robust clustering approach to discover pseudo-domains. We show that pre-training with DDM can show up to 3.5% improvement in linear probing accuracy on state-of-the-art self-supervised models including SimCLR, MoCo, BYOL, DINO, SimSiam and Barlow Twins on multi-domain benchmarks including PACS, DomainNet and WILDS. Models trained with DDM show significantly improved generalization (7.4%) to unseen domains compared to baselines. Therefore, DDM can efficiently adapt self-supervised encoders to provide high-quality, generalizable representations for diverse multi-domain data.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_03999v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当前最先进的自监督方法在单个领域上训练时是有效的，但在看不见的领域上表现出有限的泛化能力。我们观察到，即使在混合域上训练，这些模型的泛化能力也很差，这使得它们不适合在不同的现实世界设置下部署。因此，我们提出了一种通用的、轻量级的域解纠缠模块（DDM），该模块可以插入任何自监督编码器，以在具有或不具有共享类的多个不同域上有效地执行表示学习。在根据自监督损失进行预训练期间，DDM通过将表示空间拆分为域变体和域不变部分，在表示空间中强制解纠缠。当域标签不可用时，DDM使用稳健的集群方法来发现伪域。我们发现，在包括PACS、DomainNet和WILDS在内的多领域基准测试上，在包括SimCLR、MoCo、BYOL、DINO、SimSiam和Barlow Twins在内的最先进的自监督模型上，使用DDM的预训练可以显示高达3.5%的线性探测精度提高。与基线相比，用DDM训练的模型对看不见的领域的泛化能力显著提高（7.4%）。因此，DDM可以有效地调整自监督编码器，为不同的多域数据提供高质量、可推广的表示。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.03999v1" target="_blank">2309.03999v1</a>
                              </td>
                              <td>Adapting Self-Supervised Representations to Multi-Domain Setups</td>
                              <td>Neha Kalibhat</td>
                              <td>2023-09-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_03999v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.03999v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_03893v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DiffusionEngine: Diffusion Model is Scalable Data Engine for Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_03893v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_03893v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_03893v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Data is the cornerstone of deep learning. This paper reveals that the recently developed Diffusion Model is a scalable data engine for object detection. Existing methods for scaling up detection-oriented data often require manual collection or generative models to obtain target images, followed by data augmentation and labeling to produce training pairs, which are costly, complex, or lacking diversity. To address these issues, we presentDiffusionEngine (DE), a data scaling-up engine that provides high-quality detection-oriented training pairs in a single stage. DE consists of a pre-trained diffusion model and an effective Detection-Adapter, contributing to generating scalable, diverse and generalizable detection data in a plug-and-play manner. Detection-Adapter is learned to align the implicit semantic and location knowledge in off-the-shelf diffusion models with detection-aware signals to make better bounding-box predictions. Additionally, we contribute two datasets, i.e., COCO-DE and VOC-DE, to scale up existing detection benchmarks for facilitating follow-up research. Extensive experiments demonstrate that data scaling-up via DE can achieve significant improvements in diverse scenarios, such as various detection algorithms, self-supervised pre-training, data-sparse, label-scarce, cross-domain, and semi-supervised learning. For example, when using DE with a DINO-based adapter to scale up data, mAP is improved by 3.1% on COCO, 7.6% on VOC, and 11.5% on Clipart.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_03893v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>数据是深度学习的基石。本文揭示了最近开发的扩散模型是一个可扩展的对象检测数据引擎。现有的用于放大面向检测的数据的方法通常需要手动收集或生成模型来获得目标图像，然后进行数据扩充和标记以产生训练对，这是昂贵的、复杂的或缺乏多样性的。为了解决这些问题，我们提出了DiffusionEngine（DE），这是一个数据扩展引擎，可以在单个阶段提供高质量的面向检测的训练对。DE由预先训练的扩散模型和有效的检测适配器组成，有助于以即插即用的方式生成可扩展、多样和可推广的检测数据。检测适配器被学习来将现成的扩散模型中的隐含语义和位置知识与检测感知信号对齐，以做出更好的边界框预测。此外，我们贡献了两个数据集，即COCO-DE和VOC-DE，以扩大现有的检测基准，促进后续研究。大量实验表明，通过DE扩展数据可以在各种场景中实现显著改进，如各种检测算法、自监督预训练、数据稀疏、标签稀缺、跨域和半监督学习。例如，当使用DE和基于DINO的适配器来放大数据时，COCO的mAP提高了3.1%，VOC提高了7.6%，Clipart提高了11.5%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.03893v1" target="_blank">2309.03893v1</a>
                              </td>
                              <td>DiffusionEngine: Diffusion Model is Scalable Data Engine for Object Detection</td>
                              <td>Manlin Zhang</td>
                              <td>2023-09-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_03893v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.03893v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_03173v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PDiscoNet: Semantically consistent part discovery for fine-grained recognition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_03173v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_03173v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_03173v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Fine-grained classification often requires recognizing specific object parts, such as beak shape and wing patterns for birds. Encouraging a fine-grained classification model to first detect such parts and then using them to infer the class could help us gauge whether the model is indeed looking at the right details better than with interpretability methods that provide a single attribution map. We propose PDiscoNet to discover object parts by using only image-level class labels along with priors encouraging the parts to be: discriminative, compact, distinct from each other, equivariant to rigid transforms, and active in at least some of the images. In addition to using the appropriate losses to encode these priors, we propose to use part-dropout, where full part feature vectors are dropped at once to prevent a single part from dominating in the classification, and part feature vector modulation, which makes the information coming from each part distinct from the perspective of the classifier. Our results on CUB, CelebA, and PartImageNet show that the proposed method provides substantially better part discovery performance than previous methods while not requiring any additional hyper-parameter tuning and without penalizing the classification performance. The code is available at https://github.com/robertdvdk/part_detection.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_03173v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>细粒度分类通常需要识别特定的物体部分，例如鸟类的喙形和翅膀图案。鼓励细粒度分类模型首先检测这些部分，然后使用它们来推断类，可以帮助我们衡量模型是否确实比提供单个属性图的可解释性方法更好地查看了正确的细节。我们建议PDiscoNet通过仅使用图像级别的类标签以及先验来发现对象部分，这些先验鼓励这些部分是：有区别的、紧凑的、彼此不同的、等变的到刚性的变换，并且在至少一些图像中是活动的。除了使用适当的损失对这些先验进行编码外，我们还建议使用部分丢弃和部分特征向量调制，部分丢弃是指同时丢弃全部部分特征向量，以防止单个部分在分类中占主导地位，部分特征向量调制使来自每个部分的信息从分类器的角度来看是不同的。我们在CUB、CelebA和PartImageNet上的结果表明，所提出的方法比以前的方法提供了更好的零件发现性能，同时不需要任何额外的超参数调整，也不影响分类性能。代码可在https://github.com/robertdvdk/part_detection.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.03173v1" target="_blank">2309.03173v1</a>
                              </td>
                              <td>PDiscoNet: Semantically consistent part discovery for fine-grained recognition</td>
                              <td>Robert van der Klis</td>
                              <td>2023-09-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_03173v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.03173v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_05163v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Self-supervision for medical image classification: state-of-the-art performance with ~100 labeled training samples per class</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_05163v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_05163v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_05163v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Is self-supervised deep learning (DL) for medical image analysis already a serious alternative to the de facto standard of end-to-end trained supervised DL? We tackle this question for medical image classification, with a particular focus on one of the currently most limiting factors of the field: the (non-)availability of labeled data. Based on three common medical imaging modalities (bone marrow microscopy, gastrointestinal endoscopy, dermoscopy) and publicly available data sets, we analyze the performance of self-supervised DL within the self-distillation with no labels (DINO) framework. After learning an image representation without use of image labels, conventional machine learning classifiers are applied. The classifiers are fit using a systematically varied number of labeled data (1-1000 samples per class). Exploiting the learned image representation, we achieve state-of-the-art classification performance for all three imaging modalities and data sets with only a fraction of between 1% and 10% of the available labeled data and about 100 labeled samples per class.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_05163v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>用于医学图像分析的自监督深度学习（DL）是否已经是端到端训练的监督DL的事实标准的一个重要替代方案？我们解决了医学图像分类的这个问题，特别关注该领域目前最受限制的因素之一：标记数据的（非）可用性。基于三种常见的医学成像模式（骨髓显微镜、胃肠镜、皮肤镜）和公开的数据集，我们在无标签自蒸馏（DINO）框架内分析了自监督DL的性能。在不使用图像标签的情况下学习图像表示之后，应用传统的机器学习分类器。分类器使用系统变化数量的标记数据（每类1-1000个样本）进行拟合。利用学习的图像表示，我们对所有三种成像模态和数据集都实现了最先进的分类性能，每类仅占可用标记数据的1%至10%，每个类别约有100个标记样本。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.05163v2" target="_blank">2304.05163v2</a>
                              </td>
                              <td>Self-supervision for medical image classification: state-of-the-art performance with ~100 labeled training samples per class</td>
                              <td>Maximilian Nielsen</td>
                              <td>2023-04-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_05163v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.05163v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_13396v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Zero-guidance Segmentation Using Zero Segment Labels</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_13396v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_13396v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_13396v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>CLIP has enabled new and exciting joint vision-language applications, one of which is open-vocabulary segmentation, which can locate any segment given an arbitrary text query. In our research, we ask whether it is possible to discover semantic segments without any user guidance in the form of text queries or predefined classes, and label them using natural language automatically? We propose a novel problem zero-guidance segmentation and the first baseline that leverages two pre-trained generalist models, DINO and CLIP, to solve this problem without any fine-tuning or segmentation dataset. The general idea is to first segment an image into small over-segments, encode them into CLIP's visual-language space, translate them into text labels, and merge semantically similar segments together. The key challenge, however, is how to encode a visual segment into a segment-specific embedding that balances global and local context information, both useful for recognition. Our main contribution is a novel attention-masking technique that balances the two contexts by analyzing the attention layers inside CLIP. We also introduce several metrics for the evaluation of this new task. With CLIP's innate knowledge, our method can precisely locate the Mona Lisa painting among a museum crowd. Project page: https://zero-guide-seg.github.io/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_13396v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>CLIP实现了新的、令人兴奋的联合视觉语言应用，其中之一是开放式词汇分割，它可以在给定任意文本查询的情况下定位任何片段。在我们的研究中，我们问是否有可能在没有任何用户指导的情况下，以文本查询或预定义类的形式发现语义片段，并使用自然语言自动标记它们？我们提出了一种新的问题零引导分割和第一个基线，该基线利用两个预先训练的广义模型DINO和CLIP来解决这个问题，而不需要任何微调或分割数据集。一般的想法是首先将图像分割成小片段，将它们编码到CLIP的视觉语言空间中，将它们翻译成文本标签，并将语义相似的片段合并在一起。然而，关键的挑战是如何将视觉片段编码为特定片段的嵌入，以平衡全局和局部上下文信息，这两种信息都对识别有用。我们的主要贡献是一种新颖的注意力掩蔽技术，该技术通过分析CLIP内部的注意力层来平衡两种上下文。我们还介绍了用于评估这项新任务的几个指标。凭借CLIP与生俱来的知识，我们的方法可以在博物馆人群中准确定位蒙娜丽莎的画作。项目页面：https://zero-guide-seg.github.io/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.13396v3" target="_blank">2303.13396v3</a>
                              </td>
                              <td>Zero-guidance Segmentation Using Zero Segment Labels</td>
                              <td>Pitchaporn Rewatbowornwong</td>
                              <td>2023-03-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_13396v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.13396v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_16271v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Emergence of Segmentation with Minimalistic White-Box Transformers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_16271v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_16271v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_16271v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Transformer-like models for vision tasks have recently proven effective for a wide range of downstream applications such as segmentation and detection. Previous works have shown that segmentation properties emerge in vision transformers (ViTs) trained using self-supervised methods such as DINO, but not in those trained on supervised classification tasks. In this study, we probe whether segmentation emerges in transformer-based models solely as a result of intricate self-supervised learning mechanisms, or if the same emergence can be achieved under much broader conditions through proper design of the model architecture. Through extensive experimental results, we demonstrate that when employing a white-box transformer-like architecture known as CRATE, whose design explicitly models and pursues low-dimensional structures in the data distribution, segmentation properties, at both the whole and parts levels, already emerge with a minimalistic supervised training recipe. Layer-wise finer-grained analysis reveals that the emergent properties strongly corroborate the designed mathematical functions of the white-box network. Our results suggest a path to design white-box foundation models that are simultaneously highly performant and mathematically fully interpretable. Code is at \url{https://github.com/Ma-Lab-Berkeley/CRATE}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_16271v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>用于视觉任务的类转换器模型最近已被证明对广泛的下游应用（如分割和检测）有效。先前的工作表明，在使用自监督方法（如DINO）训练的视觉变换器（ViTs）中出现了分割特性，但在使用监督分类任务训练的视觉转换器中没有。在这项研究中，我们探讨了在基于转换器的模型中，分割是否仅仅是由于复杂的自监督学习机制而出现的，或者是否可以通过正确设计模型架构在更广泛的条件下实现同样的出现。通过大量的实验结果，我们证明，当使用一种被称为CRATE的类似白盒变压器的架构时，其设计明确地建模并追求数据分布中的低维结构，在整体和部分层面上的分割特性已经出现了一个极简主义的监督训练配方。逐层细粒度分析表明，涌现特性有力地证实了白盒网络设计的数学函数。我们的研究结果为设计同时具有高性能和数学上完全可解释性的白盒基础模型提供了一条途径。代码位于\url{https://github.com/Ma-Lab-Berkeley/CRATE}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.16271v1" target="_blank">2308.16271v1</a>
                              </td>
                              <td>Emergence of Segmentation with Minimalistic White-Box Transformers</td>
                              <td>Yaodong Yu</td>
                              <td>2023-08-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_16271v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.16271v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_14710v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">VideoCutLER: Surprisingly Simple Unsupervised Video Instance Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_14710v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_14710v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_14710v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Existing approaches to unsupervised video instance segmentation typically rely on motion estimates and experience difficulties tracking small or divergent motions. We present VideoCutLER, a simple method for unsupervised multi-instance video segmentation without using motion-based learning signals like optical flow or training on natural videos. Our key insight is that using high-quality pseudo masks and a simple video synthesis method for model training is surprisingly sufficient to enable the resulting video model to effectively segment and track multiple instances across video frames. We show the first competitive unsupervised learning results on the challenging YouTubeVIS-2019 benchmark, achieving 50.7% APvideo^50 , surpassing the previous state-of-the-art by a large margin. VideoCutLER can also serve as a strong pretrained model for supervised video instance segmentation tasks, exceeding DINO by 15.9% on YouTubeVIS-2019 in terms of APvideo.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_14710v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现有的无监督视频实例分割方法通常依赖于运动估计，并且在跟踪小的或发散的运动时遇到困难。我们提出了VideoCutLER，这是一种无监督多实例视频分割的简单方法，无需使用基于运动的学习信号，如光流或自然视频训练。我们的关键见解是，使用高质量的伪掩码和简单的视频合成方法进行模型训练，令人惊讶地足以使生成的视频模型能够有效地分割和跟踪视频帧中的多个实例。我们在具有挑战性的YouTubeVIS-2019基准上展示了第一个竞争性的无监督学习结果，实现了50.7%的APvideo^50，大大超过了以前的最先进水平。VideoCutLER还可以作为监督视频实例分割任务的强大预训练模型，在YouTubeVIS-2019上的APvideo超过DINO 15.9%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.14710v1" target="_blank">2308.14710v1</a>
                              </td>
                              <td>VideoCutLER: Surprisingly Simple Unsupervised Video Instance Segmentation</td>
                              <td>Xudong Wang</td>
                              <td>2023-08-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_14710v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.14710v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_14597v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Adversarial Attacks on Foundational Vision Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_14597v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_14597v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_14597v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Rapid progress is being made in developing large, pretrained, task-agnostic foundational vision models such as CLIP, ALIGN, DINOv2, etc. In fact, we are approaching the point where these models do not have to be finetuned downstream, and can simply be used in zero-shot or with a lightweight probing head. Critically, given the complexity of working at this scale, there is a bottleneck where relatively few organizations in the world are executing the training then sharing the models on centralized platforms such as HuggingFace and torch.hub. The goal of this work is to identify several key adversarial vulnerabilities of these models in an effort to make future designs more robust. Intuitively, our attacks manipulate deep feature representations to fool an out-of-distribution (OOD) detector which will be required when using these open-world-aware models to solve closed-set downstream tasks. Our methods reliably make in-distribution (ID) images (w.r.t. a downstream task) be predicted as OOD and vice versa while existing in extremely low-knowledge-assumption threat models. We show our attacks to be potent in whitebox and blackbox settings, as well as when transferred across foundational model types (e.g., attack DINOv2 with CLIP)! This work is only just the beginning of a long journey towards adversarially robust foundational vision models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_14597v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在开发大型、预训练的任务识别基础视觉模型（如CLIP、ALIGN、DINOv2等）方面取得了快速进展。事实上，我们正在接近这样一个点，即这些模型不必在下游进行微调，可以简单地用于零样本或轻型探测头。至关重要的是，鉴于在这种规模下工作的复杂性，存在一个瓶颈，即世界上相对较少的组织执行培训，然后在HuggingFace和torch.hub等集中平台上共享模型。这项工作的目标是识别这些模型的几个关键对抗性漏洞，以使未来的设计更加稳健。直观地说，我们的攻击操纵深度特征表示来欺骗分布外（OOD）检测器，这是在使用这些开放世界感知模型来解决闭集下游任务时所需要的。我们的方法可靠地使分布中（ID）图像（w.r.t.下游任务）被预测为OOD，反之亦然，同时存在于极低知识假设的威胁模型中。我们展示了我们的攻击在白盒和黑盒设置中以及在基础模型类型之间传输时的强大性（例如，使用CLIP攻击DINOv2）！这项工作只是迈向对抗性稳健基础视觉模型的漫长旅程的开始。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.14597v1" target="_blank">2308.14597v1</a>
                              </td>
                              <td>Adversarial Attacks on Foundational Vision Models</td>
                              <td>Nathan Inkawhich</td>
                              <td>2023-08-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_14597v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.14597v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_14461v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Spatio-Temporal Analysis of Patient-Derived Organoid Videos Using Deep Learning for the Prediction of Drug Efficacy</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_14461v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_14461v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_14461v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Over the last ten years, Patient-Derived Organoids (PDOs) emerged as the most reliable technology to generate ex-vivo tumor avatars. PDOs retain the main characteristics of their original tumor, making them a system of choice for pre-clinical and clinical studies. In particular, PDOs are attracting interest in the field of Functional Precision Medicine (FPM), which is based upon an ex-vivo drug test in which living tumor cells (such as PDOs) from a specific patient are exposed to a panel of anti-cancer drugs. Currently, the Adenosine Triphosphate (ATP) based cell viability assay is the gold standard test to assess the sensitivity of PDOs to drugs. The readout is measured at the end of the assay from a global PDO population and therefore does not capture single PDO responses and does not provide time resolution of drug effect. To this end, in this study, we explore for the first time the use of powerful large foundation models for the automatic processing of PDO data. In particular, we propose a novel imaging-based high-throughput screening method to assess real-time drug efficacy from a time-lapse microscopy video of PDOs. The recently proposed SAM algorithm for segmentation and DINOv2 model are adapted in a comprehensive pipeline for processing PDO microscopy frames. Moreover, an attention mechanism is proposed for fusing temporal and spatial features in a multiple instance learning setting to predict ATP. We report better results than other non-time-resolved methods, indicating that the temporality of data is an important factor for the prediction of ATP. Extensive ablations shed light on optimizing the experimental setting and automating the prediction both in real-time and for forecasting.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_14461v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在过去的十年里，患者衍生类器官（PDO）成为产生离体肿瘤化身的最可靠技术。PDO保留了其原始肿瘤的主要特征，使其成为临床前和临床研究的首选系统。特别是，PDO在功能精确医学（FPM）领域引起了人们的兴趣，该领域基于体外药物测试，其中来自特定患者的活肿瘤细胞（如PDO）暴露于一组抗癌药物。目前，基于三磷酸腺苷（ATP）的细胞活力测定是评估PDO对药物敏感性的金标准测试。在测定结束时从全局PDO群体中测量读数，因此不捕获单个PDO反应，也不提供药物作用的时间分辨率。为此，在本研究中，我们首次探索使用强大的大型基础模型来自动处理PDO数据。特别是，我们提出了一种新的基于成像的高通量筛选方法，从PDO的延时显微镜视频中评估实时药物疗效。最近提出的用于分割的SAM算法和DINOv2模型适用于处理PDO显微镜框架的综合管道。此外，提出了一种注意力机制，用于在多实例学习环境中融合时间和空间特征来预测ATP。我们报告了比其他非时间分辨方法更好的结果，表明数据的时间性是预测ATP的一个重要因素。广泛的消融揭示了优化实验设置和自动化实时预测和预测。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.14461v1" target="_blank">2308.14461v1</a>
                              </td>
                              <td>Spatio-Temporal Analysis of Patient-Derived Organoid Videos Using Deep Learning for the Prediction of Drug Efficacy</td>
                              <td>Leo Fillioux</td>
                              <td>2023-08-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_14461v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.14461v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_14392v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">1st Place Solution for the 5th LSVOS Challenge: Video Instance Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_14392v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_14392v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_14392v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video instance segmentation is a challenging task that serves as the cornerstone of numerous downstream applications, including video editing and autonomous driving. In this report, we present further improvements to the SOTA VIS method, DVIS. First, we introduce a denoising training strategy for the trainable tracker, allowing it to achieve more stable and accurate object tracking in complex and long videos. Additionally, we explore the role of visual foundation models in video instance segmentation. By utilizing a frozen VIT-L model pre-trained by DINO v2, DVIS demonstrates remarkable performance improvements. With these enhancements, our method achieves 57.9 AP and 56.0 AP in the development and test phases, respectively, and ultimately ranked 1st in the VIS track of the 5th LSVOS Challenge. The code will be available at https://github.com/zhang-tao-whu/DVIS.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_14392v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频实例分割是一项具有挑战性的任务，是许多下游应用程序的基石，包括视频编辑和自动驾驶。在本报告中，我们介绍了对SOTA VIS方法DVIS的进一步改进。首先，我们为可训练跟踪器引入了一种去噪训练策略，使其能够在复杂和长视频中实现更稳定和准确的目标跟踪。此外，我们还探讨了视觉基础模型在视频实例分割中的作用。通过使用由DINO v2预先训练的冷冻VIT-L模型，DVIS展示了显著的性能改进。通过这些增强，我们的方法在开发和测试阶段分别达到57.9 AP和56.0 AP，并最终在第五届LSVOS挑战赛的VIS赛道中排名第一。代码将在https://github.com/zhang-tao-whu/DVIS.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.14392v1" target="_blank">2308.14392v1</a>
                              </td>
                              <td>1st Place Solution for the 5th LSVOS Challenge: Video Instance Segmentation</td>
                              <td>Tao Zhang</td>
                              <td>2023-08-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_14392v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.14392v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_14070v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DETDet: Dual Ensemble Teeth Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_14070v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_14070v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_14070v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The field of dentistry is in the era of digital transformation. Particularly, artificial intelligence is anticipated to play a significant role in digital dentistry. AI holds the potential to significantly assist dental practitioners and elevate diagnostic accuracy. In alignment with this vision, the 2023 MICCAI DENTEX challenge aims to enhance the performance of dental panoramic X-ray diagnosis and enumeration through technological advancement. In response, we introduce DETDet, a Dual Ensemble Teeth Detection network. DETDet encompasses two distinct modules dedicated to enumeration and diagnosis. Leveraging the advantages of teeth mask data, we employ Mask-RCNN for the enumeration module. For the diagnosis module, we adopt an ensemble model comprising DiffusionDet and DINO. To further enhance precision scores, we integrate a complementary module to harness the potential of unlabeled data. The code for our approach will be made accessible at https://github.com/Bestever-choi/Evident</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_14070v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>牙科领域正处于数字化转型时代。特别是，预计人工智能将在数字牙科中发挥重要作用。人工智能具有显著帮助牙科医生和提高诊断准确性的潜力。与这一愿景相一致，2023年MICCAI DENTEX挑战旨在通过技术进步提高牙科全景X射线诊断和计数的性能。作为回应，我们介绍了DETDet，一种双集成牙齿检测网络。DETDet包含两个专门用于枚举和诊断的不同模块。利用牙齿掩码数据的优势，我们将mask RCNN用于枚举模块。对于诊断模块，我们采用了包括DiffusionDet和DINO的集成模型。为了进一步提高精度分数，我们集成了一个互补模块来利用未标记数据的潜力。我们方法的代码将在https://github.com/Bestever-choi/Evident</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.14070v1" target="_blank">2308.14070v1</a>
                              </td>
                              <td>DETDet: Dual Ensemble Teeth Detection</td>
                              <td>Kyoungyeon Choi</td>
                              <td>2023-08-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_14070v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.14070v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_11067v4_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CNOS: A Strong Baseline for CAD-based Novel Object Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_11067v4_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_11067v4_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_11067v4_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a simple three-stage approach to segment unseen objects in RGB images using their CAD models. Leveraging recent powerful foundation models, DINOv2 and Segment Anything, we create descriptors and generate proposals, including binary masks for a given input RGB image. By matching proposals with reference descriptors created from CAD models, we achieve precise object ID assignment along with modal masks. We experimentally demonstrate that our method achieves state-of-the-art results in CAD-based novel object segmentation, surpassing existing approaches on the seven core datasets of the BOP challenge by 19.8% AP using the same BOP evaluation protocol. Our source code is available at https://github.com/nv-nguyen/cnos.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_11067v4_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种简单的三阶段方法，使用RGB图像中的CAD模型来分割看不见的对象。利用最近强大的基础模型DINOv2和Segment Anything，我们创建描述符并生成建议，包括给定输入RGB图像的二进制掩码。通过将方案与从CAD模型创建的参考描述符相匹配，我们实现了精确的对象ID分配以及模式掩码。我们通过实验证明，我们的方法在基于CAD的新对象分割中取得了最先进的结果，使用相同的BOP评估协议，在BOP挑战的七个核心数据集上超过了现有方法19.8%AP。我们的源代码可在https://github.com/nv-nguyen/cnos.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.11067v4" target="_blank">2307.11067v4</a>
                              </td>
                              <td>CNOS: A Strong Baseline for CAD-based Novel Object Segmentation</td>
                              <td>Van Nguyen Nguyen</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_11067v4_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.11067v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2006_01236v6_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Aperiodicity, Star-freeness, and First-order Logic Definability of Structured Context-Free Languages</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2006_01236v6_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2006_01236v6_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2006_01236v6_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A classic result in formal language theory is the equivalence among non-counting, or aperiodic, regular languages, and languages defined through star-free regular expressions, or first-order logic. Past attempts to extend this result beyond the realm of regular languages have met with difficulties: for instance it is known that star-free tree languages may violate the non-counting property and there are aperiodic tree languages that cannot be defined through first-order logic. We extend such classic equivalence results to a significant family of deterministic context-free languages, the operator-precedence languages (OPL), which strictly includes the widely investigated visibly pushdown, alias input-driven, family and other structured context-free languages. The OP model originated in the '60s for defining programming languages and is still used by high performance compilers; its rich algebraic properties have been investigated initially in connection with grammar learning and recently completed with further closure properties and with monadic second order logic definition. We introduce an extension of regular expressions, the OP-expressions (OPE) which define the OPLs and, under the star-free hypothesis, define first-order definable and non-counting OPLs. Then, we prove, through a fairly articulated grammar transformation, that aperiodic OPLs are first-order definable. Thus, the classic equivalence of star-freeness, aperiodicity, and first-order definability is established for the large and powerful class of OPLs. We argue that the same approach can be exploited to obtain analogous results for visibly pushdown languages too.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2006_01236v6_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>形式语言理论中的一个经典结果是非计数或非周期的正则语言与通过无星正则表达式或一阶逻辑定义的语言之间的等价。过去试图将这一结果扩展到正则语言之外的尝试遇到了困难：例如，已知无星树语言可能违反不计数性质，并且存在无法通过一阶逻辑定义的非周期树语言。我们将这种经典等价结果扩展到一个重要的确定上下文无关语言家族，即算子优先语言（OPL），它严格包括广泛研究的可见下推、别名输入驱动、家族和其他结构化上下文无关语言。OP模型起源于60年代，用于定义编程语言，目前仍被高性能编译器使用；它丰富的代数性质最初是在语法学习中研究的，最近又完成了进一步的闭包性质和一元二阶逻辑定义。我们引入了正则表达式的一个扩展，即OP表达式（OPE），它定义了OPL，并且在无星假设下，定义了一阶可定义和不计数的OPL。然后，我们通过一个相当清晰的语法转换证明了非周期OPL是一阶可定义的。因此，对于大而有力的OPL类，建立了星自由度、非周期性和一阶可定义性的经典等价性。我们认为，同样的方法也可以用于获得明显下推语言的类似结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2006.01236v6" target="_blank">2006.01236v6</a>
                              </td>
                              <td>Aperiodicity, Star-freeness, and First-order Logic Definability of Structured Context-Free Languages</td>
                              <td>Dino Mandrioli</td>
                              <td>2020-06-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2006_01236v6_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2006.01236v6" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_12127v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Masking Strategies for Background Bias Removal in Computer Vision Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_12127v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_12127v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_12127v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Models for fine-grained image classification tasks, where the difference between some classes can be extremely subtle and the number of samples per class tends to be low, are particularly prone to picking up background-related biases and demand robust methods to handle potential examples with out-of-distribution (OOD) backgrounds. To gain deeper insights into this critical problem, our research investigates the impact of background-induced bias on fine-grained image classification, evaluating standard backbone models such as Convolutional Neural Network (CNN) and Vision Transformers (ViT). We explore two masking strategies to mitigate background-induced bias: Early masking, which removes background information at the (input) image level, and late masking, which selectively masks high-level spatial features corresponding to the background. Extensive experiments assess the behavior of CNN and ViT models under different masking strategies, with a focus on their generalization to OOD backgrounds. The obtained findings demonstrate that both proposed strategies enhance OOD performance compared to the baseline models, with early masking consistently exhibiting the best OOD performance. Notably, a ViT variant employing GAP-Pooled Patch token-based classification combined with early masking achieves the highest OOD robustness.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_12127v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>细粒度图像分类任务的模型，其中一些类别之间的差异可能非常微妙，并且每个类别的样本数量往往很低，特别容易发现与背景相关的偏差，并且需要稳健的方法来处理具有分布外（OOD）背景的潜在示例。为了深入了解这一关键问题，我们的研究调查了背景引起的偏见对细粒度图像分类的影响，评估了卷积神经网络（CNN）和视觉变换器（ViT）等标准骨干模型。我们探索了两种掩蔽策略来减轻背景引起的偏差：早期掩蔽，它去除（输入）图像级别的背景信息，以及后期掩蔽，它选择性地掩蔽与背景相对应的高级空间特征。大量实验评估了CNN和ViT模型在不同掩蔽策略下的行为，重点是它们对OOD背景的泛化。所获得的结果表明，与基线模型相比，两种提出的策略都提高了OOD性能，早期掩蔽始终表现出最佳的OOD性能。值得注意的是，采用基于GAP池补丁令牌的分类与早期掩蔽相结合的ViT变体实现了最高的OOD鲁棒性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.12127v1" target="_blank">2308.12127v1</a>
                              </td>
                              <td>Masking Strategies for Background Bias Removal in Computer Vision Models</td>
                              <td>Ananthu Aniraj</td>
                              <td>2023-08-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_12127v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.12127v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_10782v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Sparse Linear Concept Discovery Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_10782v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_10782v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_10782v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The recent mass adoption of DNNs, even in safety-critical scenarios, has shifted the focus of the research community towards the creation of inherently intrepretable models. Concept Bottleneck Models (CBMs) constitute a popular approach where hidden layers are tied to human understandable concepts allowing for investigation and correction of the network's decisions. However, CBMs usually suffer from: (i) performance degradation and (ii) lower interpretability than intended due to the sheer amount of concepts contributing to each decision. In this work, we propose a simple yet highly intuitive interpretable framework based on Contrastive Language Image models and a single sparse linear layer. In stark contrast to related approaches, the sparsity in our framework is achieved via principled Bayesian arguments by inferring concept presence via a data-driven Bernoulli distribution. As we experimentally show, our framework not only outperforms recent CBM approaches accuracy-wise, but it also yields high per example concept sparsity, facilitating the individual investigation of the emerging concepts.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_10782v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近DNN的大规模采用，即使是在安全关键的场景中，也将研究界的重点转移到了创建固有的可预测模型上。概念瓶颈模型（CBM）是一种流行的方法，其中隐藏层与人类可理解的概念联系在一起，从而可以调查和纠正网络的决策。然而，CBM通常会受到以下影响：（i）性能下降，（ii）由于每个决策涉及的概念太多，可解释性低于预期。在这项工作中，我们提出了一个基于对比语言图像模型和单个稀疏线性层的简单但高度直观的可解释框架。与相关方法形成鲜明对比的是，我们框架中的稀疏性是通过原则贝叶斯论证实现的，该论证通过数据驱动的伯努利分布推断概念存在。正如我们的实验所示，我们的框架不仅在精度方面优于最近的CBM方法，而且它还产生了高的每个示例概念稀疏性，有助于对新兴概念的单独研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.10782v1" target="_blank">2308.10782v1</a>
                              </td>
                              <td>Sparse Linear Concept Discovery Models</td>
                              <td>Konstantinos P. Panousis</td>
                              <td>2023-08-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_10782v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.10782v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </div>
  <div class="text-muted text-center py-2">
    <div class="container">
      <div class="row">
        <div class="col-md-12 my-4">
          <p class="mb-1">© 2023 Li Yingping - Powered by Arxiv API.</p>
        </div>
      </div>
    </div>
  </div>
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
    integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
    crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
    integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
    crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js"
    integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy"
    crossorigin="anonymous"></script>
  <pingendo onclick="window.open('https://pingendo.com/', '_blank')"
    style="cursor:pointer;position: fixed;bottom: 20px;right:20px;padding:4px;background-color: #00b0eb;border-radius: 8px; width:220px;display:flex;flex-direction:row;align-items:center;justify-content:center;font-size:14px;color:white">
    Made with Pingendo Free&nbsp;&nbsp;<img src="https://pingendo.com/site-assets/Pingendo_logo_big.png" class="d-block"
      alt="Pingendo logo" height="16"></pingendo>
</body>

</html>