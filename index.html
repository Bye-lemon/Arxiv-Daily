<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- PAGE settings -->
  <link rel="icon" href="https://templates.pingendo.com/assets/Pingendo_favicon.ico">
  <title>Arxiv Daily</title>
  <!-- CSS dependencies -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"
    type="text/css">
  <link rel="stylesheet" href="assets/wireframe.css">
</head>

<body class="bg-light">
  <div class="py-5">
    <div class="container">
      <div class="row">
        <div class="text-center col-md-7 mx-auto"> <i class="fa d-block fa-bullseye fa-5x mb-4 text-info"></i>
          <h2><b>Arxiv Daily - 2023-08-16</b></h2>
        </div>
      </div>
    </div>
  </div>
  <div class="py-2">
    <div class="container">
      <div class="row">
        <div class="col-md-12">
          <ul class="nav nav-tabs">
            
            <li class="nav-item"> <a href="" class="nav-link active"
                data-toggle="pill" data-target="#SLAM"><i class="fa fa-home"></i> SLAM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#SFM"><i class="fa fa-home"></i> SFM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#LLM"><i class="fa fa-home"></i> LLM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#SAM"><i class="fa fa-home"></i> SAM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#CLIP"><i class="fa fa-home"></i> CLIP</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#DINO"><i class="fa fa-home"></i> DINO</a>
            </li>
            
          </ul>
          <div class="tab-content mt-2">
            
            <div class="tab-pane fade active show" id="SLAM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2308_07723v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Extended Preintegration for Relative State Estimation of Leader-Follower Platform</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07723v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07723v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07723v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Relative state estimation using exteroceptive sensors suffers from limitations of the field of view (FOV) and false detection, that the proprioceptive sensor (IMU) data are usually engaged to compensate. Recently ego-motion constraint obtained by Inertial measurement unit (IMU) preintegration has been extensively used in simultaneous localization and mapping (SLAM) to alleviate the computation burden. This paper introduces an extended preintegration incorporating the IMU preintegration of two platforms to formulate the motion constraint of relative state. One merit of this analytic constraint is that it can be seamlessly integrated into the unified graph optimization framework to implement the relative state estimation in a high-performance real-time tracking thread, another point is a full smoother design with this precise constraint to optimize the 3D coordinate and refine the state for the refinement thread. We compare extensively in simulations the proposed algorithms with two existing approaches to confirm our outperformance. In the real virtual reality (VR) application design with the proposed estimator, we properly realize the visual tracking of the six degrees of freedom (6DoF) controller suitable for almost all scenarios, including the challenging environment with missing features, light mutation, dynamic scenes, etc. The demo video is at https://www.youtube.com/watch?v=0idb9Ls2iAM. For the benefit of the community, we make the source code public.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07723v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用外部感觉传感器的相对状态估计受到视场（FOV）和错误检测的限制，本体感觉传感器（IMU）数据通常用于补偿这些限制。近年来，通过惯性测量单元（IMU）预集成获得的自我运动约束已被广泛应用于同时定位和映射（SLAM）中，以减轻计算负担。本文引入了一个扩展的预集成，结合了两个平台的IMU预集成来制定相对状态的运动约束。这种分析约束的一个优点是，它可以无缝集成到统一的图形优化框架中，以在高性能实时跟踪线程中实现相对状态估计，另一点是，利用这种精确的约束进行完全平滑的设计，以优化三维坐标并细化细化线程的状态。我们在仿真中广泛比较了所提出的算法与两种现有方法，以确认我们的性能。在使用所提出的估计器的真实虚拟现实（VR）应用程序设计中，我们正确地实现了适用于几乎所有场景的六自由度（6DoF）控制器的视觉跟踪，包括具有缺失特征的挑战性环境、光线突变、动态场景等。演示视频位于https://www.youtube.com/watch?v=0idb9Ls2iAM.为了社区的利益，我们公开了源代码。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07723v1" target="_blank">2308.07723v1</a>
                              </td>
                              <td>Extended Preintegration for Relative State Estimation of Leader-Follower Platform</td>
                              <td>Ruican Xia</td>
                              <td>2023-08-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07723v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07723v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_14325v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BEVPlace: Learning LiDAR-based Place Recognition using Bird's Eye View Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_14325v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_14325v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_14325v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Place recognition is a key module for long-term SLAM systems. Current LiDAR-based place recognition methods usually use representations of point clouds such as unordered points or range images. These methods achieve high recall rates of retrieval, but their performance may degrade in the case of view variation or scene changes. In this work, we explore the potential of a different representation in place recognition, i.e. bird's eye view (BEV) images. We observe that the structural contents of BEV images are less influenced by rotations and translations of point clouds. We validate that, without any delicate design, a simple VGGNet trained on BEV images achieves comparable performance with the state-of-the-art place recognition methods in scenes of slight viewpoint changes. For more robust place recognition, we design a rotation-invariant network called BEVPlace. We use group convolution to extract rotation-equivariant local features from the images and NetVLAD for global feature aggregation. In addition, we observe that the distance between BEV features is correlated with the geometry distance of point clouds. Based on the observation, we develop a method to estimate the position of the query cloud, extending the usage of place recognition. The experiments conducted on large-scale public datasets show that our method 1) achieves state-of-the-art performance in terms of recall rates, 2) is robust to view changes, 3) shows strong generalization ability, and 4) can estimate the positions of query point clouds. Source codes are publicly available at https://github.com/zjuluolun/BEVPlace.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_14325v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>位置识别是长期SLAM系统的一个关键模块。当前基于激光雷达的位置识别方法通常使用点云的表示，如无序点或距离图像。这些方法实现了高的检索召回率，但在视图变化或场景变化的情况下，它们的性能可能会降低。在这项工作中，我们探索了不同的位置识别表示的潜力，即鸟瞰图（BEV）图像。我们观察到，BEV图像的结构内容较少受到点云的旋转和平移的影响。我们验证了，在没有任何精细设计的情况下，在纯电动汽车图像上训练的简单VGGNet在视点轻微变化的场景中实现了与最先进的位置识别方法相当的性能。为了更稳健的位置识别，我们设计了一个称为BEVPlace的旋转不变网络。我们使用组卷积从图像中提取旋转等变局部特征，并使用NetVLAD进行全局特征聚合。此外，我们观察到BEV特征之间的距离与点云的几何距离相关。在观察的基础上，我们开发了一种估计查询云位置的方法，扩展了位置识别的用途。在大规模公共数据集上进行的实验表明，我们的方法1）在召回率方面达到了最先进的性能，2）对视图变化具有鲁棒性，3）具有较强的泛化能力，4）可以估计查询点云的位置。源代码可在https://github.com/zjuluolun/BEVPlace.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.14325v3" target="_blank">2302.14325v3</a>
                              </td>
                              <td>BEVPlace: Learning LiDAR-based Place Recognition using Bird's Eye View Images</td>
                              <td>Lun Luo</td>
                              <td>2023-02-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_14325v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.14325v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07275v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On Semidefinite Relaxations for Matrix-Weighted State-Estimation Problems in Robotics</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07275v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07275v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07275v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, there has been remarkable progress in the development of so-called certifiable perception methods, which leverage semidefinite, convex relaxations to find global optima of perception problems in robotics. However, many of these relaxations rely on simplifying assumptions that facilitate the problem formulation, such as an isotropic measurement noise distribution. In this paper, we explore the tightness of the semidefinite relaxations of matrix-weighted (anisotropic) state-estimation problems and reveal the limitations lurking therein: matrix-weighted factors can cause convex relaxations to lose tightness. In particular, we show that the semidefinite relaxations of localization problems with matrix weights may be tight only for low noise levels. We empirically explore the factors that contribute to this loss of tightness and demonstrate that redundant constraints can be used to regain tightness, albeit at the expense of real-time performance. As a second technical contribution of this paper, we show that the state-of-the-art relaxation of scalar-weighted SLAM cannot be used when matrix weights are considered. We provide an alternate formulation and show that its SDP relaxation is not tight (even for very low noise levels) unless specific redundant constraints are used. We demonstrate the tightness of our formulations on both simulated and real-world data.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07275v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，在开发所谓的可证明感知方法方面取得了显著进展，该方法利用半定凸松弛来寻找机器人感知问题的全局最优解。然而，这些松弛中的许多依赖于简化假设，这些假设有助于问题的公式化，例如各向同性的测量噪声分布。在本文中，我们探索了矩阵加权（各向异性）状态估计问题的半定松弛的紧密性，并揭示了其中隐藏的局限性：矩阵加权因子会导致凸松弛失去紧密性。特别地，我们证明了具有矩阵权重的定位问题的半定松弛可能仅在低噪声水平下是紧的。我们实证研究了导致这种紧密性损失的因素，并证明了冗余约束可以用来恢复紧密性，尽管是以牺牲实时性能为代价的。作为本文的第二个技术贡献，我们证明了在考虑矩阵权重时，不能使用标量加权SLAM的最新松弛。我们提供了一种替代的公式，并表明除非使用特定的冗余约束，否则其SDP弛豫是不严格的（即使对于非常低的噪声水平也是如此）。我们在模拟和真实世界的数据上证明了我们的公式的严密性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07275v1" target="_blank">2308.07275v1</a>
                              </td>
                              <td>On Semidefinite Relaxations for Matrix-Weighted State-Estimation Problems in Robotics</td>
                              <td>Connor Holmes</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07275v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07275v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09531v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LOG-LIO: A LiDAR-Inertial Odometry with Efficient Local Geometric Information Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09531v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09531v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09531v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Local geometric information, i.e. normal and distribution of points, is crucial for LiDAR-based simultaneous localization and mapping (SLAM) because it provides constraints for data association, which further determines the direction of optimization and ultimately affects the accuracy of localization. However, estimating normal and distribution of points are time-consuming tasks even with the assistance of kdtree or volumetric maps. To achieve fast normal estimation, we look into the structure of LiDAR scan and propose a ring-based fast approximate least squares (Ring FALS) method. With the Ring structural information, estimating the normal requires only the range information of the points when a new scan arrives. To efficiently estimate the distribution of points, we extend the ikd-tree to manage the map in voxels and update the distribution of points in each voxel incrementally while maintaining its consistency with the normal estimation. We further fix the distribution after its convergence to balance the time consumption and the correctness of representation. Based on the extracted and maintained local geometric information, we devise a robust and accurate hierarchical data association scheme where point-to-surfel association is prioritized over point-to-plane. Extensive experiments on diverse public datasets demonstrate the advantages of our system compared to other state-of-the-art methods. Our open source implementation is available at https://github.com/tiev-tongji/LOG-LIO.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09531v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>局部几何信息，即点的法线和分布，对于基于激光雷达的同时定位和映射（SLAM）至关重要，因为它为数据关联提供了约束，从而进一步决定了优化的方向，并最终影响定位的准确性。然而，即使在kdtree或体积图的帮助下，估计点的正态和分布也是耗时的任务。为了实现快速正态估计，我们研究了激光雷达扫描的结构，并提出了一种基于环的快速近似最小二乘法（ring-FALS）。利用环形结构信息，当新的扫描到达时，估计法线只需要点的范围信息。为了有效地估计点的分布，我们扩展了ikd树来管理体素中的映射，并在保持其与正常估计的一致性的同时逐步更新每个体素中点的分布。我们在收敛后进一步固定分布，以平衡时间消耗和表示的正确性。基于提取和维护的局部几何信息，我们设计了一种稳健而准确的分层数据关联方案，其中点到表面的关联优先于点到平面。在不同的公共数据集上进行的大量实验表明，与其他最先进的方法相比，我们的系统具有优势。我们的开源实现可在https://github.com/tiev-tongji/LOG-LIO.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09531v2" target="_blank">2307.09531v2</a>
                              </td>
                              <td>LOG-LIO: A LiDAR-Inertial Odometry with Efficient Local Geometric Information Estimation</td>
                              <td>Kai Huang</td>
                              <td>2023-07-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09531v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09531v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2109_06479v5_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Large-scale Autonomous Flight with Real-time Semantic SLAM under Dense Forest Canopy</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2109_06479v5_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2109_06479v5_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2109_06479v5_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Semantic maps represent the environment using a set of semantically meaningful objects. This representation is storage-efficient, less ambiguous, and more informative, thus facilitating large-scale autonomy and the acquisition of actionable information in highly unstructured, GPS-denied environments. In this letter, we propose an integrated system that can perform large-scale autonomous flights and real-time semantic mapping in challenging under-canopy environments. We detect and model tree trunks and ground planes from LiDAR data, which are associated across scans and used to constrain robot poses as well as tree trunk models. The autonomous navigation module utilizes a multi-level planning and mapping framework and computes dynamically feasible trajectories that lead the UAV to build a semantic map of the user-defined region of interest in a computationally and storage efficient manner. A drift-compensation mechanism is designed to minimize the odometry drift using semantic SLAM outputs in real time, while maintaining planner optimality and controller stability. This leads the UAV to execute its mission accurately and safely at scale. Code is released at: https://github.com/KumarRobotics/kr_autonomous_flight and https://github.com/KumarRobotics/sloam.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2109_06479v5_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语义映射使用一组语义上有意义的对象来表示环境。这种表示具有存储效率、不那么模糊、信息量更大的特点，从而有助于在高度非结构化、拒绝GPS的环境中实现大规模自主和可操作信息的获取。在这封信中，我们提出了一个集成系统，该系统可以在具有挑战性的树冠下环境中执行大规模自主飞行和实时语义映射。我们从激光雷达数据中检测树干和地平面并对其进行建模，这些数据在扫描中相互关联，用于约束机器人姿势和树干模型。自主导航模块利用多级规划和映射框架，并计算动态可行的轨迹，这些轨迹引导无人机以计算和存储高效的方式构建用户定义的感兴趣区域的语义图。设计了一种漂移补偿机制，使用语义SLAM输出实时最小化里程计漂移，同时保持规划器的最优性和控制器的稳定性。这使得无人机能够在规模上准确、安全地执行任务。代码发布于：https://github.com/KumarRobotics/kr_autonomous_flight和https://github.com/KumarRobotics/sloam.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2109.06479v5" target="_blank">2109.06479v5</a>
                              </td>
                              <td>Large-scale Autonomous Flight with Real-time Semantic SLAM under Dense Forest Canopy</td>
                              <td>Xu Liu</td>
                              <td>2021-09-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2109_06479v5_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2109.06479v5" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_06573v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">4DRVO-Net: Deep 4D Radar-Visual Odometry Using Multi-Modal and Multi-Scale Adaptive Fusion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_06573v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_06573v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_06573v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Four-dimensional (4D) radar--visual odometry (4DRVO) integrates complementary information from 4D radar and cameras, making it an attractive solution for achieving accurate and robust pose estimation. However, 4DRVO may exhibit significant tracking errors owing to three main factors: 1) sparsity of 4D radar point clouds; 2) inaccurate data association and insufficient feature interaction between the 4D radar and camera; and 3) disturbances caused by dynamic objects in the environment, affecting odometry estimation. In this paper, we present 4DRVO-Net, which is a method for 4D radar--visual odometry. This method leverages the feature pyramid, pose warping, and cost volume (PWC) network architecture to progressively estimate and refine poses. Specifically, we propose a multi-scale feature extraction network called Radar-PointNet++ that fully considers rich 4D radar point information, enabling fine-grained learning for sparse 4D radar point clouds. To effectively integrate the two modalities, we design an adaptive 4D radar--camera fusion module (A-RCFM) that automatically selects image features based on 4D radar point features, facilitating multi-scale cross-modal feature interaction and adaptive multi-modal feature fusion. In addition, we introduce a velocity-guided point-confidence estimation module to measure local motion patterns, reduce the influence of dynamic objects and outliers, and provide continuous updates during pose refinement. We demonstrate the excellent performance of our method and the effectiveness of each module design on both the VoD and in-house datasets. Our method outperforms all learning-based and geometry-based methods for most sequences in the VoD dataset. Furthermore, it has exhibited promising performance that closely approaches that of the 64-line LiDAR odometry results of A-LOAM without mapping optimization.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_06573v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>四维（4D）雷达——视觉里程计（4DRVO）集成了来自4D雷达和相机的互补信息，使其成为实现精确和稳健姿态估计的有吸引力的解决方案。然而，由于三个主要因素，4DRVO可能表现出显著的跟踪误差：1）4D雷达点云的稀疏性；2） 4D雷达和相机之间不准确的数据关联和不充分的特征交互；以及3）由环境中的动态物体引起的干扰，影响里程计估计。在本文中，我们提出了4DRVO-Net，这是一种用于4D雷达的方法——视觉里程计。该方法利用特征金字塔、姿态扭曲和成本体积（PWC）网络架构来逐步估计和细化姿态。具体而言，我们提出了一种称为Radar PointNet++的多尺度特征提取网络，该网络充分考虑了丰富的4D雷达点信息，实现了稀疏4D雷达点云的细粒度学习。为了有效地集成这两种模态，我们设计了一个自适应4D雷达-相机融合模块（A-RCFM），该模块基于4D雷达点特征自动选择图像特征，便于多尺度跨模态特征交互和自适应多模态特征融合。此外，我们引入了一个速度引导点置信度估计模块来测量局部运动模式，减少动态对象和异常值的影响，并在姿态细化过程中提供连续更新。我们在VoD和内部数据集上展示了我们的方法的卓越性能以及每个模块设计的有效性。对于VoD数据集中的大多数序列，我们的方法优于所有基于学习和基于几何的方法。此外，在没有映射优化的情况下，它表现出了非常有希望的性能，与A-LOAM的64线LiDAR里程计结果非常接近。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.06573v1" target="_blank">2308.06573v1</a>
                              </td>
                              <td>4DRVO-Net: Deep 4D Radar-Visual Odometry Using Multi-Modal and Multi-Scale Adaptive Fusion</td>
                              <td>Guirong Zhuo</td>
                              <td>2023-08-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_06573v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.06573v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_06147v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Efficient Large-scale AUV-based Visual Seafloor Mapping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_06147v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_06147v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_06147v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Driven by the increasing number of marine data science applications, there is a growing interest in surveying and exploring the vast, uncharted terrain of the deep sea with robotic platforms. Despite impressive results achieved by many on-land visual mapping algorithms in the past decades, transferring these methods from land to the deep sea remains a challenge due to harsh environmental conditions. Typically, deep-sea exploration involves the use of autonomous underwater vehicles (AUVs) equipped with high-resolution cameras and artificial illumination systems. However, images obtained in this manner often suffer from heterogeneous illumination and quality degradation due to attenuation and scattering, on top of refraction of light rays. All of this together often lets on-land SLAM approaches fail underwater or makes Structure-from-Motion approaches drift or omit difficult images, resulting in gaps, jumps or weakly registered areas. In this work, we present a system that incorporates recent developments in underwater imaging and visual mapping to facilitate automated robotic 3D reconstruction of hectares of seafloor. Our approach is efficient in that it detects and reconsiders difficult, weakly registered areas, to avoid omitting images and to make better use of limited dive time; on the other hand it is computationally efficient; leveraging a hybrid approach combining benefits from SLAM and Structure-from-Motion that runs much faster than incremental reconstructions while achieving at least on-par performance. The proposed system has been extensively tested and evaluated during several research cruises, demonstrating its robustness and practicality in real-world conditions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_06147v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在越来越多的海洋数据科学应用的推动下，人们对使用机器人平台测量和探索广阔、未知的深海地形越来越感兴趣。尽管在过去几十年中，许多陆地视觉地图算法取得了令人印象深刻的成果，但由于恶劣的环境条件，将这些方法从陆地转移到深海仍然是一个挑战。通常，深海探测涉及使用配备高分辨率相机和人工照明系统的自动水下航行器。然而，以这种方式获得的图像除了光线的折射之外，还经常由于衰减和散射而遭受不均匀照明和质量下降。所有这些加在一起通常会使陆上SLAM方法在水下失败，或者使“运动结构”方法漂移或忽略困难的图像，从而导致间隙、跳跃或弱配准区域。在这项工作中，我们提出了一个系统，该系统结合了水下成像和视觉地图的最新发展，以促进机器人对公顷海底的自动3D重建。我们的方法是有效的，因为它检测并重新考虑困难的、弱配准的区域，以避免遗漏图像，并更好地利用有限的潜水时间；另一方面，它在计算上是高效的；利用结合SLAM和Structure from Motion的优点的混合方法，该方法比增量重建运行得快得多，同时至少实现了同等性能。所提出的系统在几次研究巡航中进行了广泛的测试和评估，证明了其在现实世界条件下的稳健性和实用性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.06147v1" target="_blank">2308.06147v1</a>
                              </td>
                              <td>Efficient Large-scale AUV-based Visual Seafloor Mapping</td>
                              <td>Mengkun She</td>
                              <td>2023-08-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_06147v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.06147v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_05504v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Detecting and Classifying Bio-Inspired Artificial Landmarks Using In-Air 3D Sonar</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_05504v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_05504v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_05504v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Various autonomous applications rely on recognizing specific known landmarks in their environment. For example, Simultaneous Localization And Mapping (SLAM) is an important technique that lays the foundation for many common tasks, such as navigation and long-term object tracking. This entails building a map on the go based on sensory inputs which are prone to accumulating errors. Recognizing landmarks in the environment plays a vital role in correcting these errors and further improving the accuracy of SLAM. The most popular choice of sensors for conducting SLAM today is optical sensors such as cameras or LiDAR sensors. These can use landmarks such as QR codes as a prerequisite. However, such sensors become unreliable in certain conditions, e.g., foggy, dusty, reflective, or glass-rich environments. Sonar has proven to be a viable alternative to manage such situations better. However, acoustic sensors also require a different type of landmark. In this paper, we put forward a method to detect the presence of bio-mimetic acoustic landmarks using support vector machines trained on the frequency bands of the reflecting acoustic echoes using an embedded real-time imaging sonar.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_05504v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>各种自主应用程序依赖于识别其环境中的特定已知地标。例如，同步定位和映射（SLAM）是一种重要的技术，它为许多常见任务（如导航和长期目标跟踪）奠定了基础。这需要根据容易累积错误的感官输入在旅途中构建地图。识别环境中的地标对于纠正这些错误和进一步提高SLAM的准确性起着至关重要的作用。如今，用于进行SLAM的最受欢迎的传感器选择是光学传感器，例如相机或激光雷达传感器。这些可以使用诸如二维码之类的地标作为先决条件。然而，这种传感器在某些条件下变得不可靠，例如雾蒙蒙、多尘、反光或富含玻璃的环境。声纳已经被证明是一种可行的替代方案，可以更好地管理这种情况。然而，声学传感器也需要不同类型的地标。在本文中，我们提出了一种方法，使用嵌入式实时成像声纳在反射声回波的频带上训练的支持向量机来检测生物模拟声学标志的存在。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.05504v2" target="_blank">2308.05504v2</a>
                              </td>
                              <td>Detecting and Classifying Bio-Inspired Artificial Landmarks Using In-Air 3D Sonar</td>
                              <td>Maarten de Backer</td>
                              <td>2023-08-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_05504v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.05504v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_05620v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Robust and Rapidly Deployable Waypoint Navigation Architecture for Long-Duration Operations in GPS-Denied Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_05620v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_05620v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_05620v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>For long-duration operations in GPS-denied environments, accurate and repeatable waypoint navigation is an essential capability. While simultaneous localization and mapping (SLAM) works well for single-session operations, repeated, multi-session operations require robots to navigate to the same spot(s) accurately and precisely each and every time. Localization and navigation errors can build up from one session to the next if they are not accounted for. Localization using a global reference map works well, but there are no publicly available packages for quickly building maps and navigating with them. We propose a new architecture using a combination of two publicly available packages with a newly released package to create a fully functional multi-session navigation system for ground vehicles. The system takes just a few hours from the beginning of the first manual scan to perform autonomous waypoint navigation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_05620v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对于拒绝GPS的环境中的长时间操作，准确和可重复的航路点导航是一项必不可少的能力。虽然同时定位和映射（SLAM）适用于单会话操作，但重复的多会话操作需要机器人每次准确无误地导航到同一地点。如果不考虑本地化和导航错误，它们可能会从一个会话累积到下一个会话。使用全局参考地图的本地化效果很好，但没有公开的包可以快速构建地图并使用它们导航。我们提出了一种新的架构，将两个公开可用的软件包与一个新发布的软件包相结合，为地面车辆创建一个功能齐全的多会话导航系统。该系统从第一次手动扫描开始只需几个小时即可执行自主航路点导航。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.05620v1" target="_blank">2308.05620v1</a>
                              </td>
                              <td>A Robust and Rapidly Deployable Waypoint Navigation Architecture for Long-Duration Operations in GPS-Denied Environments</td>
                              <td>Erik Pearson</td>
                              <td>2023-08-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_05620v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.05620v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_05593v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Robust Lifelong Indoor LiDAR Localization using the Area Graph</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_05593v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_05593v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_05593v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Lifelong indoor localization in a given map is the basis for navigation of autonomous mobile robots. In this letter, we address the problem of robust localization in cluttered indoor environments like office spaces and corridors using 3D LiDAR point clouds in a given Area Graph, which is a hierarchical, topometric semantic map representation that uses polygons to demark areas such as rooms, corridors or buildings. This representation is very compact, can represent different floors of buildings through its hierarchy and provides semantic information that helps with localization, like poses of doors and glass. In contrast to this, commonly used map representations, such as occupancy grid maps or point clouds, lack these features and require frequent updates in response to environmental changes (e.g. moved furniture), unlike our approach, which matches against lifelong architectural features such as walls and doors. For that we apply filtering to remove clutter from the 3D input point cloud and then employ further scoring and weight functions for localization. Given a broad initial guess from WiFi localization, our experiments show that our global localization and the weighted point to line ICP pose tracking perform very well, even when compared to localization and SLAM algorithms that use the current, feature-rich cluttered map for localization.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_05593v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>给定地图中的终身室内定位是自主移动机器人导航的基础。在这封信中，我们在给定的区域图中使用3D激光雷达点云解决了在杂乱的室内环境（如办公空间和走廊）中的稳健定位问题，该图是一种分层的拓扑语义地图表示，使用多边形来划分房间、走廊或建筑物等区域。这种表示非常紧凑，可以通过其层次结构来表示建筑物的不同楼层，并提供有助于定位的语义信息，如门和玻璃的姿势。与此相反，通常使用的地图表示，如占用网格地图或点云，缺乏这些功能，需要根据环境变化（如移动的家具）进行频繁更新，而我们的方法与墙壁和门等终身建筑特征相匹配。为此，我们应用滤波从3D输入点云中去除杂波，然后使用进一步的评分和权重函数进行定位。给定WiFi定位的广泛初始猜测，我们的实验表明，即使与使用当前特征丰富的杂乱地图进行定位的定位和SLAM算法相比，我们的全局定位和加权点对线ICP姿态跟踪也表现得非常好。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.05593v1" target="_blank">2308.05593v1</a>
                              </td>
                              <td>Robust Lifelong Indoor LiDAR Localization using the Area Graph</td>
                              <td>Fujing Xie</td>
                              <td>2023-08-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_05593v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.05593v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_05515v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Mono-hydra: Real-time 3D scene graph construction from monocular camera input with IMU</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_05515v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_05515v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_05515v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The ability of robots to autonomously navigate through 3D environments depends on their comprehension of spatial concepts, ranging from low-level geometry to high-level semantics, such as objects, places, and buildings. To enable such comprehension, 3D scene graphs have emerged as a robust tool for representing the environment as a layered graph of concepts and their relationships. However, building these representations using monocular vision systems in real-time remains a difficult task that has not been explored in depth. This paper puts forth a real-time spatial perception system Mono-Hydra, combining a monocular camera and an IMU sensor setup, focusing on indoor scenarios. However, the proposed approach is adaptable to outdoor applications, offering flexibility in its potential uses. The system employs a suite of deep learning algorithms to derive depth and semantics. It uses a robocentric visual-inertial odometry (VIO) algorithm based on square-root information, thereby ensuring consistent visual odometry with an IMU and a monocular camera. This system achieves sub-20 cm error in real-time processing at 15 fps, enabling real-time 3D scene graph construction using a laptop GPU (NVIDIA 3080). This enhances decision-making efficiency and effectiveness in simple camera setups, augmenting robotic system agility. We make Mono-Hydra publicly available at: https://github.com/UAV-Centre-ITC/Mono_Hydra</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_05515v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>机器人在三维环境中自主导航的能力取决于他们对空间概念的理解，从低级几何到高级语义，如物体、场所和建筑。为了实现这种理解，3D场景图已经成为一种强大的工具，用于将环境表示为概念及其关系的分层图。然而，使用单目视觉系统实时构建这些表示仍然是一项尚未深入探索的艰巨任务。本文提出了一种实时空间感知系统Mono Hydra，将单眼相机和IMU传感器相结合，专注于室内场景。然而，所提出的方法适用于户外应用，为其潜在用途提供了灵活性。该系统采用了一套深度学习算法来推导深度和语义。它使用基于平方根信息的以机器人为中心的视觉惯性里程计（VIO）算法，从而确保IMU和单眼相机的视觉里程计一致。该系统在每秒15帧的实时处理中实现了低于20厘米的误差，从而能够使用笔记本电脑GPU（NVIDIA 3080）构建实时3D场景图。这提高了简单相机设置中的决策效率和有效性，增强了机器人系统的灵活性。我们将Mono Hydra公开发布在：https://github.com/UAV-Centre-ITC/Mono_Hydra</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.05515v1" target="_blank">2308.05515v1</a>
                              </td>
                              <td>Mono-hydra: Real-time 3D scene graph construction from monocular camera input with IMU</td>
                              <td>U. V. B. L. Udugama</td>
                              <td>2023-08-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_05515v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.05515v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_13381v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Cosys-AirSim: A Real-Time Simulation Framework Expanded for Complex Industrial Applications</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_13381v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_13381v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_13381v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Within academia and industry, there has been a need for expansive simulation frameworks that include model-based simulation of sensors, mobile vehicles, and the environment around them. To this end, the modular, real-time, and open-source AirSim framework has been a popular community-built system that fulfills some of those needs. However, the framework required adding systems to serve some complex industrial applications, including designing and testing new sensor modalities, Simultaneous Localization And Mapping (SLAM), autonomous navigation algorithms, and transfer learning with machine learning models. In this work, we discuss the modification and additions to our open-source version of the AirSim simulation framework, including new sensor modalities, vehicle types, and methods to generate realistic environments with changeable objects procedurally. Furthermore, we show the various applications and use cases the framework can serve.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_13381v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在学术界和工业界，一直需要广泛的模拟框架，包括传感器、移动车辆及其周围环境的基于模型的模拟。为此，模块化、实时和开源的AirSim框架已经成为一个流行的社区构建系统，可以满足其中的一些需求。然而，该框架需要添加系统来服务于一些复杂的工业应用，包括设计和测试新的传感器模态、同步定位和映射（SLAM）、自主导航算法以及使用机器学习模型的迁移学习。在这项工作中，我们讨论了对AirSim模拟框架开源版本的修改和添加，包括新的传感器模式、车辆类型和通过程序生成具有可变对象的真实环境的方法。此外，我们还展示了该框架可以提供的各种应用程序和用例。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.13381v3" target="_blank">2303.13381v3</a>
                              </td>
                              <td>Cosys-AirSim: A Real-Time Simulation Framework Expanded for Complex Industrial Applications</td>
                              <td>Wouter Jansen</td>
                              <td>2023-03-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_13381v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.13381v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_05444v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">How-to Augmented Lagrangian on Factor Graphs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_05444v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_05444v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_05444v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Factor graphs are a very powerful graphical representation, used to model many problems in robotics. They are widely spread in the areas of Simultaneous Localization and Mapping (SLAM), computer vision, and localization. In this paper we describe an approach to fill the gap with other areas, such as optimal control, by presenting an extension of Factor Graph Solvers to constrained optimization. The core idea of our method is to encapsulate the Augmented Lagrangian (AL) method in factors of the graph that can be integrated straightforwardly in existing factor graph solvers. We show the generality of our approach by addressing three applications, arising from different areas: pose estimation, rotation synchronization and Model Predictive Control (MPC) of a pseudo-omnidirectional platform. We implemented our approach using C++ and ROS. Besides the generality of the approach, application results show that we can favorably compare against domain specific approaches.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_05444v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>因子图是一种非常强大的图形表示，用于对机器人中的许多问题进行建模。它们广泛应用于同步定位和映射（SLAM）、计算机视觉和定位领域。在本文中，我们描述了一种通过将因子图求解器扩展到约束优化来填补其他领域（如最优控制）空白的方法。我们方法的核心思想是将增广拉格朗日（AL）方法封装在图的因子中，这些因子可以在现有的因子图求解器中直接集成。我们通过解决来自不同领域的三个应用来展示我们方法的通用性：伪全向平台的姿态估计、旋转同步和模型预测控制（MPC）。我们使用C++和ROS实现了我们的方法。除了该方法的通用性之外，应用结果表明，我们可以与特定领域的方法进行比较。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.05444v1" target="_blank">2308.05444v1</a>
                              </td>
                              <td>How-to Augmented Lagrangian on Factor Graphs</td>
                              <td>Barbara Bazzana</td>
                              <td>2023-08-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_05444v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.05444v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_05443v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Occupancy Grid Map to Pose Graph-based Map: Robust BIM-based 2D-LiDAR Localization for Lifelong Indoor Navigation in Changing and Dynamic Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_05443v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_05443v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_05443v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Several studies rely on the de facto standard Adaptive Monte Carlo Localization (AMCL) method to localize a robot in an Occupancy Grid Map (OGM) extracted from a building information model (BIM model). However, most of these studies assume that the BIM model precisely represents the real world, which is rarely true. Discrepancies between the reference BIM model and the real world (Scan-BIM deviations) are not only due to furniture or clutter but also the usual as-planned and as-built deviations that exist with any model created in the design phase. These deviations affect the accuracy of AMCL drastically. This paper proposes an open-source method to generate appropriate Pose Graph-based maps from BIM models for robust 2D-LiDAR localization in changing and dynamic environments. First, 2D OGMs are automatically generated from complex BIM models. These OGMs only represent structural elements allowing indoor autonomous robot navigation. Then, an efficient technique converts these 2D OGMs into Pose Graph-based maps enabling more accurate robot pose tracking. Finally, we leverage the different map representations for accurate, robust localization with a combination of state-of-the-art algorithms. Moreover, we provide a quantitative comparison of various state-of-the-art localization algorithms in three simulated scenarios with varying levels of Scan-BIM deviations and dynamic agents. More precisely, we compare two Particle Filter (PF) algorithms: AMCL and General Monte Carlo Localization (GMCL); and two Graph-based Localization (GBL) methods: Google's Cartographer and SLAM Toolbox, solving the global localization and pose tracking problems. The numerous experiments demonstrate that the proposed method contributes to a robust localization with an as-designed BIM model or a sparse OGM in changing and dynamic environments, outperforming the conventional AMCL in accuracy and robustness.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_05443v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>一些研究依赖于事实上的标准自适应蒙特卡罗定位（AMCL）方法来在从建筑信息模型（BIM模型）中提取的占用网格图（OGM）中定位机器人。然而，这些研究大多认为BIM模型准确地代表了现实世界，这很少是真的。参考BIM模型与现实世界之间的差异（扫描BIM偏差）不仅是由于家具或杂物造成的，而且是设计阶段创建的任何模型存在的通常的计划偏差和竣工偏差。这些偏差极大地影响了AMCL的准确性。本文提出了一种开源方法，从BIM模型中生成合适的基于姿势图的地图，用于在变化和动态环境中进行稳健的2D激光雷达定位。首先，二维OGM是从复杂的BIM模型中自动生成的。这些OGM仅代表允许室内自主机器人导航的结构元件。然后，一种有效的技术将这些2D OGM转换为基于姿势图的地图，从而实现更准确的机器人姿势跟踪。最后，我们结合最先进的算法，利用不同的地图表示进行准确、稳健的定位。此外，我们在三个模拟场景中对各种最先进的定位算法进行了定量比较，这些场景具有不同水平的扫描BIM偏差和动态代理。更准确地说，我们比较了两种粒子滤波器（PF）算法：AMCL和通用蒙特卡罗定位（GMCL）；以及两种基于图的定位（GBL）方法：谷歌的制图器和SLAM工具箱，解决了全局定位和姿态跟踪问题。大量实验表明，所提出的方法有助于在变化和动态环境中使用设计的BIM模型或稀疏OGM进行稳健定位，在准确性和稳健性方面优于传统的AMCL。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.05443v1" target="_blank">2308.05443v1</a>
                              </td>
                              <td>Occupancy Grid Map to Pose Graph-based Map: Robust BIM-based 2D-LiDAR Localization for Lifelong Indoor Navigation in Changing and Dynamic Environments</td>
                              <td>Miguel Arturo Vega Torres</td>
                              <td>2023-08-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_05443v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.05443v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_14283v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GAPSLAM: Blending Gaussian Approximation and Particle Filters for Real-Time Non-Gaussian SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_14283v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_14283v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_14283v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Inferring the posterior distribution in SLAM is critical for evaluating the uncertainty in localization and mapping, as well as supporting subsequent planning tasks aiming to reduce uncertainty for safe navigation. However, real-time full posterior inference techniques, such as Gaussian approximation and particle filters, either lack expressiveness for representing non-Gaussian posteriors or suffer from performance degeneracy when estimating high-dimensional posteriors. Inspired by the complementary strengths of Gaussian approximation and particle filters$\unicode{x2013}$scalability and non-Gaussian estimation, respectively$\unicode{x2013}$we blend these two approaches to infer marginal posteriors in SLAM. Specifically, Gaussian approximation provides robot pose distributions on which particle filters are conditioned to sample landmark marginals. In return, the maximum a posteriori point among these samples can be used to reset linearization points in the nonlinear optimization solver of the Gaussian approximation, facilitating the pursuit of global optima. We demonstrate the scalability, generalizability, and accuracy of our algorithm for real-time full posterior inference on realworld range-only SLAM and object-based bearing-only SLAM datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_14283v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>推断SLAM中的后验分布对于评估定位和映射的不确定性以及支持旨在减少安全导航不确定性的后续规划任务至关重要。然而，实时全后验推理技术，如高斯近似和粒子滤波器，要么缺乏表示非高斯后验的表现力，要么在估计高维后验时存在性能退化。受高斯近似和粒子滤波器$\unicode{x2013}$可伸缩性和非高斯估计的互补优势的启发，我们分别将这两种方法混合在一起，以推断SLAM中的边际后验。具体而言，高斯近似提供了机器人姿态分布，粒子滤波器根据该分布对地标边缘进行采样。作为回报，这些样本中的最大后验点可以用于重置高斯近似的非线性优化求解器中的线性化点，从而促进全局最优的追求。我们展示了我们的算法在真实世界的仅测距SLAM和基于对象的仅方位SLAM数据集上进行实时全后验推理的可扩展性、可推广性和准确性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.14283v2" target="_blank">2303.14283v2</a>
                              </td>
                              <td>GAPSLAM: Blending Gaussian Approximation and Particle Filters for Real-Time Non-Gaussian SLAM</td>
                              <td>Qiangqiang Huang</td>
                              <td>2023-03-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_14283v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.14283v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_04000v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-level Map Construction for Dynamic Scenes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_04000v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_04000v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_04000v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In dynamic scenes, both localization and mapping in visual SLAM face significant challenges. In recent years, numerous outstanding research works have proposed effective solutions for the localization problem. However, there has been a scarcity of excellent works focusing on constructing long-term consistent maps in dynamic scenes, which severely hampers map applications. To address this issue, we have designed a multi-level map construction system tailored for dynamic scenes. In this system, we employ multi-object tracking algorithms, DBSCAN clustering algorithm, and depth information to rectify the results of object detection, accurately extract static point clouds, and construct dense point cloud maps and octree maps. We propose a plane map construction algorithm specialized for dynamic scenes, involving the extraction, filtering, data association, and fusion optimization of planes in dynamic environments, thus creating a plane map. Additionally, we introduce an object map construction algorithm targeted at dynamic scenes, which includes object parameterization, data association, and update optimization. Extensive experiments on public datasets and real-world scenarios validate the accuracy of the multi-level maps constructed in this study and the robustness of the proposed algorithms. Furthermore, we demonstrate the practical application prospects of our algorithms by utilizing the constructed object maps for dynamic object tracking.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_04000v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在动态场景中，视觉SLAM中的定位和映射都面临着重大挑战。近年来，许多优秀的研究工作为本地化问题提出了有效的解决方案。然而，一直缺乏专注于在动态场景中构建长期一致地图的优秀作品，这严重阻碍了地图应用。为了解决这个问题，我们设计了一个针对动态场景的多层次地图构建系统。在该系统中，我们使用多目标跟踪算法、DBSCAN聚类算法和深度信息来校正目标检测结果，准确提取静态点云，并构建密集点云图和八叉树图。我们提出了一种专门用于动态场景的平面图构建算法，包括动态环境中平面的提取、过滤、数据关联和融合优化，从而创建平面图。此外，我们还介绍了一种针对动态场景的对象地图构建算法，包括对象参数化、数据关联和更新优化。在公共数据集和真实世界场景上进行的大量实验验证了本研究构建的多级地图的准确性和所提出算法的稳健性。此外，我们还利用构建的目标图进行动态目标跟踪，展示了我们算法的实际应用前景。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.04000v1" target="_blank">2308.04000v1</a>
                              </td>
                              <td>Multi-level Map Construction for Dynamic Scenes</td>
                              <td>Xinggang Hu</td>
                              <td>2023-08-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_04000v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.04000v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_02799v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">VoxelMap++: Mergeable Voxel Mapping Method for Online LiDAR(-inertial) Odometry</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_02799v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_02799v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_02799v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents VoxelMap++: a voxel mapping method with plane merging which can effectively improve the accuracy and efficiency of LiDAR(-inertial) based simultaneous localization and mapping (SLAM). This map is a collection of voxels that contains one plane feature with 3DOF representation and corresponding covariance estimation. Considering total map will contain a large number of coplanar features (kid planes), these kid planes' 3DOF estimation can be regarded as the measurements with covariance of a larger plane (father plane). Thus, we design a plane merging module based on union-find which can save resources and further improve the accuracy of plane fitting. This module can distinguish the kid planes in different voxels and merge these kid planes to estimate the father plane. After merging, the father plane 3DOF representation will be more accurate than the kids plane and the uncertainty will decrease significantly which can further improve the performance of LiDAR(-inertial) odometry. Experiments on challenging environments such as corridors and forests demonstrate the high accuracy and efficiency of our method compared to other state-of-the-art methods (see our attached video). By the way, our implementation VoxelMap++ is open-sourced on GitHub which is applicable for both non-repetitive scanning LiDARs and traditional scanning LiDAR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_02799v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种具有平面合并的体素映射方法VoxelMap++，它可以有效地提高基于激光雷达（惯性）的同时定位和映射（SLAM）的精度和效率。该图是体素的集合，其包含具有3DOF表示和相应协方差估计的一个平面特征。考虑到全图将包含大量共面特征（子平面），这些子平面的3DOF估计可以被视为具有较大平面（父平面）协方差的测量。因此，我们设计了一个基于并集查找的平面合并模块，可以节省资源，进一步提高平面拟合的精度。该模块可以区分不同体素中的子平面，并将这些子平面合并以估计父平面。合并后，父平面3DOF表示将比子平面更准确，并且不确定性将显著降低，这可以进一步提高激光雷达（惯性）里程计的性能。在走廊和森林等具有挑战性的环境中进行的实验表明，与其他最先进的方法相比，我们的方法具有很高的准确性和效率（见我们随附的视频）。顺便说一句，我们的VoxelMap++实现是在GitHub上开源的，适用于非重复扫描激光雷达和传统扫描激光雷达。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.02799v1" target="_blank">2308.02799v1</a>
                              </td>
                              <td>VoxelMap++: Mergeable Voxel Mapping Method for Online LiDAR(-inertial) Odometry</td>
                              <td>Yifei Yuan</td>
                              <td>2023-08-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_02799v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.02799v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_02670v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">EDI: ESKF-based Disjoint Initialization for Visual-Inertial SLAM Systems</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_02670v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_02670v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_02670v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual-inertial initialization can be classified into joint and disjoint approaches. Joint approaches tackle both the visual and the inertial parameters together by aligning observations from feature-bearing points based on IMU integration then use a closed-form solution with visual and acceleration observations to find initial velocity and gravity. In contrast, disjoint approaches independently solve the Structure from Motion (SFM) problem and determine inertial parameters from up-to-scale camera poses obtained from pure monocular SLAM. However, previous disjoint methods have limitations, like assuming negligible acceleration bias impact or accurate rotation estimation by pure monocular SLAM. To address these issues, we propose EDI, a novel approach for fast, accurate, and robust visual-inertial initialization. Our method incorporates an Error-state Kalman Filter (ESKF) to estimate gyroscope bias and correct rotation estimates from monocular SLAM, overcoming dependence on pure monocular SLAM for rotation estimation. To estimate the scale factor without prior information, we offer a closed-form solution for initial velocity, scale, gravity, and acceleration bias estimation. To address gravity and acceleration bias coupling, we introduce weights in the linear least-squares equations, ensuring acceleration bias observability and handling outliers. Extensive evaluation on the EuRoC dataset shows that our method achieves an average scale error of 5.8% in less than 3 seconds, outperforming other state-of-the-art disjoint visual-inertial initialization approaches, even in challenging environments and with artificial noise corruption.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_02670v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉惯性初始化可以分为联合方法和不相交方法。联合方法通过基于IMU积分对齐特征承载点的观测结果，将视觉和惯性参数结合在一起，然后使用视觉和加速度观测的闭合形式解来找到初始速度和重力。相反，不相交的方法独立地解决了运动结构（SFM）问题，并根据从纯单目SLAM获得的高比例相机姿态确定惯性参数。然而，以前的不相交方法有局限性，比如假设加速度偏差影响可以忽略不计，或者通过纯单目SLAM进行精确的旋转估计。为了解决这些问题，我们提出了EDI，这是一种快速、准确和稳健的视觉惯性初始化的新方法。我们的方法结合了误差状态卡尔曼滤波器（ESKF）来估计陀螺仪偏差，并校正单目SLAM的旋转估计，克服了对纯单目SLAM旋转估计的依赖。为了在没有先验信息的情况下估计比例因子，我们为初始速度、比例、重力和加速度偏差估计提供了一个闭合形式的解决方案。为了解决重力和加速度偏差的耦合问题，我们在线性最小二乘方程中引入了权重，以确保加速度偏差的可观察性并处理异常值。对EuRoC数据集的广泛评估表明，我们的方法在不到3秒内实现了5.8%的平均尺度误差，即使在具有挑战性的环境和人工噪声破坏的情况下，也优于其他最先进的不相交视觉惯性初始化方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.02670v1" target="_blank">2308.02670v1</a>
                              </td>
                              <td>EDI: ESKF-based Disjoint Initialization for Visual-Inertial SLAM Systems</td>
                              <td>Weihan Wang</td>
                              <td>2023-08-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_02670v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.02670v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_07595v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AirVO: An Illumination-Robust Point-Line Visual Odometry</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_07595v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_07595v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_07595v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper proposes an illumination-robust visual odometry (VO) system that incorporates both accelerated learning-based corner point algorithms and an extended line feature algorithm. To be robust to dynamic illumination, the proposed system employs the convolutional neural network (CNN) and graph neural network (GNN) to detect and match reliable and informative corner points. Then point feature matching results and the distribution of point and line features are utilized to match and triangulate lines. By accelerating CNN and GNN parts and optimizing the pipeline, the proposed system is able to run in real-time on low-power embedded platforms. The proposed VO was evaluated on several datasets with varying illumination conditions, and the results show that it outperforms other state-of-the-art VO systems in terms of accuracy and robustness. The open-source nature of the proposed system allows for easy implementation and customization by the research community, enabling further development and improvement of VO for various applications.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_07595v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种照明鲁棒视觉里程计（VO）系统，该系统结合了基于加速学习的角点算法和扩展线特征算法。为了对动态照明具有鲁棒性，该系统采用卷积神经网络（CNN）和图神经网络（GNN）来检测和匹配可靠且信息丰富的角点。然后利用点特征匹配结果以及点和线特征的分布对线进行匹配和三角化。通过加速CNN和GNN部件并优化管道，所提出的系统能够在低功耗嵌入式平台上实时运行。在几个具有不同照明条件的数据集上对所提出的VO进行了评估，结果表明，它在准确性和鲁棒性方面优于其他最先进的VO系统。所提出的系统的开源性质允许研究界轻松实施和定制，从而能够为各种应用程序进一步开发和改进VO。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.07595v3" target="_blank">2212.07595v3</a>
                              </td>
                              <td>AirVO: An Illumination-Robust Point-Line Visual Odometry</td>
                              <td>Kuan Xu</td>
                              <td>2022-12-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_07595v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.07595v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_16500v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AirLine: Efficient Learnable Line Detection with Local Edge Voting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_16500v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_16500v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_16500v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Line detection is widely used in many robotic tasks such as scene recognition, 3D reconstruction, and simultaneous localization and mapping (SLAM). Compared to points, lines can provide both low-level and high-level geometrical information for downstream tasks. In this paper, we propose a novel learnable edge-based line detection algorithm, AirLine, which can be applied to various tasks. In contrast to existing learnable endpoint-based methods, which are sensitive to the geometrical condition of environments, AirLine can extract line segments directly from edges, resulting in a better generalization ability for unseen environments. To balance efficiency and accuracy, we introduce a region-grow algorithm and a local edge voting scheme for line parameterization. To the best of our knowledge, AirLine is one of the first learnable edge-based line detection methods. Our extensive experiments have shown that it retains state-of-the-art-level precision, yet with a 3 to 80 times runtime acceleration compared to other learning-based methods, which is critical for low-power robots.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_16500v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>直线检测被广泛应用于许多机器人任务中，如场景识别、三维重建和同时定位与映射（SLAM）。与点相比，线可以为下游任务提供低级和高级几何信息。在本文中，我们提出了一种新的可学习的基于边缘的直线检测算法AirLine，该算法可以应用于各种任务。与现有的对环境几何条件敏感的基于端点的可学习方法相比，AirLine可以直接从边缘提取线段，从而对看不见的环境具有更好的泛化能力。为了平衡效率和准确性，我们引入了一种区域增长算法和一种用于线参数化的局部边缘投票方案。据我们所知，AirLine是首批可学习的基于边缘的线路检测方法之一。我们的大量实验表明，它保持了最先进的精度，但与其他基于学习的方法相比，运行时加速是其他方法的3到80倍，这对低功耗机器人至关重要。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.16500v2" target="_blank">2303.16500v2</a>
                              </td>
                              <td>AirLine: Efficient Learnable Line Detection with Local Edge Voting</td>
                              <td>Xiao Lin</td>
                              <td>2023-03-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_16500v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.16500v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01553v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Uncertainty analysis for accurate ground truth trajectories with robotic total stations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01553v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01553v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01553v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In the context of robotics, accurate ground truth positioning is essential for the development of Simultaneous Localization and Mapping (SLAM) and control algorithms. Robotic Total Stations (RTSs) provide accurate and precise reference positions in different types of outdoor environments, especially when compared to the limited accuracy of Global Navigation Satellite System (GNSS) in cluttered areas. Three RTSs give the possibility to obtain the six-Degrees Of Freedom (DOF) reference pose of a robotic platform. However, the uncertainty of every pose is rarely computed for trajectory evaluation. As evaluation algorithms are getting increasingly precise, it becomes crucial to take into account this uncertainty. We propose a method to compute this six-DOF uncertainty from the fusion of three RTSs based on Monte Carlo (MC) methods. This solution relies on point-to-point minimization to propagate the noise of RTSs on the pose of the robotic platform. Five main noise sources are identified to model this uncertainty: noise inherent to the instrument, tilt noise, atmospheric factors, time synchronization noise, and extrinsic calibration noise. Based on extensive experimental work, we compare the impact of each noise source on the prism uncertainty and the final estimated pose. Tested on more than 50 km of trajectories, our comparison highlighted the importance of the calibration noise and the measurement distance, which should be ideally under 75 m. Moreover, it has been noted that the uncertainty on the pose of the robot is not prominently affected by one particular noise source, compared to the others.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01553v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在机器人技术的背景下，准确的地面实况定位对于同步定位和映射（SLAM）和控制算法的开发至关重要。机器人全站仪（RTS）在不同类型的户外环境中提供精确的参考位置，尤其是与全球导航卫星系统（GNSS）在杂乱区域的有限精度相比。三个RTS提供了获得机器人平台的六自由度（DOF）参考姿态的可能性。然而，很少为轨迹评估计算每个姿势的不确定性。随着评估算法越来越精确，考虑这种不确定性变得至关重要。我们提出了一种基于蒙特卡罗（MC）方法计算三个RTS融合的六自由度不确定性的方法。该解决方案依赖于点对点最小化来在机器人平台的姿态上传播RTS的噪声。确定了五个主要的噪声源来模拟这种不确定性：仪器固有的噪声、倾斜噪声、大气因素、时间同步噪声和外部校准噪声。基于大量的实验工作，我们比较了每个噪声源对棱镜不确定性和最终估计姿态的影响。在50多公里的轨迹上进行了测试，我们的比较强调了校准噪声和测量距离的重要性，理想情况下测量距离应在75米以下。此外，值得注意的是，与其他噪声源相比，机器人姿态的不确定性不受一个特定噪声源的显著影响。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01553v1" target="_blank">2308.01553v1</a>
                              </td>
                              <td>Uncertainty analysis for accurate ground truth trajectories with robotic total stations</td>
                              <td>Maxime Vaidis</td>
                              <td>2023-08-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01553v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01553v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01398v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Small Form Factor Aerial Research Vehicle for Pick-and-Place Tasks with Onboard Real-Time Object Detection and Visual Odometry</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01398v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01398v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01398v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper introduces a novel, small form-factor, aerial vehicle research platform for agile object detection, classification, tracking, and interaction tasks. General-purpose hardware components were designed to augment a given aerial vehicle and enable it to perform safe and reliable grasping. These components include a custom collision tolerant cage and low-cost Gripper Extension Package, which we call GREP, for object grasping. Small vehicles enable applications in highly constrained environments, but are often limited by computational resources. This work evaluates the challenges of pick-and-place tasks, with entirely onboard computation of object pose and visual odometry based state estimation on a small platform, and demonstrates experiments with enough accuracy to reliably grasp objects. In a total of 70 trials across challenging cases such as cluttered environments, obstructed targets, and multiple instances of the same target, we demonstrated successfully grasping the target in 93% of trials. Both the hardware component designs and software framework are released as open-source, since our intention is to enable easy reproduction and application on a wide range of small vehicles.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01398v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了一种新颖的小型飞行器研究平台，用于敏捷物体的检测、分类、跟踪和交互任务。通用硬件组件的设计是为了增强给定的飞行器，使其能够进行安全可靠的抓取。这些组件包括一个自定义的防撞笼和低成本的Gripper扩展包，我们称之为GREP，用于抓取物体。小型车辆能够在高度受限的环境中应用，但通常受到计算资源的限制。这项工作评估了拾取和放置任务的挑战，在小型平台上对物体姿态进行了完全车载计算，并基于视觉里程计进行了状态估计，并展示了具有足够精度的实验，以可靠地抓取物体。在总共70项具有挑战性的试验中，如杂乱的环境、障碍目标和同一目标的多个实例，我们在93%的试验中成功地抓住了目标。硬件组件设计和软件框架都是开源的，因为我们的目的是在各种小型车辆上轻松复制和应用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01398v1" target="_blank">2308.01398v1</a>
                              </td>
                              <td>A Small Form Factor Aerial Research Vehicle for Pick-and-Place Tasks with Onboard Real-Time Object Detection and Visual Odometry</td>
                              <td>Cora A. Dimmig</td>
                              <td>2023-08-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01398v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01398v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07607v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SubT-MRS: A Subterranean, Multi-Robot, Multi-Spectral and Multi-Degraded Dataset for Robust SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07607v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07607v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07607v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, significant progress has been made in the field of simultaneous localization and mapping (SLAM) research. However, current state-of-the-art solutions still struggle with limited accuracy and robustness in real-world applications. One major reason is the lack of datasets that fully capture the conditions faced by robots in the wild. To address this problem, we present SubT-MRS, an extremely challenging real-world dataset designed to push the limits of SLAM and perception algorithms.   SubT-MRS is a multi-modal, multi-robot dataset collected mainly from subterranean environments having multi-degraded conditions including structureless corridors, varying lighting conditions, and perceptual obscurants such as smoke and dust. Furthermore, the dataset packages information from a diverse range of time-synchronized sensors, including LiDAR, visual cameras, thermal cameras, and IMUs captured using varied vehicular motions like aerial, legged, and wheeled, to support research in sensor fusion, which is essential for achieving accurate and robust robotic perception in complex environments. To evaluate the accuracy of SLAM systems, we also provide a dense 3D model with sub-centimeter-level accuracy, as well as accurate 6DoF ground truth. Our benchmarking approach includes several state-of-the-art methods to demonstrate the challenges our datasets introduce, particularly in the case of multi-degraded environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07607v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，同步定位与映射（SLAM）研究领域取得了重大进展。然而，当前最先进的解决方案在现实应用中仍然难以达到有限的准确性和稳健性。一个主要原因是缺乏能够完全捕捉机器人在野外所面临状况的数据集。为了解决这个问题，我们提出了SubT-MRS，这是一个极具挑战性的真实世界数据集，旨在突破SLAM和感知算法的极限。SubT-MRS是一个多模态、多机器人数据集，主要从具有多重退化条件的地下环境中收集，包括无结构走廊、不同的照明条件和烟雾和灰尘等感知遮蔽物。此外，该数据集封装了来自各种时间同步传感器的信息，包括激光雷达、视觉相机、热像仪和使用各种车辆运动（如空中、腿部和轮式）捕获的IMU，以支持传感器融合研究，这对于在复杂环境中实现准确和稳健的机器人感知至关重要。为了评估SLAM系统的精度，我们还提供了一个亚厘米级精度的密集3D模型，以及精确的6DoF地面实况。我们的基准测试方法包括几种最先进的方法，以证明我们的数据集带来的挑战，特别是在多重退化环境的情况下。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07607v2" target="_blank">2307.07607v2</a>
                              </td>
                              <td>SubT-MRS: A Subterranean, Multi-Robot, Multi-Spectral and Multi-Degraded Dataset for Robust SLAM</td>
                              <td>Shibo Zhao</td>
                              <td>2023-07-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07607v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07607v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01125v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Stereo Visual Odometry with Deep Learning-Based Point and Line Feature Matching using an Attention Graph Neural Network</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01125v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01125v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01125v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Robust feature matching forms the backbone for most Visual Simultaneous Localization and Mapping (vSLAM), visual odometry, 3D reconstruction, and Structure from Motion (SfM) algorithms. However, recovering feature matches from texture-poor scenes is a major challenge and still remains an open area of research. In this paper, we present a Stereo Visual Odometry (StereoVO) technique based on point and line features which uses a novel feature-matching mechanism based on an Attention Graph Neural Network that is designed to perform well even under adverse weather conditions such as fog, haze, rain, and snow, and dynamic lighting conditions such as nighttime illumination and glare scenarios. We perform experiments on multiple real and synthetic datasets to validate the ability of our method to perform StereoVO under low visibility weather and lighting conditions through robust point and line matches. The results demonstrate that our method achieves more line feature matches than state-of-the-art line matching algorithms, which when complemented with point feature matches perform consistently well in adverse weather and dynamic lighting conditions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01125v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>稳健的特征匹配构成了大多数视觉同步定位和映射（vSLAM）、视觉里程计、3D重建和运动结构（SfM）算法的支柱。然而，从纹理差的场景中恢复特征匹配是一个重大挑战，并且仍然是一个开放的研究领域。在本文中，我们提出了一种基于点和线特征的立体视觉Odometry（StereoVO）技术，该技术使用了一种新的基于注意力图神经网络的特征匹配机制，即使在雾、霾、雨和雪等恶劣天气条件以及夜间照明和眩光等动态照明条件下也能表现良好。我们在多个真实和合成数据集上进行了实验，以验证我们的方法通过稳健的点和线匹配在低能见度天气和照明条件下执行StereoVO的能力。结果表明，与最先进的线匹配算法相比，我们的方法实现了更多的线特征匹配，当与点特征匹配相补充时，线匹配算法在恶劣天气和动态照明条件下始终表现良好。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01125v1" target="_blank">2308.01125v1</a>
                              </td>
                              <td>Stereo Visual Odometry with Deep Learning-Based Point and Line Feature Matching using an Attention Graph Neural Network</td>
                              <td>Shenbagaraj Kannapiran</td>
                              <td>2023-08-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01125v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01125v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_13513v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Preliminary Design of the Dragonfly Navigation Filter</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_13513v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_13513v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_13513v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Dragonfly is scheduled to begin exploring Titan by 2034 using a series of multi-kilometer surface flights. This paper outlines the preliminary design of the navigation filter for the Dragonfly Mobility subsystem. The software architecture and filter formulation for lidar, visual odometry, pressure sensors, and redundant IMUs are described in detail. Special discussion is given to developments to achieve multi-kilometer surface flights, including optimizing sequential image baselines, modeling correlating image processing errors, and an efficient approximation to the Simultaneous Localization and Mapping (SLAM) problem.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_13513v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Dragonfly计划在2034年开始探索泰坦，进行一系列长达数公里的地面飞行。本文概述了Dragonfly Mobility子系统导航滤波器的初步设计。详细描述了激光雷达、视觉里程计、压力传感器和冗余IMU的软件架构和滤波器公式。特别讨论了实现多公里地面飞行的发展，包括优化顺序图像基线、建模相关图像处理误差，以及同时定位和映射（SLAM）问题的有效近似。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.13513v2" target="_blank">2307.13513v2</a>
                              </td>
                              <td>Preliminary Design of the Dragonfly Navigation Filter</td>
                              <td>Ben Schilling</td>
                              <td>2023-07-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_13513v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.13513v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_04036v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NR-SLAM: Non-Rigid Monocular SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_04036v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_04036v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_04036v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we present NR-SLAM, a novel non-rigid monocular SLAM system founded on the combination of a Dynamic Deformation Graph with a Visco-Elastic deformation model. The former enables our system to represent the dynamics of the deforming environment as the camera explores, while the later allows us to model general deformations in a simple way. The presented system is able to automatically initialize and extend a map modeled by a sparse point cloud in deforming environments, that is refined with a sliding-window Deformable Bundle Adjustment. This map serves as base for the estimation of the camera motion and deformation and enables us to represent arbitrary surface topologies, overcoming the limitations of previous methods. To assess the performance of our system in challenging deforming scenarios, we evaluate it in several representative medical datasets. In our experiments, NR-SLAM outperforms previous deformable SLAM systems, achieving millimeter reconstruction accuracy and bringing automated medical intervention closer. For the benefit of the community, we make the source code public.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_04036v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了NR-SLAM，这是一种基于动态变形图和粘弹性变形模型的新型非刚性单目SLAM系统。前者使我们的系统能够在相机探索时表示变形环境的动力学，而后者使我们能够以简单的方式对一般变形进行建模。所提出的系统能够在变形环境中自动初始化和扩展由稀疏点云建模的地图，该地图通过滑动窗口可变形束调整进行细化。该地图是估计相机运动和变形的基础，使我们能够表示任意表面拓扑，克服了以前方法的局限性。为了评估我们的系统在具有挑战性的变形场景中的性能，我们在几个具有代表性的医学数据集中对其进行了评估。在我们的实验中，NR-SLAM优于以前的可变形SLAM系统，实现了毫米级重建精度，并使自动化医疗干预更加接近。为了社区的利益，我们公开了源代码。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.04036v1" target="_blank">2308.04036v1</a>
                              </td>
                              <td>NR-SLAM: Non-Rigid Monocular SLAM</td>
                              <td>Juan J. Gomez Rodriguez</td>
                              <td>2023-08-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_04036v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.04036v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_00235v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Demonstrating Autonomous 3D Path Planning on a Novel Scalable UGV-UAV Morphing Robot</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_00235v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_00235v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_00235v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Some animals exhibit multi-modal locomotion capability to traverse a wide range of terrains and environments, such as amphibians that can swim and walk or birds that can fly and walk. This capability is extremely beneficial for expanding the animal's habitat range and they can choose the most energy efficient mode of locomotion in a given environment. The robotic biomimicry of this multi-modal locomotion capability can be very challenging but offer the same advantages. However, the expanded range of locomotion also increases the complexity of performing localization and path planning. In this work, we present our morphing multi-modal robot, which is capable of ground and aerial locomotion, and the implementation of readily available SLAM and path planning solutions to navigate a complex indoor environment.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_00235v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>一些动物表现出多模式运动能力，可以穿越各种地形和环境，例如会游泳和行走的两栖动物或会飞行和行走的鸟类。这种能力对扩大动物的栖息地范围非常有益，它们可以在特定环境中选择最节能的运动模式。这种多模式运动能力的机器人仿生可能非常具有挑战性，但也具有相同的优势。然而，移动范围的扩大也增加了执行定位和路径规划的复杂性。在这项工作中，我们介绍了我们的变形多模态机器人，它能够进行地面和空中运动，并实现了现成的SLAM和路径规划解决方案，以在复杂的室内环境中导航。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.00235v1" target="_blank">2308.00235v1</a>
                              </td>
                              <td>Demonstrating Autonomous 3D Path Planning on a Novel Scalable UGV-UAV Morphing Robot</td>
                              <td>Eric Sihite</td>
                              <td>2023-08-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_00235v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.00235v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_16710v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning whom to trust in navigation: dynamically switching between classical and neural planning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_16710v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_16710v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_16710v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Navigation of terrestrial robots is typically addressed either with localization and mapping (SLAM) followed by classical planning on the dynamically created maps, or by machine learning (ML), often through end-to-end training with reinforcement learning (RL) or imitation learning (IL). Recently, modular designs have achieved promising results, and hybrid algorithms that combine ML with classical planning have been proposed. Existing methods implement these combinations with hand-crafted functions, which cannot fully exploit the complementary nature of the policies and the complex regularities between scene structure and planning performance. Our work builds on the hypothesis that the strengths and weaknesses of neural planners and classical planners follow some regularities, which can be learned from training data, in particular from interactions. This is grounded on the assumption that, both, trained planners and the mapping algorithms underlying classical planning are subject to failure cases depending on the semantics of the scene and that this dependence is learnable: for instance, certain areas, objects or scene structures can be reconstructed easier than others. We propose a hierarchical method composed of a high-level planner dynamically switching between a classical and a neural planner. We fully train all neural policies in simulation and evaluate the method in both simulation and real experiments with a LoCoBot robot, showing significant gains in performance, in particular in the real environment. We also qualitatively conjecture on the nature of data regularities exploited by the high-level planner.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_16710v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>陆地机器人的导航通常通过定位和映射（SLAM），然后在动态创建的地图上进行经典规划，或者通过机器学习（ML），通常通过强化学习（RL）或模仿学习（IL）的端到端训练来解决。最近，模块化设计取得了有希望的结果，并提出了将ML与经典规划相结合的混合算法。现有的方法通过手工制作的功能来实现这些组合，无法充分利用策略的互补性以及场景结构和规划性能之间的复杂规律。我们的工作建立在这样一个假设的基础上，即神经规划者和经典规划者的优势和劣势遵循一些规律，这些规律可以从训练数据中学习，特别是从互动中学习。这是基于这样一种假设，即受过训练的规划者和经典规划背后的映射算法都会根据场景的语义发生故障，并且这种依赖性是可学习的：例如，某些区域、对象或场景结构可以比其他区域、对象和场景结构更容易重建。我们提出了一种由高级规划器在经典规划器和神经规划器之间动态切换组成的分层方法。我们在模拟中充分训练了所有神经策略，并在模拟和实际实验中使用LoCoBot机器人对该方法进行了评估，显示出显著的性能提升，特别是在实际环境中。我们还定性地推测了高级规划师所利用的数据规律的性质。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.16710v1" target="_blank">2307.16710v1</a>
                              </td>
                              <td>Learning whom to trust in navigation: dynamically switching between classical and neural planning</td>
                              <td>Sombit Dey</td>
                              <td>2023-07-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_16710v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.16710v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_05162v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">EVOLIN Benchmark: Evaluation of Line Detection and Association</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_05162v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_05162v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_05162v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Lines are interesting geometrical features commonly seen in indoor and urban environments. There is missing a complete benchmark where one can evaluate lines from a sequential stream of images in all its stages: Line detection, Line Association and Pose error. To do so, we present a complete and exhaustive benchmark for visual lines in a SLAM front-end, both for RGB and RGBD, by providing a plethora of complementary metrics. We have also labelled data from well-known SLAM datasets in order to have all in one poses and accurately annotated lines. In particular, we have evaluated 17 line detection algorithms, 5 line associations methods and the resultant pose error for aligning a pair of frames with several combinations of detector-association. We have packaged all methods and evaluations metrics and made them publicly available on web-page https://prime-slam.github.io/evolin/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_05162v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>线条是室内和城市环境中常见的有趣的几何特征。缺少一个完整的基准，在该基准中，可以评估所有阶段的连续图像流中的线条：线条检测、线条关联和姿势错误。为此，我们通过提供大量补充指标，为SLAM前端的RGB和RGBD视觉线条提供了一个完整而详尽的基准。我们还标记了来自知名SLAM数据集的数据，以便具有一体式姿势和精确注释的线条。特别地，我们评估了17种线检测算法、5种线关联方法以及用于将一对帧与检测器关联的几种组合对准的结果姿态误差。我们已经将所有方法和评估指标打包，并在网页上公开https://prime-slam.github.io/evolin/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.05162v2" target="_blank">2303.05162v2</a>
                              </td>
                              <td>EVOLIN Benchmark: Evaluation of Line Detection and Association</td>
                              <td>Kirill Ivanov</td>
                              <td>2023-03-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_05162v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.05162v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_10561v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LiDAR-Based Place Recognition For Autonomous Driving: A Survey</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_10561v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_10561v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_10561v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>LiDAR-based place recognition (LPR) plays a pivotal role in autonomous driving, which assists Simultaneous Localization and Mapping (SLAM) systems in reducing accumulated errors and achieving reliable localization. However, existing reviews predominantly concentrate on visual place recognition (VPR) methods. Despite the recent remarkable progress in LPR, to the best of our knowledge, there is no dedicated systematic review in this area. This paper bridges the gap by providing a comprehensive review of place recognition methods employing LiDAR sensors, thus facilitating and encouraging further research. We commence by delving into the problem formulation of place recognition, exploring existing challenges, and describing relations to previous surveys. Subsequently, we conduct an in-depth review of related research, which offers detailed classifications, strengths and weaknesses, and architectures. Finally, we summarize existing datasets, commonly used evaluation metrics, and comprehensive evaluation results from various methods on public datasets. This paper can serve as a valuable tutorial for newcomers entering the field of place recognition and for researchers interested in long-term robot localization. We pledge to maintain an up-to-date project on our website https://github.com/ShiPC-AI/LPR-Survey.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_10561v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于激光雷达的位置识别（LPR）在自动驾驶中发挥着关键作用，它有助于同时定位和映射（SLAM）系统减少累积误差，实现可靠的定位。然而，现有的综述主要集中在视觉位置识别（VPR）方法上。尽管LPR最近取得了显著进展，但据我们所知，在这一领域还没有专门的系统审查。本文通过对使用激光雷达传感器的位置识别方法进行全面综述来弥补这一差距，从而促进和鼓励进一步的研究。我们首先深入研究地点识别的问题公式，探索现有的挑战，并描述与以前调查的关系。随后，我们对相关研究进行了深入回顾，提供了详细的分类、优势和劣势以及架构。最后，我们总结了现有的数据集、常用的评估指标，以及在公共数据集上各种方法的综合评估结果。本文可以为进入位置识别领域的新手和对长期机器人定位感兴趣的研究人员提供宝贵的指导。我们保证在我们的网站上保持最新的项目https://github.com/ShiPC-AI/LPR-Survey.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.10561v2" target="_blank">2306.10561v2</a>
                              </td>
                              <td>LiDAR-Based Place Recognition For Autonomous Driving: A Survey</td>
                              <td>Yongjun Zhang</td>
                              <td>2023-06-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_10561v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.10561v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_04747v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Graph-based Optimization Framework for Hand-Eye Calibration for Multi-Camera Setups</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_04747v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_04747v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_04747v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Hand-eye calibration is the problem of estimating the spatial transformation between a reference frame, usually the base of a robot arm or its gripper, and the reference frame of one or multiple cameras. Generally, this calibration is solved as a non-linear optimization problem, what instead is rarely done is to exploit the underlying graph structure of the problem itself. Actually, the problem of hand-eye calibration can be seen as an instance of the Simultaneous Localization and Mapping (SLAM) problem. Inspired by this fact, in this work we present a pose-graph approach to the hand-eye calibration problem that extends a recent state-of-the-art solution in two different ways: i) by formulating the solution to eye-on-base setups with one camera; ii) by covering multi-camera robotic setups. The proposed approach has been validated in simulation against standard hand-eye calibration methods. Moreover, a real application is shown. In both scenarios, the proposed approach overcomes all alternative methods. We release with this paper an open-source implementation of our graph-based optimization framework for multi-camera setups.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_04747v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>手眼校准是估计参考系（通常是机械臂或其夹具的底座）与一个或多个相机的参考系之间的空间变换的问题。通常，这种校准是作为一个非线性优化问题来解决的，相反，很少做的是利用问题本身的底层图结构。实际上，手眼校准问题可以看作是同时定位和映射（SLAM）问题的一个例子。受此启发，在这项工作中，我们提出了一种解决手眼校准问题的姿势图方法，该方法以两种不同的方式扩展了最近最先进的解决方案：i）通过用一台相机将解决方案公式化为基于眼睛的设置；ii）通过覆盖多摄像机机器人设置。所提出的方法已经在模拟中与标准手眼校准方法进行了验证。此外，还展示了一个实际应用。在这两种情况下，所提出的方法都克服了所有备选方法。我们在本文中发布了一个基于图形的多摄像头优化框架的开源实现。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.04747v2" target="_blank">2303.04747v2</a>
                              </td>
                              <td>A Graph-based Optimization Framework for Hand-Eye Calibration for Multi-Camera Setups</td>
                              <td>Daniele Evangelista</td>
                              <td>2023-03-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_04747v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.04747v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_15005v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FLiCR: A Fast and Lightweight LiDAR Point Cloud Compression Based on Lossy RI</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_15005v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_15005v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_15005v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Light detection and ranging (LiDAR) sensors are becoming available on modern mobile devices and provide a 3D sensing capability. This new capability is beneficial for perceptions in various use cases, but it is challenging for resource-constrained mobile devices to use the perceptions in real-time because of their high computational complexity. In this context, edge computing can be used to enable LiDAR online perceptions, but offloading the perceptions on the edge server requires a low-latency, lightweight, and efficient compression due to the large volume of LiDAR point clouds data.   This paper presents FLiCR, a fast and lightweight LiDAR point cloud compression method for enabling edge-assisted online perceptions. FLiCR is based on range images (RI) as an intermediate representation (IR), and dictionary coding for compressing RIs. FLiCR achieves its benefits by leveraging lossy RIs, and we show the efficiency of bytestream compression is largely improved with quantization and subsampling. In addition, we identify the limitation of current quality metrics for presenting the entropy of a point cloud, and introduce a new metric that reflects both point-wise and entropy-wise qualities for lossy IRs. The evaluation results show FLiCR is more suitable for edge-assisted real-time perceptions than the existing LiDAR compressions, and we demonstrate the effectiveness of our compression and metric with the evaluations on 3D object detection and LiDAR SLAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_15005v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>光探测和测距（LiDAR）传感器在现代移动设备上变得可用，并提供3D传感能力。这种新功能有利于各种使用情况下的感知，但由于资源受限的移动设备计算复杂度高，因此实时使用感知具有挑战性。在这种情况下，边缘计算可以用于实现激光雷达在线感知，但由于激光雷达点云数据量大，在边缘服务器上卸载感知需要低延迟、轻量和高效的压缩。本文介绍了FLiCR，这是一种快速、轻量级的激光雷达点云压缩方法，用于实现边缘辅助在线感知。FLiCR基于作为中间表示（IR）的距离图像（RI）和用于压缩RI的字典编码。FLiCR通过利用有损RI来实现其优势，我们表明，通过量化和子采样，字节流压缩的效率大大提高。此外，我们确定了当前质量度量用于表示点云熵的局限性，并引入了一种新的度量，该度量反映了有损IR的逐点和逐熵质量。评估结果表明，与现有的激光雷达压缩相比，FLiCR更适合于边缘辅助实时感知，我们通过对3D目标检测和激光雷达SLAM的评估证明了我们的压缩和度量的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.15005v1" target="_blank">2307.15005v1</a>
                              </td>
                              <td>FLiCR: A Fast and Lightweight LiDAR Point Cloud Compression Based on Lossy RI</td>
                              <td>Jin Heo</td>
                              <td>2023-07-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_15005v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.15005v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01188v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Event-based Stereo Visual Odometry with Native Temporal Resolution via Continuous-time Gaussian Process Regression</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01188v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01188v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01188v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Event-based cameras asynchronously capture individual visual changes in a scene. This makes them more robust than traditional frame-based cameras to highly dynamic motions and poor illumination. It also means that every measurement in a scene can occur at a unique time.   Handling these different measurement times is a major challenge of using event-based cameras. It is often addressed in visual odometry (VO) pipelines by approximating temporally close measurements as occurring at one common time. This grouping simplifies the estimation problem but, absent additional sensors, sacrifices the inherent temporal resolution of event-based cameras.   This paper instead presents a complete stereo VO pipeline that estimates directly with individual event-measurement times without requiring any grouping or approximation in the estimation state. It uses continuous-time trajectory estimation to maintain the temporal fidelity and asynchronous nature of event-based cameras through Gaussian process regression with a physically motivated prior. Its performance is evaluated on the MVSEC dataset, where it achieves 7.9e-3 and 5.9e-3 RMS relative error on two independent sequences, outperforming the existing publicly available event-based stereo VO pipeline by two and four times, respectively.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01188v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于事件的摄影机异步捕捉场景中的各个视觉变化。这使得它们比传统的基于帧的相机对高动态运动和较差的照明更具鲁棒性。这也意味着场景中的每个测量都可以在唯一的时间发生。处理这些不同的测量时间是使用基于事件的相机的主要挑战。它通常在视觉里程计（VO）管道中通过将时间上的近距离测量近似为在一个公共时间发生来解决。这种分组简化了估计问题，但在没有额外传感器的情况下，牺牲了基于事件的相机固有的时间分辨率。相反，本文提出了一个完整的立体VO管道，该管道直接用单个事件测量时间进行估计，而不需要在估计状态下进行任何分组或近似。它使用连续时间轨迹估计，通过具有物理动机先验的高斯过程回归来保持基于事件的相机的时间保真度和异步性质。它的性能在MVSEC数据集上进行了评估，在两个独立序列上实现了7.9e-3和5.9e-3的RMS相对误差，分别比现有的公开可用的基于事件的立体声VO管道高出两倍和四倍。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01188v2" target="_blank">2306.01188v2</a>
                              </td>
                              <td>Event-based Stereo Visual Odometry with Native Temporal Resolution via Continuous-time Gaussian Process Regression</td>
                              <td>Jianeng Wang</td>
                              <td>2023-06-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01188v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01188v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_05086v4_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Stereo Event-based Visual-Inertial Odometry</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_05086v4_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_05086v4_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_05086v4_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Event-based cameras are new type vision sensors whose pixels work independently and respond asynchronously to brightness change with microsecond resolution, instead of providing standard intensity frames. Compared with traditional cameras, event-based cameras have low latency, no motion blur, and high dynamic range (HDR), which provide possibilities for robots to deal with some challenging scenes. We propose a visual-inertial odometry for stereo event-based cameras based on Error-State Kalman Filter (ESKF). The visual module updates the pose relies on the edge alignment of a semi-dense 3D map to a 2D image, and the IMU module updates pose by median integral. We evaluate our method on public datasets with general 6-DoF motion and compare the results against ground truth. We show that our proposed pipeline provides improved accuracy over the result of the state-of-the-art visual odometry for stereo event-based cameras, while running in real-time on a standard CPU (low-resolution cameras). To the best of our knowledge, this is the first published visual-inertial odometry for stereo event-based cameras.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_05086v4_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于事件的相机是一种新型的视觉传感器，其像素独立工作，并以微秒的分辨率异步响应亮度变化，而不是提供标准的强度帧。与传统相机相比，基于事件的相机具有低延迟、无运动模糊和高动态范围（HDR），这为机器人处理一些具有挑战性的场景提供了可能性。我们提出了一种基于误差状态卡尔曼滤波器（ESKF）的立体事件相机视觉惯性里程计。视觉模块根据半密集3D地图到2D图像的边缘对齐来更新姿态，IMU模块通过中值积分来更新姿态。我们在具有一般6-DoF运动的公共数据集上评估了我们的方法，并将结果与地面实况进行了比较。我们表明，我们提出的管道在标准CPU（低分辨率相机）上实时运行的同时，为基于立体事件的相机提供了比最先进的视觉里程计结果更高的精度。据我们所知，这是第一个发表的用于基于立体事件的相机的视觉惯性里程计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.05086v4" target="_blank">2303.05086v4</a>
                              </td>
                              <td>Stereo Event-based Visual-Inertial Odometry</td>
                              <td>Kunfeng Wang</td>
                              <td>2023-03-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_05086v4_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.05086v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_06524v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SST: Real-time End-to-end Monocular 3D Reconstruction via Sparse Spatial-Temporal Guidance</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_06524v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_06524v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_06524v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Real-time monocular 3D reconstruction is a challenging problem that remains unsolved. Although recent end-to-end methods have demonstrated promising results, tiny structures and geometric boundaries are hardly captured due to their insufficient supervision neglecting spatial details and oversimplified feature fusion ignoring temporal cues. To address the problems, we propose an end-to-end 3D reconstruction network SST, which utilizes Sparse estimated points from visual SLAM system as additional Spatial guidance and fuses Temporal features via a novel cross-modal attention mechanism, achieving more detailed reconstruction results. We propose a Local Spatial-Temporal Fusion module to exploit more informative spatial-temporal cues from multi-view color information and sparse priors, as well a Global Spatial-Temporal Fusion module to refine the local TSDF volumes with the world-frame model from coarse to fine. Extensive experiments on ScanNet and 7-Scenes demonstrate that SST outperforms all state-of-the-art competitors, whilst keeping a high inference speed at 59 FPS, enabling real-world applications with real-time requirements.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_06524v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>实时单目三维重建是一个尚未解决的具有挑战性的问题。尽管最近的端到端方法已经证明了有希望的结果，但由于忽略空间细节的监督不足和忽略时间线索的特征融合过于简单，很难捕捉到微小的结构和几何边界。为了解决这些问题，我们提出了一种端到端的3D重建网络SST，该网络利用视觉SLAM系统的稀疏估计点作为额外的空间引导，并通过一种新的跨模态注意力机制融合时间特征，获得更详细的重建结果。我们提出了一个局部时空融合模块来利用来自多视图颜色信息和稀疏先验的更具信息性的时空线索，以及一个全局时空融合模块，用世界帧模型从粗到细细化局部TSDF体积。在ScanNet和7-Scenes上进行的大量实验表明，SST优于所有最先进的竞争对手，同时保持59 FPS的高推理速度，使现实世界的应用程序具有实时要求。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.06524v2" target="_blank">2212.06524v2</a>
                              </td>
                              <td>SST: Real-time End-to-end Monocular 3D Reconstruction via Sparse Spatial-Temporal Guidance</td>
                              <td>Chenyangguang Zhang</td>
                              <td>2022-12-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_06524v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.06524v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_12836v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GNSS-stereo-inertial SLAM for arable farming</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_12836v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_12836v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_12836v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The accelerating pace in the automation of agricultural tasks demands highly accurate and robust localization systems for field robots. Simultaneous Localization and Mapping (SLAM) methods inevitably accumulate drift on exploratory trajectories and primarily rely on place revisiting and loop closing to keep a bounded global localization error. Loop closure techniques are significantly challenging in agricultural fields, as the local visual appearance of different views is very similar and might change easily due to weather effects. A suitable alternative in practice is to employ global sensor positioning systems jointly with the rest of the robot sensors. In this paper we propose and implement the fusion of global navigation satellite system (GNSS), stereo views, and inertial measurements for localization purposes. Specifically, we incorporate, in a tightly coupled manner, GNSS measurements into the stereo-inertial ORB-SLAM3 pipeline. We thoroughly evaluate our implementation in the sequences of the Rosario data set, recorded by an autonomous robot in soybean fields, and our own in-house data. Our data includes measurements from a conventional GNSS, rarely included in evaluations of state-of-the-art approaches. We characterize the performance of GNSS-stereo-inertial SLAM in this application case, reporting pose error reductions between 10% and 30% compared to visual-inertial and loosely coupled GNSS-stereo-inertial baselines. In addition to such analysis, we also release the code of our implementation as open source.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_12836v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>农业任务自动化的加速要求现场机器人具有高度准确和稳健的定位系统。同时定位和映射（SLAM）方法不可避免地会在探索轨迹上积累漂移，并且主要依靠位置重访和闭环来保持有界的全局定位误差。闭环技术在农业领域具有重大挑战性，因为不同视图的局部视觉外观非常相似，并且可能很容易因天气影响而发生变化。实践中合适的替代方案是将全局传感器定位系统与机器人传感器的其余部分联合使用。在本文中，我们提出并实现了全球导航卫星系统（GNSS）、立体视图和惯性测量的融合，用于定位目的。具体而言，我们以紧密耦合的方式将全球导航卫星系统的测量纳入立体惯性ORB-SLAM3管道。我们彻底评估了我们在罗萨里奥数据集序列中的实现，该数据集由大豆田中的自主机器人记录，以及我们自己的内部数据。我们的数据包括传统全球导航卫星系统的测量结果，很少包括在对最先进方法的评估中。我们在这个应用案例中描述了GNSS立体惯性SLAM的性能，报告称与视觉惯性和松耦合GNSS立体惯导基线相比，姿态误差降低了10%至30%。除了这样的分析，我们还将实现的代码作为开源发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.12836v1" target="_blank">2307.12836v1</a>
                              </td>
                              <td>GNSS-stereo-inertial SLAM for arable farming</td>
                              <td>Javier Cremona</td>
                              <td>2023-07-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_12836v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.12836v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_12326v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Scale jump-aware pose graph relaxation for monocular SLAM with re-initializations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_12326v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_12326v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_12326v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Pose graph relaxation has become an indispensable addition to SLAM enabling efficient global registration of sensor reference frames under the objective of satisfying pair-wise relative transformation constraints. The latter may be given by incremental motion estimation or global place recognition. While the latter case enables loop closures and drift compensation, care has to be taken in the monocular case in which local estimates of structure and displacements can differ from reality not just in terms of noise, but also in terms of a scale factor. Owing to the accumulation of scale propagation errors, this scale factor is drifting over time, hence scale-drift aware pose graph relaxation has been introduced. We extend this idea to cases in which the relative scale between subsequent sensor frames is unknown, a situation that can easily occur if monocular SLAM enters re-initialization and no reliable overlap between successive local maps can be identified. The approach is realized by a hybrid pose graph formulation that combines the regular similarity consistency terms with novel, scale-blind constraints. We apply the technique to the practically relevant case of small indoor service robots capable of effectuating purely rotational displacements, a condition that can easily cause tracking failures. We demonstrate that globally consistent trajectories can be recovered even if multiple re-initializations occur along the loop, and present an in-depth study of success and failure cases.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_12326v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>姿态图松弛已成为SLAM不可或缺的补充，能够在满足成对相对变换约束的目标下实现传感器参考帧的有效全局配准。后者可以通过增量运动估计或全局位置识别来给出。虽然后一种情况能够实现闭环和漂移补偿，但在单目情况下必须小心，在这种情况下，结构和位移的局部估计不仅在噪声方面，而且在比例因子方面都可能与现实不同。由于尺度传播误差的累积，该尺度因子随着时间的推移而漂移，因此引入了尺度漂移感知的姿态图松弛。我们将这一想法扩展到后续传感器帧之间的相对比例未知的情况，如果单目SLAM进入重新初始化，并且无法识别连续局部映射之间的可靠重叠，这种情况很容易发生。该方法通过一种混合姿态图公式来实现，该公式将规则相似性一致性项与新颖的尺度盲约束相结合。我们将该技术应用于能够实现纯旋转位移的小型室内服务机器人的实际相关案例，这种情况很容易导致跟踪故障。我们证明，即使在循环中发生多次重新初始化，也可以恢复全局一致的轨迹，并对成功和失败案例进行了深入研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.12326v1" target="_blank">2307.12326v1</a>
                              </td>
                              <td>Scale jump-aware pose graph relaxation for monocular SLAM with re-initializations</td>
                              <td>Runze Yuan</td>
                              <td>2023-07-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_12326v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.12326v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10984v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Metric3D: Towards Zero-shot Metric 3D Prediction from A Single Image</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10984v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10984v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10984v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Reconstructing accurate 3D scenes from images is a long-standing vision task. Due to the ill-posedness of the single-image reconstruction problem, most well-established methods are built upon multi-view geometry. State-of-the-art (SOTA) monocular metric depth estimation methods can only handle a single camera model and are unable to perform mixed-data training due to the metric ambiguity. Meanwhile, SOTA monocular methods trained on large mixed datasets achieve zero-shot generalization by learning affine-invariant depths, which cannot recover real-world metrics. In this work, we show that the key to a zero-shot single-view metric depth model lies in the combination of large-scale data training and resolving the metric ambiguity from various camera models. We propose a canonical camera space transformation module, which explicitly addresses the ambiguity problems and can be effortlessly plugged into existing monocular models. Equipped with our module, monocular models can be stably trained with over 8 million images with thousands of camera models, resulting in zero-shot generalization to in-the-wild images with unseen camera settings. Experiments demonstrate SOTA performance of our method on 7 zero-shot benchmarks. Notably, our method won the championship in the 2nd Monocular Depth Estimation Challenge. Our method enables the accurate recovery of metric 3D structures on randomly collected internet images, paving the way for plausible single-image metrology. The potential benefits extend to downstream tasks, which can be significantly improved by simply plugging in our model. For example, our model relieves the scale drift issues of monocular-SLAM (Fig. 1), leading to high-quality metric scale dense mapping. The code is available at https://github.com/YvanYin/Metric3D.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10984v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从图像中重建精确的3D场景是一项长期的视觉任务。由于单图像重建问题的不适定性，大多数公认的方法都建立在多视图几何的基础上。现有技术（SOTA）单目度量深度估计方法只能处理单个相机模型，并且由于度量模糊性而无法执行混合数据训练。同时，在大型混合数据集上训练的SOTA单目方法通过学习仿射不变深度来实现零样本泛化，而仿射不变深度不能恢复真实世界的度量。在这项工作中，我们表明零样本单视图测量深度模型的关键在于结合大规模数据训练和解决各种相机模型的测量模糊性。我们提出了一个规范的相机空间转换模块，它明确地解决了模糊问题，并且可以毫不费力地插入到现有的单目模型中。配备我们的模块，单目模型可以通过数千个相机模型稳定地训练超过800万张图像，从而实现对具有看不见相机设置的现场图像的零样本泛化。实验证明了我们的方法在7个零样本基准上的SOTA性能。值得注意的是，我们的方法在第二届单目深度估计挑战赛中获得了冠军。我们的方法能够在随机收集的互联网图像上准确恢复度量三维结构，为合理的单图像度量铺平了道路。潜在的好处延伸到下游任务，只需插入我们的模型就可以显著改善这些任务。例如，我们的模型缓解了单目SLAM的尺度漂移问题（图1），导致了高质量的度量尺度密集映射。代码可在https://github.com/YvanYin/Metric3D.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10984v1" target="_blank">2307.10984v1</a>
                              </td>
                              <td>Metric3D: Towards Zero-shot Metric 3D Prediction from A Single Image</td>
                              <td>Wei Yin</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10984v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10984v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_07894v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">iSLAM: Imperative SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_07894v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_07894v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_07894v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localization and mapping (SLAM) stands as one of the critical challenges in robot navigation. Recent advancements suggest that methods based on supervised learning deliver impressive performance in front-end odometry, while traditional optimization-based methods still play a vital role in the back-end for minimizing estimation drift. In this paper, we found that such decoupled paradigm can lead to only sub-optimal performance, consequently curtailing system capabilities and generalization potential. To solve this problem, we proposed a novel self-supervised learning framework, imperative SLAM (iSLAM), which fosters reciprocal correction between the front-end and back-end, thus enhancing performance without necessitating any external supervision. Specifically, we formulate a SLAM system as a bi-level optimization problem so that the two components are bidirectionally connected. As a result, the front-end model is able to learn global geometric knowledge obtained through pose graph optimization by back-propagating the residuals from the back-end. This significantly improves the generalization ability of the entire system and thus achieves the accuracy improvement up to 45%. To the best of our knowledge, iSLAM is the first SLAM system showing that the front-end and back-end can learn jointly and mutually contribute to each other in a self-supervised manner.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_07894v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和映射（SLAM）是机器人导航中的关键挑战之一。最近的进展表明，基于监督学习的方法在前端里程计中提供了令人印象深刻的性能，而传统的基于优化的方法在后端仍然发挥着至关重要的作用，以最大限度地减少估计漂移。在本文中，我们发现这种解耦范式只能导致次优性能，从而削弱系统能力和泛化潜力。为了解决这个问题，我们提出了一种新的自我监督学习框架，即命令式SLAM（iSLAM），它促进了前端和后端之间的相互校正，从而在不需要任何外部监督的情况下提高了性能。具体来说，我们将SLAM系统公式化为双层优化问题，使两个组件双向连接。因此，前端模型能够通过从后端反向传播残差来学习通过位姿图优化获得的全局几何知识。这显著提高了整个系统的泛化能力，从而实现了高达45%的精度提高。据我们所知，iSLAM是第一个SLAM系统，表明前端和后端可以以自我监督的方式共同学习并相互贡献。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.07894v3" target="_blank">2306.07894v3</a>
                              </td>
                              <td>iSLAM: Imperative SLAM</td>
                              <td>Taimeng Fu</td>
                              <td>2023-06-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_07894v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.07894v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10015v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Optimizing the extended Fourier Mellin Transformation Algorithm</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10015v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10015v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10015v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With the increasing application of robots, stable and efficient Visual Odometry (VO) algorithms are becoming more and more important. Based on the Fourier Mellin Transformation (FMT) algorithm, the extended Fourier Mellin Transformation (eFMT) is an image registration approach that can be applied to downward-looking cameras, for example on aerial and underwater vehicles. eFMT extends FMT to multi-depth scenes and thus more application scenarios. It is a visual odometry method which estimates the pose transformation between three overlapping images. On this basis, we develop an optimized eFMT algorithm that improves certain aspects of the method and combines it with back-end optimization for the small loop of three consecutive frames. For this we investigate the extraction of uncertainty information from the eFMT registration, the related objective function and the graph-based optimization. Finally, we design a series of experiments to investigate the properties of this approach and compare it with other VO and SLAM (Simultaneous Localization and Mapping) algorithms. The results show the superior accuracy and speed of our o-eFMT approach, which is published as open source.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10015v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着机器人应用的日益广泛，稳定高效的视觉测距算法变得越来越重要。基于傅立叶-梅林变换（FMT）算法，扩展傅立叶-梅林转换（eFMT）是一种图像配准方法，可应用于向下看的相机，例如航空和水下航行器。eFMT将FMT扩展到多深度场景，从而扩展到更多的应用场景。这是一种视觉里程计方法，用于估计三个重叠图像之间的姿态变换。在此基础上，我们开发了一种优化的eFMT算法，该算法改进了该方法的某些方面，并将其与三个连续帧的小循环的后端优化相结合。为此，我们研究了从eFMT配准、相关目标函数和基于图的优化中提取不确定性信息。最后，我们设计了一系列实验来研究该方法的性能，并将其与其他VO和SLAM（同步定位和映射）算法进行了比较。结果表明，我们的o-eFMT方法具有卓越的准确性和速度，该方法以开源形式发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10015v1" target="_blank">2307.10015v1</a>
                              </td>
                              <td>Optimizing the extended Fourier Mellin Transformation Algorithm</td>
                              <td>Wenqing Jiang</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10015v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10015v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09044v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D-SeqMOS: A Novel Sequential 3D Moving Object Segmentation in Autonomous Driving</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09044v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09044v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09044v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>For the SLAM system in robotics and autonomous driving, the accuracy of front-end odometry and back-end loop-closure detection determine the whole intelligent system performance. But the LiDAR-SLAM could be disturbed by current scene moving objects, resulting in drift errors and even loop-closure failure. Thus, the ability to detect and segment moving objects is essential for high-precision positioning and building a consistent map. In this paper, we address the problem of moving object segmentation from 3D LiDAR scans to improve the odometry and loop-closure accuracy of SLAM. We propose a novel 3D Sequential Moving-Object-Segmentation (3D-SeqMOS) method that can accurately segment the scene into moving and static objects, such as moving and static cars. Different from the existing projected-image method, we process the raw 3D point cloud and build a 3D convolution neural network for MOS task. In addition, to make full use of the spatio-temporal information of point cloud, we propose a point cloud residual mechanism using the spatial features of current scan and the temporal features of previous residual scans. Besides, we build a complete SLAM framework to verify the effectiveness and accuracy of 3D-SeqMOS. Experiments on SemanticKITTI dataset show that our proposed 3D-SeqMOS method can effectively detect moving objects and improve the accuracy of LiDAR odometry and loop-closure detection. The test results show our 3D-SeqMOS outperforms the state-of-the-art method by 12.4%. We extend the proposed method to the SemanticKITTI: Moving Object Segmentation competition and achieve the 2nd in the leaderboard, showing its effectiveness.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09044v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对于机器人和自动驾驶领域的SLAM系统来说，前端里程计和后端闭环检测的准确性决定了整个智能系统的性能。但激光雷达SLAM可能会受到当前场景移动物体的干扰，导致漂移误差，甚至环路闭合失败。因此，检测和分割移动物体的能力对于高精度定位和构建一致的地图至关重要。在本文中，我们解决了从三维激光雷达扫描中分割运动对象的问题，以提高SLAM的里程计和闭环精度。我们提出了一种新的3D顺序运动对象分割（3D-SeqMOS）方法，该方法可以准确地将场景分割为运动和静态对象，如运动和静态汽车。与现有的投影图像方法不同，我们对原始的三维点云进行了处理，并构建了用于MOS任务的三维卷积神经网络。此外，为了充分利用点云的时空信息，我们利用当前扫描的空间特征和先前残差扫描的时间特征，提出了一种点云残差机制。此外，我们建立了一个完整的SLAM框架来验证3D SeqMOS的有效性和准确性。在SemanticKITTI数据集上的实验表明，我们提出的3D SeqMOS方法可以有效地检测运动物体，并提高激光雷达里程计和闭环检测的准确性。测试结果表明，我们的3D SeqMOS比最先进的方法高12.4%。我们将所提出的方法扩展到SemanticKITTI:运动对象分割竞赛中，并在排行榜上排名第二，显示了它的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09044v1" target="_blank">2307.09044v1</a>
                              </td>
                              <td>3D-SeqMOS: A Novel Sequential 3D Moving Object Segmentation in Autonomous Driving</td>
                              <td>Qipeng Li</td>
                              <td>2023-07-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09044v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09044v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_08221v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NDT-Map-Code: A 3D global descriptor for real-time loop closure detection in lidar SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_08221v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_08221v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_08221v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Loop-closure detection, also known as place recognition, aiming to identify previously visited locations, is an essential component of a SLAM system. Existing research on lidar-based loop closure heavily relies on dense point cloud and 360 FOV lidars. This paper proposes an out-of-the-box NDT (Normal Distribution Transform) based global descriptor, NDT-Map-Code, designed for both on-road driving and underground valet parking scenarios. NDT-Map-Code can be directly extracted from the NDT map without the need for a dense point cloud, resulting in excellent scalability and low maintenance cost. The NDT representation is leveraged to identify representative patterns, which are further encoded according to their spatial location (bearing, range, and height). Experimental results on the NIO underground parking lot dataset and the KITTI dataset demonstrate that our method achieves significantly better performance compared to the state-of-the-art.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_08221v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>闭环检测，也称为位置识别，旨在识别以前访问过的位置，是SLAM系统的重要组成部分。现有的基于激光雷达的闭环研究在很大程度上依赖于密集点云和360视场激光雷达。本文提出了一种开箱即用的基于无损检测（正态分布变换）的全局描述符无损检测地图代码，该描述符专为道路驾驶和地下代客泊车场景设计。无损检测图代码可以直接从无损检测图中提取，无需密集的点云，具有良好的可扩展性和较低的维护成本。无损检测表示用于识别代表性图案，并根据其空间位置（方位、范围和高度）对其进行进一步编码。在NIO地下停车场数据集和KITTI数据集上的实验结果表明，与最先进的方法相比，我们的方法实现了显著更好的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.08221v1" target="_blank">2307.08221v1</a>
                              </td>
                              <td>NDT-Map-Code: A 3D global descriptor for real-time loop closure detection in lidar SLAM</td>
                              <td>Lizhou Liao</td>
                              <td>2023-07-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_08221v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.08221v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07936v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Joint Beam Management and SLAM for mmWave Communication Systems</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07936v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07936v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07936v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The millimeter-wave (mmWave) communication technology, which employs large-scale antenna arrays, enables inherent sensing capabilities. Simultaneous localization and mapping (SLAM) can utilize channel multipath angle estimates to realize integrated sensing and communication design in 6G communication systems. However, existing works have ignored the significant overhead required by the mmWave beam management when implementing SLAM with angle estimates. This study proposes a joint beam management and SLAM design that utilizes the strong coupling between the radio map and channel multipath for simultaneous beam management, localization, and mapping. In this approach, we first propose a hierarchical sweeping and sensing service design. The path angles are estimated in the hierarchical sweeping, enabling angle-based SLAM with the aid of an inertial measurement unit (IMU) to realize sensing service. Then, feature-aided tracking is proposed that utilizes prior angle information generated from the radio map and IMU. Finally, a switching module is introduced to enable flexible switching between hierarchical sweeping and feature-aided tracking. Simulations show that the proposed joint design can achieve sub-meter level localization and mapping accuracy (with an error < 0.5 m). Moreover, the beam management overhead can be reduced by approximately 40% in different wireless environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07936v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>毫米波（mmWave）通信技术采用了大规模天线阵列，实现了固有的传感能力。同时定位和映射（SLAM）可以利用信道多径角度估计来实现6G通信系统中的集成传感和通信设计。然而，现有工作忽略了毫米波波束管理在使用角度估计实现SLAM时所需的大量开销。本研究提出了一种联合波束管理和SLAM设计，该设计利用无线电映射和信道多径之间的强耦合来同时进行波束管理、定位和映射。在这种方法中，我们首先提出了一种分层的扫描和感知服务设计。在分层扫描中估计路径角度，使基于角度的SLAM能够在惯性测量单元（IMU）的帮助下实现传感服务。然后，提出了利用无线电地图和IMU生成的先验角度信息的特征辅助跟踪。最后，引入了一个切换模块，以实现分层扫描和特征辅助跟踪之间的灵活切换。仿真表明，所提出的联合设计可以实现亚米级的定位和映射精度（误差＜0.5m）。此外，在不同的无线环境中，波束管理开销可以减少大约40%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07936v1" target="_blank">2307.07936v1</a>
                              </td>
                              <td>Joint Beam Management and SLAM for mmWave Communication Systems</td>
                              <td>Hang Que</td>
                              <td>2023-07-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07936v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07936v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07763v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Tightly-Coupled LiDAR-Visual SLAM Based on Geometric Features for Mobile Agents</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07763v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07763v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07763v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The mobile robot relies on SLAM (Simultaneous Localization and Mapping) to provide autonomous navigation and task execution in complex and unknown environments. However, it is hard to develop a dedicated algorithm for mobile robots due to dynamic and challenging situations, such as poor lighting conditions and motion blur. To tackle this issue, we propose a tightly-coupled LiDAR-visual SLAM based on geometric features, which includes two sub-systems (LiDAR and monocular visual SLAM) and a fusion framework. The fusion framework associates the depth and semantics of the multi-modal geometric features to complement the visual line landmarks and to add direction optimization in Bundle Adjustment (BA). This further constrains visual odometry. On the other hand, the entire line segment detected by the visual subsystem overcomes the limitation of the LiDAR subsystem, which can only perform the local calculation for geometric features. It adjusts the direction of linear feature points and filters out outliers, leading to a higher accurate odometry system. Finally, we employ a module to detect the subsystem's operation, providing the LiDAR subsystem's output as a complementary trajectory to our system while visual subsystem tracking fails. The evaluation results on the public dataset M2DGR, gathered from ground robots across various indoor and outdoor scenarios, show that our system achieves more accurate and robust pose estimation compared to current state-of-the-art multi-modal methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07763v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>移动机器人依靠SLAM（同步定位和映射）在复杂和未知的环境中提供自主导航和任务执行。然而，由于动态和具有挑战性的情况，例如较差的照明条件和运动模糊，很难为移动机器人开发专用算法。为了解决这个问题，我们提出了一种基于几何特征的紧密耦合激光雷达视觉SLAM，它包括两个子系统（激光雷达和单目视觉SLAM）和一个融合框架。融合框架将多模态几何特征的深度和语义相关联，以补充视觉线地标，并在束调整（BA）中添加方向优化。这进一步限制了视觉里程计。另一方面，视觉子系统检测到的整个线段克服了激光雷达子系统只能对几何特征进行局部计算的局限性。它调整线性特征点的方向并过滤掉异常值，从而实现更高精度的里程计系统。最后，我们使用一个模块来检测子系统的操作，在视觉子系统跟踪失败时，将激光雷达子系统的输出作为我们系统的补充轨迹。从各种室内和室外场景中的地面机器人收集的公共数据集M2DGR的评估结果表明，与当前最先进的多模态方法相比，我们的系统实现了更准确、更稳健的姿态估计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07763v1" target="_blank">2307.07763v1</a>
                              </td>
                              <td>Tightly-Coupled LiDAR-Visual SLAM Based on Geometric Features for Mobile Agents</td>
                              <td>Ke Cao</td>
                              <td>2023-07-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07763v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07763v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07296v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Reinforcement Learning with Frontier-Based Exploration via Autonomous Environment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07296v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07296v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07296v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Active Simultaneous Localisation and Mapping (SLAM) is a critical problem in autonomous robotics, enabling robots to navigate to new regions while building an accurate model of their surroundings. Visual SLAM is a popular technique that uses virtual elements to enhance the experience. However, existing frontier-based exploration strategies can lead to a non-optimal path in scenarios where there are multiple frontiers with similar distance. This issue can impact the efficiency and accuracy of Visual SLAM, which is crucial for a wide range of robotic applications, such as search and rescue, exploration, and mapping. To address this issue, this research combines both an existing Visual-Graph SLAM known as ExploreORB with reinforcement learning. The proposed algorithm allows the robot to learn and optimize exploration routes through a reward-based system to create an accurate map of the environment with proper frontier selection. Frontier-based exploration is used to detect unexplored areas, while reinforcement learning optimizes the robot's movement by assigning rewards for optimal frontier points. Graph SLAM is then used to integrate the robot's sensory data and build an accurate map of the environment. The proposed algorithm aims to improve the efficiency and accuracy of ExploreORB by optimizing the exploration process of frontiers to build a more accurate map. To evaluate the effectiveness of the proposed approach, experiments will be conducted in various virtual environments using Gazebo, a robot simulation software. Results of these experiments will be compared with existing methods to demonstrate the potential of the proposed approach as an optimal solution for SLAM in autonomous robotics.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07296v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>主动同步定位和映射（SLAM）是自主机器人的一个关键问题，它使机器人能够导航到新的区域，同时建立其周围环境的准确模型。视觉SLAM是一种流行的技术，它使用虚拟元素来增强体验。然而，在存在距离相似的多个边界的情况下，现有的基于边界的勘探策略可能会导致非最优路径。这个问题可能会影响视觉SLAM的效率和准确性，而视觉SLAM对于搜索和救援、勘探和测绘等广泛的机器人应用至关重要。为了解决这个问题，本研究将现有的可视化图形SLAM ExploreORB与强化学习相结合。所提出的算法允许机器人通过基于奖励的系统学习和优化探索路线，以创建具有适当边界选择的准确环境地图。基于边界的探索用于检测未探索的区域，而强化学习通过为最佳边界点分配奖励来优化机器人的运动。然后使用图形SLAM来整合机器人的感官数据，并构建准确的环境地图。所提出的算法旨在通过优化边界探索过程来构建更准确的地图，从而提高ExploreORB的效率和准确性。为了评估所提出方法的有效性，将使用机器人模拟软件Gazebo在各种虚拟环境中进行实验。将这些实验的结果与现有方法进行比较，以证明所提出的方法作为自主机器人SLAM的最佳解决方案的潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07296v1" target="_blank">2307.07296v1</a>
                              </td>
                              <td>Reinforcement Learning with Frontier-Based Exploration via Autonomous Environment</td>
                              <td>Kenji Leong</td>
                              <td>2023-07-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07296v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07296v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_07308v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NeuSE: Neural SE(3)-Equivariant Embedding for Consistent Spatial Understanding with Objects</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_07308v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_07308v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_07308v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present NeuSE, a novel Neural SE(3)-Equivariant Embedding for objects, and illustrate how it supports object SLAM for consistent spatial understanding with long-term scene changes. NeuSE is a set of latent object embeddings created from partial object observations. It serves as a compact point cloud surrogate for complete object models, encoding full shape information while transforming SE(3)-equivariantly in tandem with the object in the physical world. With NeuSE, relative frame transforms can be directly derived from inferred latent codes. Our proposed SLAM paradigm, using NeuSE for object shape and pose characterization, can operate independently or in conjunction with typical SLAM systems. It directly infers SE(3) camera pose constraints that are compatible with general SLAM pose graph optimization, while also maintaining a lightweight object-centric map that adapts to real-world changes. Our approach is evaluated on synthetic and real-world sequences featuring changed objects and shows improved localization accuracy and change-aware mapping capability, when working either standalone or jointly with a common SLAM pipeline.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_07308v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了NeuSE，一种新的用于对象的Neural SE（3）-等变量嵌入，并说明了它如何支持对象SLAM在长期场景变化的情况下实现一致的空间理解。NeuSE是根据部分对象观测创建的一组潜在对象嵌入。它充当完整对象模型的紧凑点云代理，对完整形状信息进行编码，同时与物理世界中的对象等变地变换SE（3）。使用NeuSE，可以直接从推断的潜在代码中导出相对帧变换。我们提出的SLAM范式，使用NeuSE进行物体形状和姿态表征，可以独立操作，也可以与典型的SLAM系统结合操作。它直接推断SE（3）相机姿势约束，这些约束与通用SLAM姿势图优化兼容，同时还保持了一个适应现实世界变化的轻量级以对象为中心的贴图。我们的方法在以变化对象为特征的合成序列和真实世界序列上进行了评估，并在独立或与通用SLAM管道联合工作时显示出改进的定位精度和变化感知映射能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.07308v2" target="_blank">2303.07308v2</a>
                              </td>
                              <td>NeuSE: Neural SE(3)-Equivariant Embedding for Consistent Spatial Understanding with Objects</td>
                              <td>Jiahui Fu</td>
                              <td>2023-03-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_07308v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.07308v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="SFM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2308_01246v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Tirtha -- An Automated Platform to Crowdsource Images and Create 3D Models of Heritage Sites</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01246v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01246v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01246v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Digital preservation of Cultural Heritage (CH) sites is crucial to protect them against damage from natural disasters or human activities. Creating 3D models of CH sites has become a popular method of digital preservation thanks to advancements in computer vision and photogrammetry. However, the process is time-consuming, expensive, and typically requires specialized equipment and expertise, posing challenges in resource-limited developing countries. Additionally, the lack of an open repository for 3D models hinders research and public engagement with their heritage. To address these issues, we propose Tirtha, a web platform for crowdsourcing images of CH sites and creating their 3D models. Tirtha utilizes state-of-the-art Structure from Motion (SfM) and Multi-View Stereo (MVS) techniques. It is modular, extensible and cost-effective, allowing for the incorporation of new techniques as photogrammetry advances. Tirtha is accessible through a web interface at https://tirtha.niser.ac.in and can be deployed on-premise or in a cloud environment. In our case studies, we demonstrate the pipeline's effectiveness by creating 3D models of temples in Odisha, India, using crowdsourced images. These models are available for viewing, interaction, and download on the Tirtha website. Our work aims to provide a dataset of crowdsourced images and 3D reconstructions for research in computer vision, heritage conservation, and related domains. Overall, Tirtha is a step towards democratizing digital preservation, primarily in resource-limited developing countries.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01246v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文化遗产（CH）遗址的数字保护对于保护它们免受自然灾害或人类活动的破坏至关重要。由于计算机视觉和摄影测量的进步，创建CH遗址的3D模型已成为一种流行的数字保存方法。然而，这一过程耗时、昂贵，通常需要专门的设备和专业知识，这对资源有限的发展中国家构成了挑战。此外，缺乏开放的3D模型存储库阻碍了研究和公众对其遗产的参与。为了解决这些问题，我们提出了Tirtha，一个用于众包CH网站图像并创建其3D模型的网络平台。Tirtha采用了最先进的运动结构（SfM）和多视图立体（MVS）技术。它是模块化的、可扩展的和具有成本效益的，允许随着摄影测量的发展而结合新技术。Tirtha可以通过以下网站的web界面访问：https://tirtha.niser.ac.in并且可以在内部部署或在云环境中部署。在我们的案例研究中，我们通过使用众包图像创建印度奥迪沙寺庙的3D模型来证明该管道的有效性。这些模型可在Tirtha网站上查看、交互和下载。我们的工作旨在为计算机视觉、遗产保护和相关领域的研究提供众包图像和3D重建的数据集。总的来说，Tirtha是朝着数字保护民主化迈出的一步，主要是在资源有限的发展中国家。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01246v2" target="_blank">2308.01246v2</a>
                              </td>
                              <td>Tirtha -- An Automated Platform to Crowdsource Images and Create 3D Models of Heritage Sites</td>
                              <td>Jyotirmaya Shivottam</td>
                              <td>2023-08-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01246v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01246v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_06794v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MMF-Track: Multi-modal Multi-level Fusion for 3D Single Object Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_06794v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_06794v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_06794v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>3D single object tracking plays a crucial role in computer vision. Mainstream methods mainly rely on point clouds to achieve geometry matching between target template and search area. However, textureless and incomplete point clouds make it difficult for single-modal trackers to distinguish objects with similar structures. To overcome the limitations of geometry matching, we propose a Multi-modal Multi-level Fusion Tracker (MMF-Track), which exploits the image texture and geometry characteristic of point clouds to track 3D target. Specifically, we first propose a Space Alignment Module (SAM) to align RGB images with point clouds in 3D space, which is the prerequisite for constructing inter-modal associations. Then, in feature interaction level, we design a Feature Interaction Module (FIM) based on dual-stream structure, which enhances intra-modal features in parallel and constructs inter-modal semantic associations. Meanwhile, in order to refine each modal feature, we introduce a Coarse-to-Fine Interaction Module (CFIM) to realize the hierarchical feature interaction at different scales. Finally, in similarity fusion level, we propose a Similarity Fusion Module (SFM) to aggregate geometry and texture clues from the target. Experiments show that our method achieves state-of-the-art performance on KITTI (39% Success and 42% Precision gains against previous multi-modal method) and is also competitive on NuScenes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_06794v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三维单目标跟踪在计算机视觉中起着至关重要的作用。主流的方法主要依靠点云来实现目标模板和搜索区域之间的几何匹配。然而，无纹理和不完整的点云使单模态跟踪器难以区分具有相似结构的对象。为了克服几何匹配的局限性，我们提出了一种多模态多级融合跟踪器（MMF Track），该跟踪器利用点云的图像纹理和几何特征来跟踪三维目标。具体来说，我们首先提出了一个空间对齐模块（SAM）来将RGB图像与3D空间中的点云对齐，这是构建模态间关联的先决条件。然后，在特征交互层面，我们设计了一个基于双流结构的特征交互模块，该模块并行增强模态内特征，构建模态间语义关联。同时，为了细化每个模态特征，我们引入了一个从粗到细的交互模块（CFIM）来实现不同尺度的层次特征交互。最后，在相似性融合层面，我们提出了一个相似性融合模块（SFM）来聚合来自目标的几何和纹理线索。实验表明，我们的方法在KITTI上实现了最先进的性能（与以前的多模态方法相比，成功率为39%，精度提高了42%），在NuScenes上也具有竞争力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.06794v2" target="_blank">2305.06794v2</a>
                              </td>
                              <td>MMF-Track: Multi-modal Multi-level Fusion for 3D Single Object Tracking</td>
                              <td>Zhiheng Li</td>
                              <td>2023-05-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_06794v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.06794v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_06147v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Efficient Large-scale AUV-based Visual Seafloor Mapping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_06147v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_06147v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_06147v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Driven by the increasing number of marine data science applications, there is a growing interest in surveying and exploring the vast, uncharted terrain of the deep sea with robotic platforms. Despite impressive results achieved by many on-land visual mapping algorithms in the past decades, transferring these methods from land to the deep sea remains a challenge due to harsh environmental conditions. Typically, deep-sea exploration involves the use of autonomous underwater vehicles (AUVs) equipped with high-resolution cameras and artificial illumination systems. However, images obtained in this manner often suffer from heterogeneous illumination and quality degradation due to attenuation and scattering, on top of refraction of light rays. All of this together often lets on-land SLAM approaches fail underwater or makes Structure-from-Motion approaches drift or omit difficult images, resulting in gaps, jumps or weakly registered areas. In this work, we present a system that incorporates recent developments in underwater imaging and visual mapping to facilitate automated robotic 3D reconstruction of hectares of seafloor. Our approach is efficient in that it detects and reconsiders difficult, weakly registered areas, to avoid omitting images and to make better use of limited dive time; on the other hand it is computationally efficient; leveraging a hybrid approach combining benefits from SLAM and Structure-from-Motion that runs much faster than incremental reconstructions while achieving at least on-par performance. The proposed system has been extensively tested and evaluated during several research cruises, demonstrating its robustness and practicality in real-world conditions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_06147v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在越来越多的海洋数据科学应用的推动下，人们对使用机器人平台测量和探索广阔、未知的深海地形越来越感兴趣。尽管在过去几十年中，许多陆地视觉地图算法取得了令人印象深刻的成果，但由于恶劣的环境条件，将这些方法从陆地转移到深海仍然是一个挑战。通常，深海探测涉及使用配备高分辨率相机和人工照明系统的自动水下航行器。然而，以这种方式获得的图像除了光线的折射之外，还经常由于衰减和散射而遭受不均匀照明和质量下降。所有这些加在一起通常会使陆上SLAM方法在水下失败，或者使“运动结构”方法漂移或忽略困难的图像，从而导致间隙、跳跃或弱配准区域。在这项工作中，我们提出了一个系统，该系统结合了水下成像和视觉地图的最新发展，以促进机器人对公顷海底的自动3D重建。我们的方法是有效的，因为它检测并重新考虑困难的、弱配准的区域，以避免遗漏图像，并更好地利用有限的潜水时间；另一方面，它在计算上是高效的；利用结合SLAM和Structure from Motion的优点的混合方法，该方法比增量重建运行得快得多，同时至少实现了同等性能。所提出的系统在几次研究巡航中进行了广泛的测试和评估，证明了其在现实世界条件下的稳健性和实用性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.06147v1" target="_blank">2308.06147v1</a>
                              </td>
                              <td>Efficient Large-scale AUV-based Visual Seafloor Mapping</td>
                              <td>Mengkun She</td>
                              <td>2023-08-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_06147v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.06147v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_10544v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">EC-SfM: Efficient Covisibility-based Structure-from-Motion for Both Sequential and Unordered Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_10544v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_10544v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_10544v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Structure-from-Motion is a technology used to obtain scene structure through image collection, which is a fundamental problem in computer vision. For unordered Internet images, SfM is very slow due to the lack of prior knowledge about image overlap. For sequential images, knowing the large overlap between adjacent frames, SfM can adopt a variety of acceleration strategies, which are only applicable to sequential data. To further improve the reconstruction efficiency and break the gap of strategies between these two kinds of data, this paper presents an efficient covisibility-based incremental SfM. Different from previous methods, we exploit covisibility and registration dependency to describe the image connection which is suitable to any kind of data. Based on this general image connection, we propose a unified framework to efficiently reconstruct sequential images, unordered images, and the mixture of these two. Experiments on the unordered images and mixed data verify the effectiveness of the proposed method, which is three times faster than the state of the art on feature matching, and an order of magnitude faster on reconstruction without sacrificing the accuracy. The source code is publicly available at https://github.com/openxrlab/xrsfm</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_10544v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>运动结构是一种通过图像采集获得场景结构的技术，是计算机视觉中的一个基本问题。对于无序的互联网图像，由于缺乏图像重叠的先验知识，SfM非常慢。对于序列图像，由于知道相邻帧之间有很大的重叠，SfM可以采用各种加速策略，这些策略仅适用于序列数据。为了进一步提高重建效率，打破这两种数据之间策略的差距，本文提出了一种有效的基于共视性的增量SfM。与以往的方法不同，我们利用共视性和配准依赖性来描述适用于任何类型数据的图像连接。基于这种通用的图像连接，我们提出了一个统一的框架来有效地重建序列图像、无序图像以及这两者的混合图像。在无序图像和混合数据上的实验验证了所提出方法的有效性，该方法在特征匹配方面比现有技术快三倍，在不牺牲精度的情况下重建速度快一个数量级。源代码可在https://github.com/openxrlab/xrsfm</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.10544v2" target="_blank">2302.10544v2</a>
                              </td>
                              <td>EC-SfM: Efficient Covisibility-based Structure-from-Motion for Both Sequential and Unordered Images</td>
                              <td>Zhichao Ye</td>
                              <td>2023-02-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_10544v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.10544v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_02670v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">EDI: ESKF-based Disjoint Initialization for Visual-Inertial SLAM Systems</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_02670v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_02670v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_02670v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual-inertial initialization can be classified into joint and disjoint approaches. Joint approaches tackle both the visual and the inertial parameters together by aligning observations from feature-bearing points based on IMU integration then use a closed-form solution with visual and acceleration observations to find initial velocity and gravity. In contrast, disjoint approaches independently solve the Structure from Motion (SFM) problem and determine inertial parameters from up-to-scale camera poses obtained from pure monocular SLAM. However, previous disjoint methods have limitations, like assuming negligible acceleration bias impact or accurate rotation estimation by pure monocular SLAM. To address these issues, we propose EDI, a novel approach for fast, accurate, and robust visual-inertial initialization. Our method incorporates an Error-state Kalman Filter (ESKF) to estimate gyroscope bias and correct rotation estimates from monocular SLAM, overcoming dependence on pure monocular SLAM for rotation estimation. To estimate the scale factor without prior information, we offer a closed-form solution for initial velocity, scale, gravity, and acceleration bias estimation. To address gravity and acceleration bias coupling, we introduce weights in the linear least-squares equations, ensuring acceleration bias observability and handling outliers. Extensive evaluation on the EuRoC dataset shows that our method achieves an average scale error of 5.8% in less than 3 seconds, outperforming other state-of-the-art disjoint visual-inertial initialization approaches, even in challenging environments and with artificial noise corruption.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_02670v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉惯性初始化可以分为联合方法和不相交方法。联合方法通过基于IMU积分对齐特征承载点的观测结果，将视觉和惯性参数结合在一起，然后使用视觉和加速度观测的闭合形式解来找到初始速度和重力。相反，不相交的方法独立地解决了运动结构（SFM）问题，并根据从纯单目SLAM获得的高比例相机姿态确定惯性参数。然而，以前的不相交方法有局限性，比如假设加速度偏差影响可以忽略不计，或者通过纯单目SLAM进行精确的旋转估计。为了解决这些问题，我们提出了EDI，这是一种快速、准确和稳健的视觉惯性初始化的新方法。我们的方法结合了误差状态卡尔曼滤波器（ESKF）来估计陀螺仪偏差，并校正单目SLAM的旋转估计，克服了对纯单目SLAM旋转估计的依赖。为了在没有先验信息的情况下估计比例因子，我们为初始速度、比例、重力和加速度偏差估计提供了一个闭合形式的解决方案。为了解决重力和加速度偏差的耦合问题，我们在线性最小二乘方程中引入了权重，以确保加速度偏差的可观察性并处理异常值。对EuRoC数据集的广泛评估表明，我们的方法在不到3秒内实现了5.8%的平均尺度误差，即使在具有挑战性的环境和人工噪声破坏的情况下，也优于其他最先进的不相交视觉惯性初始化方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.02670v1" target="_blank">2308.02670v1</a>
                              </td>
                              <td>EDI: ESKF-based Disjoint Initialization for Visual-Inertial SLAM Systems</td>
                              <td>Weihan Wang</td>
                              <td>2023-08-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_02670v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.02670v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01125v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Stereo Visual Odometry with Deep Learning-Based Point and Line Feature Matching using an Attention Graph Neural Network</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01125v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01125v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01125v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Robust feature matching forms the backbone for most Visual Simultaneous Localization and Mapping (vSLAM), visual odometry, 3D reconstruction, and Structure from Motion (SfM) algorithms. However, recovering feature matches from texture-poor scenes is a major challenge and still remains an open area of research. In this paper, we present a Stereo Visual Odometry (StereoVO) technique based on point and line features which uses a novel feature-matching mechanism based on an Attention Graph Neural Network that is designed to perform well even under adverse weather conditions such as fog, haze, rain, and snow, and dynamic lighting conditions such as nighttime illumination and glare scenarios. We perform experiments on multiple real and synthetic datasets to validate the ability of our method to perform StereoVO under low visibility weather and lighting conditions through robust point and line matches. The results demonstrate that our method achieves more line feature matches than state-of-the-art line matching algorithms, which when complemented with point feature matches perform consistently well in adverse weather and dynamic lighting conditions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01125v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>稳健的特征匹配构成了大多数视觉同步定位和映射（vSLAM）、视觉里程计、3D重建和运动结构（SfM）算法的支柱。然而，从纹理差的场景中恢复特征匹配是一个重大挑战，并且仍然是一个开放的研究领域。在本文中，我们提出了一种基于点和线特征的立体视觉Odometry（StereoVO）技术，该技术使用了一种新的基于注意力图神经网络的特征匹配机制，即使在雾、霾、雨和雪等恶劣天气条件以及夜间照明和眩光等动态照明条件下也能表现良好。我们在多个真实和合成数据集上进行了实验，以验证我们的方法通过稳健的点和线匹配在低能见度天气和照明条件下执行StereoVO的能力。结果表明，与最先进的线匹配算法相比，我们的方法实现了更多的线特征匹配，当与点特征匹配相补充时，线匹配算法在恶劣天气和动态照明条件下始终表现良好。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01125v1" target="_blank">2308.01125v1</a>
                              </td>
                              <td>Stereo Visual Odometry with Deep Learning-Based Point and Line Feature Matching using an Attention Graph Neural Network</td>
                              <td>Shenbagaraj Kannapiran</td>
                              <td>2023-08-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01125v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01125v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_11702v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SACReg: Scene-Agnostic Coordinate Regression for Visual Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_11702v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_11702v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_11702v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Scene coordinates regression (SCR), i.e., predicting 3D coordinates for every pixel of a given image, has recently shown promising potential. However, existing methods remain mostly scene-specific or limited to small scenes and thus hardly scale to realistic datasets. In this paper, we propose a new paradigm where a single generic SCR model is trained once to be then deployed to new test scenes, regardless of their scale and without further finetuning. For a given query image, it collects inputs from off-the-shelf image retrieval techniques and Structure-from-Motion databases: a list of relevant database images with sparse pointwise 2D-3D annotations. The model is based on the transformer architecture and can take a variable number of images and sparse 2D-3D annotations as input. It is trained on a few diverse datasets and significantly outperforms other scene regression approaches on several benchmarks, including scene-specific models, for visual localization. In particular, we set a new state of the art on the Cambridge localization benchmark, even outperforming feature-matching-based approaches.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_11702v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>场景坐标回归（SCR），即预测给定图像的每个像素的3D坐标，最近显示出了很有前途的潜力。然而，现有的方法大多是针对场景的，或者仅限于小场景，因此很难扩展到真实的数据集。在本文中，我们提出了一种新的范式，其中单个通用SCR模型被训练一次，然后部署到新的测试场景中，而不考虑其规模，也不需要进一步的微调。对于给定的查询图像，它从现成的图像检索技术和运动数据库的结构中收集输入：具有稀疏逐点2D-3D注释的相关数据库图像列表。该模型基于转换器架构，并且可以采用可变数量的图像和稀疏的2D-3D注释作为输入。它在几个不同的数据集上进行了训练，在视觉定位方面，它在几个基准测试（包括特定场景的模型）上显著优于其他场景回归方法。特别是，我们在剑桥本地化基准上设定了一个新的技术水平，甚至优于基于特征匹配的方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.11702v2" target="_blank">2307.11702v2</a>
                              </td>
                              <td>SACReg: Scene-Agnostic Coordinate Regression for Visual Localization</td>
                              <td>Jerome Revaud</td>
                              <td>2023-07-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_11702v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.11702v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_15055v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_15055v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_15055v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_15055v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce PointOdyssey, a large-scale synthetic dataset, and data generation framework, for the training and evaluation of long-term fine-grained tracking algorithms. Our goal is to advance the state-of-the-art by placing emphasis on long videos with naturalistic motion. Toward the goal of naturalism, we animate deformable characters using real-world motion capture data, we build 3D scenes to match the motion capture environments, and we render camera viewpoints using trajectories mined via structure-from-motion on real videos. We create combinatorial diversity by randomizing character appearance, motion profiles, materials, lighting, 3D assets, and atmospheric effects. Our dataset currently includes 104 videos, averaging 2,000 frames long, with orders of magnitude more correspondence annotations than prior work. We show that existing methods can be trained from scratch in our dataset and outperform the published variants. Finally, we introduce modifications to the PIPs point tracking method, greatly widening its temporal receptive field, which improves its performance on PointOdyssey as well as on two real-world benchmarks. Our data and code are publicly available at: https://pointodyssey.com</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_15055v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了PointOdyssey，一个大规模的合成数据集和数据生成框架，用于长期细粒度跟踪算法的训练和评估。我们的目标是通过强调自然运动的长视频来推进最先进的技术。为了实现自然主义的目标，我们使用真实世界的运动捕捉数据制作可变形角色的动画，我们构建3D场景以匹配运动捕捉环境，我们使用通过真实视频上的运动结构挖掘的轨迹来渲染相机视点。我们通过随机化角色外观、运动剖面、材质、照明、3D资产和大气效果来创造组合多样性。我们的数据集目前包括104个视频，平均2000帧长，与之前的工作相比，对应注释多了几个数量级。我们表明，现有的方法可以在我们的数据集中从头开始训练，并且优于已发布的变体。最后，我们介绍了对PIP点跟踪方法的修改，极大地拓宽了其时间感受野，这提高了其在PointOdyssey和两个真实世界基准上的性能。我们的数据和代码可在以下网址公开获取：https://pointodyssey.com</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.15055v1" target="_blank">2307.15055v1</a>
                              </td>
                              <td>PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking</td>
                              <td>Yang Zheng</td>
                              <td>2023-07-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_15055v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.15055v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_07250v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fusing Structure from Motion and Simulation-Augmented Pose Regression from Optical Flow for Challenging Indoor Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_07250v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_07250v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_07250v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The localization of objects is a crucial task in various applications such as robotics, virtual and augmented reality, and the transportation of goods in warehouses. Recent advances in deep learning have enabled the localization using monocular visual cameras. While structure from motion (SfM) predicts the absolute pose from a point cloud, absolute pose regression (APR) methods learn a semantic understanding of the environment through neural networks. However, both fields face challenges caused by the environment such as motion blur, lighting changes, repetitive patterns, and feature-less structures. This study aims to address these challenges by incorporating additional information and regularizing the absolute pose using relative pose regression (RPR) methods. RPR methods suffer under different challenges, i.e., motion blur. The optical flow between consecutive images is computed using the Lucas-Kanade algorithm, and the relative pose is predicted using an auxiliary small recurrent convolutional network. The fusion of absolute and relative poses is a complex task due to the mismatch between the global and local coordinate systems. State-of-the-art methods fusing absolute and relative poses use pose graph optimization (PGO) to regularize the absolute pose predictions using relative poses. In this work, we propose recurrent fusion networks to optimally align absolute and relative pose predictions to improve the absolute pose prediction. We evaluate eight different recurrent units and construct a simulation environment to pre-train the APR and RPR networks for better generalized training. Additionally, we record a large database of different scenarios in a challenging large-scale indoor environment that mimics a warehouse with transportation robots. We conduct hyperparameter searches and experiments to show the effectiveness of our recurrent fusion method compared to PGO.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_07250v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>物体的定位在机器人、虚拟和增强现实以及仓库中的货物运输等各种应用中是一项至关重要的任务。深度学习的最新进展使得能够使用单目视觉相机进行定位。虽然运动结构（SfM）从点云预测绝对姿态，但绝对姿态回归（APR）方法通过神经网络学习对环境的语义理解。然而，这两个领域都面临着环境带来的挑战，如运动模糊、照明变化、重复模式和无特征结构。本研究旨在通过结合额外信息和使用相对姿态回归（RPR）方法规范绝对姿态来应对这些挑战。RPR方法面临不同的挑战，即运动模糊。使用Lucas Kanade算法计算连续图像之间的光流，并使用辅助的小递归卷积网络预测相对姿态。由于全局坐标系和局部坐标系之间的不匹配，绝对姿态和相对姿态的融合是一项复杂的任务。融合绝对姿态和相对姿态的现有技术方法使用姿态图优化（PGO）来使用相对姿态正则化绝对姿态预测。在这项工作中，我们提出了递归融合网络来优化绝对和相对姿态预测，以改进绝对姿态预测。我们评估了八个不同的递归单元，并构建了一个模拟环境来预训练APR和RPR网络，以便更好地进行广义训练。此外，我们在一个具有挑战性的大型室内环境中记录了不同场景的大型数据库，该环境模拟了带有运输机器人的仓库。我们进行了超参数搜索和实验，以显示与PGO相比，我们的递归融合方法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.07250v2" target="_blank">2304.07250v2</a>
                              </td>
                              <td>Fusing Structure from Motion and Simulation-Augmented Pose Regression from Optical Flow for Challenging Indoor Environments</td>
                              <td>Felix Ott</td>
                              <td>2023-04-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_07250v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.07250v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09981v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Lazy Visual Localization via Motion Averaging</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09981v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09981v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09981v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual (re)localization is critical for various applications in computer vision and robotics. Its goal is to estimate the 6 degrees of freedom (DoF) camera pose for each query image, based on a set of posed database images. Currently, all leading solutions are structure-based that either explicitly construct 3D metric maps from the database with structure-from-motion, or implicitly encode the 3D information with scene coordinate regression models. On the contrary, visual localization without reconstructing the scene in 3D offers clear benefits. It makes deployment more convenient by reducing database pre-processing time, releasing storage requirements, and remaining unaffected by imperfect reconstruction, etc. In this technical report, we demonstrate that it is possible to achieve high localization accuracy without reconstructing the scene from the database. The key to achieving this owes to a tailored motion averaging over database-query pairs. Experiments show that our visual localization proposal, LazyLoc, achieves comparable performance against state-of-the-art structure-based methods. Furthermore, we showcase the versatility of LazyLoc, which can be easily extended to handle complex configurations such as multi-query co-localization and camera rigs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09981v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉（再）定位对于计算机视觉和机器人的各种应用至关重要。其目标是基于一组摆姿势的数据库图像，估计每个查询图像的6个自由度（DoF）相机姿势。目前，所有领先的解决方案都是基于结构的，它们要么从数据库中显式地构建具有运动结构的3D度量图，要么用场景坐标回归模型隐式地编码3D信息。相反，在不重建3D场景的情况下进行视觉定位提供了明显的好处。它通过减少数据库预处理时间、释放存储需求、不受不完美重建的影响等方式使部署更加方便。在本技术报告中，我们证明了在不从数据库重建场景的情况下实现高定位精度是可能的。实现这一点的关键在于对数据库查询对进行定制的运动平均。实验表明，我们的视觉定位方案LazyLoc与最先进的基于结构的方法相比，具有相当的性能。此外，我们还展示了LazyLoc的多功能性，它可以很容易地扩展到处理复杂的配置，如多查询协同定位和相机钻机。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09981v1" target="_blank">2307.09981v1</a>
                              </td>
                              <td>Lazy Visual Localization via Motion Averaging</td>
                              <td>Siyan Dong</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09981v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09981v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07524v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Reducing Causality to Functions with Structural Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07524v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07524v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07524v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The precise definition of causality is currently an open problem in philosophy and statistics. We believe causality should be defined as functions (in mathematics) that map causes to effects. We propose a reductive definition of causality based on Structural Functional Model (SFM). Using delta compression and contrastive forward inference, SFM can produce causal utterances like "X causes Y" and "X is the cause of Y" that match our intuitions. We compile a dataset of causal scenarios and use SFM in all of them. SFM is compatible with but not reducible to probability theory. We also compare SFM with other theories of causation and apply SFM to downstream problems like free will, causal explanation, and mental causation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07524v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>因果关系的精确定义目前是哲学和统计学中一个悬而未决的问题。我们认为因果关系应该被定义为（在数学中）将原因映射到效果的函数。基于结构函数模型，我们提出了因果关系的简化定义。使用delta压缩和对比前向推理，SFM可以产生与我们的直觉相匹配的因果话语，如“X导致Y”和“X是Y的原因”。我们编译了一个因果场景的数据集，并在所有场景中使用SFM。SFM与概率论是相容的，但不可简化为概率论。我们还将SFM与其他因果关系理论进行了比较，并将SFM应用于自由意志、因果解释和精神因果关系等下游问题。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07524v1" target="_blank">2307.07524v1</a>
                              </td>
                              <td>Reducing Causality to Functions with Structural Models</td>
                              <td>Tianyi Miao</td>
                              <td>2023-07-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07524v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07524v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04520v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Efficient Match Pair Retrieval for Large-scale UAV Images via Graph Indexed Global Descriptor</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04520v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04520v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04520v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>SfM (Structure from Motion) has been extensively used for UAV (Unmanned Aerial Vehicle) image orientation. Its efficiency is directly influenced by feature matching. Although image retrieval has been extensively used for match pair selection, high computational costs are consumed due to a large number of local features and the large size of the used codebook. Thus, this paper proposes an efficient match pair retrieval method and implements an integrated workflow for parallel SfM reconstruction. First, an individual codebook is trained online by considering the redundancy of UAV images and local features, which avoids the ambiguity of training codebooks from other datasets. Second, local features of each image are aggregated into a single high-dimension global descriptor through the VLAD (Vector of Locally Aggregated Descriptors) aggregation by using the trained codebook, which remarkably reduces the number of features and the burden of nearest neighbor searching in image indexing. Third, the global descriptors are indexed via the HNSW (Hierarchical Navigable Small World) based graph structure for the nearest neighbor searching. Match pairs are then retrieved by using an adaptive threshold selection strategy and utilized to create a view graph for divide-and-conquer based parallel SfM reconstruction. Finally, the performance of the proposed solution has been verified using three large-scale UAV datasets. The test results demonstrate that the proposed solution accelerates match pair retrieval with a speedup ratio ranging from 36 to 108 and improves the efficiency of SfM reconstruction with competitive accuracy in both relative and absolute orientation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04520v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>SfM（Structure from Motion）已被广泛用于无人机（UAV）的图像定向。其效率直接受到特征匹配的影响。尽管图像检索已被广泛用于匹配对选择，但由于大量的局部特征和所使用的码本的大尺寸，消耗了高计算成本。因此，本文提出了一种高效的匹配对检索方法，并实现了一个用于并行SfM重建的集成工作流。首先，考虑无人机图像和局部特征的冗余性，在线训练单个码本，避免了其他数据集训练码本的模糊性。其次，通过使用训练后的码本进行VLAD（Vector of Locally aggregated Descriptors）聚合，将每个图像的局部特征聚合为单个高维全局描述符，显著减少了图像索引中特征的数量和最近邻搜索的负担。第三，通过基于HNSW（分层导航小世界）的图结构对全局描述符进行索引，用于最近邻居搜索。然后通过使用自适应阈值选择策略来检索匹配对，并用于创建用于基于分治的并行SfM重建的视图图。最后，使用三个大型无人机数据集验证了所提出的解决方案的性能。测试结果表明，所提出的解决方案以36到108的加速比加速了匹配对检索，并在相对和绝对方向上以具有竞争力的精度提高了SfM重建的效率。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04520v1" target="_blank">2307.04520v1</a>
                              </td>
                              <td>Efficient Match Pair Retrieval for Large-scale UAV Images via Graph Indexed Global Descriptor</td>
                              <td>San Jiang</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04520v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04520v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03404v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RGB-D Mapping and Tracking in a Plenoxel Radiance Field</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03404v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03404v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03404v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Building on the success of Neural Radiance Fields (NeRFs), recent years have seen significant advances in the domain of novel view synthesis. These models capture the scene's volumetric radiance field, creating highly convincing dense photorealistic models through the use of simple, differentiable rendering equations. Despite their popularity, these algorithms suffer from severe ambiguities in visual data inherent to the RGB sensor, which means that although images generated with view synthesis can visually appear very believable, the underlying 3D model will often be wrong. This considerably limits the usefulness of these models in practical applications like Robotics and Extended Reality (XR), where an accurate dense 3D reconstruction otherwise would be of significant value. In this technical report, we present the vital differences between view synthesis models and 3D reconstruction models. We also comment on why a depth sensor is essential for modeling accurate geometry in general outward-facing scenes using the current paradigm of novel view synthesis methods. Focusing on the structure-from-motion task, we practically demonstrate this need by extending the Plenoxel radiance field model: Presenting an analytical differential approach for dense mapping and tracking with radiance fields based on RGB-D data without a neural network. Our method achieves state-of-the-art results in both the mapping and tracking tasks while also being faster than competing neural network-based approaches.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03404v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在神经辐射场（NeRFs）成功的基础上，近年来在新视图合成领域取得了重大进展。这些模型捕捉了场景的体积辐射场，通过使用简单、可微分的渲染方程创建了令人信服的密集真实感模型。尽管这些算法很受欢迎，但RGB传感器固有的视觉数据存在严重的模糊性，这意味着尽管通过视图合成生成的图像在视觉上看起来非常可信，但底层的3D模型往往是错误的。这大大限制了这些模型在机器人和扩展现实（XR）等实际应用中的有用性，否则，精确的密集3D重建将具有重要价值。在本技术报告中，我们介绍了视图合成模型和三维重建模型之间的重要区别。我们还评论了为什么深度传感器对于使用当前新型视图合成方法的范式在一般面向外的场景中建模精确的几何体至关重要。专注于运动任务中的结构，我们通过扩展Plenoxel辐射场模型来实际证明这一需求：在没有神经网络的情况下，基于RGB-D数据，提出了一种用于辐射场密集映射和跟踪的解析微分方法。我们的方法在映射和跟踪任务中都取得了最先进的结果，同时也比竞争对手的基于神经网络的方法更快。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03404v1" target="_blank">2307.03404v1</a>
                              </td>
                              <td>RGB-D Mapping and Tracking in a Plenoxel Radiance Field</td>
                              <td>Andreas L. Teigen</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03404v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03404v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_01817v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Human Trajectory Forecasting with Explainable Behavioral Uncertainty</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_01817v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_01817v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_01817v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Human trajectory forecasting helps to understand and predict human behaviors, enabling applications from social robots to self-driving cars, and therefore has been heavily investigated. Most existing methods can be divided into model-free and model-based methods. Model-free methods offer superior prediction accuracy but lack explainability, while model-based methods provide explainability but cannot predict well. Combining both methodologies, we propose a new Bayesian Neural Stochastic Differential Equation model BNSP-SFM, where a behavior SDE model is combined with Bayesian neural networks (BNNs). While the NNs provide superior predictive power, the SDE offers strong explainability with quantifiable uncertainty in behavior and observation. We show that BNSP-SFM achieves up to a 50% improvement in prediction accuracy, compared with 11 state-of-the-art methods. BNSP-SFM also generalizes better to drastically different scenes with different environments and crowd densities (~ 20 times higher than the testing data). Finally, BNSP-SFM can provide predictions with confidence to better explain potential causes of behaviors. The code will be released upon acceptance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_01817v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人类轨迹预测有助于理解和预测人类行为，实现从社交机器人到自动驾驶汽车的应用，因此受到了大量研究。大多数现有的方法可以分为无模型方法和基于模型的方法。无模型方法提供了优越的预测精度但缺乏可解释性，而基于模型的方法提供了可解释性但不能很好地预测。结合这两种方法，我们提出了一个新的贝叶斯神经随机微分方程模型BNSP-SFM，其中行为SDE模型与贝叶斯神经网络（BNNs）相结合。虽然神经网络提供了卓越的预测能力，但SDE提供了强大的可解释性，在行为和观察方面具有可量化的不确定性。我们表明，与11种最先进的方法相比，BNSP-SFM的预测精度提高了50%。BNSP-SFM还可以更好地推广到具有不同环境和人群密度的截然不同的场景（比测试数据高出约20倍）。最后，BNSP-SFM可以提供有信心的预测，以更好地解释行为的潜在原因。该代码将在验收后发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.01817v1" target="_blank">2307.01817v1</a>
                              </td>
                              <td>Human Trajectory Forecasting with Explainable Behavioral Uncertainty</td>
                              <td>Jiangbei Yue</td>
                              <td>2023-07-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_01817v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.01817v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16917v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Drunkard's Odometry: Estimating Camera Motion in Deforming Scenes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16917v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16917v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16917v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Estimating camera motion in deformable scenes poses a complex and open research challenge. Most existing non-rigid structure from motion techniques assume to observe also static scene parts besides deforming scene parts in order to establish an anchoring reference. However, this assumption does not hold true in certain relevant application cases such as endoscopies. Deformable odometry and SLAM pipelines, which tackle the most challenging scenario of exploratory trajectories, suffer from a lack of robustness and proper quantitative evaluation methodologies. To tackle this issue with a common benchmark, we introduce the Drunkard's Dataset, a challenging collection of synthetic data targeting visual navigation and reconstruction in deformable environments. This dataset is the first large set of exploratory camera trajectories with ground truth inside 3D scenes where every surface exhibits non-rigid deformations over time. Simulations in realistic 3D buildings lets us obtain a vast amount of data and ground truth labels, including camera poses, RGB images and depth, optical flow and normal maps at high resolution and quality. We further present a novel deformable odometry method, dubbed the Drunkard's Odometry, which decomposes optical flow estimates into rigid-body camera motion and non-rigid scene deformations. In order to validate our data, our work contains an evaluation of several baselines as well as a novel tracking error metric which does not require ground truth data. Dataset and code: https://davidrecasens.github.io/TheDrunkard'sOdometry/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16917v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在可变形场景中估计相机运动是一个复杂而开放的研究挑战。大多数现有的非刚性结构运动技术假设除了变形场景部分之外，还观察静态场景部分，以建立锚定参考。然而，这一假设在某些相关应用案例中并不成立，例如内镜。可变形里程计和SLAM管道解决了最具挑战性的探索轨迹场景，但缺乏稳健性和适当的定量评估方法。为了用一个通用的基准来解决这个问题，我们引入了Drunkard的数据集，这是一个具有挑战性的合成数据集，旨在在可变形环境中进行视觉导航和重建。该数据集是第一个在3D场景中具有地面实况的大型探索相机轨迹集，其中每个表面随着时间的推移都表现出非刚性变形。在逼真的3D建筑中进行模拟可以让我们获得大量的数据和地面实况标签，包括相机姿态、RGB图像和深度、光流和高分辨率和高质量的法线图。我们进一步提出了一种新的可变形里程计方法，称为Drunkard里程计，该方法将光流估计分解为刚体相机运动和非刚体场景变形。为了验证我们的数据，我们的工作包括对几个基线的评估，以及一种新的跟踪误差度量，该度量不需要地面实况数据。数据集和代码：https://davidrecasens.github.io/TheDrunkard'国内/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16917v1" target="_blank">2306.16917v1</a>
                              </td>
                              <td>The Drunkard's Odometry: Estimating Camera Motion in Deforming Scenes</td>
                              <td>David Recasens</td>
                              <td>2023-06-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16917v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16917v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_15667v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_15667v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_15667v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_15667v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Camera pose estimation is a long-standing computer vision problem that to date often relies on classical methods, such as handcrafted keypoint matching, RANSAC and bundle adjustment. In this paper, we propose to formulate the Structure from Motion (SfM) problem inside a probabilistic diffusion framework, modelling the conditional distribution of camera poses given input images. This novel view of an old problem has several advantages. (i) The nature of the diffusion framework mirrors the iterative procedure of bundle adjustment. (ii) The formulation allows a seamless integration of geometric constraints from epipolar geometry. (iii) It excels in typically difficult scenarios such as sparse views with wide baselines. (iv) The method can predict intrinsics and extrinsics for an arbitrary amount of images. We demonstrate that our method PoseDiffusion significantly improves over the classic SfM pipelines and the learned approaches on two real-world datasets. Finally, it is observed that our method can generalize across datasets without further training. Project page: https://posediffusion.github.io/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_15667v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>相机姿态估计是一个长期存在的计算机视觉问题，迄今为止，它通常依赖于经典的方法，如手工关键点匹配、RANSAC和束调整。在本文中，我们建议在概率扩散框架内公式化运动结构（SfM）问题，对给定输入图像的相机姿态的条件分布进行建模。这种对老问题的新颖看法有几个优点。（i） 扩散框架的性质反映了束调整的迭代过程。（ii）该公式允许来自核极几何的几何约束的无缝集成。（iii）它在典型的困难场景中表现出色，例如具有宽基线的稀疏视图。（iv）该方法可以预测任意数量的图像的内在和外在。我们在两个真实世界的数据集上证明了我们的方法PoseDiffusion比经典的SfM管道和学习的方法有了显著的改进。最后，我们观察到，我们的方法可以在不需要进一步训练的情况下在数据集之间进行推广。项目页面：https://posediffusion.github.io/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.15667v2" target="_blank">2306.15667v2</a>
                              </td>
                              <td>PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment</td>
                              <td>Jianyuan Wang</td>
                              <td>2023-06-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_15667v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.15667v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_15669v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Detector-Free Structure from Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_15669v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_15669v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_15669v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a new structure-from-motion framework to recover accurate camera poses and point clouds from unordered images. Traditional SfM systems typically rely on the successful detection of repeatable keypoints across multiple views as the first step, which is difficult for texture-poor scenes, and poor keypoint detection may break down the whole SfM system. We propose a new detector-free SfM framework to draw benefits from the recent success of detector-free matchers to avoid the early determination of keypoints, while solving the multi-view inconsistency issue of detector-free matchers. Specifically, our framework first reconstructs a coarse SfM model from quantized detector-free matches. Then, it refines the model by a novel iterative refinement pipeline, which iterates between an attention-based multi-view matching module to refine feature tracks and a geometry refinement module to improve the reconstruction accuracy. Experiments demonstrate that the proposed framework outperforms existing detector-based SfM systems on common benchmark datasets. We also collect a texture-poor SfM dataset to demonstrate the capability of our framework to reconstruct texture-poor scenes. Based on this framework, we take $\textit{first place}$ in Image Matching Challenge 2023.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_15669v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们从运动框架中提出了一种新的结构，以从无序图像中恢复准确的相机姿态和点云。传统的SfM系统通常依赖于跨多个视图的可重复关键点的成功检测作为第一步，这对于纹理较差的场景来说是困难的，并且较差的关键点检测可能会破坏整个SfM体系。我们提出了一种新的无检测器SfM框架，以从无检测器匹配器最近的成功中获益，避免早期确定关键点，同时解决无检测器匹配的多视图不一致问题。具体来说，我们的框架首先从量化的无检测器匹配中重建粗略的SfM模型。然后，它通过一种新的迭代精化流水线对模型进行精化，该流水线在基于注意力的多视图匹配模块和几何精化模块之间迭代以精化特征轨迹，从而提高重建精度。实验表明，该框架在通用基准数据集上优于现有的基于检测器的SfM系统。我们还收集了一个纹理较差的SfM数据集，以证明我们的框架重建纹理较差场景的能力。基于这个框架，我们在2023年的图像匹配挑战中获得$\textit｛first place｝$。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.15669v1" target="_blank">2306.15669v1</a>
                              </td>
                              <td>Detector-Free Structure from Motion</td>
                              <td>Xingyi He</td>
                              <td>2023-06-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_15669v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.15669v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_12770v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D Reconstruction of Spherical Images based on Incremental Structure from Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_12770v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_12770v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_12770v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>3D reconstruction plays an increasingly important role in modern photogrammetric systems. Conventional satellite or aerial-based remote sensing (RS) platforms can provide the necessary data sources for the 3D reconstruction of large-scale landforms and cities. Even with low-altitude UAVs (Unmanned Aerial Vehicles), 3D reconstruction in complicated situations, such as urban canyons and indoor scenes, is challenging due to the frequent tracking failures between camera frames and high data collection costs. Recently, spherical images have been extensively exploited due to the capability of recording surrounding environments from one camera exposure. Classical 3D reconstruction pipelines, however, cannot be used for spherical images. Besides, there exist few software packages for 3D reconstruction of spherical images. Based on the imaging geometry of spherical cameras, this study investigates the algorithms for the relative orientation using spherical correspondences, absolute orientation using 3D correspondences between scene and spherical points, and the cost functions for BA (bundle adjustment) optimization. In addition, an incremental SfM (Structure from Motion) workflow has been proposed for spherical images using the above-mentioned algorithms. The proposed solution is finally verified by using three spherical datasets captured by both consumer-grade and professional spherical cameras. The results demonstrate that the proposed SfM workflow can achieve the successful 3D reconstruction of complex scenes and provide useful clues for the implementation in open-source software packages. The source code of the designed SfM workflow would be made publicly available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_12770v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三维重建在现代摄影测量系统中发挥着越来越重要的作用。传统的卫星或航空遥感平台可以为大规模地形和城市的三维重建提供必要的数据源。即使使用低空无人机，由于相机帧之间频繁的跟踪故障和高昂的数据收集成本，在城市峡谷和室内场景等复杂情况下的3D重建也具有挑战性。最近，球形图像由于能够通过一台相机曝光记录周围环境而被广泛利用。然而，经典的3D重建管道不能用于球面图像。此外，用于球面图像的三维重建的软件包很少。基于球面相机的成像几何，研究了使用球面对应关系的相对方位、使用场景与球面点之间的3D对应关系的绝对方位以及BA（束调整）优化的成本函数的算法。此外，已经提出了使用上述算法的球面图像的增量SfM（运动结构）工作流程。通过使用消费者级和专业球形相机拍摄的三个球形数据集，最终验证了所提出的解决方案。结果表明，所提出的SfM工作流可以成功地实现复杂场景的三维重建，并为开源软件包的实现提供了有用的线索。设计的SfM工作流程的源代码将公开。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.12770v2" target="_blank">2306.12770v2</a>
                              </td>
                              <td>3D Reconstruction of Spherical Images based on Incremental Structure from Motion</td>
                              <td>San Jiang</td>
                              <td>2023-06-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_12770v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.12770v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09109v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09109v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09109v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09109v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in neural reconstruction enable high-quality 3D object reconstruction from casually captured image collections. Current techniques mostly analyze their progress on relatively simple image collections where Structure-from-Motion (SfM) techniques can provide ground-truth (GT) camera poses. We note that SfM techniques tend to fail on in-the-wild image collections such as image search results with varying backgrounds and illuminations. To enable systematic research progress on 3D reconstruction from casual image captures, we propose NAVI: a new dataset of category-agnostic image collections of objects with high-quality 3D scans along with per-image 2D-3D alignments providing near-perfect GT camera parameters. These 2D-3D alignments allow us to extract accurate derivative annotations such as dense pixel correspondences, depth and segmentation maps. We demonstrate the use of NAVI image collections on different problem settings and show that NAVI enables more thorough evaluations that were not possible with existing datasets. We believe NAVI is beneficial for systematic research progress on 3D reconstruction and correspondence estimation. Project page: https://navidataset.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09109v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经重建的最新进展使得能够从随意捕获的图像集合中重建高质量的3D对象。当前的技术主要分析它们在相对简单的图像采集上的进展，其中运动结构（SfM）技术可以提供地面实况（GT）相机姿态。我们注意到，SfM技术在野生图像集合中往往失败，例如具有不同背景和照明的图像搜索结果。为了实现从偶然图像捕获的3D重建的系统研究进展，我们提出了NAVI：一个具有高质量3D扫描的对象的类别不可知图像集合的新数据集，以及提供近乎完美的GT相机参数的每张图像2D-3D对齐。这些2D-3D比对允许我们提取精确的导数注释，例如密集像素对应、深度和分割图。我们展示了NAVI图像集合在不同问题设置中的使用，并表明NAVI能够实现现有数据集无法实现的更彻底的评估。我们相信NAVI有利于三维重建和对应关系估计的系统研究进展。项目页面：https://navidataset.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09109v1" target="_blank">2306.09109v1</a>
                              </td>
                              <td>NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations</td>
                              <td>Varun Jampani</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09109v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09109v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09012v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Yes, we CANN: Constrained Approximate Nearest Neighbors for local feature-based visual localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09012v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09012v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09012v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large-scale visual localization systems continue to rely on 3D point clouds built from image collections using structure-from-motion. While the 3D points in these models are represented using local image features, directly matching a query image's local features against the point cloud is challenging due to the scale of the nearest-neighbor search problem. Many recent approaches to visual localization have thus proposed a hybrid method, where first a global (per image) embedding is used to retrieve a small subset of database images, and local features of the query are matched only against those. It seems to have become common belief that global embeddings are critical for said image-retrieval in visual localization, despite the significant downside of having to compute two feature types for each query image. In this paper, we take a step back from this assumption and propose Constrained Approximate Nearest Neighbors (CANN), a joint solution of k-nearest-neighbors across both the geometry and appearance space using only local features. We first derive the theoretical foundation for k-nearest-neighbor retrieval across multiple metrics and then showcase how CANN improves visual localization. Our experiments on public localization benchmarks demonstrate that our method significantly outperforms both state-of-the-art global feature-based retrieval and approaches using local feature aggregation schemes. Moreover, it is an order of magnitude faster in both index and query time than feature aggregation schemes for these datasets. Code will be released.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09012v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大规模视觉定位系统继续依赖于使用来自运动的结构从图像集合构建的3D点云。虽然这些模型中的3D点是使用局部图像特征来表示的，但由于最近邻居搜索问题的规模，将查询图像的局部特征与点云直接匹配是具有挑战性的。因此，许多最近的视觉定位方法都提出了一种混合方法，其中首先使用全局（每图像）嵌入来检索数据库图像的一小部分，并且查询的局部特征仅与这些特征相匹配。人们似乎普遍认为，全局嵌入对于视觉定位中的图像检索至关重要，尽管必须为每个查询图像计算两种特征类型有很大的缺点。在本文中，我们从这一假设后退了一步，提出了约束近似最近邻（CANN），这是一种仅使用局部特征在几何和外观空间上的k个最近邻的联合解决方案。我们首先推导了跨多个度量的k近邻检索的理论基础，然后展示了CANN如何改进视觉定位。我们在公共定位基准上的实验表明，我们的方法显著优于最先进的基于全局特征的检索和使用局部特征聚合方案的方法。此外，它在索引和查询时间上都比这些数据集的特征聚合方案快一个数量级。将发布代码。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09012v1" target="_blank">2306.09012v1</a>
                              </td>
                              <td>Yes, we CANN: Constrained Approximate Nearest Neighbors for local feature-based visual localization</td>
                              <td>Dror Aiger</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09012v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09012v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_06360v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D reconstruction using Structure for Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_06360v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_06360v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_06360v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We are working towards 3D reconstruction of indoor spaces using a pair of HDR cameras in a stereo vision configuration mounted on an indoor mobile floor robot that captures various textures and spatial features as 2D images and this data is simultaneously utilized as a feed to our algorithm which will allow us to visualize the depth map.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_06360v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们正在使用安装在室内移动地板机器人上的一对立体视觉配置的HDR相机对室内空间进行3D重建，该机器人将各种纹理和空间特征捕获为2D图像，这些数据同时被用作我们算法的反馈，这将使我们能够可视化深度图。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.06360v1" target="_blank">2306.06360v1</a>
                              </td>
                              <td>3D reconstruction using Structure for Motion</td>
                              <td>Kshitij Karnawat</td>
                              <td>2023-06-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_06360v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.06360v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_05410v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_05410v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_05410v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_05410v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A critical obstacle preventing NeRF models from being deployed broadly in the wild is their reliance on accurate camera poses. Consequently, there is growing interest in extending NeRF models to jointly optimize camera poses and scene representation, which offers an alternative to off-the-shelf SfM pipelines which have well-understood failure modes. Existing approaches for unposed NeRF operate under limited assumptions, such as a prior pose distribution or coarse pose initialization, making them less effective in a general setting. In this work, we propose a novel approach, LU-NeRF, that jointly estimates camera poses and neural radiance fields with relaxed assumptions on pose configuration. Our approach operates in a local-to-global manner, where we first optimize over local subsets of the data, dubbed mini-scenes. LU-NeRF estimates local pose and geometry for this challenging few-shot task. The mini-scene poses are brought into a global reference frame through a robust pose synchronization step, where a final global optimization of pose and scene can be performed. We show our LU-NeRF pipeline outperforms prior attempts at unposed NeRF without making restrictive assumptions on the pose prior. This allows us to operate in the general SE(3) pose setting, unlike the baselines. Our results also indicate our model can be complementary to feature-based SfM pipelines as it compares favorably to COLMAP on low-texture and low-resolution images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_05410v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>阻碍NeRF模型在野外广泛部署的一个关键障碍是它们对精确相机姿势的依赖。因此，人们对扩展NeRF模型以联合优化相机姿态和场景表示越来越感兴趣，这为具有众所周知的故障模式的现成SfM管道提供了一种替代方案。现有的无基NeRF方法在有限的假设下运行，例如先前的姿态分布或粗略的姿态初始化，这使得它们在一般情况下的效果较差。在这项工作中，我们提出了一种新的方法，即LU NeRF，该方法通过对姿势配置的宽松假设来联合估计相机姿势和神经辐射场。我们的方法以局部到全局的方式运行，首先对数据的局部子集进行优化，称为迷你场景。LU NeRF估计了这项具有挑战性的少镜头任务的局部姿态和几何结构。通过稳健的姿态同步步骤，将迷你场景姿态带入全局参考帧，其中可以执行姿态和场景的最终全局优化。我们展示了我们的LU NeRF流水线在没有对姿势先验进行限制性假设的情况下，在未建模的NeRF上优于先前的尝试。这使我们能够在一般的SE（3）姿势设置中操作，而不是基线。我们的结果还表明，我们的模型可以与基于特征的SfM管道互补，因为它在低纹理和低分辨率图像上优于COLMAP。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.05410v1" target="_blank">2306.05410v1</a>
                              </td>
                              <td>LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs</td>
                              <td>Zezhou Cheng</td>
                              <td>2023-06-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_05410v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.05410v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_08422v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A vision-based autonomous UAV inspection framework for unknown tunnel construction sites with dynamic obstacles</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_08422v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_08422v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_08422v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Tunnel construction using the drill-and-blast method requires the 3D measurement of the excavation front to evaluate underbreak locations. Considering the inspection and measurement task's safety, cost, and efficiency, deploying lightweight autonomous robots, such as unmanned aerial vehicles (UAV), becomes more necessary and popular. Most of the previous works use a prior map for inspection viewpoint determination and do not consider dynamic obstacles. To maximally increase the level of autonomy, this paper proposes a vision-based UAV inspection framework for dynamic tunnel environments without using a prior map. Our approach utilizes a hierarchical planning scheme, decomposing the inspection problem into different levels. The high-level decision maker first determines the task for the robot and generates the target point. Then, the mid-level path planner finds the waypoint path and optimizes the collision-free static trajectory. Finally, the static trajectory will be fed into the low-level local planner to avoid dynamic obstacles and navigate to the target point. Besides, our framework contains a novel dynamic map module that can simultaneously track dynamic obstacles and represent static obstacles based on an RGB-D camera. After inspection, the Structure-from-Motion (SfM) pipeline is applied to generate the 3D shape of the target. To our best knowledge, this is the first time autonomous inspection has been realized in unknown and dynamic tunnel environments. Our flight experiments in a real tunnel prove that our method can autonomously inspect the tunnel excavation front surface.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_08422v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>采用钻爆法的隧道施工需要对开挖前沿进行三维测量，以评估欠挖位置。考虑到检测任务的安全性、成本和效率，部署无人机等轻型自主机器人变得更加必要和流行。以前的大多数工作都使用先验地图来确定检查视点，并且没有考虑动态障碍物。为了最大限度地提高自主性，本文提出了一种基于视觉的无人机动态隧道环境检测框架，无需使用先验地图。我们的方法采用分层规划方案，将检查问题分解为不同的级别。高级决策者首先确定机器人的任务并生成目标点。然后，中级路径规划器找到航路点路径并优化无碰撞静态轨迹。最后，静态轨迹将被输入到低级局部规划器中，以避开动态障碍并导航到目标点。此外，我们的框架包含一个新的动态地图模块，该模块可以基于RGB-D相机同时跟踪动态障碍物和表示静态障碍物。检查后，应用运动结构（SfM）管道生成目标的3D形状。据我们所知，这是首次在未知和动态的隧道环境中实现自主检测。我们在实际隧道中的飞行实验证明，我们的方法可以自主检测隧道开挖前表面。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.08422v2" target="_blank">2301.08422v2</a>
                              </td>
                              <td>A vision-based autonomous UAV inspection framework for unknown tunnel construction sites with dynamic obstacles</td>
                              <td>Zhefan Xu</td>
                              <td>2023-01-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_08422v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.08422v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01938v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Self-supervised Interest Point Detection and Description for Fisheye and Perspective Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01938v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01938v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01938v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Keypoint detection and matching is a fundamental task in many computer vision problems, from shape reconstruction, to structure from motion, to AR/VR applications and robotics. It is a well-studied problem with remarkable successes such as SIFT, and more recent deep learning approaches. While great robustness is exhibited by these techniques with respect to noise, illumination variation, and rigid motion transformations, less attention has been placed on image distortion sensitivity. In this work, we focus on the case when this is caused by the geometry of the cameras used for image acquisition, and consider the keypoint detection and matching problem between the hybrid scenario of a fisheye and a projective image. We build on a state-of-the-art approach and derive a self-supervised procedure that enables training an interest point detector and descriptor network. We also collected two new datasets for additional training and testing in this unexplored scenario, and we demonstrate that current approaches are suboptimal because they are designed to work in traditional projective conditions, while the proposed approach turns out to be the most effective.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01938v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>关键点检测和匹配是许多计算机视觉问题中的一项基本任务，从形状重建到结构从运动到AR/VR应用和机器人。这是一个研究得很好的问题，取得了显著的成功，如SIFT和最近的深度学习方法。虽然这些技术在噪声、照明变化和刚性运动变换方面表现出了很大的鲁棒性，但对图像失真敏感性的关注较少。在这项工作中，我们重点关注由用于图像采集的相机的几何形状引起的情况，并考虑鱼眼和投影图像的混合场景之间的关键点检测和匹配问题。我们建立在最先进的方法之上，并推导出一个自监督程序，该程序能够训练兴趣点检测器和描述符网络。我们还收集了两个新的数据集，用于在这个未探索的场景中进行额外的训练和测试，我们证明了当前的方法是次优的，因为它们被设计为在传统的投影条件下工作，而所提出的方法被证明是最有效的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01938v1" target="_blank">2306.01938v1</a>
                              </td>
                              <td>Self-supervised Interest Point Detection and Description for Fisheye and Perspective Images</td>
                              <td>Marcela Mera-Trujillo</td>
                              <td>2023-06-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01938v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01938v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_00180v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_00180v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_00180v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_00180v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Reconstruction of 3D neural fields from posed images has emerged as a promising method for self-supervised representation learning. The key challenge preventing the deployment of these 3D scene learners on large-scale video data is their dependence on precise camera poses from structure-from-motion, which is prohibitively expensive to run at scale. We propose a method that jointly reconstructs camera poses and 3D neural scene representations online and in a single forward pass. We estimate poses by first lifting frame-to-frame optical flow to 3D scene flow via differentiable rendering, preserving locality and shift-equivariance of the image processing backbone. SE(3) camera pose estimation is then performed via a weighted least-squares fit to the scene flow field. This formulation enables us to jointly supervise pose estimation and a generalizable neural scene representation via re-rendering the input video, and thus, train end-to-end and fully self-supervised on real-world video datasets. We demonstrate that our method performs robustly on diverse, real-world video, notably on sequences traditionally challenging to optimization-based pose estimation techniques.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_00180v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从姿态图像重建三维神经场已成为自监督表示学习的一种很有前途的方法。阻止将这些3D场景学习器部署在大规模视频数据上的关键挑战是，它们依赖于从结构到运动的精确相机姿势，这在规模上运行成本高得令人望而却步。我们提出了一种在线和单次前向联合重建相机姿态和3D神经场景表示的方法。我们通过可微分渲染将逐帧光流提升到3D场景流来估计姿态，保持图像处理主干的局部性和平移等变性。然后通过对场景流场的加权最小二乘拟合来执行SE（3）相机姿态估计。该公式使我们能够通过重新渲染输入视频来联合监督姿势估计和可推广的神经场景表示，从而在真实世界的视频数据集上进行端到端和完全自监督的训练。我们证明了我们的方法在不同的真实世界视频上表现稳健，尤其是在传统上对基于优化的姿态估计技术具有挑战性的序列上。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.00180v1" target="_blank">2306.00180v1</a>
                              </td>
                              <td>FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow</td>
                              <td>Cameron Smith</td>
                              <td>2023-05-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_00180v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.00180v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_16342v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">InterFormer: Interactive Local and Global Features Fusion for Automatic Speech Recognition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_16342v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_16342v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_16342v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The local and global features are both essential for automatic speech recognition (ASR). Many recent methods have verified that simply combining local and global features can further promote ASR performance. However, these methods pay less attention to the interaction of local and global features, and their series architectures are rigid to reflect local and global relationships. To address these issues, this paper proposes InterFormer for interactive local and global features fusion to learn a better representation for ASR. Specifically, we combine the convolution block with the transformer block in a parallel design. Besides, we propose a bidirectional feature interaction module (BFIM) and a selective fusion module (SFM) to implement the interaction and fusion of local and global features, respectively. Extensive experiments on public ASR datasets demonstrate the effectiveness of our proposed InterFormer and its superior performance over the other Transformer and Conformer models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_16342v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>局部特征和全局特征对于自动语音识别（ASR）都是必不可少的。最近的许多方法已经证明，简单地结合局部和全局特征可以进一步提高ASR性能。然而，这些方法很少关注局部和全局特征的相互作用，并且它们的串联架构是刚性的，以反映局部和全局关系。为了解决这些问题，本文提出了用于交互式局部和全局特征融合的InterFormer，以学习ASR的更好表示。具体地说，我们在并行设计中将卷积块与变换器块相结合。此外，我们提出了一个双向特征交互模块（BFIM）和一个选择性融合模块（SFM），分别实现局部和全局特征的交互和融合。在公共ASR数据集上进行的大量实验证明了我们提出的InterFormer的有效性及其优于其他Transformer和Conformer模型的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.16342v2" target="_blank">2305.16342v2</a>
                              </td>
                              <td>InterFormer: Interactive Local and Global Features Fusion for Automatic Speech Recognition</td>
                              <td>Zhi-Hao Lai</td>
                              <td>2023-05-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_16342v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.16342v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_12036v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SIDAR: Synthetic Image Dataset for Alignment & Restoration</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_12036v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_12036v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_12036v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Image alignment and image restoration are classical computer vision tasks. However, there is still a lack of datasets that provide enough data to train and evaluate end-to-end deep learning models. Obtaining ground-truth data for image alignment requires sophisticated structure-from-motion methods or optical flow systems that often do not provide enough data variance, i.e., typically providing a high number of image correspondences, while only introducing few changes of scenery within the underlying image sequences. Alternative approaches utilize random perspective distortions on existing image data. However, this only provides trivial distortions, lacking the complexity and variance of real-world scenarios. Instead, our proposed data augmentation helps to overcome the issue of data scarcity by using 3D rendering: images are added as textures onto a plane, then varying lighting conditions, shadows, and occlusions are added to the scene. The scene is rendered from multiple viewpoints, generating perspective distortions more consistent with real-world scenarios, with homographies closely resembling those of camera projections rather than randomized homographies. For each scene, we provide a sequence of distorted images with corresponding occlusion masks, homographies, and ground-truth labels. The resulting dataset can serve as a training and evaluation set for a multitude of tasks involving image alignment and artifact removal, such as deep homography estimation, dense image matching, 2D bundle adjustment, inpainting, shadow removal, denoising, content retrieval, and background subtraction. Our data generation pipeline is customizable and can be applied to any existing dataset, serving as a data augmentation to further improve the feature learning of any existing method.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_12036v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>图像对齐和图像恢复是经典的计算机视觉任务。然而，仍然缺乏提供足够数据来训练和评估端到端深度学习模型的数据集。获得用于图像对准的地面实况数据需要来自运动方法或光流系统的复杂结构，这些运动方法或光学流系统通常不能提供足够的数据方差，即，通常提供大量的图像对应，而在底层图像序列内只引入很少的风景变化。替代方法利用现有图像数据上的随机透视失真。然而，这只提供了微不足道的扭曲，缺乏现实世界场景的复杂性和多样性。相反，我们提出的数据增强通过使用3D渲染有助于克服数据稀缺的问题：将图像作为纹理添加到平面上，然后将不同的照明条件、阴影和遮挡添加到场景中。场景从多个视点渲染，生成与真实世界场景更一致的透视扭曲，单应性与相机投影的单应性非常相似，而不是随机单应性。对于每个场景，我们提供一系列失真的图像，这些图像具有相应的遮挡遮罩、单应性和基本事实标签。所得数据集可以作为涉及图像对齐和伪影去除的大量任务的训练和评估集，例如深度单应性估计、密集图像匹配、2D束调整、修复、阴影去除、去噪、内容检索和背景减法。我们的数据生成管道是可定制的，可以应用于任何现有的数据集，作为数据扩充，进一步改进任何现有方法的特征学习。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.12036v1" target="_blank">2305.12036v1</a>
                              </td>
                              <td>SIDAR: Synthetic Image Dataset for Alignment & Restoration</td>
                              <td>Monika Kwiatkowski</td>
                              <td>2023-05-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_12036v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.12036v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_08810v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AutoRecon: Automated 3D Object Discovery and Reconstruction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_08810v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_08810v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_08810v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A fully automated object reconstruction pipeline is crucial for digital content creation. While the area of 3D reconstruction has witnessed profound developments, the removal of background to obtain a clean object model still relies on different forms of manual labor, such as bounding box labeling, mask annotations, and mesh manipulations. In this paper, we propose a novel framework named AutoRecon for the automated discovery and reconstruction of an object from multi-view images. We demonstrate that foreground objects can be robustly located and segmented from SfM point clouds by leveraging self-supervised 2D vision transformer features. Then, we reconstruct decomposed neural scene representations with dense supervision provided by the decomposed point clouds, resulting in accurate object reconstruction and segmentation. Experiments on the DTU, BlendedMVS and CO3D-V2 datasets demonstrate the effectiveness and robustness of AutoRecon.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_08810v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>完全自动化的对象重建管道对于数字内容创建至关重要。虽然3D重建领域已经取得了深刻的发展，但去除背景以获得干净的对象模型仍然依赖于不同形式的手工劳动，如边界框标记、遮罩注释和网格操作。在本文中，我们提出了一个名为AutoRecon的新框架，用于从多视图图像中自动发现和重建对象。我们证明，通过利用自监督2D视觉变换器特征，可以从SfM点云中稳健地定位和分割前景对象。然后，我们在分解的点云提供的密集监督下重建分解的神经场景表示，从而实现精确的对象重建和分割。在DTU、BlendedMVS和CO3D-V2数据集上的实验证明了AutoRecon的有效性和稳健性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.08810v1" target="_blank">2305.08810v1</a>
                              </td>
                              <td>AutoRecon: Automated 3D Object Discovery and Reconstruction</td>
                              <td>Yuang Wang</td>
                              <td>2023-05-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_08810v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.08810v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_05301v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Eiffel Tower: A Deep-Sea Underwater Dataset for Long-Term Visual Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_05301v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_05301v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_05301v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual localization plays an important role in the positioning and navigation of robotics systems within previously visited environments. When visits occur over long periods of time, changes in the environment related to seasons or day-night cycles present a major challenge. Under water, the sources of variability are due to other factors such as water conditions or growth of marine organisms. Yet it remains a major obstacle and a much less studied one, partly due to the lack of data. This paper presents a new deep-sea dataset to benchmark underwater long-term visual localization. The dataset is composed of images from four visits to the same hydrothermal vent edifice over the course of five years. Camera poses and a common geometry of the scene were estimated using navigation data and Structure-from-Motion. This serves as a reference when evaluating visual localization techniques. An analysis of the data provides insights about the major changes observed throughout the years. Furthermore, several well-established visual localization methods are evaluated on the dataset, showing there is still room for improvement in underwater long-term visual localization. The data is made publicly available at https://www.seanoe.org/data/00810/92226/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_05301v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉定位在机器人系统在先前访问的环境中的定位和导航中起着重要作用。当访问发生在长时间内时，与季节或昼夜周期相关的环境变化是一个重大挑战。在水下，变化的来源是由于其他因素，如水条件或海洋生物的生长。然而，它仍然是一个主要障碍，也是一个研究较少的障碍，部分原因是缺乏数据。本文提出了一个新的深海数据集，用于对水下长期视觉定位进行基准测试。该数据集由五年内四次访问同一热液喷口建筑物的图像组成。使用导航数据和“运动结构”来估计摄影机姿态和场景的常见几何体。这可作为评估视觉定位技术时的参考。对数据的分析提供了多年来观察到的主要变化的见解。此外，在数据集上评估了几种公认的视觉定位方法，表明水下长期视觉定位仍有改进空间。数据可在https://www.seanoe.org/data/00810/92226/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.05301v1" target="_blank">2305.05301v1</a>
                              </td>
                              <td>Eiffel Tower: A Deep-Sea Underwater Dataset for Long-Term Visual Localization</td>
                              <td>Clémentin Boittiaux</td>
                              <td>2023-05-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_05301v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.05301v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_05268v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rotation Synchronization via Deep Matrix Factorization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_05268v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_05268v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_05268v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we address the rotation synchronization problem, where the objective is to recover absolute rotations starting from pairwise ones, where the unknowns and the measures are represented as nodes and edges of a graph, respectively. This problem is an essential task for structure from motion and simultaneous localization and mapping. We focus on the formulation of synchronization via neural networks, which has only recently begun to be explored in the literature. Inspired by deep matrix completion, we express rotation synchronization in terms of matrix factorization with a deep neural network. Our formulation exhibits implicit regularization properties and, more importantly, is unsupervised, whereas previous deep approaches are supervised. Our experiments show that we achieve comparable accuracy to the closest competitors in most scenes, while working under weaker assumptions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_05268v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们解决了旋转同步问题，其中目标是从成对旋转开始恢复绝对旋转，其中未知数和测度分别表示为图的节点和边。这个问题是结构从运动和同时定位和映射的一个重要任务。我们专注于通过神经网络进行同步的公式化，直到最近才开始在文献中进行探索。受深度矩阵完备的启发，我们用深度神经网络的矩阵分解来表达旋转同步。我们的公式具有隐式正则化性质，更重要的是，它是无监督的，而以前的深度方法是有监督的。我们的实验表明，在大多数场景中，我们实现了与最接近的竞争对手相当的准确性，同时在较弱的假设下工作。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.05268v1" target="_blank">2305.05268v1</a>
                              </td>
                              <td>Rotation Synchronization via Deep Matrix Factorization</td>
                              <td>Gk Tejus</td>
                              <td>2023-05-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_05268v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.05268v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2210_05020v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Spectral Sparsification for Communication-Efficient Collaborative Rotation and Translation Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2210_05020v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2210_05020v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2210_05020v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose fast and communication-efficient optimization algorithms for multi-robot rotation averaging and translation estimation problems that arise from collaborative simultaneous localization and mapping (SLAM), structure-from-motion (SfM), and camera network localization applications. Our methods are based on theoretical relations between the Hessians of the underlying Riemannian optimization problems and the Laplacians of suitably weighted graphs. We leverage these results to design a collaborative solver in which robots coordinate with a central server to perform approximate second-order optimization, by solving a Laplacian system at each iteration. Crucially, our algorithms permit robots to employ spectral sparsification to sparsify intermediate dense matrices before communication, and hence provide a mechanism to trade off accuracy with communication efficiency with provable guarantees. We perform rigorous theoretical analysis of our methods and prove that they enjoy (local) linear rate of convergence. Furthermore, we show that our methods can be combined with graduated non-convexity to achieve outlier-robust estimation. Extensive experiments on real-world SLAM and SfM scenarios demonstrate the superior convergence rate and communication efficiency of our methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2210_05020v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们为多机器人旋转平均和平移估计问题提出了快速且高效的优化算法，这些问题源于协同同步定位和映射（SLAM）、运动结构（SfM）和相机网络定位应用。我们的方法基于基本黎曼优化问题的Hessians和适当加权图的Laplacian之间的理论关系。我们利用这些结果设计了一个协作求解器，在该求解器中，机器人与中央服务器协调，通过在每次迭代中求解拉普拉斯系统来执行近似的二阶优化。至关重要的是，我们的算法允许机器人在通信前使用频谱稀疏化来稀疏中间密集矩阵，因此提供了一种在通信效率与精度之间进行权衡的机制，并提供了可证明的保证。我们对我们的方法进行了严格的理论分析，并证明它们具有（局部）线性收敛率。此外，我们证明了我们的方法可以与分级非凸性相结合，以实现异常值的鲁棒估计。在真实世界的SLAM和SfM场景上进行的大量实验证明了我们的方法优越的收敛速度和通信效率。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2210.05020v3" target="_blank">2210.05020v3</a>
                              </td>
                              <td>Spectral Sparsification for Communication-Efficient Collaborative Rotation and Translation Estimation</td>
                              <td>Yulun Tian</td>
                              <td>2022-10-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2210_05020v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2210.05020v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_10664v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Comparative Neural Radiance Field (NeRF) 3D Analysis of Camera Poses from HoloLens Trajectories and Structure from Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_10664v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_10664v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_10664v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural Radiance Fields (NeRFs) are trained using a set of camera poses and associated images as input to estimate density and color values for each position. The position-dependent density learning is of particular interest for photogrammetry, enabling 3D reconstruction by querying and filtering the NeRF coordinate system based on the object density. While traditional methods like Structure from Motion are commonly used for camera pose calculation in pre-processing for NeRFs, the HoloLens offers an interesting interface for extracting the required input data directly. We present a workflow for high-resolution 3D reconstructions almost directly from HoloLens data using NeRFs. Thereby, different investigations are considered: Internal camera poses from the HoloLens trajectory via a server application, and external camera poses from Structure from Motion, both with an enhanced variant applied through pose refinement. Results show that the internal camera poses lead to NeRF convergence with a PSNR of 25\,dB with a simple rotation around the x-axis and enable a 3D reconstruction. Pose refinement enables comparable quality compared to external camera poses, resulting in improved training process with a PSNR of 27\,dB and a better 3D reconstruction. Overall, NeRF reconstructions outperform the conventional photogrammetric dense reconstruction using Multi-View Stereo in terms of completeness and level of detail.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_10664v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用一组相机姿势和相关图像作为输入来训练神经辐射场（NeRF），以估计每个位置的密度和颜色值。位置相关密度学习对摄影测量特别感兴趣，它通过基于对象密度查询和过滤NeRF坐标系来实现3D重建。虽然在NeRF的预处理中，像“运动结构”这样的传统方法通常用于相机姿态计算，但HoloLens提供了一个有趣的界面，用于直接提取所需的输入数据。我们提出了一个使用NeRF几乎直接从HoloLens数据进行高分辨率3D重建的工作流程。因此，考虑了不同的研究：通过服务器应用程序从HoloLens轨迹中获得的内部相机姿势，以及从运动中获得的结构中获得的外部相机姿势，两者都通过姿势细化应用了增强的变体。结果表明，在绕x轴简单旋转的情况下，内部相机姿态导致NeRF收敛，PSNR为25dB，并能够进行3D重建。与外部相机姿势相比，姿势细化能够实现相当的质量，从而改进训练过程，PSNR为27dB，并实现更好的3D重建。总体而言，NeRF重建在完整性和细节水平方面优于使用多视图立体的传统摄影测量密集重建。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.10664v1" target="_blank">2304.10664v1</a>
                              </td>
                              <td>A Comparative Neural Radiance Field (NeRF) 3D Analysis of Camera Poses from HoloLens Trajectories and Structure from Motion</td>
                              <td>Miriam Jäger</td>
                              <td>2023-04-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_10664v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.10664v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2103_13875v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Finding Geometric Models by Clustering in the Consensus Space</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2103_13875v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2103_13875v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2103_13875v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a new algorithm for finding an unknown number of geometric models, e.g., homographies. The problem is formalized as finding dominant model instances progressively without forming crisp point-to-model assignments. Dominant instances are found via a RANSAC-like sampling and a consolidation process driven by a model quality function considering previously proposed instances. New ones are found by clustering in the consensus space. This new formulation leads to a simple iterative algorithm with state-of-the-art accuracy while running in real-time on a number of vision problems - at least two orders of magnitude faster than the competitors on two-view motion estimation. Also, we propose a deterministic sampler reflecting the fact that real-world data tend to form spatially coherent structures. The sampler returns connected components in a progressively densified neighborhood-graph. We present a number of applications where the use of multiple geometric models improves accuracy. These include pose estimation from multiple generalized homographies; trajectory estimation of fast-moving objects; and we also propose a way of using multiple homographies in global SfM algorithms. Source code: https://github.com/danini/clustering-in-consensus-space.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2103_13875v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的算法来寻找未知数量的几何模型，例如单应性。该问题被形式化为逐步找到主导模型实例，而不形成清晰的点到模型分配。主要实例是通过类似RANSAC的采样和由考虑先前提出的实例的模型质量函数驱动的合并过程来发现的。通过在一致性空间中进行聚类来发现新的一致性。这种新的公式产生了一种简单的迭代算法，具有最先进的精度，同时实时处理许多视觉问题——在双视图运动估计方面比竞争对手快至少两个数量级。此外，我们提出了一个确定性采样器，反映了真实世界的数据往往形成空间相干结构的事实。采样器返回逐步加密的邻域图中的连接分量。我们介绍了许多应用，其中使用多个几何模型可以提高精度。这些包括来自多个广义单应性的姿态估计；快速移动物体的轨迹估计；并且我们还提出了一种在全局SfM算法中使用多个单应性的方法。源代码：https://github.com/danini/clustering-in-consensus-space.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2103.13875v2" target="_blank">2103.13875v2</a>
                              </td>
                              <td>Finding Geometric Models by Clustering in the Consensus Space</td>
                              <td>Daniel Barath</td>
                              <td>2021-03-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2103_13875v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2103.13875v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_05947v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Visual Localization using Imperfect 3D Models from the Internet</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_05947v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_05947v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_05947v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual localization is a core component in many applications, including augmented reality (AR). Localization algorithms compute the camera pose of a query image w.r.t. a scene representation, which is typically built from images. This often requires capturing and storing large amounts of data, followed by running Structure-from-Motion (SfM) algorithms. An interesting, and underexplored, source of data for building scene representations are 3D models that are readily available on the Internet, e.g., hand-drawn CAD models, 3D models generated from building footprints, or from aerial images. These models allow to perform visual localization right away without the time-consuming scene capturing and model building steps. Yet, it also comes with challenges as the available 3D models are often imperfect reflections of reality. E.g., the models might only have generic or no textures at all, might only provide a simple approximation of the scene geometry, or might be stretched. This paper studies how the imperfections of these models affect localization accuracy. We create a new benchmark for this task and provide a detailed experimental evaluation based on multiple 3D models per scene. We show that 3D models from the Internet show promise as an easy-to-obtain scene representation. At the same time, there is significant room for improvement for visual localization pipelines. To foster research on this interesting and challenging task, we release our benchmark at v-pnk.github.io/cadloc.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_05947v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉定位是包括增强现实（AR）在内的许多应用程序的核心组件。定位算法计算查询图像的相机姿态，该图像通常是根据图像构建的场景表示。这通常需要捕获和存储大量数据，然后运行运动结构（SfM）算法。用于建筑场景表示的一个有趣且未充分探索的数据源是在互联网上容易获得的3D模型，例如手绘CAD模型、从建筑足迹或从航空图像生成的3D模型。这些模型允许立即执行视觉定位，而无需耗时的场景捕捉和模型构建步骤。然而，它也带来了挑战，因为可用的3D模型往往是对现实的不完美反映。例如，模型可能只具有通用纹理或根本没有纹理，可能只提供场景几何体的简单近似，或者可能被拉伸。本文研究了这些模型的缺陷如何影响定位精度。我们为这项任务创建了一个新的基准，并基于每个场景的多个3D模型提供了详细的实验评估。我们表明，来自互联网的3D模型有望成为一种易于获得的场景表示。同时，视觉定位管道还有很大的改进空间。为了促进对这项有趣且具有挑战性的任务的研究，我们在v-pnk.github.io/cadloc上发布了我们的基准。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.05947v1" target="_blank">2304.05947v1</a>
                              </td>
                              <td>Visual Localization using Imperfect 3D Models from the Internet</td>
                              <td>Vojtech Panek</td>
                              <td>2023-04-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_05947v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.05947v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_03930v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Photometric Correction for Infrared Sensors</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_03930v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_03930v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_03930v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Infrared thermography has been widely used in several domains to capture and measure temperature distributions across surfaces and objects. This methodology can be further expanded to 3D applications if the spatial distribution of the temperature distribution is available. Structure from Motion (SfM) is a photometric range imaging technique that makes it possible to obtain 3D renderings from a cloud of 2D images. To explore the possibility of 3D reconstruction via SfM from infrared images, this article proposes a photometric correction model for infrared sensors based on temperature constancy. Photometric correction is accomplished by estimating the scene irradiance as the values from the solution to a differential equation for microbolometer pixel excitation with unknown coefficients and initial conditions. The model was integrated into an SfM framework and experimental evaluations demonstrate the contribution of the photometric correction for improving the estimates of both the camera motion and the scene structure. Further, experiments show that the reconstruction quality from the corrected infrared imagery achieves performance on par with state-of-the-art reconstruction using RGB sensors.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_03930v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>红外热成像技术已广泛应用于多个领域，用于捕捉和测量表面和物体的温度分布。如果温度分布的空间分布可用，则该方法可以进一步扩展到3D应用。运动结构（SfM）是一种光度范围成像技术，可以从2D图像云中获得3D渲染。为了探索从红外图像中通过SfM进行三维重建的可能性，本文提出了一种基于温度恒定性的红外传感器光度校正模型。光度校正是通过将场景辐照度估计为具有未知系数和初始条件的微测辐射热计像素激发的微分方程的解的值来实现的。该模型被集成到SfM框架中，实验评估证明了光度校正对改善相机运动和场景结构估计的贡献。此外，实验表明，校正后的红外图像的重建质量达到了与使用RGB传感器的最先进重建相当的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.03930v1" target="_blank">2304.03930v1</a>
                              </td>
                              <td>Photometric Correction for Infrared Sensors</td>
                              <td>Jincheng Zhang</td>
                              <td>2023-04-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_03930v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.03930v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_03560v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DualRefine: Self-Supervised Depth and Pose Estimation Through Iterative Epipolar Sampling and Refinement Toward Equilibrium</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_03560v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_03560v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_03560v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised multi-frame depth estimation achieves high accuracy by computing matching costs of pixel correspondences between adjacent frames, injecting geometric information into the network. These pixel-correspondence candidates are computed based on the relative pose estimates between the frames. Accurate pose predictions are essential for precise matching cost computation as they influence the epipolar geometry. Furthermore, improved depth estimates can, in turn, be used to align pose estimates.   Inspired by traditional structure-from-motion (SfM) principles, we propose the DualRefine model, which tightly couples depth and pose estimation through a feedback loop. Our novel update pipeline uses a deep equilibrium model framework to iteratively refine depth estimates and a hidden state of feature maps by computing local matching costs based on epipolar geometry. Importantly, we used the refined depth estimates and feature maps to compute pose updates at each step. This update in the pose estimates slowly alters the epipolar geometry during the refinement process. Experimental results on the KITTI dataset demonstrate competitive depth prediction and odometry prediction performance surpassing published self-supervised baselines.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_03560v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督多帧深度估计通过计算相邻帧之间像素对应关系的匹配代价，将几何信息注入网络，实现了高精度。这些像素对应候选是基于帧之间的相对姿态估计来计算的。精确的姿态预测对于精确的匹配成本计算至关重要，因为它们会影响核极几何。此外，改进的深度估计反过来可以用于对准姿态估计。受传统运动结构（SfM）原理的启发，我们提出了DualRefine模型，该模型通过反馈回路将深度和姿态估计紧密耦合。我们新颖的更新管道使用深度平衡模型框架，通过基于核极几何计算局部匹配成本，迭代细化深度估计和特征图的隐藏状态。重要的是，我们使用精细的深度估计和特征图来计算每一步的姿势更新。姿态估计的这种更新在细化过程中缓慢地改变了极线几何结构。KITTI数据集上的实验结果表明，竞争性深度预测和里程计预测性能超过了已公布的自监督基线。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.03560v1" target="_blank">2304.03560v1</a>
                              </td>
                              <td>DualRefine: Self-Supervised Depth and Pose Estimation Through Iterative Epipolar Sampling and Refinement Toward Equilibrium</td>
                              <td>Antyanta Bangunharcana</td>
                              <td>2023-04-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_03560v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.03560v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_03426v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Convex Minimization with Integer Minima in $\widetilde O(n^4)$ Time</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_03426v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_03426v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_03426v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Given a convex function $f$ on $\mathbb{R}^n$ with an integer minimizer, we show how to find an exact minimizer of $f$ using $O(n^2 \log n)$ calls to a separation oracle and $O(n^4 \log n)$ time. The previous best polynomial time algorithm for this problem given in [Jiang, SODA 2021, JACM 2022] achieves $\widetilde{O}(n^2)$ oracle complexity. However, the overall runtime of Jiang's algorithm is at least $\widetilde{\Omega}(n^8)$, due to expensive sub-routines such as the Lenstra-Lenstra-Lov\'asz (LLL) algorithm [Lenstra, Lenstra, Lov\'asz, Math. Ann. 1982] and random walk based cutting plane method [Bertsimas, Vempala, JACM 2004]. Our significant speedup is obtained by a nontrivial combination of a faster version of the LLL algorithm due to [Neumaier, Stehl\'e, ISSAC 2016] that gives similar guarantees, the volumetric center cutting plane method (CPM) by [Vaidya, FOCS 1989] and its fast implementation given in [Jiang, Lee, Song, Wong, STOC 2020].   For the special case of submodular function minimization (SFM), our result implies a strongly polynomial time algorithm for this problem using $O(n^3 \log n)$ calls to an evaluation oracle and $O(n^4 \log n)$ additional arithmetic operations. Both the oracle complexity and the number of arithmetic operations of our more general algorithm are better than the previous best-known runtime algorithms for this specific problem given in [Lee, Sidford, Wong, FOCS 2015] and [Dadush, V\'egh, Zambelli, SODA 2018, MOR 2021].</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_03426v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>给定$\mathbb｛R｝^n$上具有整数极小值的凸函数$f$，我们展示了如何使用对分离预言机的$O（n^2\logn）$调用和$O（n ^4\logn）$时间来找到$f$的精确极小值。[Jiang，SODA 2021，JACM 2022]中给出的该问题的先前最佳多项式时间算法实现了$\widetilde{O}（n^2）$oracle复杂度。然而，由于Lenstra-Lonstra-Lov'asz（LLL）算法[Lenstra，Lenstra，Lov'asz，Math.Ann.1982]和基于随机行走的切割平面方法[Bertsimas，Vempala，JACM 2004]等昂贵的子程序，姜算法的总体运行时间至少为$\widetilde｛\Omega｝（n^8）$。我们的显著加速是通过[Neumaier，Stehl\'e，ISSAC 2016]的LLL算法的更快版本、[Vaidya，FOCS 1989]的体积中心切割平面法（CPM）及其在[Jiang，Lee，Song，Wong，STOC 2020]中给出的快速实现的非平凡组合而获得的。对于子模函数最小化（SFM）的特殊情况，我们的结果暗示了这个问题的一个强多项式时间算法，使用$O（n^3\logn）$对评估预言机的调用和$O（n ^4\log n）$额外的算术运算。对于这一特定问题，我们更通用的算法的预言机复杂性和算术运算次数都优于[Lee，Sidford，Wong，FOCS 2015]和[Dadush，V\'egh，Zambelli，SODA 2018，MOR 2021]中给出的以前最著名的运行时算法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.03426v1" target="_blank">2304.03426v1</a>
                              </td>
                              <td>Convex Minimization with Integer Minima in $\widetilde O(n^4)$ Time</td>
                              <td>Haotian Jiang</td>
                              <td>2023-04-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_03426v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.03426v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_02420v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Semantic Validation in Structure from Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_02420v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_02420v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_02420v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Structure from Motion (SfM) challenge in computer vision is the process of recovering the 3D structure of a scene from a series of projective measurements that are calculated from a collection of 2D images, taken from different perspectives. SfM consists of three main steps; feature detection and matching, camera motion estimation, and recovery of 3D structure from estimated intrinsic and extrinsic parameters and features.   A problem encountered in SfM is that scenes lacking texture or with repetitive features can cause erroneous feature matching between frames. Semantic segmentation offers a route to validate and correct SfM models by labelling pixels in the input images with the use of a deep convolutional neural network. The semantic and geometric properties associated with classes in the scene can be taken advantage of to apply prior constraints to each class of object. The SfM pipeline COLMAP and semantic segmentation pipeline DeepLab were used. This, along with planar reconstruction of the dense model, were used to determine erroneous points that may be occluded from the calculated camera position, given the semantic label, and thus prior constraint of the reconstructed plane. Herein, semantic segmentation is integrated into SfM to apply priors on the 3D point cloud, given the object detection in the 2D input images. Additionally, the semantic labels of matched keypoints are compared and inconsistent semantically labelled points discarded. Furthermore, semantic labels on input images are used for the removal of objects associated with motion in the output SfM models. The proposed approach is evaluated on a data-set of 1102 images of a repetitive architecture scene. This project offers a novel method for improved validation of 3D SfM models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_02420v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>计算机视觉中的运动结构（SfM）挑战是从一系列投影测量中恢复场景的3D结构的过程，这些投影测量是从不同视角拍摄的2D图像集合中计算出来的。SfM包括三个主要步骤；特征检测和匹配，相机运动估计，以及从估计的内在和外在参数和特征中恢复3D结构。SfM中遇到的问题是，缺乏纹理或具有重复特征的场景可能导致帧之间的错误特征匹配。语义分割通过使用深度卷积神经网络标记输入图像中的像素，提供了一种验证和校正SfM模型的途径。可以利用与场景中的类相关联的语义和几何特性来将先验约束应用于对象的每个类。使用了SfM流水线COLMAP和语义分割流水线DeepLab。这与密集模型的平面重建一起，用于确定在给定语义标签的情况下可能从计算的相机位置遮挡的错误点，从而确定重建平面的先验约束。在此，在给定2D输入图像中的对象检测的情况下，语义分割被集成到SfM中，以在3D点云上应用先验。此外，对匹配关键点的语义标签进行比较，并丢弃语义上不一致的标记点。此外，输入图像上的语义标签用于去除与输出SfM模型中的运动相关联的对象。在重复建筑场景的1102个图像的数据集上评估所提出的方法。该项目提供了一种改进三维SfM模型验证的新方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.02420v1" target="_blank">2304.02420v1</a>
                              </td>
                              <td>Semantic Validation in Structure from Motion</td>
                              <td>Joseph Rowell</td>
                              <td>2023-04-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_02420v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.02420v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_13551v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SfM-TTR: Using Structure from Motion for Test-Time Refinement of Single-View Depth Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_13551v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_13551v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_13551v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Estimating a dense depth map from a single view is geometrically ill-posed, and state-of-the-art methods rely on learning depth's relation with visual appearance using deep neural networks. On the other hand, Structure from Motion (SfM) leverages multi-view constraints to produce very accurate but sparse maps, as matching across images is typically limited by locally discriminative texture. In this work, we combine the strengths of both approaches by proposing a novel test-time refinement (TTR) method, denoted as SfM-TTR, that boosts the performance of single-view depth networks at test time using SfM multi-view cues. Specifically, and differently from the state of the art, we use sparse SfM point clouds as test-time self-supervisory signal, fine-tuning the network encoder to learn a better representation of the test scene. Our results show how the addition of SfM-TTR to several state-of-the-art self-supervised and supervised networks improves significantly their performance, outperforming previous TTR baselines mainly based on photometric multi-view consistency. The code is available at https://github.com/serizba/SfM-TTR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_13551v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从单个视图估计密集深度图在几何上是不适定的，最先进的方法依赖于使用深度神经网络学习深度与视觉外观的关系。另一方面，运动结构（SfM）利用多视图约束来生成非常精确但稀疏的地图，因为图像之间的匹配通常受到局部判别纹理的限制。在这项工作中，我们结合了这两种方法的优势，提出了一种新的测试时间细化（TTR）方法，称为SfM-TTR，该方法在测试时使用SfM多视图线索来提高单视图深度网络的性能。具体而言，与现有技术不同的是，我们使用稀疏的SfM点云作为测试时间自监督信号，微调网络编码器以学习测试场景的更好表示。我们的结果表明，在几个最先进的自监督和监督网络中添加SfM-TTR可以显著提高其性能，优于以前主要基于光度多视图一致性的TTR基线。代码可在https://github.com/serizba/SfM-TTR.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.13551v2" target="_blank">2211.13551v2</a>
                              </td>
                              <td>SfM-TTR: Using Structure from Motion for Test-Time Refinement of Single-View Depth Networks</td>
                              <td>Sergio Izquierdo</td>
                              <td>2022-11-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_13551v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.13551v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_17504v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D Line Mapping Revisited</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_17504v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_17504v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_17504v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In contrast to sparse keypoints, a handful of line segments can concisely encode the high-level scene layout, as they often delineate the main structural elements. In addition to offering strong geometric cues, they are also omnipresent in urban landscapes and indoor scenes. Despite their apparent advantages, current line-based reconstruction methods are far behind their point-based counterparts. In this paper we aim to close the gap by introducing LIMAP, a library for 3D line mapping that robustly and efficiently creates 3D line maps from multi-view imagery. This is achieved through revisiting the degeneracy problem of line triangulation, carefully crafted scoring and track building, and exploiting structural priors such as line coincidence, parallelism, and orthogonality. Our code integrates seamlessly with existing point-based Structure-from-Motion methods and can leverage their 3D points to further improve the line reconstruction. Furthermore, as a byproduct, the method is able to recover 3D association graphs between lines and points / vanishing points (VPs). In thorough experiments, we show that LIMAP significantly outperforms existing approaches for 3D line mapping. Our robust 3D line maps also open up new research directions. We show two example applications: visual localization and bundle adjustment, where integrating lines alongside points yields the best results. Code is available at https://github.com/cvg/limap.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_17504v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>与稀疏关键点相比，少数线段可以简洁地对高级场景布局进行编码，因为它们通常描绘主要的结构元素。除了提供强烈的几何线索外，它们还在城市景观和室内场景中无处不在。尽管有明显的优势，但目前基于线的重建方法远远落后于基于点的重建方法。在本文中，我们的目标是通过引入LIMAP来缩小差距，LIMAP是一个用于3D线图绘制的库，可以从多视图图像中稳健有效地创建3D线图。这是通过重新审视线三角测量的退化问题、精心制作的评分和轨迹构建，以及利用线重合、平行和正交等结构先验来实现的。我们的代码与现有的基于点的运动结构方法无缝集成，可以利用它们的3D点来进一步改进线重建。此外，作为副产品，该方法能够恢复线和点/消失点（VP）之间的3D关联图。在深入的实验中，我们表明LIMAP显著优于现有的3D线映射方法。我们强大的3D折线图也开辟了新的研究方向。我们展示了两个示例应用程序：视觉定位和束调整，其中将线与点一起积分会产生最佳结果。代码可在https://github.com/cvg/limap.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.17504v1" target="_blank">2303.17504v1</a>
                              </td>
                              <td>3D Line Mapping Revisited</td>
                              <td>Shaohui Liu</td>
                              <td>2023-03-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_17504v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.17504v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_15069v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FeatureBooster: Boosting Feature Descriptors with a Lightweight Neural Network</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_15069v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_15069v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_15069v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce a lightweight network to improve descriptors of keypoints within the same image. The network takes the original descriptors and the geometric properties of keypoints as the input, and uses an MLP-based self-boosting stage and a Transformer-based cross-boosting stage to enhance the descriptors. The boosted descriptors can be either real-valued or binary ones. We use the proposed network to boost both hand-crafted (ORB, SIFT) and the state-of-the-art learning-based descriptors (SuperPoint, ALIKE) and evaluate them on image matching, visual localization, and structure-from-motion tasks. The results show that our method significantly improves the performance of each task, particularly in challenging cases such as large illumination changes or repetitive patterns. Our method requires only 3.2ms on desktop GPU and 27ms on embedded GPU to process 2000 features, which is fast enough to be applied to a practical system. The code and trained weights are publicly available at github.com/SJTU-ViSYS/FeatureBooster.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_15069v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们引入了一个轻量级网络来改进同一图像中关键点的描述符。该网络以原始描述符和关键点的几何特性为输入，并使用基于MLP的自提升级和基于Transformer的交叉提升级来增强描述符。增强的描述符可以是实数描述符，也可以是二进制描述符。我们使用所提出的网络来增强手工制作的（ORB，SIFT）和最先进的基于学习的描述符（SuperPoint，ALIKE），并在图像匹配、视觉定位和运动任务的结构方面对它们进行评估。结果表明，我们的方法显著提高了每个任务的性能，特别是在具有挑战性的情况下，如大的照明变化或重复模式。我们的方法只需要在台式GPU上3.2ms，在嵌入式GPU上27ms就可以处理2000个特征，这足够快，可以应用于实际系统。代码和训练过的重量可在github.com/SJTU-ViSYS/FeatureBooster上公开获取。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.15069v3" target="_blank">2211.15069v3</a>
                              </td>
                              <td>FeatureBooster: Boosting Feature Descriptors with a Lightweight Neural Network</td>
                              <td>Xinjiang Wang</td>
                              <td>2022-11-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_15069v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.15069v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_15060v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TMO: Textured Mesh Acquisition of Objects with a Mobile Device by using Differentiable Rendering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_15060v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_15060v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_15060v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present a new pipeline for acquiring a textured mesh in the wild with a single smartphone which offers access to images, depth maps, and valid poses. Our method first introduces an RGBD-aided structure from motion, which can yield filtered depth maps and refines camera poses guided by corresponding depth. Then, we adopt the neural implicit surface reconstruction method, which allows for high-quality mesh and develops a new training process for applying a regularization provided by classical multi-view stereo methods. Moreover, we apply a differentiable rendering to fine-tune incomplete texture maps and generate textures which are perceptually closer to the original scene. Our pipeline can be applied to any common objects in the real world without the need for either in-the-lab environments or accurate mask images. We demonstrate results of captured objects with complex shapes and validate our method numerically against existing 3D reconstruction and texture mapping methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_15060v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的管道，可以通过一部智能手机在野外获取纹理网格，该智能手机可以访问图像、深度图和有效姿势。我们的方法首先引入了一种基于运动的RGBD辅助结构，该结构可以生成过滤后的深度图，并根据相应的深度细化相机姿态。然后，我们采用了神经隐式曲面重建方法，该方法可以获得高质量的网格，并开发了一种新的训练过程，用于应用经典多视图立体方法提供的正则化。此外，我们应用可微分渲染来微调不完整的纹理贴图，并生成在感知上更接近原始场景的纹理。我们的管道可以应用于现实世界中的任何常见对象，而无需实验室环境或精确的掩模图像。我们展示了具有复杂形状的捕捉对象的结果，并与现有的3D重建和纹理映射方法进行了数值验证。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.15060v1" target="_blank">2303.15060v1</a>
                              </td>
                              <td>TMO: Textured Mesh Acquisition of Objects with a Mobile Device by using Differentiable Rendering</td>
                              <td>Jaehoon Choi</td>
                              <td>2023-03-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_15060v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.15060v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_12018v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Level-S$^2$fM: Structure from Motion on Neural Level Set of Implicit Surfaces</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_12018v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_12018v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_12018v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a neural incremental Structure-from-Motion (SfM) approach, Level-S$^2$fM, which estimates the camera poses and scene geometry from a set of uncalibrated images by learning coordinate MLPs for the implicit surfaces and the radiance fields from the established keypoint correspondences. Our novel formulation poses some new challenges due to inevitable two-view and few-view configurations in the incremental SfM pipeline, which complicates the optimization of coordinate MLPs for volumetric neural rendering with unknown camera poses. Nevertheless, we demonstrate that the strong inductive basis conveying in the 2D correspondences is promising to tackle those challenges by exploiting the relationship between the ray sampling schemes. Based on this, we revisit the pipeline of incremental SfM and renew the key components, including two-view geometry initialization, the camera poses registration, the 3D points triangulation, and Bundle Adjustment, with a fresh perspective based on neural implicit surfaces. By unifying the scene geometry in small MLP networks through coordinate MLPs, our Level-S$^2$fM treats the zero-level set of the implicit surface as an informative top-down regularization to manage the reconstructed 3D points, reject the outliers in correspondences via querying SDF, and refine the estimated geometries by NBA (Neural BA). Not only does our Level-S$^2$fM lead to promising results on camera pose estimation and scene geometry reconstruction, but it also shows a promising way for neural implicit rendering without knowing camera extrinsic beforehand.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_12018v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种基于运动的神经增量结构（SfM）方法，即S$^2$fM级，该方法通过从建立的关键点对应关系中学习隐式表面的坐标MLP和辐射场，从一组未校准的图像中估计相机姿态和场景几何。由于增量SfM管道中不可避免的两视图和少视图配置，我们的新公式提出了一些新的挑战，这使用于具有未知相机姿态的体积神经渲染的坐标MLP的优化变得复杂。然而，我们证明了在2D对应关系中传递的强归纳基有望通过利用射线采样方案之间的关系来解决这些挑战。基于此，我们重新审视了增量SfM的管道，并更新了关键组件，包括两视图几何初始化、相机姿态配准、3D点三角测量和束调整，以基于神经隐式曲面的全新视角。通过坐标MLP统一小型MLP网络中的场景几何结构，我们的Level-S$^2$fM将隐式曲面的零级集视为自上而下的信息正则化，以管理重建的3D点，通过查询SDF拒绝对应关系中的异常值，并通过NBA（Neural BA）细化估计的几何结构。我们的S$^2$fM级不仅在相机姿态估计和场景几何重建方面取得了有希望的结果，而且它还为神经隐式渲染提供了一种很有前途的方法，而无需事先了解相机的外在情况。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.12018v2" target="_blank">2211.12018v2</a>
                              </td>
                              <td>Level-S$^2$fM: Structure from Motion on Neural Level Set of Implicit Surfaces</td>
                              <td>Yuxi Xiao</td>
                              <td>2022-11-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_12018v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.12018v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="LLM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2308_07921v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07921v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07921v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07921v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent progress in large language models (LLMs) like GPT-4 and PaLM-2 has brought significant advancements in addressing math reasoning problems. In particular, OpenAI's latest version of GPT-4, known as GPT-4 Code Interpreter, shows remarkable performance on challenging math datasets. In this paper, we explore the effect of code on enhancing LLMs' reasoning capability by introducing different constraints on the \textit{Code Usage Frequency} of GPT-4 Code Interpreter. We found that its success can be largely attributed to its powerful skills in generating and executing code, evaluating the output of code execution, and rectifying its solution when receiving unreasonable outputs. Based on this insight, we propose a novel and effective prompting method, explicit \uline{c}ode-based \uline{s}elf-\uline{v}erification~(CSV), to further boost the mathematical reasoning potential of GPT-4 Code Interpreter. This method employs a zero-shot prompt on GPT-4 Code Interpreter to encourage it to use code to self-verify its answers. In instances where the verification state registers as ``False'', the model shall automatically amend its solution, analogous to our approach of rectifying errors during a mathematics examination. Furthermore, we recognize that the states of the verification result indicate the confidence of a solution, which can improve the effectiveness of majority voting. With GPT-4 Code Interpreter and CSV, we achieve an impressive zero-shot accuracy on MATH dataset \textbf{(53.9\% $\to$ 84.3\%)}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07921v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>GPT-4和PaLM-2等大型语言模型（LLM）的最新进展为解决数学推理问题带来了重大进展。特别是，OpenAI的最新版本GPT-4，即GPT-4代码解释器，在具有挑战性的数学数据集上表现出了非凡的性能。在本文中，我们通过对GPT-4代码解释器的\textit｛代码使用频率｝引入不同的约束，探讨了代码对增强LLM推理能力的影响。我们发现，它的成功很大程度上可以归功于它在生成和执行代码、评估代码执行的输出以及在收到不合理输出时纠正其解决方案方面的强大技能。基于这一见解，我们提出了一种新颖有效的提示方法，即基于代码的显式自我验证~（CSV），以进一步提高GPT-4代码解释器的数学推理潜力。此方法在GPT-4代码解释器上使用零样本提示，以鼓励它使用代码来自我验证其答案。在验证状态注册为“错误”的情况下，模型应自动修改其解决方案，类似于我们在数学考试中纠正错误的方法。此外，我们认识到，验证结果的状态表明了解决方案的可信度，这可以提高多数投票的有效性。使用GPT-4代码解释器和CSV，我们在MATH数据集\textbf｛（53.9\%$\to$84.3\%）｝上实现了令人印象深刻的零样本精度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07921v1" target="_blank">2308.07921v1</a>
                              </td>
                              <td>Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification</td>
                              <td>Aojun Zhou</td>
                              <td>2023-08-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07921v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07921v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07902v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Through the Lens of Core Competency: Survey on Evaluation of Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07902v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07902v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07902v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>From pre-trained language model (PLM) to large language model (LLM), the field of natural language processing (NLP) has witnessed steep performance gains and wide practical uses. The evaluation of a research field guides its direction of improvement. However, LLMs are extremely hard to thoroughly evaluate for two reasons. First of all, traditional NLP tasks become inadequate due to the excellent performance of LLM. Secondly, existing evaluation tasks are difficult to keep up with the wide range of applications in real-world scenarios. To tackle these problems, existing works proposed various benchmarks to better evaluate LLMs. To clarify the numerous evaluation tasks in both academia and industry, we investigate multiple papers concerning LLM evaluations. We summarize 4 core competencies of LLM, including reasoning, knowledge, reliability, and safety. For every competency, we introduce its definition, corresponding benchmarks, and metrics. Under this competency architecture, similar tasks are combined to reflect corresponding ability, while new tasks can also be easily added into the system. Finally, we give our suggestions on the future direction of LLM's evaluation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07902v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从预训练语言模型（PLM）到大型语言模型（LLM），自然语言处理（NLP）领域已经见证了巨大的性能提升和广泛的实际应用。对一个研究领域的评估指导其改进方向。然而，LLM非常难以彻底评估，原因有两个。首先，由于LLM的优异性能，传统的NLP任务变得不足。其次，现有的评估任务很难跟上现实世界场景中广泛的应用。为了解决这些问题，现有的工作提出了各种基准，以更好地评估LLM。为了阐明学术界和工业界的众多评估任务，我们调查了多篇关于LLM评估的论文。我们总结了LLM的4个核心能力，包括推理、知识、可靠性和安全性。对于每种能力，我们都会介绍其定义、相应的基准和度量标准。在这种能力架构下，类似的任务被组合起来以反映相应的能力，而新的任务也可以很容易地添加到系统中。最后，我们对LLM评估的未来方向提出了建议。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07902v1" target="_blank">2308.07902v1</a>
                              </td>
                              <td>Through the Lens of Core Competency: Survey on Evaluation of Large Language Models</td>
                              <td>Ziyu Zhuang</td>
                              <td>2023-08-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07902v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07902v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07891v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Link-Context Learning for Multimodal LLMs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07891v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07891v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07891v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The ability to learn from context with novel concepts, and deliver appropriate responses are essential in human conversations. Despite current Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being trained on mega-scale datasets, recognizing unseen images or understanding novel concepts in a training-free manner remains a challenge. In-Context Learning (ICL) explores training-free few-shot learning, where models are encouraged to ``learn to learn" from limited tasks and generalize to unseen tasks. In this work, we propose link-context learning (LCL), which emphasizes "reasoning from cause and effect" to augment the learning capabilities of MLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal relationship between the support set and the query set. By providing demonstrations with causal links, LCL guides the model to discern not only the analogy but also the underlying causal associations between data points, which empowers MLLMs to recognize unseen images and understand novel concepts more effectively. To facilitate the evaluation of this novel approach, we introduce the ISEKAI dataset, comprising exclusively of unseen generated image-label pairs designed for link-context learning. Extensive experiments show that our LCL-MLLM exhibits strong link-context learning capabilities to novel concepts over vanilla MLLMs. Code and data will be released at https://github.com/isekai-portal/Link-Context-Learning.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07891v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在人类对话中，用新颖的概念从上下文中学习并做出适当回应的能力至关重要。尽管目前的多模式大语言模型（MLLM）和大语言模型是在超大规模的数据集上训练的，但以无训练的方式识别看不见的图像或理解新概念仍然是一个挑战。上下文学习（ICL）探索了无训练的少镜头学习，鼓励模型从有限的任务中“学会学习”，并推广到看不见的任务。在这项工作中，我们提出了链接上下文学习（LCL），它强调“从因果推理”“以增强MLLMs的学习能力。LCL通过明确加强支持集和查询集之间的因果关系，超越了传统的ICL。通过提供因果链接的演示，LCL引导模型不仅识别类比，还识别数据点之间的潜在因果关联，这使MLLMs能够识别看不见的图像和不足更有效地理解新颖的概念。为了促进对这种新方法的评估，我们引入了ISEKAI数据集，该数据集专门由为链接上下文学习而设计的看不见的生成图像标签对组成。大量实验表明，与普通MLLM相比，我们的LCL-MLLM表现出与新概念的强大链接上下文学习能力。代码和数据将在https://github.com/isekai-portal/Link-Context-Learning.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07891v1" target="_blank">2308.07891v1</a>
                              </td>
                              <td>Link-Context Learning for Multimodal LLMs</td>
                              <td>Yan Tai</td>
                              <td>2023-08-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07891v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07891v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_08456v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PoetryDiffusion: Towards Joint Semantic and Metrical Manipulation in Poetry Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_08456v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_08456v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_08456v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Controllable text generation is a challenging and meaningful field in natural language generation (NLG). Especially, poetry generation is a typical one with well-defined and strict conditions for text generation which is an ideal playground for the assessment of current methodologies. While prior works succeeded in controlling either semantic or metrical aspects of poetry generation, simultaneously addressing both remains a challenge. In this paper, we pioneer the use of the Diffusion model for generating sonnets and Chinese SongCi poetry to tackle such challenges. In terms of semantics, our PoetryDiffusion model, built upon the Diffusion model, generates entire sentences or poetry by comprehensively considering the entirety of sentence information. This approach enhances semantic expression, distinguishing it from autoregressive and large language models (LLMs). For metrical control, the separation feature of diffusion generation and its constraint control module enable us to flexibly incorporate a novel metrical controller to manipulate and evaluate metrics (format and rhythm). The denoising process in PoetryDiffusion allows for gradual enhancement of semantics and flexible integration of the metrical controller which can calculate and impose penalties on states that stray significantly from the target control distribution. Experimental results on two datasets demonstrate that our model outperforms existing models in automatic evaluation of semantic, metrical, and overall performance as well as human evaluation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_08456v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>可控文本生成是自然语言生成中一个富有挑战性和意义的领域。尤其是诗歌生成是一个典型的文本生成条件明确而严格的文本生成，是评估当前方法论的理想场所。虽然先前的作品成功地控制了诗歌生成的语义或韵律方面，但同时解决这两者仍然是一个挑战。在本文中，我们率先使用扩散模型来生成十四行诗和中国宋词，以应对这些挑战。在语义方面，我们的PoetryDiffusion模型建立在Diffusion模式的基础上，通过综合考虑句子信息的整体性来生成完整的句子或诗歌。这种方法增强了语义表达，将其与自回归和大型语言模型（LLM）区分开来。对于度量控制，扩散生成的分离特性及其约束控制模块使我们能够灵活地结合一种新的度量控制器来操作和评估度量（格式和节奏）。PoetryDiffusion中的去噪过程允许语义的逐渐增强和度量控制器的灵活集成，度量控制器可以计算显著偏离目标控制分布的状态并对其施加惩罚。在两个数据集上的实验结果表明，我们的模型在语义、度量和整体性能的自动评估以及人类评估方面优于现有模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.08456v2" target="_blank">2306.08456v2</a>
                              </td>
                              <td>PoetryDiffusion: Towards Joint Semantic and Metrical Manipulation in Poetry Generation</td>
                              <td>Zhiyuan Hu</td>
                              <td>2023-06-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_08456v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.08456v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07847v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Robustness Over Time: Understanding Adversarial Examples' Effectiveness on Longitudinal Versions of Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07847v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07847v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07847v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) have led to significant improvements in many tasks across various domains, such as code interpretation, response generation, and ambiguity handling. These LLMs, however, when upgrading, primarily prioritize enhancing user experience while neglecting security, privacy, and safety implications. Consequently, unintended vulnerabilities or biases can be introduced. Previous studies have predominantly focused on specific versions of the models and disregard the potential emergence of new attack vectors targeting the updated versions. Through the lens of adversarial examples within the in-context learning framework, this longitudinal study addresses this gap by conducting a comprehensive assessment of the robustness of successive versions of LLMs, vis-\`a-vis GPT-3.5. We conduct extensive experiments to analyze and understand the impact of the robustness in two distinct learning categories: zero-shot learning and few-shot learning. Our findings indicate that, in comparison to earlier versions of LLMs, the updated versions do not exhibit the anticipated level of robustness against adversarial attacks. In addition, our study emphasizes the increased effectiveness of synergized adversarial queries in most zero-shot learning and few-shot learning cases. We hope that our study can lead to a more refined assessment of the robustness of LLMs over time and provide valuable insights of these models for both developers and users.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07847v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）在各个领域的许多任务中都有了显著的改进，例如代码解释、响应生成和歧义处理。然而，在升级时，这些LLM主要优先考虑增强用户体验，而忽略了安全、隐私和安全影响。因此，可能会引入意外的漏洞或偏见。先前的研究主要集中在模型的特定版本上，而忽略了针对更新版本的新攻击载体的潜在出现。通过上下文学习框架内对抗性例子的视角，这项纵向研究通过对LLM的连续版本的稳健性进行全面评估来解决这一差距，相对于GPT-3.5。我们进行了广泛的实验来分析和理解鲁棒性在两个不同的学习类别中的影响：零样本学习和少热点学习。我们的研究结果表明，与早期版本的LLM相比，更新版本对对抗性攻击没有表现出预期的稳健性水平。此外，我们的研究强调了在大多数零样本学习和少数热点学习情况下，协同对抗性查询的有效性提高。我们希望我们的研究能够对LLM的稳健性进行更精细的评估，并为开发人员和用户提供这些模型的宝贵见解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07847v1" target="_blank">2308.07847v1</a>
                              </td>
                              <td>Robustness Over Time: Understanding Adversarial Examples' Effectiveness on Longitudinal Versions of Large Language Models</td>
                              <td>Yugeng Liu</td>
                              <td>2023-08-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07847v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07847v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07758v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Backward Reasoning in Large Language Models for Verification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07758v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07758v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07758v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., ``\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three LLMs. Experimental results demonstrate that FOBAR achieves state-of-the-art performance on various reasoning benchmarks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07758v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管链（CoT）提示在各种推理任务中表现出了良好的性能。最近，自一致性\citep｛wang2023selfconsistency｝提出对一组不同的推理链进行采样，这些推理链可能会导致不同的答案，同时选择获得最多选票的答案。在本文中，我们提出了一种新的方法来使用反向推理来验证候选答案。我们用$｛\bf x｝$屏蔽问题中的令牌，并要求LLM在\textit｛a simple template｝提供候选答案时预测屏蔽的令牌，即“\textit”｛\textbf｛如果我们知道上述问题的答案是\｛a candidate answer｝，那么未知变量$｛\bf x｝美元的值是多少？｝直观地，如果所提供的候选答案是正确的，则LLM有望成功预测掩蔽令牌。我们进一步提出FOBAR结合正向和反向推理来估计候选答案的概率。我们对六个数据集和三个LLM进行了广泛的实验。实验结果表明，FOBAR在各种推理基准上都达到了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07758v1" target="_blank">2308.07758v1</a>
                              </td>
                              <td>Backward Reasoning in Large Language Models for Verification</td>
                              <td>Weisen Jiang</td>
                              <td>2023-08-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07758v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07758v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07107v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Large Language Models for Information Retrieval: A Survey</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07107v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07107v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07107v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>As a primary means of information acquisition, information retrieval (IR) systems, such as search engines, have integrated themselves into our daily lives. These systems also serve as components of dialogue, question-answering, and recommender systems. The trajectory of IR has evolved dynamically from its origins in term-based methods to its integration with advanced neural models. While the neural models excel at capturing complex contextual signals and semantic nuances, thereby reshaping the IR landscape, they still face challenges such as data scarcity, interpretability, and the generation of contextually plausible yet potentially inaccurate responses. This evolution requires a combination of both traditional methods (such as term-based sparse retrieval methods with rapid response) and modern neural architectures (such as language models with powerful language understanding capacity). Meanwhile, the emergence of large language models (LLMs), typified by ChatGPT and GPT-4, has revolutionized natural language processing due to their remarkable language understanding, generation, generalization, and reasoning abilities. Consequently, recent research has sought to leverage LLMs to improve IR systems. Given the rapid evolution of this research trajectory, it is necessary to consolidate existing methodologies and provide nuanced insights through a comprehensive overview. In this survey, we delve into the confluence of LLMs and IR systems, including crucial aspects such as query rewriters, retrievers, rerankers, and readers. Additionally, we explore promising directions within this expanding field.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07107v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>作为信息获取的主要手段，搜索引擎等信息检索系统已经融入了我们的日常生活。这些系统也是对话、问答和推荐系统的组成部分。IR的轨迹从起源于基于术语的方法到与高级神经模型的集成，一直在动态演变。尽管神经模型擅长捕捉复杂的上下文信号和语义细微差别，从而重塑IR景观，但它们仍然面临着数据稀缺、可解释性以及生成上下文合理但可能不准确的响应等挑战。这种进化需要传统方法（如具有快速响应的基于术语的稀疏检索方法）和现代神经架构（如具有强大语言理解能力的语言模型）的结合。与此同时，以ChatGPT和GPT-4为代表的大型语言模型（LLM）的出现，由于其卓越的语言理解、生成、泛化和推理能力，使自然语言处理发生了革命性的变化。因此，最近的研究试图利用LLM来改进IR系统。鉴于这一研究轨迹的快速演变，有必要巩固现有的方法，并通过全面的概述提供细致入微的见解。在这项调查中，我们深入研究了LLM和IR系统的融合，包括查询重写器、检索器、重新排序器和读取器等关键方面。此外，我们还在这个不断扩大的领域中探索有希望的方向。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07107v2" target="_blank">2308.07107v2</a>
                              </td>
                              <td>Large Language Models for Information Retrieval: A Survey</td>
                              <td>Yutao Zhu</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07107v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07107v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07702v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Better Zero-Shot Reasoning with Role-Play Prompting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07702v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07702v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07702v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Modern large language models (LLMs), such as ChatGPT, exhibit a remarkable capacity for role-playing, enabling them to embody not only human characters but also non-human entities like a Linux terminal. This versatility allows them to simulate complex human-like interactions and behaviors within various contexts, as well as to emulate specific objects or systems. While these capabilities have enhanced user engagement and introduced novel modes of interaction, the influence of role-playing on LLMs' reasoning abilities remains underexplored. In this study, we introduce a strategically designed role-play prompting methodology and assess its performance under the zero-shot setting across twelve diverse reasoning benchmarks, encompassing arithmetic, commonsense reasoning, symbolic reasoning, and more. Leveraging models such as ChatGPT and Llama 2, our empirical results illustrate that role-play prompting consistently surpasses the standard zero-shot approach across most datasets. Notably, accuracy on AQuA rises from 53.5% to 63.8%, and on Last Letter from 23.8% to 84.2%. Beyond enhancing contextual understanding, we posit that role-play prompting serves as an implicit Chain-of-Thought (CoT) trigger, thereby improving the quality of reasoning. By comparing our approach with the Zero-Shot-CoT technique, which prompts the model to "think step by step", we further demonstrate that role-play prompting can generate a more effective CoT. This highlights its potential to augment the reasoning capabilities of LLMs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07702v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现代大型语言模型（LLM），如ChatGPT，表现出非凡的角色扮演能力，使它们不仅能够体现人类角色，还能够体现Linux终端等非人类实体。这种多功能性使他们能够模拟各种环境中复杂的类人交互和行为，以及模拟特定的对象或系统。虽然这些功能增强了用户参与度并引入了新的交互模式，但角色扮演对LLM推理能力的影响仍有待深入研究。在这项研究中，我们介绍了一种策略性设计的角色扮演提示方法，并评估了其在零样本设置下的性能，包括算术、常识推理、符号推理等十二个不同的推理基准。利用ChatGPT和Llama 2等模型，我们的实证结果表明，在大多数数据集中，角色扮演提示始终超过标准的零样本方法。值得注意的是，AQuA的准确性从53.5%上升到63.8%，Last Letter的准确性从23.8%上升到84.2%。除了增强上下文理解，我们还认为角色扮演提示是一种隐含的思维链触发因素，从而提高推理质量。通过将我们的方法与促使模型“循序渐进地思考”的Zero-ShotCoT技术进行比较，我们进一步证明了角色扮演提示可以生成更有效的CoT。这突出了其增强LLM推理能力的潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07702v1" target="_blank">2308.07702v1</a>
                              </td>
                              <td>Better Zero-Shot Reasoning with Role-Play Prompting</td>
                              <td>Aobo Kong</td>
                              <td>2023-08-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07702v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07702v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07662v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Gradient-Based Post-Training Quantization: Challenging the Status Quo</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07662v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07662v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07662v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Quantization has become a crucial step for the efficient deployment of deep neural networks, where floating point operations are converted to simpler fixed point operations. In its most naive form, it simply consists in a combination of scaling and rounding transformations, leading to either a limited compression rate or a significant accuracy drop. Recently, Gradient-based post-training quantization (GPTQ) methods appears to be constitute a suitable trade-off between such simple methods and more powerful, yet expensive Quantization-Aware Training (QAT) approaches, particularly when attempting to quantize LLMs, where scalability of the quantization process is of paramount importance. GPTQ essentially consists in learning the rounding operation using a small calibration set. In this work, we challenge common choices in GPTQ methods. In particular, we show that the process is, to a certain extent, robust to a number of variables (weight selection, feature augmentation, choice of calibration set). More importantly, we derive a number of best practices for designing more efficient and scalable GPTQ methods, regarding the problem formulation (loss, degrees of freedom, use of non-uniform quantization schemes) or optimization process (choice of variable and optimizer). Lastly, we propose a novel importance-based mixed-precision technique. Those guidelines lead to significant performance improvements on all the tested state-of-the-art GPTQ methods and networks (e.g. +6.819 points on ViT for 4-bit quantization), paving the way for the design of scalable, yet effective quantization methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07662v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>量化已经成为高效部署深度神经网络的关键一步，在深度神经网络中，浮点运算被转换为更简单的定点运算。在最简单的形式中，它只是由缩放和舍入变换的组合组成，导致压缩率有限或精度显著下降。最近，基于梯度的训练后量化（GPTQ）方法似乎构成了这种简单方法和更强大但更昂贵的量化感知训练（QAT）方法之间的适当权衡，特别是在试图量化LLM时，其中量化过程的可扩展性至关重要。GPTQ主要包括使用一个小的校准集学习舍入操作。在这项工作中，我们对GPTQ方法中的常见选择提出了挑战。特别地，我们证明了该过程在一定程度上对许多变量（权重选择、特征增强、校准集的选择）是鲁棒的。更重要的是，我们推导了一些最佳实践，用于设计更高效和可扩展的GPTQ方法，涉及问题公式（损失、自由度、非均匀量化方案的使用）或优化过程（变量和优化器的选择）。最后，我们提出了一种新的基于重要性的混合精度技术。这些指南显著提高了所有测试过的最先进的GPTQ方法和网络的性能（例如，4位量化的ViT得分为+6.819），为设计可扩展但有效的量化方法铺平了道路。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07662v1" target="_blank">2308.07662v1</a>
                              </td>
                              <td>Gradient-Based Post-Training Quantization: Challenging the Status Quo</td>
                              <td>Edouard Yvinec</td>
                              <td>2023-08-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07662v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07662v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07645v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Steering Language Generation: Harnessing Contrastive Expert Guidance and Negative Prompting for Coherent and Diverse Synthetic Data Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07645v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07645v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07645v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) hold immense potential to generate synthetic data of high quality and utility, which has numerous applications from downstream model training to practical data utilisation. However, contemporary models, despite their impressive capacities, consistently struggle to produce both coherent and diverse data. To address the coherency issue, we introduce contrastive expert guidance, where the difference between the logit distributions of fine-tuned and base language models is emphasised to ensure domain adherence. In order to ensure diversity, we utilise existing real and synthetic examples as negative prompts to the model. We deem this dual-pronged approach to logit reshaping as STEER: Semantic Text Enhancement via Embedding Repositioning. STEER operates at inference-time and systematically guides the LLMs to strike a balance between adherence to the data distribution (ensuring semantic fidelity) and deviation from prior synthetic examples or existing real datasets (ensuring diversity and authenticity). This delicate balancing act is achieved by dynamically moving towards or away from chosen representations in the latent space. STEER demonstrates improved performance over previous synthetic data generation techniques, exhibiting better balance between data diversity and coherency across three distinct tasks: hypothesis generation, toxic and non-toxic comment generation, and commonsense reasoning task generation. We demonstrate how STEER allows for fine-tuned control over the diversity-coherency trade-off via its hyperparameters, highlighting its versatility.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07645v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）在生成高质量和实用性的合成数据方面具有巨大的潜力，从下游模型训练到实际数据利用都有许多应用。然而，尽管当代模型的能力令人印象深刻，但它们始终难以产生连贯和多样的数据。为了解决一致性问题，我们引入了对比专家指导，其中强调了微调和基本语言模型的logit分布之间的差异，以确保领域一致性。为了确保多样性，我们利用现有的真实和合成例子作为模型的负面提示。我们将这种双管齐下的logit重塑方法视为STEER：通过嵌入重新定位增强语义文本。STEER在推理时操作，并系统地指导LLM在遵守数据分布（确保语义保真度）和偏离先前合成示例或现有真实数据集（确保多样性和真实性）之间取得平衡。这种微妙的平衡行为是通过在潜在空间中动态地朝向或远离所选择的表示来实现的。与以前的合成数据生成技术相比，STEER的性能有所提高，在三个不同的任务（假设生成、有毒和无毒评论生成以及常识推理任务生成）中，数据多样性和一致性之间表现出更好的平衡。我们展示了STEER如何通过其超参数对多样性-一致性权衡进行微调控制，突出了其多功能性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07645v1" target="_blank">2308.07645v1</a>
                              </td>
                              <td>Steering Language Generation: Harnessing Contrastive Expert Guidance and Negative Prompting for Coherent and Diverse Synthetic Data Generation</td>
                              <td>Charles O'Neill</td>
                              <td>2023-08-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07645v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07645v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07641v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Ternary Singular Value Decomposition as a Better Parameterized Form in Linear Mapping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07641v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07641v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07641v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present a simple yet novel parameterized form of linear mapping to achieves remarkable network compression performance: a pseudo SVD called Ternary SVD (TSVD).   Unlike vanilla SVD, TSVD limits the $U$ and $V$ matrices in SVD to ternary matrices form in $\{\pm 1, 0\}$. This means that instead of using the expensive multiplication instructions, TSVD only requires addition instructions when computing $U(\cdot)$ and $V(\cdot)$.   We provide direct and training transition algorithms for TSVD like Post Training Quantization and Quantization Aware Training respectively. Additionally, we analyze the convergence of the direct transition algorithms in theory.   In experiments, we demonstrate that TSVD can achieve state-of-the-art network compression performance in various types of networks and tasks, including current baseline models such as ConvNext, Swim, BERT, and large language model like OPT.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07641v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种简单而新颖的线性映射参数化形式，以实现显著的网络压缩性能：一种称为三值SVD（TSVD）的伪SVD。与普通SVD不同，TSVD将SVD中的$U$和$V$矩阵限制为$\｛\pm 1，0\｝$形式的三元矩阵。这意味着TSVD在计算$U（\cdot）$和$V（\cdot）$时只需要加法指令，而不是使用昂贵的乘法指令。我们分别为TSVD提供了直接和训练转换算法，如训练后量化和量化感知训练。此外，我们还从理论上分析了直接转换算法的收敛性。在实验中，我们证明TSVD可以在各种类型的网络和任务中实现最先进的网络压缩性能，包括当前的基线模型，如ConvNext、Swim、BERT，以及大型语言模型，如OPT。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07641v1" target="_blank">2308.07641v1</a>
                              </td>
                              <td>Ternary Singular Value Decomposition as a Better Parameterized Form in Linear Mapping</td>
                              <td>Boyu Chen</td>
                              <td>2023-08-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07641v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07641v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07635v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LLM-Mini-CEX: Automatic Evaluation of Large Language Model for Diagnostic Conversation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07635v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07635v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07635v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>There is an increasing interest in developing LLMs for medical diagnosis to improve diagnosis efficiency. Despite their alluring technological potential, there is no unified and comprehensive evaluation criterion, leading to the inability to evaluate the quality and potential risks of medical LLMs, further hindering the application of LLMs in medical treatment scenarios. Besides, current evaluations heavily rely on labor-intensive interactions with LLMs to obtain diagnostic dialogues and human evaluation on the quality of diagnosis dialogue. To tackle the lack of unified and comprehensive evaluation criterion, we first initially establish an evaluation criterion, termed LLM-specific Mini-CEX to assess the diagnostic capabilities of LLMs effectively, based on original Mini-CEX. To address the labor-intensive interaction problem, we develop a patient simulator to engage in automatic conversations with LLMs, and utilize ChatGPT for evaluating diagnosis dialogues automatically. Experimental results show that the LLM-specific Mini-CEX is adequate and necessary to evaluate medical diagnosis dialogue. Besides, ChatGPT can replace manual evaluation on the metrics of humanistic qualities and provides reproducible and automated comparisons between different LLMs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07635v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人们对开发用于医学诊断的LLM以提高诊断效率越来越感兴趣。尽管LLM具有诱人的技术潜力，但由于缺乏统一、全面的评估标准，导致无法评估医疗LLM的质量和潜在风险，进一步阻碍了LLM在医疗场景中的应用。此外，目前的评估在很大程度上依赖于与LLM的劳动密集型互动来获得诊断对话和人类对诊断对话质量的评估。为了解决缺乏统一全面的评估标准的问题，我们首先在原有的Mini-CEX基础上，初步建立了一个评估标准，称为LLM特异性Mini-CEX，以有效评估LLM的诊断能力。为了解决劳动密集型交互问题，我们开发了一个患者模拟器，与LLM进行自动对话，并利用ChatGPT自动评估诊断对话。实验结果表明，LLM特异性Mini-CEX对于评估医学诊断对话是充分和必要的。此外，ChatGPT可以取代对人文素质指标的手动评估，并在不同LLM之间提供可重复和自动的比较。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07635v1" target="_blank">2308.07635v1</a>
                              </td>
                              <td>LLM-Mini-CEX: Automatic Evaluation of Large Language Model for Diagnostic Conversation</td>
                              <td>Xiaoming Shi</td>
                              <td>2023-08-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07635v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07635v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07633v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Survey on Model Compression for Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07633v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07633v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07633v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) have revolutionized natural language processing tasks with remarkable success. However, their formidable size and computational demands present significant challenges for practical deployment, especially in resource-constrained environments. As these challenges become increasingly pertinent, the field of model compression has emerged as a pivotal research area to alleviate these limitations. This paper presents a comprehensive survey that navigates the landscape of model compression techniques tailored specifically for LLMs. Addressing the imperative need for efficient deployment, we delve into various methodologies, encompassing quantization, pruning, knowledge distillation, and more. Within each of these techniques, we highlight recent advancements and innovative approaches that contribute to the evolving landscape of LLM research. Furthermore, we explore benchmarking strategies and evaluation metrics that are essential for assessing the effectiveness of compressed LLMs. By providing insights into the latest developments and practical implications, this survey serves as an invaluable resource for both researchers and practitioners. As LLMs continue to evolve, this survey aims to facilitate enhanced efficiency and real-world applicability, establishing a foundation for future advancements in the field.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07633v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）以显著的成功彻底改变了自然语言处理任务。然而，它们强大的规模和计算需求给实际部署带来了重大挑战，尤其是在资源受限的环境中。随着这些挑战变得越来越重要，模型压缩领域已成为缓解这些限制的关键研究领域。本文对专门为LLM量身定制的模型压缩技术进行了全面的调查。为了满足高效部署的迫切需要，我们深入研究了各种方法，包括量化、修剪、知识提炼等。在每一种技术中，我们都强调了有助于LLM研究不断发展的最新进展和创新方法。此外，我们探索了对评估压缩LLM的有效性至关重要的基准测试策略和评估指标。通过深入了解最新发展和实际意义，这项调查为研究人员和从业者提供了宝贵的资源。随着LLM的不断发展，这项调查旨在促进提高效率和现实世界的适用性，为该领域的未来进步奠定基础。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07633v1" target="_blank">2308.07633v1</a>
                              </td>
                              <td>A Survey on Model Compression for Large Language Models</td>
                              <td>Xunyu Zhu</td>
                              <td>2023-08-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07633v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07633v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07610v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LogPrompt: Prompt Engineering Towards Zero-Shot and Interpretable Log Analysis</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07610v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07610v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07610v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Automated log analysis is crucial in modern software-intensive systems for ensuring reliability and resilience throughout software maintenance and engineering life cycles. Existing methods perform tasks such as log parsing and log anomaly detection by providing a single prediction value without interpretation. However, given the increasing volume of system events, the limited interpretability of analysis results hinders analysts' trust and their ability to take appropriate actions. Moreover, these methods require substantial in-domain training data, and their performance declines sharply (by up to 62.5%) in online scenarios involving unseen logs from new domains, a common occurrence due to rapid software updates. In this paper, we propose LogPrompt, a novel zero-shot and interpretable log analysis approach. LogPrompt employs large language models (LLMs) to perform zero-shot log analysis tasks via a suite of advanced prompt strategies tailored for log tasks, which enhances LLMs' performance by up to 107.5% compared with simple prompts. Experiments on nine publicly available evaluation datasets across two tasks demonstrate that LogPrompt, despite using no training data, outperforms existing approaches trained on thousands of logs by up to around 50%. We also conduct a human evaluation of LogPrompt's interpretability, with six practitioners possessing over 10 years of experience, who highly rated the generated content in terms of usefulness and readability (averagely 4.42/5). LogPrompt also exhibits remarkable compatibility with open-source and smaller-scale LLMs, making it flexible for practical deployment.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07610v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在现代软件密集型系统中，自动化日志分析对于确保整个软件维护和工程生命周期的可靠性和弹性至关重要。现有方法通过提供单个预测值而不进行解释来执行诸如日志解析和日志异常检测之类的任务。然而，鉴于系统事件的数量不断增加，分析结果的可解释性有限，阻碍了分析师的信任和采取适当行动的能力。此外，这些方法需要大量的域内训练数据，在涉及新域中看不见的日志的在线场景中，它们的性能会急剧下降（高达62.5%），这是由于软件快速更新而常见的情况。在本文中，我们提出了LogPrompt，一种新的零样本和可解释的日志分析方法。LogPrompt采用大型语言模型（LLM），通过一套针对日志任务定制的高级提示策略来执行零样本日志分析任务，与简单提示相比，该策略将LLM的性能提高了107.5%。在两个任务的九个公开可用的评估数据集上进行的实验表明，尽管没有使用训练数据，但LogPrompt的性能比在数千个日志上训练的现有方法高出50%左右。我们还对LogPrompt的可解释性进行了人工评估，六名从业人员拥有超过10年的经验，他们对生成的内容的有用性和可读性给予了高度评价（平均4.42/5）。LogPrompt还与开源和小型LLM表现出显著的兼容性，使其能够灵活地进行实际部署。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07610v1" target="_blank">2308.07610v1</a>
                              </td>
                              <td>LogPrompt: Prompt Engineering Towards Zero-Shot and Interpretable Log Analysis</td>
                              <td>Yilun Liu</td>
                              <td>2023-08-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07610v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07610v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07074v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07074v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07074v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07074v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Foundation language models obtain the instruction-following ability through supervised fine-tuning (SFT). Diversity and complexity are considered critical factors of a successful SFT dataset, while their definitions remain obscure and lack quantitative analyses. In this work, we propose InsTag, an open-set fine-grained tagger, to tag samples within SFT datasets based on semantics and intentions and define instruction diversity and complexity regarding tags. We obtain 6.6K tags to describe comprehensive user queries. Then we analyze popular open-sourced SFT datasets and find that the model ability grows with more diverse and complex data. Based on this observation, we propose a data selector based on InsTag to select 6K diverse and complex samples from open-source datasets and fine-tune models on InsTag-selected data. The resulting models, TagLM, outperform open-source models based on considerably larger SFT data evaluated by MT-Bench, echoing the importance of query diversity and complexity. We open-source InsTag in https://github.com/OFA-Sys/InsTag.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07074v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基础语言模型通过监督微调（SFT）获得指令跟随能力。多样性和复杂性被认为是成功的SFT数据集的关键因素，而它们的定义仍然模糊不清，缺乏定量分析。在这项工作中，我们提出了一种开放集细粒度标记器InsTag，用于基于语义和意图在SFT数据集中标记样本，并定义与标记相关的指令多样性和复杂性。我们获得了6.6K标签来描述全面的用户查询。然后，我们分析了流行的开源SFT数据集，发现模型能力随着数据的多样性和复杂性而增长。基于这一观察结果，我们提出了一种基于InsTag的数据选择器，从开源数据集中选择6K不同和复杂的样本，并对InsTag选择的数据进行微调模型。由此产生的模型TagLM优于基于MT Bench评估的相当大的SFT数据的开源模型，反映了查询多样性和复杂性的重要性。我们在https://github.com/OFA-Sys/InsTag.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07074v2" target="_blank">2308.07074v2</a>
                              </td>
                              <td>#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models</td>
                              <td>Keming Lu</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07074v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07074v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03025v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Style Over Substance: Evaluation Biases for Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03025v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03025v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03025v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>As large language models (LLMs) continue to advance, accurately and comprehensively evaluating their performance becomes increasingly challenging. Human evaluations are conventionally considered the gold standard in natural language generation, but recent advancements incorporate state-of-the-art LLMs as proxies for human judges in evaluation processes. However, the extent to which humans and LLMs are capable evaluators remains uncertain. This study investigates the behavior of crowd-sourced and expert annotators, as well as LLMs, when comparing outputs from different models. To achieve this, we curate a dataset of intentionally flawed machine-generated answers. Our findings reveal a concerning bias in the evaluation process, as answers with factual errors are rated more favorably than answers that are too short or contained grammatical errors. To address this issue, we propose independently evaluating machine-generated text across multiple dimensions, rather than merging all the evaluation aspects into a single score. We instantiate this idea with the Elo rating system, resulting in the Multi-Elo Rating System. Empirical results from our study reveal that this proposed approach significantly enhances the quality of LLM-based evaluations, particularly in terms of factual accuracy. However, there is no significant improvement in crowd-sourced-based evaluations, indicating the need for further investigation and refinement.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03025v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着大型语言模型（LLM）的不断发展，准确、全面地评估其性能变得越来越具有挑战性。人类评估通常被认为是自然语言生成的黄金标准，但最近的进步将最先进的LLM作为人类法官在评估过程中的代理。然而，人类和LLM在多大程度上是有能力的评估者仍不确定。这项研究调查了众包和专家注释器以及LLM在比较不同模型的输出时的行为。为了实现这一点，我们策划了一个故意有缺陷的机器生成答案的数据集。我们的研究结果揭示了评估过程中令人担忧的偏见，因为有事实错误的答案比太短或包含语法错误的答案更受好评。为了解决这个问题，我们建议跨多个维度独立评估机器生成的文本，而不是将所有评估方面合并为一个分数。我们用Elo评级系统来实例化这个想法，从而产生了多Elo评级体系。我们研究的实证结果表明，所提出的方法显著提高了基于LLM的评估的质量，特别是在事实准确性方面。然而，基于众包的评价没有显著改善，这表明需要进一步调查和完善。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03025v2" target="_blank">2307.03025v2</a>
                              </td>
                              <td>Style Over Substance: Evaluation Biases for Large Language Models</td>
                              <td>Minghao Wu</td>
                              <td>2023-07-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03025v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03025v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07540v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CALYPSO: LLMs as Dungeon Masters' Assistants</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07540v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07540v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07540v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The role of a Dungeon Master, or DM, in the game Dungeons & Dragons is to perform multiple tasks simultaneously. The DM must digest information about the game setting and monsters, synthesize scenes to present to other players, and respond to the players' interactions with the scene. Doing all of these tasks while maintaining consistency within the narrative and story world is no small feat of human cognition, making the task tiring and unapproachable to new players. Large language models (LLMs) like GPT-3 and ChatGPT have shown remarkable abilities to generate coherent natural language text. In this paper, we conduct a formative evaluation with DMs to establish the use cases of LLMs in D&D and tabletop gaming generally. We introduce CALYPSO, a system of LLM-powered interfaces that support DMs with information and inspiration specific to their own scenario. CALYPSO distills game context into bite-sized prose and helps brainstorm ideas without distracting the DM from the game. When given access to CALYPSO, DMs reported that it generated high-fidelity text suitable for direct presentation to players, and low-fidelity ideas that the DM could develop further while maintaining their creative agency. We see CALYPSO as exemplifying a paradigm of AI-augmented tools that provide synchronous creative assistance within established game worlds, and tabletop gaming more broadly.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07540v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>地下城大师（DM）在《龙与地下城》游戏中的角色是同时执行多项任务。DM必须消化有关游戏设置和怪物的信息，合成场景以呈现给其他玩家，并对玩家与场景的互动做出反应。在保持叙事和故事世界的一致性的同时完成所有这些任务是人类认知的一项不小的壮举，这使得新玩家无法完成这项任务。像GPT-3和ChatGPT这样的大型语言模型（LLM）在生成连贯的自然语言文本方面表现出了非凡的能力。在本文中，我们用DM进行了形成性评估，以建立LLM在D&D和桌面游戏中的一般用例。我们介绍了CALYPSO，这是一个LLM支持的接口系统，它为DM提供了特定于其自身场景的信息和灵感。CALYPSO将游戏背景提炼成一口大小的散文，并帮助集思广益，而不会分散DM对游戏的注意力。当被允许访问CALYPSO时，DM报告说，它生成了适合直接呈现给玩家的高保真文本，以及DM可以在保持其创造性代理的同时进一步发展的低保真度想法。我们认为CALYPSO是人工智能增强工具的典范，它在既定的游戏世界和更广泛的桌面游戏中提供同步的创造性帮助。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07540v1" target="_blank">2308.07540v1</a>
                              </td>
                              <td>CALYPSO: LLMs as Dungeon Masters' Assistants</td>
                              <td>Andrew Zhu</td>
                              <td>2023-08-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07540v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07540v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07525v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Delphic Costs and Benefits in Web Search: A utilitarian and historical analysis</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07525v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07525v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07525v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present a new framework to conceptualize and operationalize the total user experience of search, by studying the entirety of a search journey from an utilitarian point of view.   Web search engines are widely perceived as "free". But search requires time and effort: in reality there are many intermingled non-monetary costs (e.g. time costs, cognitive costs, interactivity costs) and the benefits may be marred by various impairments, such as misunderstanding and misinformation. This characterization of costs and benefits appears to be inherent to the human search for information within the pursuit of some larger task: most of the costs and impairments can be identified in interactions with any web search engine, interactions with public libraries, and even in interactions with ancient oracles. To emphasize this innate connection, we call these costs and benefits Delphic, in contrast to explicitly financial costs and benefits.   Our main thesis is that the users' satisfaction with a search engine mostly depends on their experience of Delphic cost and benefits, in other words on their utility. The consumer utility is correlated with classic measures of search engine quality, such as ranking, precision, recall, etc., but is not completely determined by them. To argue our thesis, we catalog the Delphic costs and benefits and show how the development of search engines over the last quarter century, from classic Information Retrieval roots to the integration of Large Language Models, was driven to a great extent by the quest of decreasing Delphic costs and increasing Delphic benefits.   We hope that the Delphic costs framework will engender new ideas and new research for evaluating and improving the web experience for everyone.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07525v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一个新的框架，通过从功利主义的角度研究整个搜索过程，来概念化和操作搜索的总用户体验。网络搜索引擎被广泛认为是“免费的”。但搜索需要时间和精力：事实上，存在许多混合的非货币成本（如时间成本、认知成本、互动成本），其好处可能会因误解和错误信息等各种障碍而受损。这种成本和收益的表征似乎是人类在追求更大任务时搜索信息所固有的：大多数成本和损害可以在与任何网络搜索引擎的交互、与公共图书馆的交互，甚至与古代神谕的交互中识别出来。为了强调这种内在联系，我们将这些成本和收益称为Delphic，而不是明确的财务成本和收益。我们的主要论点是，用户对搜索引擎的满意度主要取决于他们对Delphic成本和收益的体验，换句话说，取决于他们的效用。消费者效用与搜索引擎质量的经典衡量标准相关，如排名、精确度、召回率等，但并不完全由它们决定。为了论证我们的论文，我们列举了Delphic的成本和收益，并展示了过去25年来搜索引擎的发展，从经典的信息检索根源到大型语言模型的集成，在很大程度上是由减少Delphic成本和增加Delphic收益的追求驱动的。我们希望Delphic成本框架将产生新的想法和新的研究，以评估和改善每个人的网络体验。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07525v1" target="_blank">2308.07525v1</a>
                              </td>
                              <td>Delphic Costs and Benefits in Web Search: A utilitarian and historical analysis</td>
                              <td>Andrei Z. Broder</td>
                              <td>2023-08-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07525v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07525v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07308v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07308v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07308v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07308v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have skyrocketed in popularity in recent years due to their ability to generate high-quality text in response to human prompting. However, these models have been shown to have the potential to generate harmful content in response to user prompting (e.g., giving users instructions on how to commit crimes). There has been a focus in the literature on mitigating these risks, through methods like aligning models with human values through reinforcement learning. However, it has been shown that even aligned language models are susceptible to adversarial attacks that bypass their restrictions on generating harmful text. We propose a simple approach to defending against these attacks by having a large language model filter its own responses. Our current results show that even if a model is not fine-tuned to be aligned with human values, it is possible to stop it from presenting harmful content to users by validating the content using a language model.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07308v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，大型语言模型（LLM）因其能够响应人类提示生成高质量文本而大受欢迎。然而，这些模型已被证明有可能根据用户提示（例如，向用户提供如何犯罪的指示）生成有害内容。文献中一直关注通过强化学习使模型与人类价值观相一致等方法来减轻这些风险。然而，研究表明，即使是对齐的语言模型也容易受到对抗性攻击，这些攻击绕过了它们对生成有害文本的限制。我们提出了一种简单的方法来防御这些攻击，方法是让一个大型语言模型过滤自己的响应。我们目前的研究结果表明，即使一个模型没有经过微调以与人类价值观相一致，也有可能通过使用语言模型验证内容来阻止它向用户呈现有害内容。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07308v2" target="_blank">2308.07308v2</a>
                              </td>
                              <td>LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked</td>
                              <td>Alec Helbling</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07308v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07308v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07517v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Synergi: A Mixed-Initiative System for Scholarly Synthesis and Sensemaking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07517v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07517v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07517v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Efficiently reviewing scholarly literature and synthesizing prior art are crucial for scientific progress. Yet, the growing scale of publications and the burden of knowledge make synthesis of research threads more challenging than ever. While significant research has been devoted to helping scholars interact with individual papers, building research threads scattered across multiple papers remains a challenge. Most top-down synthesis (and LLMs) make it difficult to personalize and iterate on the output, while bottom-up synthesis is costly in time and effort. Here, we explore a new design space of mixed-initiative workflows. In doing so we develop a novel computational pipeline, Synergi, that ties together user input of relevant seed threads with citation graphs and LLMs, to expand and structure them, respectively. Synergi allows scholars to start with an entire threads-and-subthreads structure generated from papers relevant to their interests, and to iterate and customize on it as they wish. In our evaluation, we find that Synergi helps scholars efficiently make sense of relevant threads, broaden their perspectives, and increases their curiosity. We discuss future design implications for thread-based, mixed-initiative scholarly synthesis support tools.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07517v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>有效地回顾学术文献和综合现有技术对科学进步至关重要。然而，出版物规模的不断扩大和知识的负担使研究线索的综合比以往任何时候都更具挑战性。尽管大量研究致力于帮助学者与个别论文互动，但构建分散在多篇论文中的研究线索仍然是一项挑战。大多数自上而下的合成（和LLM）都很难对输出进行个性化和迭代，而自下而上的合成在时间和精力上都很昂贵。在这里，我们探索了一个混合倡议工作流的新设计空间。在这样做的过程中，我们开发了一个新的计算管道Synergi，它将相关种子线程的用户输入与引用图和LLM联系在一起，以分别扩展和构建它们。Synergi允许学者从与他们兴趣相关的论文中生成的整个线程和子线程结构开始，并根据他们的意愿对其进行迭代和定制。在我们的评估中，我们发现Synergi有助于学者有效地理解相关线索，拓宽他们的视角，增加他们的好奇心。我们讨论了基于线程的混合倡议学术综合支持工具的未来设计含义。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07517v1" target="_blank">2308.07517v1</a>
                              </td>
                              <td>Synergi: A Mixed-Initiative System for Scholarly Synthesis and Sensemaking</td>
                              <td>Hyeonsu B. Kang</td>
                              <td>2023-08-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07517v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07517v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07505v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Data Race Detection Using Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07505v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07505v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07505v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) are demonstrating significant promise as an alternate strategy to facilitate analyses and optimizations of high-performance computing programs, circumventing the need for resource-intensive manual tool creation. In this paper, we explore a novel LLM-based data race detection approach combining prompting engineering and fine-tuning techniques. We create a dedicated dataset named DRB-ML, which is derived from DataRaceBench, with fine-grain labels showing the presence of data race pairs and their associated variables, line numbers, and read/write information. DRB-ML is then used to evaluate representative LLMs and fine-tune open-source ones. Our experiment shows that LLMs can be a viable approach to data race detection. However, they still cannot compete with traditional data race detection tools when we need detailed information about variable pairs causing data races.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07505v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）作为一种替代策略，有助于高性能计算程序的分析和优化，从而避免了资源密集型手动工具创建的需要。在本文中，我们探索了一种新的基于LLM的数据竞赛检测方法，该方法结合了提示工程和微调技术。我们创建了一个名为DRB-ML的专用数据集，该数据集源自DataRaceBeach，带有精细标签，显示数据竞赛对及其相关变量、行号和读/写信息的存在。然后使用DRB-ML来评估具有代表性的LLM，并对开源LLM进行微调。我们的实验表明LLM是一种可行的数据竞赛检测方法。然而，当我们需要有关导致数据竞赛的变量对的详细信息时，它们仍然无法与传统的数据竞赛检测工具竞争。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07505v1" target="_blank">2308.07505v1</a>
                              </td>
                              <td>Data Race Detection Using Large Language Models</td>
                              <td>Le Chen</td>
                              <td>2023-08-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07505v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07505v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_05713v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Testing GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on math and science problems</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_05713v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_05713v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_05713v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This report describes a test of the large language model GPT-4 with the Wolfram Alpha and the Code Interpreter plug-ins on 105 original problems in science and math, at the high school and college levels, carried out in June-August 2023. Our tests suggest that the plug-ins significantly enhance GPT's ability to solve these problems. Having said that, there are still often "interface" failures; that is, GPT often has trouble formulating problems in a way that elicits useful answers from the plug-ins. Fixing these interface failures seems like a central challenge in making GPT a reliable tool for college-level calculation problems.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_05713v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本报告描述了2023年6月至8月在高中和大学级别对大型语言模型GPT-4进行的一项测试，该测试使用Wolfram Alpha和代码解释器插件，针对105道科学和数学原始问题。我们的测试表明，这些插件显著增强了GPT解决这些问题的能力。话虽如此，仍然经常存在“接口”故障；也就是说，GPT在从插件中获取有用答案的方式中经常难以制定问题。修复这些接口故障似乎是使GPT成为解决大学级计算问题的可靠工具的核心挑战。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.05713v2" target="_blank">2308.05713v2</a>
                              </td>
                              <td>Testing GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on math and science problems</td>
                              <td>Ernest Davis</td>
                              <td>2023-08-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_05713v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.05713v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07499v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Detecting The Corruption Of Online Questionnaires By Artificial Intelligence</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07499v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07499v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07499v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Online questionnaires that use crowd-sourcing platforms to recruit participants have become commonplace, due to their ease of use and low costs. Artificial Intelligence (AI) based Large Language Models (LLM) have made it easy for bad actors to automatically fill in online forms, including generating meaningful text for open-ended tasks. These technological advances threaten the data quality for studies that use online questionnaires. This study tested if text generated by an AI for the purpose of an online study can be detected by both humans and automatic AI detection systems. While humans were able to correctly identify authorship of text above chance level (76 percent accuracy), their performance was still below what would be required to ensure satisfactory data quality. Researchers currently have to rely on the disinterest of bad actors to successfully use open-ended responses as a useful tool for ensuring data quality. Automatic AI detection systems are currently completely unusable. If AIs become too prevalent in submitting responses then the costs associated with detecting fraudulent submissions will outweigh the benefits of online questionnaires. Individual attention checks will no longer be a sufficient tool to ensure good data quality. This problem can only be systematically addressed by crowd-sourcing platforms. They cannot rely on automatic AI detection systems and it is unclear how they can ensure data quality for their paying clients.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07499v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用众包平台招募参与者的在线问卷由于其易用性和低成本而变得司空见惯。基于人工智能（AI）的大型语言模型（LLM）使不良行为者很容易自动填写在线表格，包括为开放式任务生成有意义的文本。这些技术进步威胁到使用在线问卷的研究的数据质量。这项研究测试了人工智能为在线研究生成的文本是否可以被人类和自动人工智能检测系统检测到。尽管人类能够正确识别出高于偶然水平（76%的准确率）的文本作者，但他们的表现仍低于确保令人满意的数据质量所需的水平。研究人员目前必须依靠不良行为者的不感兴趣，才能成功地使用开放式回答作为确保数据质量的有用工具。自动AI检测系统目前完全无法使用。如果人工智能在提交回复时过于普遍，那么检测欺诈提交的相关成本将超过在线问卷的好处。个人注意力检查将不再是确保良好数据质量的充分工具。只有众包平台才能系统地解决这个问题。他们不能依赖自动人工智能检测系统，也不清楚如何确保付费客户的数据质量。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07499v1" target="_blank">2308.07499v1</a>
                              </td>
                              <td>Detecting The Corruption Of Online Questionnaires By Artificial Intelligence</td>
                              <td>Benjamin Lebrun</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07499v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07499v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07462v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Playing with Words: Comparing the Vocabulary and Lexical Richness of ChatGPT and Humans</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07462v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07462v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07462v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The introduction of Artificial Intelligence (AI) generative language models such as GPT (Generative Pre-trained Transformer) and tools such as ChatGPT has triggered a revolution that can transform how text is generated. This has many implications, for example, as AI-generated text becomes a significant fraction of the text in many disciplines, would this have an effect on the language capabilities of readers and also on the training of newer AI tools? Would it affect the evolution of languages? Focusing on one specific aspect of the language: words; will the use of tools such as ChatGPT increase or reduce the vocabulary used or the lexical richness (understood as the number of different words used in a written or oral production) when writing a given text? This has implications for words, as those not included in AI-generated content will tend to be less and less popular and may eventually be lost. In this work, we perform an initial comparison of the vocabulary and lexical richness of ChatGPT and humans when performing the same tasks. In more detail, two datasets containing the answers to different types of questions answered by ChatGPT and humans are used, and the analysis shows that ChatGPT tends to use fewer distinct words and lower lexical richness than humans. These results are very preliminary and additional datasets and ChatGPT configurations have to be evaluated to extract more general conclusions. Therefore, further research is needed to understand how the use of ChatGPT and more broadly generative AI tools will affect the vocabulary and lexical richness in different types of text and languages.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07462v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>GPT（generative Pre-trained Transformer）等人工智能（AI）生成语言模型和ChatGPT等工具的引入引发了一场可以改变文本生成方式的革命。这有很多含义，例如，当人工智能生成的文本在许多学科中成为文本的重要组成部分时，这会对读者的语言能力以及更新的人工智能工具的培训产生影响吗？它会影响语言的进化吗？关注语言的一个特定方面：单词；在编写给定文本时，使用ChatGPT等工具会增加或减少所使用的词汇或词汇丰富度（理解为书面或口头作品中使用的不同单词的数量）吗？这对单词有影响，因为人工智能生成的内容中没有包含的单词往往越来越不受欢迎，最终可能会丢失。在这项工作中，我们对ChatGPT和人类在执行相同任务时的词汇和词汇丰富度进行了初步比较。更详细地说，使用了两个数据集，其中包含ChatGPT和人类回答的不同类型问题的答案，分析表明，与人类相比，ChatGPT倾向于使用更少的不同单词和更低的词汇丰富度。这些结果是非常初步的，必须评估额外的数据集和ChatGPT配置，以提取更一般的结论。因此，需要进一步研究，以了解ChatGPT和更广泛的生成人工智能工具的使用将如何影响不同类型文本和语言中的词汇和词汇丰富度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07462v1" target="_blank">2308.07462v1</a>
                              </td>
                              <td>Playing with Words: Comparing the Vocabulary and Lexical Richness of ChatGPT and Humans</td>
                              <td>Pedro Reviriego</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07462v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07462v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07429v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Semantic Similarity Loss for Neural Source Code Summarization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07429v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07429v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07429v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents an improved loss function for neural source code summarization. Code summarization is the task of writing natural language descriptions of source code. Neural code summarization refers to automated techniques for generating these descriptions using neural networks. Almost all current approaches involve neural networks as either standalone models or as part of a pretrained large language models e.g., GPT, Codex, LLaMA. Yet almost all also use a categorical cross-entropy (CCE) loss function for network optimization. Two problems with CCE are that 1) it computes loss over each word prediction one-at-a-time, rather than evaluating a whole sentence, and 2) it requires a perfect prediction, leaving no room for partial credit for synonyms. We propose and evaluate a loss function to alleviate this problem. In essence, we propose to use a semantic similarity metric to calculate loss over the whole output sentence prediction per training batch, rather than just loss for each word. We also propose to combine our loss with traditional CCE for each word, which streamlines the training process compared to baselines. We evaluate our approach over several baselines and report an improvement in the vast majority of conditions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07429v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种用于神经源代码摘要的改进损失函数。代码摘要是编写源代码的自然语言描述的任务。神经代码摘要是指使用神经网络生成这些描述的自动化技术。几乎所有当前的方法都涉及神经网络作为独立模型或作为预训练的大型语言模型的一部分，例如GPT、Codex、LLaMA。然而，几乎所有的网络优化都使用了分类交叉熵（CCE）损失函数。CCE的两个问题是：1）它一次一个地计算每个单词预测的损失，而不是评估整个句子；2）它需要完美的预测，不给同义词留下部分信用的空间。我们提出并评估了一个损失函数来缓解这个问题。本质上，我们建议使用语义相似性度量来计算每个训练批次的整个输出句子预测的损失，而不仅仅是每个单词的损失。我们还建议将我们的损失与每个单词的传统CCE相结合，与基线相比，这简化了训练过程。我们在几个基线上评估了我们的方法，并报告了绝大多数情况的改善。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07429v1" target="_blank">2308.07429v1</a>
                              </td>
                              <td>Semantic Similarity Loss for Neural Source Code Summarization</td>
                              <td>Chia-Yi Su</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07429v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07429v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07411v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exploring the Intersection of Large Language Models and Agent-Based Modeling via Prompt Engineering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07411v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07411v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07411v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The final frontier for simulation is the accurate representation of complex, real-world social systems. While agent-based modeling (ABM) seeks to study the behavior and interactions of agents within a larger system, it is unable to faithfully capture the full complexity of human-driven behavior. Large language models (LLMs), like ChatGPT, have emerged as a potential solution to this bottleneck by enabling researchers to explore human-driven interactions in previously unimaginable ways. Our research investigates simulations of human interactions using LLMs. Through prompt engineering, inspired by Park et al. (2023), we present two simulations of believable proxies of human behavior: a two-agent negotiation and a six-agent murder mystery game.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07411v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>模拟的最后一个前沿是精确表示复杂的、真实世界的社会系统。虽然基于代理的建模（ABM）试图研究更大系统中代理的行为和交互，但它无法忠实地捕捉人类驱动行为的全部复杂性。像ChatGPT这样的大型语言模型（LLM）已经成为解决这一瓶颈的潜在方案，使研究人员能够以以前难以想象的方式探索人类驱动的交互。我们的研究调查了使用LLM模拟人类互动的情况。受Park等人（2023）的启发，通过即时工程，我们提出了两种人类行为可信代理的模拟：两名特工的谈判和六名特工的谋杀推理游戏。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07411v1" target="_blank">2308.07411v1</a>
                              </td>
                              <td>Exploring the Intersection of Large Language Models and Agent-Based Modeling via Prompt Engineering</td>
                              <td>Edward Junprung</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07411v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07411v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07407v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Development and Evaluation of Three Chatbots for Postpartum Mood and Anxiety Disorders</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07407v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07407v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07407v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In collaboration with Postpartum Support International (PSI), a non-profit organization dedicated to supporting caregivers with postpartum mood and anxiety disorders, we developed three chatbots to provide context-specific empathetic support to postpartum caregivers, leveraging both rule-based and generative models. We present and evaluate the performance of our chatbots using both machine-based metrics and human-based questionnaires. Overall, our rule-based model achieves the best performance, with outputs that are close to ground truth reference and contain the highest levels of empathy. Human users prefer the rule-based chatbot over the generative chatbot for its context-specific and human-like replies. Our generative chatbot also produced empathetic responses and was described by human users as engaging. However, limitations in the training dataset often result in confusing or nonsensical responses. We conclude by discussing practical benefits of rule-based vs. generative models for supporting individuals with mental health challenges. In light of the recent surge of ChatGPT and BARD, we also discuss the possibilities and pitfalls of large language models for digital mental healthcare.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07407v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们与产后支持国际（PSI）合作，开发了三个聊天机器人，利用基于规则和生成模型，为产后护理人员提供特定情境的移情支持。产后支持国际是一个致力于支持产后情绪和焦虑障碍护理人员的非营利组织。我们使用基于机器的指标和基于人工的问卷来展示和评估聊天机器人的性能。总的来说，我们基于规则的模型实现了最佳性能，其输出接近基本事实参考，并包含最高水平的同理心。与生成聊天机器人相比，人类用户更喜欢基于规则的聊天机器人，因为它具有特定于上下文和类似人类的回复。我们的生成聊天机器人也产生了移情反应，并被人类用户描述为引人入胜。然而，训练数据集的局限性往往会导致令人困惑或无意义的反应。最后，我们讨论了基于规则与生成模型在支持有心理健康挑战的个人方面的实际好处。鉴于最近ChatGPT和BARD的激增，我们还讨论了数字心理健康的大型语言模型的可能性和陷阱。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07407v1" target="_blank">2308.07407v1</a>
                              </td>
                              <td>Development and Evaluation of Three Chatbots for Postpartum Mood and Anxiety Disorders</td>
                              <td>Xuewen Yao</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07407v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07407v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07317v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Platypus: Quick, Cheap, and Powerful Refinement of LLMs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07317v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07317v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07317v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present $\textbf{Platypus}$, a family of fine-tuned and merged Large Language Models (LLMs) that achieves the strongest performance and currently stands at first place in HuggingFace's Open LLM Leaderboard as of the release date of this work. In this work we describe (1) our curated dataset $\textbf{Open-Platypus}$, that is a subset of other open datasets and which $\textit{we release to the public}$ (2) our process of fine-tuning and merging LoRA modules in order to conserve the strong prior of pretrained LLMs, while bringing specific domain knowledge to the surface (3) our efforts in checking for test data leaks and contamination in the training data, which can inform future research. Specifically, the Platypus family achieves strong performance in quantitative LLM metrics across model sizes, topping the global Open LLM leaderboard while using just a fraction of the fine-tuning data and overall compute that are required for other state-of-the-art fine-tuned LLMs. In particular, a 13B Platypus model can be trained on $\textit{a single}$ A100 GPU using 25k questions in 5 hours. This is a testament of the quality of our Open-Platypus dataset, and opens opportunities for more improvements in the field. Project page: https://platypus-llm.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07317v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们展示了$\textbf｛Platypus｝$，这是一个经过微调和合并的大型语言模型（LLM）家族，它实现了最强的性能，截至本作品发布之日，目前在HuggingFace的开放LLM排行榜上排名第一。在这项工作中，我们描述了（1）我们策划的数据集$\textbf｛Open Platypus｝$，它是其他开放数据集的子集，并且$\textit｛我们向公众发布｝$（2）我们微调和合并LoRA模块的过程，以保持预训练LLM的强先验，同时将特定领域的知识浮出水面（3）我们在检查训练数据中的测试数据泄漏和污染方面所做的努力，这可以为未来的研究提供信息。具体而言，Platypus家族在各种模型尺寸的定量LLM指标方面取得了强大的性能，在全球开放LLM排行榜上名列前茅，同时只使用了其他最先进的微调LLM所需的微调数据和整体计算的一小部分。特别是，13B Platypus模型可以在$\textit｛一个｝$A100 GPU上使用25k个问题在5小时内进行训练。这证明了我们Open Platypus数据集的质量，并为该领域的更多改进提供了机会。项目页面：https://platypus-llm.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07317v1" target="_blank">2308.07317v1</a>
                              </td>
                              <td>Platypus: Quick, Cheap, and Powerful Refinement of LLMs</td>
                              <td>Ariel N. Lee</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07317v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07317v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07305v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Neural Authorship Attribution: Stylometric Analysis on Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07305v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07305v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07305v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) such as GPT-4, PaLM, and Llama have significantly propelled the generation of AI-crafted text. With rising concerns about their potential misuse, there is a pressing need for AI-generated-text forensics. Neural authorship attribution is a forensic effort, seeking to trace AI-generated text back to its originating LLM. The LLM landscape can be divided into two primary categories: proprietary and open-source. In this work, we delve into these emerging categories of LLMs, focusing on the nuances of neural authorship attribution. To enrich our understanding, we carry out an empirical analysis of LLM writing signatures, highlighting the contrasts between proprietary and open-source models, and scrutinizing variations within each group. By integrating stylometric features across lexical, syntactic, and structural aspects of language, we explore their potential to yield interpretable results and augment pre-trained language model-based classifiers utilized in neural authorship attribution. Our findings, based on a range of state-of-the-art LLMs, provide empirical insights into neural authorship attribution, paving the way for future investigations aimed at mitigating the threats posed by AI-generated misinformation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07305v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>GPT-4、PaLM和Llama等大型语言模型极大地推动了人工智能文本的生成。随着人们越来越担心它们可能被滥用，迫切需要人工智能生成的文本取证。神经作者归因是一项取证工作，旨在将人工智能生成的文本追溯到其原始LLM。LLM领域可以分为两大类：专有和开源。在这项工作中，我们深入研究了这些新兴的LLM类别，重点关注神经作者归因的细微差别。为了丰富我们的理解，我们对LLM写签名进行了实证分析，强调了专有模型和开源模型之间的对比，并仔细研究了每个群体中的差异。通过整合语言的词汇、句法和结构方面的风格特征，我们探索了它们产生可解释结果的潜力，并增强了用于神经作者归因的预先训练的基于语言模型的分类器。我们的发现基于一系列最先进的LLM，为神经作者归因提供了经验见解，为未来旨在减轻人工智能产生的错误信息带来的威胁的调查铺平了道路。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07305v1" target="_blank">2308.07305v1</a>
                              </td>
                              <td>Neural Authorship Attribution: Stylometric Analysis on Large Language Models</td>
                              <td>Tharindu Kumarage</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07305v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07305v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07286v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07286v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07286v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07286v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07286v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>机器翻译的自动评估是推动机器翻译系统快速迭代发展的关键工具。虽然在估计单个标量质量分数方面已经取得了相当大的进展，但当前的度量缺乏注释单个错误的更详细方案的信息性，例如多维质量度量（MQM）。在本文中，我们提出了AutoMQM，这是一种提示技术，它利用了大型语言模型（LLM）的推理和上下文学习能力，并要求它们识别和分类翻译中的错误，从而填补了这一空白。我们首先通过简单的分数预测提示来评估最近的LLM，如PaLM和PaLM-2，并通过上下文学习和微调来研究标记数据的影响。然后，我们用PaLM-2模型评估了AutoMQM，我们发现，与仅提示分数相比，它提高了性能（对于较大的模型，增益特别大），同时通过与人类注释一致的错误跨度提供了可解释性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07286v1" target="_blank">2308.07286v1</a>
                              </td>
                              <td>The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation</td>
                              <td>Patrick Fernandes</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07286v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07286v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07269v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07269v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07269v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07269v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy issues, which means they are unaware of unseen events or generate text with incorrect facts owing to the outdated/noisy data. To this end, many knowledge editing approaches for LLMs have emerged -- aiming to subtly inject/edit updated knowledge or adjust undesired behavior while minimizing the impact on unrelated inputs. Nevertheless, due to significant differences among various knowledge editing methods and the variations in task setups, there is no standard implementation framework available for the community, which hinders practitioners to apply knowledge editing to applications. To address these issues, we propose EasyEdit, an easy-to-use knowledge editing framework for LLMs. It supports various cutting-edge knowledge editing approaches and can be readily apply to many well-known LLMs such as T5, GPT-J, LlaMA, etc. Empirically, we report the knowledge editing results on LlaMA-2 with EasyEdit, demonstrating that knowledge editing surpasses traditional fine-tuning in terms of reliability and generalization. We have released the source code on GitHub at https://github.com/zjunlp/EasyEdit, along with Google Colab tutorials and comprehensive documentation for beginners to get started. Besides, we present an online system for real-time knowledge editing, and a demo video at http://knowlm.zjukg.cn/easyedit.mp4.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07269v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）通常存在知识截断或谬论问题，这意味着它们不知道看不见的事件，或者由于过时/嘈杂的数据而生成具有错误事实的文本。为此，出现了许多LLM的知识编辑方法——旨在巧妙地注入/编辑更新的知识或调整不期望的行为，同时最大限度地减少对无关输入的影响。然而，由于各种知识编辑方法之间的显著差异和任务设置的差异，社区没有可用的标准实施框架，这阻碍了从业者将知识编辑应用于应用程序。为了解决这些问题，我们提出了EasyEdit，这是一个易于使用的LLM知识编辑框架。它支持各种前沿的知识编辑方法，可以很容易地应用于许多著名的LLM，如T5、GPT-J、LlaMA等。经验上，我们用EasyEdit报告了LlaMA-2上的知识编辑结果，表明知识编辑在可靠性和通用性方面超越了传统的微调。我们已经在GitHub上发布了源代码https://github.com/zjunlp/EasyEdit，以及Google Colab教程和全面的文档，供初学者入门。此外，我们还提供了一个用于实时知识编辑的在线系统，并在http://knowlm.zjukg.cn/easyedit.mp4.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07269v1" target="_blank">2308.07269v1</a>
                              </td>
                              <td>EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models</td>
                              <td>Peng Wang</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07269v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07269v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_06259v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Self-Alignment with Instruction Backtranslation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_06259v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_06259v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_06259v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_06259v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种可扩展的方法，通过用相应的指令自动标记人类书写的文本来建立高质量的指令遵循语言模型。我们的方法名为指令反翻译，从一个在少量种子数据和给定的网络语料库上微调的语言模型开始。种子模型用于构建训练示例，方法是生成网络文档的指令提示（自增强），然后从这些候选者中选择高质量的示例（自管理）。然后使用这些数据来微调更强的模型。在我们的方法的两次迭代中对LLaMa进行微调，产生了一个优于Alpaca排行榜上所有其他基于LLaMa的模型的模型，该模型不依赖于蒸馏数据，证明了高效的自校准。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.06259v2" target="_blank">2308.06259v2</a>
                              </td>
                              <td>Self-Alignment with Instruction Backtranslation</td>
                              <td>Xian Li</td>
                              <td>2023-08-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_06259v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.06259v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07522v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07522v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07522v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07522v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in machine learning and AI, including Generative AI and LLMs, are disrupting technological innovation, product development, and society as a whole. AI's contribution to technology can come from multiple approaches that require access to large training data sets and clear performance evaluation criteria, ranging from pattern recognition and classification to generative models. Yet, AI has contributed less to fundamental science in part because large data sets of high-quality data for scientific practice and model discovery are more difficult to access. Generative AI, in general, and Large Language Models in particular, may represent an opportunity to augment and accelerate the scientific discovery of fundamental deep science with quantitative models. Here we explore and investigate aspects of an AI-driven, automated, closed-loop approach to scientific discovery, including self-driven hypothesis generation and open-ended autonomous exploration of the hypothesis space. Integrating AI-driven automation into the practice of science would mitigate current problems, including the replication of findings, systematic production of data, and ultimately democratisation of the scientific process. Realising these possibilities requires a vision for augmented AI coupled with a diversity of AI approaches able to deal with fundamental aspects of causality analysis and model discovery while enabling unbiased search across the space of putative explanations. These advances hold the promise to unleash AI's potential for searching and discovering the fundamental structure of our world beyond what human scientists have been able to achieve. Such a vision would push the boundaries of new fundamental science rather than automatize current workflows and instead open doors for technological innovation to tackle some of the greatest challenges facing humanity today.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07522v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>机器学习和人工智能的最新进展，包括Generative AI和LLM，正在颠覆技术创新、产品开发和整个社会。人工智能对技术的贡献可以来自多种方法，这些方法需要访问大型训练数据集和明确的性能评估标准，从模式识别和分类到生成模型。然而，人工智能对基础科学的贡献较小，部分原因是用于科学实践和模型发现的高质量数据的大数据集更难访问。一般来说，生成型人工智能，尤其是大型语言模型，可能代表着一个机会，可以通过定量模型来增强和加速基础深层科学的科学发现。在这里，我们探索和研究人工智能驱动的、自动化的、闭环的科学发现方法的各个方面，包括自我驱动的假设生成和对假设空间的开放式自主探索。将人工智能驱动的自动化融入科学实践将缓解当前的问题，包括研究结果的复制、数据的系统化生产，以及最终科学过程的民主化。实现这些可能性需要增强人工智能的愿景，以及能够处理因果关系分析和模型发现的基本方面的多种人工智能方法，同时能够在假定解释的空间中进行公正的搜索。这些进步有望释放人工智能的潜力，探索和发现人类科学家所能实现的世界的基本结构。这样的愿景将突破新基础科学的界限，而不是使当前的工作流程自动化，而是为技术创新打开大门，以应对当今人类面临的一些最大挑战。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07522v2" target="_blank">2307.07522v2</a>
                              </td>
                              <td>The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence</td>
                              <td>Hector Zenil</td>
                              <td>2023-07-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07522v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07522v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07201v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07201v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07201v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07201v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Text evaluation has historically posed significant challenges, often demanding substantial labor and time cost. With the emergence of large language models (LLMs), researchers have explored LLMs' potential as alternatives for human evaluation. While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality. Recognizing that best practices of human evaluation processes often involve multiple human annotators collaborating in the evaluation, we resort to a multi-agent debate framework, moving beyond single-agent prompting strategies. The multi-agent-based approach enables a group of LLMs to synergize with an array of intelligent counterparts, harnessing their distinct capabilities and expertise to enhance efficiency and effectiveness in handling intricate tasks. In this paper, we construct a multi-agent referee team called ChatEval to autonomously discuss and evaluate the quality of generated responses from different models on open-ended questions and traditional natural language generation (NLG) tasks. Our analysis shows that ChatEval transcends mere textual scoring, offering a human-mimicking evaluation process for reliable assessments. Our code is available at https://github.com/chanchimin/ChatEval.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07201v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文本评估在历史上提出了重大挑战，通常需要大量的人力和时间成本。随着大型语言模型（LLM）的出现，研究人员探索了LLM作为人类评估替代品的潜力。虽然这些基于单智能体的方法显示出了前景，但实验结果表明，还需要进一步的进步来弥补其目前的有效性和人类水平的评估质量之间的差距。认识到人类评估过程的最佳实践通常涉及多个人类注释器在评估中的协作，我们求助于多智能体辩论框架，超越了单一智能体的提示策略。基于多代理的方法使一组LLM能够与一系列智能同行协同工作，利用他们独特的能力和专业知识来提高处理复杂任务的效率和有效性。在本文中，我们构建了一个名为ChatEval的多智能体裁判团队，以自主讨论和评估不同模型对开放式问题和传统自然语言生成（NLG）任务生成的回答的质量。我们的分析表明，ChatEval超越了单纯的文本评分，为可靠的评估提供了一个模仿人类的评估过程。我们的代码可在https://github.com/chanchimin/ChatEval.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07201v1" target="_blank">2308.07201v1</a>
                              </td>
                              <td>ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate</td>
                              <td>Chi-Min Chan</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07201v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07201v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07124v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">OctoPack: Instruction Tuning Code Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07124v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07124v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07124v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07124v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>根据指令对大型语言模型（LLM）进行微调可以极大地提高自然语言任务的性能。我们使用代码应用指令调优，利用Git提交的自然结构，将代码更改与人工指令配对。我们编译CommitPack:4TB的Git提交，跨越350种编程语言。我们在16B参数StarCoder模型上，将CommitPack与其他自然和合成代码指令（xP3x、Self Instruction、OASST）进行基准测试，并在HumanEval Python基准测试（46.2%pass@1)。我们进一步介绍了HumanEvalPack，将HumanEval基准扩展到6种语言（Python、JavaScript、Java、Go、C++、Rust）的总共3个编码任务（代码修复、代码解释、代码合成）。我们的模型OctoCoder和OctoGeeX在所有许可模型中实现了HumanEvalPack的最佳性能，证明了CommitPack在推广到更广泛的语言和自然编码任务方面的优势。代码、模型和数据可在https://github.com/bigcode-project/octopack.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07124v1" target="_blank">2308.07124v1</a>
                              </td>
                              <td>OctoPack: Instruction Tuning Code Large Language Models</td>
                              <td>Niklas Muennighoff</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07124v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07124v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07134v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Natural Language is All a Graph Needs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07134v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07134v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07134v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundational model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scalable prompts based on natural language instructions, and use natural language to describe the geometric structure and node features of the graph for instruction tuning an LLMs to perform learning and inference on graphs in a generative manner. Our method exceeds all competitive GNN baselines on ogbn-arxiv, Cora and PubMed datasets, which demonstrates the effectiveness of our method and sheds light on generative language models replacing GNNs as the foundation model for graph machine learning.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07134v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大规模预训练语言模型的出现，如ChatGPT，彻底改变了人工智能的各个研究领域。基于Transformers的大型语言模型（LLM）已逐渐取代CNN和RNN，以统一计算机视觉和自然语言处理领域。与图像、视频或文本等相对独立存在的数据相比，图形是一种包含丰富结构和关系信息的数据类型。同时，自然语言作为最具表现力的媒介之一，擅长描述复杂的结构。然而，将图学习问题纳入生成语言建模框架的现有工作仍然非常有限。随着语言模型的重要性不断提高，探索LLM是否也可以取代GNN作为图的基础模型变得至关重要。在本文中，我们提出了指令微调图语言模型，系统地设计了基于自然语言指令的高度可扩展的提示，并使用自然语言描述了图的几何结构和节点特征，用于指令微调LLM，以生成的方式对图进行学习和推理。我们的方法超过了ogbn-arxiv、Cora和PubMed数据集上所有竞争性的GNN基线，这证明了我们方法的有效性，并揭示了生成语言模型取代GNN作为图机器学习的基础模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07134v1" target="_blank">2308.07134v1</a>
                              </td>
                              <td>Natural Language is All a Graph Needs</td>
                              <td>Ruosong Ye</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07134v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07134v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07120v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Mind your Language (Model): Fact-Checking LLMs and their Role in NLP Research and Practice</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07120v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07120v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07120v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Much of the recent discourse within the NLP research community has been centered around Large Language Models (LLMs), their functionality and potential -- yet not only do we not have a working definition of LLMs, but much of this discourse relies on claims and assumptions that are worth re-examining. This position paper contributes a definition of LLMs, explicates some of the assumptions made regarding their functionality, and outlines the existing evidence for and against them. We conclude with suggestions for research directions and their framing in future work.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07120v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>NLP研究界最近的大部分论述都围绕着大型语言模型（LLM）及其功能和潜力展开——然而，我们不仅没有对LLM的有效定义，而且这些论述的大部分都依赖于值得重新审视的主张和假设。本立场文件提供了LLM的定义，解释了关于其功能的一些假设，并概述了支持和反对LLM的现有证据。最后，我们对研究方向及其在未来工作中的框架提出了建议。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07120v1" target="_blank">2308.07120v1</a>
                              </td>
                              <td>Mind your Language (Model): Fact-Checking LLMs and their Role in NLP Research and Practice</td>
                              <td>Alexandra Sasha Luccioni</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07120v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07120v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01776v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Does Correction Remain A Problem For Large Language Models?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01776v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01776v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01776v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>As large language models, such as GPT, continue to advance the capabilities of natural language processing (NLP), the question arises: does the problem of correction still persist? This paper investigates the role of correction in the context of large language models by conducting two experiments. The first experiment focuses on correction as a standalone task, employing few-shot learning techniques with GPT-like models for error correction. The second experiment explores the notion of correction as a preparatory task for other NLP tasks, examining whether large language models can tolerate and perform adequately on texts containing certain levels of noise or errors. By addressing these experiments, we aim to shed light on the significance of correction in the era of large language models and its implications for various NLP applications.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01776v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着GPT等大型语言模型不断提高自然语言处理（NLP）的能力，问题来了：纠正问题仍然存在吗？本文通过两个实验研究了在大型语言模型的上下文中纠正的作用。第一个实验侧重于将校正作为一项独立任务，使用具有类似GPT模型的少量镜头学习技术进行纠错。第二个实验探讨了校正的概念，将其作为其他NLP任务的准备任务，检验大型语言模型是否能够容忍并在包含一定程度的噪声或错误的文本上充分执行。通过处理这些实验，我们旨在阐明在大型语言模型时代校正的重要性及其对各种NLP应用的影响。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01776v2" target="_blank">2308.01776v2</a>
                              </td>
                              <td>Does Correction Remain A Problem For Large Language Models?</td>
                              <td>Xiaowu Zhang</td>
                              <td>2023-08-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01776v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01776v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01861v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01861v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01861v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01861v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this work, we make the first attempt to evaluate LLMs in a more challenging code generation scenario, i.e. class-level code generation. We first manually construct the first class-level code generation benchmark ClassEval of 100 class-level Python code generation tasks with approximately 500 person-hours. Based on it, we then perform the first study of 11 state-of-the-art LLMs on class-level code generation. Based on our results, we have the following main findings. First, we find that all existing LLMs show much worse performance on class-level code generation compared to on standalone method-level code generation benchmarks like HumanEval; and the method-level coding ability cannot equivalently reflect the class-level coding ability among LLMs. Second, we find that GPT-4 and GPT-3.5 still exhibit dominate superior than other LLMs on class-level code generation, and the second-tier models includes Instruct-Starcoder, Instruct-Codegen, and Wizardcoder with very similar performance. Third, we find that generating the entire class all at once (i.e. holistic generation strategy) is the best generation strategy only for GPT-4 and GPT-3.5, while method-by-method generation (i.e. incremental and compositional) is better strategies for the other models with limited ability of understanding long instructions and utilizing the middle information. Lastly, we find the limited model ability of generating method-dependent code and discuss the frequent error types in generated classes. Our benchmark is available at https://github.com/FudanSELab/ClassEval.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01861v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项工作中，我们首次尝试在更具挑战性的代码生成场景中评估LLM，即类级代码生成。我们首先手动构建了第一个类级代码生成基准ClassEval，该基准包含100个类级Python代码生成任务，约500人时。在此基础上，我们对11种最先进的LLM在类级代码生成方面进行了首次研究。根据我们的研究结果，我们有以下主要发现。首先，我们发现，与HumanEval等独立方法级代码生成基准测试相比，所有现有的LLM在类级代码生成上的性能都要差得多；并且方法级编码能力不能等效地反映LLM之间的类级编码能力。其次，我们发现GPT-4和GPT-3.5在类级代码生成方面仍然表现出优于其他LLM的优势，第二层模型包括性能非常相似的Directive Starcoder、Directive Codegen和Wizardcoder。第三，我们发现，一次生成整个类（即整体生成策略）是仅适用于GPT-4和GPT-3.5的最佳生成策略，而对于理解长指令和利用中间信息能力有限的其他模型，逐方法生成（即增量和组合）是更好的策略。最后，我们发现了生成方法相关代码的有限模型能力，并讨论了生成类中常见的错误类型。我们的基准可在https://github.com/FudanSELab/ClassEval.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01861v2" target="_blank">2308.01861v2</a>
                              </td>
                              <td>ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation</td>
                              <td>Xueying Du</td>
                              <td>2023-08-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01861v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01861v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_00304v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_00304v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_00304v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_00304v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We consider the problem of eliciting compositional generalization capabilities in large language models (LLMs) with a novel type of prompting strategy. Compositional generalization empowers the LLMs to solve problems that are harder than the ones they have seen (i.e., easy-to-hard generalization), which is a critical reasoning capability of human-like intelligence. However, even the current state-of-the-art LLMs still struggle with this form of reasoning. To bridge this gap, we propose skills-in-context (SKiC) prompting, which instructs LLMs how to compose basic skills to resolve more complex problems. We find that it is crucial to demonstrate both the skills and the compositional examples within the same prompting context. With as few as two examplars, our SKiC prompting initiates strong synergies between skills and their composition capabilities. Notably, it empowers LLMs to solve unseen problems that require innovative skill compositions, achieving near-perfect generalization on a broad range of challenging compositionality tasks. Intriguingly, SKiC prompting unlocks the latent potential of LLMs, enabling them to leverage pre-existing internal skills acquired during earlier pre-training stages, even when these skills are not explicitly presented in the prompting context. This results in the capability of LLMs to solve unseen complex problems by activating and composing internal competencies. With such prominent features, SKiC prompting is able to achieve state-of-the-art performance on challenging mathematical reasoning benchmarks (e.g., MATH).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_00304v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们考虑了用一种新型的提示策略在大型语言模型（LLM）中激发组合泛化能力的问题。组合泛化使LLM能够解决比他们所看到的更难的问题（即，从容易到难的泛化），这是类人智能的关键推理能力。然而，即使是目前最先进的LLM仍然难以进行这种形式的推理。为了弥补这一差距，我们提出了上下文技能（SKiC）提示，指导LLM如何组成基本技能来解决更复杂的问题。我们发现，在相同的提示上下文中展示技巧和构图示例是至关重要的。只有两个例子，我们的SKiC提示启动了技能与其合成能力之间的强大协同作用。值得注意的是，它使LLM能够解决需要创新技能组成的看不见的问题，在一系列具有挑战性的组成任务上实现近乎完美的概括。有趣的是，SKiC提示释放了LLM的潜在潜力，使他们能够利用在早期培训前阶段获得的预先存在的内部技能，即使这些技能没有在提示上下文中明确呈现。这导致LLM能够通过激活和组合内部能力来解决看不见的复杂问题。凭借这些突出的功能，SKiC提示能够在具有挑战性的数学推理基准（例如MATH）上实现最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.00304v2" target="_blank">2308.00304v2</a>
                              </td>
                              <td>Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models</td>
                              <td>Jiaao Chen</td>
                              <td>2023-08-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_00304v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.00304v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_16680v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_16680v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_16680v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_16680v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Diffusion models and large language models have emerged as leading-edge generative models and have sparked a revolutionary impact on various aspects of human life. However, the practical implementation of these models has also exposed inherent risks, highlighting their dual nature and raising concerns regarding their trustworthiness. Despite the abundance of literature on this subject, a comprehensive survey specifically delving into the intersection of large-scale generative models and their trustworthiness remains largely absent. To bridge this gap, This paper investigates both the long-standing and emerging threats associated with these models across four fundamental dimensions: privacy, security, fairness, and responsibility. In this way, we construct an extensive map outlining the trustworthiness of these models, while also providing practical recommendations and identifying future directions. These efforts are crucial for promoting the trustworthy deployment of these models, ultimately benefiting society as a whole.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_16680v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>扩散模型和大语言模型已经成为前沿的生成模型，并对人类生活的各个方面产生了革命性的影响。然而，这些模式的实际实施也暴露出固有的风险，突出了其双重性，并引发了人们对其可信度的担忧。尽管有大量关于这一主题的文献，但一项专门研究大规模生成模型及其可信度的交叉点的全面调查在很大程度上仍然缺乏。为了弥补这一差距，本文从隐私、安全、公平和责任四个基本维度调查了与这些模型相关的长期和新出现的威胁。通过这种方式，我们构建了一个广泛的地图，概述了这些模型的可信度，同时也提供了实用的建议和确定了未来的方向。这些努力对于促进这些模式的可靠部署至关重要，最终使整个社会受益。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.16680v3" target="_blank">2307.16680v3</a>
                              </td>
                              <td>On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey</td>
                              <td>Mingyuan Fan</td>
                              <td>2023-07-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_16680v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.16680v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_06111v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Improving Zero-Shot Text Matching for Financial Auditing with Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_06111v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_06111v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_06111v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Auditing financial documents is a very tedious and time-consuming process. As of today, it can already be simplified by employing AI-based solutions to recommend relevant text passages from a report for each legal requirement of rigorous accounting standards. However, these methods need to be fine-tuned regularly, and they require abundant annotated data, which is often lacking in industrial environments. Hence, we present ZeroShotALI, a novel recommender system that leverages a state-of-the-art large language model (LLM) in conjunction with a domain-specifically optimized transformer-based text-matching solution. We find that a two-step approach of first retrieving a number of best matching document sections per legal requirement with a custom BERT-based model and second filtering these selections using an LLM yields significant performance improvements over existing approaches.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_06111v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>审计财务文件是一个非常乏味和耗时的过程。截至目前，它已经可以通过使用基于人工智能的解决方案来简化，为严格会计准则的每一项法律要求推荐报告中的相关文本段落。然而，这些方法需要定期进行微调，并且需要大量的注释数据，而这在工业环境中往往是缺乏的。因此，我们提出了ZeroShotALI，这是一种新颖的推荐系统，它利用了最先进的大型语言模型（LLM）和专门优化的基于领域的转换器的文本匹配解决方案。我们发现，一种分两步的方法，即首先使用基于自定义BERT的模型根据法律要求检索多个最佳匹配的文档部分，然后使用LLM对这些选择进行过滤，与现有方法相比，可以显著提高性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.06111v2" target="_blank">2308.06111v2</a>
                              </td>
                              <td>Improving Zero-Shot Text Matching for Financial Auditing with Large Language Models</td>
                              <td>Lars Hillebrand</td>
                              <td>2023-08-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_06111v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.06111v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_06966v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">EcomGPT: Instruction-tuning Large Language Model with Chain-of-Task Tasks for E-commerce</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_06966v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_06966v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_06966v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, instruction-following Large Language Models (LLMs) , represented by ChatGPT, have exhibited exceptional performance in general Natural Language Processing (NLP) tasks. However, the unique characteristics of E-commerce data pose significant challenges to general LLMs. An LLM tailored specifically for E-commerce scenarios, possessing robust cross-dataset/task generalization capabilities, is a pressing necessity. To solve this issue, in this work, we proposed the first e-commerce instruction dataset EcomInstruct, with a total of 2.5 million instruction data. EcomInstruct scales up the data size and task diversity by constructing atomic tasks with E-commerce basic data types, such as product information, user reviews. Atomic tasks are defined as intermediate tasks implicitly involved in solving a final task, which we also call Chain-of-Task tasks. We developed EcomGPT with different parameter scales by training the backbone model BLOOMZ with the EcomInstruct. Benefiting from the fundamental semantic understanding capabilities acquired from the Chain-of-Task tasks, EcomGPT exhibits excellent zero-shot generalization capabilities. Extensive experiments and human evaluations demonstrate that EcomGPT outperforms ChatGPT in term of cross-dataset/task generalization on E-commerce tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_06966v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，以ChatGPT为代表的指令跟随大型语言模型（LLM）在一般的自然语言处理（NLP）任务中表现出了非凡的性能。然而，电子商务数据的独特特征对一般LLM提出了重大挑战。迫切需要一种专门为电子商务场景量身定制的LLM，它具有强大的跨数据集/任务泛化能力。为了解决这个问题，在这项工作中，我们提出了第一个电子商务指令数据集EcomInstruction，共有250万条指令数据。EcomInstruction通过构建具有电子商务基本数据类型（如产品信息、用户评论）的原子任务，扩大了数据规模和任务多样性。原子任务被定义为隐含地涉及解决最终任务的中间任务，我们也称之为任务链任务。我们通过使用EcomInstruction训练骨干模型BLOOMZ，开发了具有不同参数尺度的EcomGPT。得益于从任务链任务中获得的基本语义理解能力，EcomGPT表现出出色的零样本泛化能力。大量的实验和人类评估表明，EcomGPT在电子商务任务的跨数据集/任务泛化方面优于ChatGPT。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.06966v1" target="_blank">2308.06966v1</a>
                              </td>
                              <td>EcomGPT: Instruction-tuning Large Language Model with Chain-of-Task Tasks for E-commerce</td>
                              <td>Yangning Li</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_06966v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.06966v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_06942v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Approximating Human-Like Few-shot Learning with GPT-based Compression</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_06942v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_06942v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_06942v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this work, we conceptualize the learning process as information compression. We seek to equip generative pre-trained models with human-like learning capabilities that enable data compression during inference. We present a novel approach that utilizes the Generative Pre-trained Transformer (GPT) to approximate Kolmogorov complexity, with the aim of estimating the optimal Information Distance for few-shot learning. We first propose using GPT as a prior for lossless text compression, achieving a noteworthy compression ratio. Experiment with LLAMA2-7B backbone achieves a compression ratio of 15.5 on enwik9. We justify the pre-training objective of GPT models by demonstrating its equivalence to the compression length, and, consequently, its ability to approximate the information distance for texts. Leveraging the approximated information distance, our method allows the direct application of GPT models in quantitative text similarity measurements. Experiment results show that our method overall achieves superior performance compared to embedding and prompt baselines on challenging NLP tasks, including semantic similarity, zero and one-shot text classification, and zero-shot text ranking.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_06942v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项工作中，我们将学习过程概念化为信息压缩。我们寻求为生成性预训练模型配备类似人类的学习能力，以便在推理过程中进行数据压缩。我们提出了一种新的方法，该方法利用生成预训练变换器（GPT）来近似Kolmogorov复杂性，目的是估计少镜头学习的最佳信息距离。我们首先提出使用GPT作为无损文本压缩的先验，实现了显著的压缩比。LLAMA2-7B骨干网的实验在enwik9上实现了15.5的压缩比。我们通过证明GPT模型与压缩长度的等价性，以及它近似文本信息距离的能力，来证明GPT的预训练目标是合理的。利用近似的信息距离，我们的方法允许GPT模型在定量文本相似性测量中直接应用。实验结果表明，在具有挑战性的NLP任务中，与嵌入和提示基线相比，我们的方法总体上取得了优异的性能，包括语义相似性、零和一次文本分类以及零样本文本排序。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.06942v1" target="_blank">2308.06942v1</a>
                              </td>
                              <td>Approximating Human-Like Few-shot Learning with GPT-based Compression</td>
                              <td>Cynthia Huang</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_06942v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.06942v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_06932v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DIVAS: An LLM-based End-to-End Framework for SoC Security Analysis and Policy-based Protection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_06932v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_06932v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_06932v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Securing critical assets in a bus-based System-On-Chip (SoC) is imperative to mitigate potential vulnerabilities and prevent unauthorized access, ensuring the integrity, availability, and confidentiality of the system. Ensuring security throughout the SoC design process is a formidable task owing to the inherent intricacies in SoC designs and the dispersion of assets across diverse IPs. Large Language Models (LLMs), exemplified by ChatGPT (OpenAI) and BARD (Google), have showcased remarkable proficiency across various domains, including security vulnerability detection and prevention in SoC designs. In this work, we propose DIVAS, a novel framework that leverages the knowledge base of LLMs to identify security vulnerabilities from user-defined SoC specifications, map them to the relevant Common Weakness Enumerations (CWEs), followed by the generation of equivalent assertions, and employ security measures through enforcement of security policies. The proposed framework is implemented using multiple ChatGPT and BARD models, and their performance was analyzed while generating relevant CWEs from the SoC specifications provided. The experimental results obtained from open-source SoC benchmarks demonstrate the efficacy of our proposed framework.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_06932v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>保护基于总线的片上系统（SoC）中的关键资产对于缓解潜在漏洞和防止未经授权的访问至关重要，从而确保系统的完整性、可用性和机密性。由于SoC设计中固有的复杂性以及资产在不同IP之间的分散性，确保整个SoC设计过程的安全性是一项艰巨的任务。以ChatGPT（OpenAI）和BARD（Google）为例的大型语言模型（LLM）在各个领域都表现出了非凡的熟练程度，包括SoC设计中的安全漏洞检测和预防。在这项工作中，我们提出了DIVAS，这是一个新的框架，它利用LLM的知识库来识别用户定义的SoC规范中的安全漏洞，将它们映射到相关的常见弱点枚举（CWE），然后生成等效断言，并通过执行安全策略来采用安全措施。所提出的框架使用多个ChatGPT和BARD模型实现，并在根据所提供的SoC规范生成相关CWE的同时分析了它们的性能。从开源SoC基准测试中获得的实验结果证明了我们提出的框架的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.06932v1" target="_blank">2308.06932v1</a>
                              </td>
                              <td>DIVAS: An LLM-based End-to-End Framework for SoC Security Analysis and Policy-based Protection</td>
                              <td>Sudipta Paria</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_06932v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.06932v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_06921v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CodeHelp: Using Large Language Models with Guardrails for Scalable Support in Programming Classes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_06921v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_06921v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_06921v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Computing educators face significant challenges in providing timely support to students, especially in large class settings. Large language models (LLMs) have emerged recently and show great promise for providing on-demand help at a large scale, but there are concerns that students may over-rely on the outputs produced by these models. In this paper, we introduce CodeHelp, a novel LLM-powered tool designed with guardrails to provide on-demand assistance to programming students without directly revealing solutions. We detail the design of the tool, which incorporates a number of useful features for instructors, and elaborate on the pipeline of prompting strategies we use to ensure generated outputs are suitable for students. To evaluate CodeHelp, we deployed it in a first-year computer and data science course with 52 students and collected student interactions over a 12-week period. We examine students' usage patterns and perceptions of the tool, and we report reflections from the course instructor and a series of recommendations for classroom use. Our findings suggest that CodeHelp is well-received by students who especially value its availability and help with resolving errors, and that for instructors it is easy to deploy and complements, rather than replaces, the support that they provide to students.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_06921v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>计算机教育工作者在为学生提供及时支持方面面临重大挑战，尤其是在大班环境中。大型语言模型（LLM）最近出现，在大规模提供按需帮助方面显示出巨大的前景，但也有人担心学生可能过度依赖这些模型产生的输出。在本文中，我们介绍了CodeHelp，这是一种新型的LLM驱动工具，设计有护栏，为编程学生提供按需帮助，而无需直接透露解决方案。我们详细介绍了该工具的设计，其中包含了许多对教师有用的功能，并详细说明了我们用于确保生成的输出适合学生的提示策略。为了评估CodeHelp，我们将其部署在一年级的计算机和数据科学课程中，共有52名学生，并收集了12周的学生互动。我们研究了学生对该工具的使用模式和看法，并报告了课程讲师的反思和一系列课堂使用建议。我们的研究结果表明，CodeHelp深受学生的欢迎，他们特别重视它的可用性和帮助解决错误，对于讲师来说，它很容易部署和补充，而不是取代他们为学生提供的支持。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.06921v1" target="_blank">2308.06921v1</a>
                              </td>
                              <td>CodeHelp: Using Large Language Models with Guardrails for Scalable Support in Programming Classes</td>
                              <td>Mark Liffiton</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_06921v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.06921v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_06911v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_06911v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_06911v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_06911v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models have made significant strides in natural language processing, paving the way for innovative applications including molecular representation and generation. However, most existing single-modality approaches cannot capture the abundant and complex information in molecular data. Here, we introduce GIT-Mol, a multi-modal large language model that integrates the structure Graph, Image, and Text information, including the Simplified Molecular Input Line Entry System (SMILES) and molecular captions. To facilitate the integration of multi-modal molecular data, we propose GIT-Former, a novel architecture capable of mapping all modalities into a unified latent space. Our study develops an innovative any-to-language molecular translation strategy and achieves a 10%-15% improvement in molecular captioning, a 5%-10% accuracy increase in property prediction, and a 20% boost in molecule generation validity compared to baseline or single-modality models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_06911v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型在自然语言处理方面取得了重大进展，为包括分子表示和生成在内的创新应用铺平了道路。然而，现有的大多数单模态方法无法捕捉分子数据中丰富而复杂的信息。在这里，我们介绍了GIT-Mol，这是一个多模态的大型语言模型，它集成了结构图、图像和文本信息，包括简化分子输入行输入系统（SMILES）和分子字幕。为了促进多模态分子数据的集成，我们提出了GIT-Former，这是一种能够将所有模态映射到统一潜在空间的新架构。我们的研究开发了一种创新的任意语言分子翻译策略，与基线或单模态模型相比，分子字幕提高了10%-15%，属性预测准确率提高了5%-10%，分子生成有效性提高了20%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>46</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.06911v1" target="_blank">2308.06911v1</a>
                              </td>
                              <td>GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text</td>
                              <td>Pengfei Liu</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_06911v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.06911v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_03549v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_03549v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_03549v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_03549v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in Large Language Models (LLMs) have achieved remarkable breakthroughs in understanding and responding to user intents. However, their performance lag behind general use cases in some expertise domains, such as Chinese medicine. Existing efforts to incorporate Chinese medicine into LLMs rely on Supervised Fine-Tuning (SFT) with single-turn and distilled dialogue data. These models lack the ability for doctor-like proactive inquiry and multi-turn comprehension and cannot always align responses with safety and professionalism experts. In this work, we introduce Zhongjing, the first Chinese medical LLaMA-based LLM that implements an entire training pipeline from pre-training to reinforcement learning with human feedback (RLHF). Additionally, we introduce a Chinese multi-turn medical dialogue dataset of 70,000 authentic doctor-patient dialogues, CMtMedQA, which significantly enhances the model's capability for complex dialogue and proactive inquiry initiation. We define a refined annotation rule and evaluation criteria given the biomedical domain's unique characteristics. Results show that our model outperforms baselines in various capacities and matches the performance of ChatGPT in a few abilities, despite having 50x training data with previous best model and 100x parameters with ChatGPT. RLHF further improves the model's instruction-following ability and safety.We also release our code, datasets and model for further research.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_03549v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）的最新进展在理解和响应用户意图方面取得了显著突破。然而，它们在某些专业领域（如中医）的性能落后于一般用例。将中药纳入LLM的现有努力依赖于具有单圈和提取对话数据的监督微调（SFT）。这些模型缺乏像医生一样的主动询问和多角度理解的能力，并且不能始终与安全和专业专家保持一致。在这项工作中，我们介绍了中静，这是第一个基于LLaMA的中国医学LLM，它实现了从预训练到人类反馈强化学习（RLHF）的整个训练管道。此外，我们还引入了一个包含70000个真实医患对话的中文多回合医学对话数据集CMtMedQA，它显著增强了该模型进行复杂对话和主动询问的能力。鉴于生物医学领域的独特特征，我们定义了一个精细的注释规则和评估标准。结果表明，尽管我们的模型具有50倍于以前最佳模型的训练数据和100倍于ChatGPT的参数，但我们的模型在各种能力上都优于基线，并在一些能力上与ChatGPT相匹配。RLHF进一步提高了模型的指令遵循能力和安全性。我们还发布了我们的代码、数据集和模型以供进一步研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>47</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.03549v2" target="_blank">2308.03549v2</a>
                              </td>
                              <td>Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue</td>
                              <td>Songhua Yang</td>
                              <td>2023-08-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_03549v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.03549v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_06907v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Generative Interpretation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_06907v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_06907v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_06907v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce generative interpretation, a new approach to estimating contractual meaning using large language models. As AI triumphalism is the order of the day, we proceed by way of grounded case studies, each illustrating the capabilities of these novel tools in distinct ways. Taking well-known contracts opinions, and sourcing the actual agreements that they adjudicated, we show that AI models can help factfinders ascertain ordinary meaning in context, quantify ambiguity, and fill gaps in parties' agreements. We also illustrate how models can calculate the probative value of individual pieces of extrinsic evidence. After offering best practices for the use of these models given their limitations, we consider their implications for judicial practice and contract theory. Using LLMs permits courts to estimate what the parties intended cheaply and accurately, and as such generative interpretation unsettles the current interpretative stalemate. Their use responds to efficiency-minded textualists and justice-oriented contextualists, who argue about whether parties will prefer cost and certainty or accuracy and fairness. Parties--and courts--would prefer a middle path, in which adjudicators strive to predict what the contract really meant, admitting just enough context to approximate reality while avoiding unguided and biased assimilation of evidence. As generative interpretation offers this possibility, we argue it can become the new workhorse of contractual interpretation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_06907v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了生成解释，这是一种使用大型语言模型来估计合同含义的新方法。由于人工智能的必胜主义是大势所趋，我们通过有根据的案例研究进行，每一个案例都以不同的方式说明了这些新工具的能力。根据众所周知的合同意见，并获取他们裁决的实际协议，我们表明，人工智能模型可以帮助事实调查者确定上下文中的普通含义，量化歧义，并填补各方协议中的空白。我们还说明了模型如何计算个别外在证据的证明价值。鉴于这些模型的局限性，在提供了使用这些模型的最佳实践后，我们考虑了它们对司法实践和合同理论的影响。使用LLM可以让法院廉价而准确地估计当事方的意图，因此生成性解释扰乱了目前的解释僵局。它们的使用回应了注重效率的文本主义者和注重正义的语境主义者，他们争论各方是更喜欢成本和确定性，还是更喜欢准确性和公平。当事人和法院更喜欢一条中间道路，在这条道路上，裁决者努力预测合同的真正含义，承认足够的背景来接近现实，同时避免对证据的无指导和有偏见的同化。由于生成性解释提供了这种可能性，我们认为它可以成为合同解释的新主力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>48</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.06907v1" target="_blank">2308.06907v1</a>
                              </td>
                              <td>Generative Interpretation</td>
                              <td>Yonathan A. Arbel</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_06907v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.06907v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_06850v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">S3C2 Summit 2023-06: Government Secure Supply Chain Summit</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_06850v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_06850v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_06850v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent years have shown increased cyber attacks targeting less secure elements in the software supply chain and causing fatal damage to businesses and organizations. Past well-known examples of software supply chain attacks are the SolarWinds or log4j incidents that have affected thousands of customers and businesses. The US government and industry are equally interested in enhancing software supply chain security. On June 7, 2023, researchers from the NSF-supported Secure Software Supply Chain Center (S3C2) conducted a Secure Software Supply Chain Summit with a diverse set of 17 practitioners from 13 government agencies. The goal of the Summit was two-fold: (1) to share our observations from our previous two summits with industry, and (2) to enable sharing between individuals at the government agencies regarding practical experiences and challenges with software supply chain security. For each discussion topic, we presented our observations and take-aways from the industry summits to spur conversation. We specifically focused on the Executive Order 14028, software bill of materials (SBOMs), choosing new dependencies, provenance and self-attestation, and large language models. The open discussions enabled mutual sharing and shed light on common challenges that government agencies see as impacting government and industry practitioners when securing their software supply chain. In this paper, we provide a summary of the Summit.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_06850v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，针对软件供应链中不太安全的元素的网络攻击有所增加，并对企业和组织造成致命损害。过去众所周知的软件供应链攻击例子是影响了数千名客户和企业的SolarWinds或log4j事件。美国政府和业界对加强软件供应链安全同样感兴趣。2023年6月7日，美国国家科学基金会支持的安全软件供应链中心（S3C2）的研究人员与来自13个政府机构的17名从业者举行了一次安全软件供应链路峰会。峰会的目标有两个：（1）与业界分享我们前两次峰会的意见；（2）使政府机构的个人能够分享软件供应链安全方面的实际经验和挑战。对于每个讨论主题，我们都会介绍我们的观察结果和行业峰会的要点，以激发对话。我们特别关注14028号行政命令、软件材料清单（SBOM）、选择新的依赖项、出处和自我证明以及大型语言模型。公开讨论实现了相互共享，并揭示了政府机构在保护软件供应链时认为会影响政府和行业从业者的共同挑战。在本文件中，我们提供了首脑会议的摘要。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>49</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.06850v1" target="_blank">2308.06850v1</a>
                              </td>
                              <td>S3C2 Summit 2023-06: Government Secure Supply Chain Summit</td>
                              <td>William Enck</td>
                              <td>2023-08-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_06850v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.06850v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="SAM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2308_07815v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ImbSAM: A Closer Look at Sharpness-Aware Minimization in Class-Imbalanced Recognition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07815v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07815v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07815v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Class imbalance is a common challenge in real-world recognition tasks, where the majority of classes have few samples, also known as tail classes. We address this challenge with the perspective of generalization and empirically find that the promising Sharpness-Aware Minimization (SAM) fails to address generalization issues under the class-imbalanced setting. Through investigating this specific type of task, we identify that its generalization bottleneck primarily lies in the severe overfitting for tail classes with limited training data. To overcome this bottleneck, we leverage class priors to restrict the generalization scope of the class-agnostic SAM and propose a class-aware smoothness optimization algorithm named Imbalanced-SAM (ImbSAM). With the guidance of class priors, our ImbSAM specifically improves generalization targeting tail classes. We also verify the efficacy of ImbSAM on two prototypical applications of class-imbalanced recognition: long-tailed classification and semi-supervised anomaly detection, where our ImbSAM demonstrates remarkable performance improvements for tail classes and anomaly. Our code implementation is available at https://github.com/cool-xuan/Imbalanced_SAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07815v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>类不平衡是现实世界识别任务中的一个常见挑战，在现实世界中，大多数类的样本很少，也称为尾类。我们从泛化的角度来应对这一挑战，并从经验上发现，有希望的清晰度感知最小化（SAM）未能解决类不平衡设置下的泛化问题。通过研究这一特定类型的任务，我们发现其泛化瓶颈主要在于训练数据有限的尾部类的严重过拟合。为了克服这一瓶颈，我们利用类先验来限制类不可知SAM的泛化范围，并提出了一种称为不平衡SAM（ImbSAM）的类感知平滑优化算法。在类先验的指导下，我们的ImbSAM专门改进了针对尾部类的泛化。我们还验证了ImbSAM在类不平衡识别的两个典型应用上的有效性：长尾分类和半监督异常检测，其中我们的ImbSAM证明了尾部类和异常的显著性能改进。我们的代码实现可在https://github.com/cool-xuan/Imbalanced_SAM.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07815v1" target="_blank">2308.07815v1</a>
                              </td>
                              <td>ImbSAM: A Closer Look at Sharpness-Aware Minimization in Class-Imbalanced Recognition</td>
                              <td>Yixuan Zhou</td>
                              <td>2023-08-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07815v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07815v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07714v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Flashpoints Signal Hidden Inherent Instabilities in Land-Use Planning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07714v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07714v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07714v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Land-use decision-making processes have a long history of producing globally pervasive systemic equity and sustainability concerns. Quantitative, optimization-based planning approaches, e.g. Multi-Objective Land Allocation (MOLA), seemingly open the possibility to improve objectivity and transparency by explicitly evaluating planning priorities by the type, amount, and location of land uses. Here, we show that optimization-based planning approaches with generic planning criteria generate a series of unstable "flashpoints" whereby tiny changes in planning priorities produce large-scale changes in the amount of land use by type. We give quantitative arguments that the flashpoints we uncover in MOLA models are examples of a more general family of instabilities that occur whenever planning accounts for factors that coordinate use on- and between-sites, regardless of whether these planning factors are formulated explicitly or implicitly. We show that instabilities lead to regions of ambiguity in land-use type that we term "gray areas". By directly mapping gray areas between flashpoints, we show that quantitative methods retain utility by reducing combinatorially large spaces of possible land-use patterns to a small, characteristic set that can engage stakeholders to arrive at more efficient and just outcomes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07714v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>长期以来，土地利用决策过程在全球范围内普遍产生系统公平和可持续性问题。定量的、基于优化的规划方法，如多目标土地分配（MOLA），似乎通过明确评估土地用途的类型、数量和位置来提高客观性和透明度。在这里，我们表明，具有通用规划标准的基于优化的规划方法会产生一系列不稳定的“爆发点”，即规划优先级的微小变化会导致按类型划分的土地利用量的大规模变化。我们给出了定量的论点，即我们在MOLA模型中发现的爆发点是一个更普遍的不稳定性家族的例子，每当规划考虑到协调场地上和场地之间使用的因素时，就会发生这种不稳定性，无论这些规划因素是明确制定的还是隐含制定的。我们表明，不稳定性会导致土地使用类型的模糊区域，我们称之为“灰色地带”。通过直接绘制爆发点之间的灰色区域，我们表明，定量方法通过将可能的土地利用模式的组合大空间减少到一个小的、有特征的集合来保持效用，该集合可以让利益相关者参与进来，以获得更高效、更公正的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07714v1" target="_blank">2308.07714v1</a>
                              </td>
                              <td>Flashpoints Signal Hidden Inherent Instabilities in Land-Use Planning</td>
                              <td>Hazhir Aliahmadi</td>
                              <td>2023-08-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07714v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07714v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07624v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Self-Prompting Large Vision Models for Few-Shot Medical Image Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07624v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07624v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07624v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advancements in large foundation models have shown promising potential in the medical industry due to their flexible prompting capability. One such model, the Segment Anything Model (SAM), a prompt-driven segmentation model, has shown remarkable performance improvements, surpassing state-of-the-art approaches in medical image segmentation. However, existing methods primarily rely on tuning strategies that require extensive data or prior prompts tailored to the specific task, making it particularly challenging when only a limited number of data samples are available. In this paper, we propose a novel perspective on self-prompting in medical vision applications. Specifically, we harness the embedding space of SAM to prompt itself through a simple yet effective linear pixel-wise classifier. By preserving the encoding capabilities of the large model, the contextual information from its decoder, and leveraging its interactive promptability, we achieve competitive results on multiple datasets (i.e. improvement of more than 15% compared to fine-tuning the mask decoder using a few images).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07624v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型基础模型的最新进展由于其灵活的提示能力，在医疗行业显示出了巨大的潜力。一种这样的模型，Segment Anything model（SAM），一种即时驱动的分割模型，已经显示出显著的性能改进，超过了医学图像分割中最先进的方法。然而，现有的方法主要依赖于调整策略，这些策略需要大量的数据或针对特定任务定制的预先提示，这使得在只有有限数量的数据样本可用的情况下特别具有挑战性。在这篇论文中，我们提出了一个新的视角来看待医学视觉应用中的自我提示。具体来说，我们利用SAM的嵌入空间，通过一个简单而有效的线性像素分类器来提示自己。通过保留大模型的编码能力、解码器的上下文信息，并利用其交互式提示性，我们在多个数据集上实现了有竞争力的结果（即，与使用少数图像微调掩码解码器相比，改进了15%以上）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07624v1" target="_blank">2308.07624v1</a>
                              </td>
                              <td>Self-Prompting Large Vision Models for Few-Shot Medical Image Segmentation</td>
                              <td>Qi Wu</td>
                              <td>2023-08-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07624v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07624v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_16586v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAMFlow: Eliminating Any Fragmentation in Optical Flow with Segment Anything Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_16586v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_16586v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_16586v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Optical Flow Estimation aims to find the 2D dense motion field between two frames. Due to the limitation of model structures and training datasets, existing methods often rely too much on local clues and ignore the integrity of objects, resulting in fragmented motion estimation. Through theoretical analysis, we find the pre-trained large vision models are helpful in optical flow estimation, and we notice that the recently famous Segment Anything Model (SAM) demonstrates a strong ability to segment complete objects, which is suitable for solving the fragmentation problem. We thus propose a solution to embed the frozen SAM image encoder into FlowFormer to enhance object perception. To address the challenge of in-depth utilizing SAM in non-segmentation tasks like optical flow estimation, we propose an Optical Flow Task-Specific Adaption scheme, including a Context Fusion Module to fuse the SAM encoder with the optical flow context encoder, and a Context Adaption Module to adapt the SAM features for optical flow task with Learned Task-Specific Embedding. Our proposed SAMFlow model reaches 0.86/2.10 clean/final EPE and 3.55/12.32 EPE/F1-all on Sintel and KITTI-15 training set, surpassing Flowformer by 8.5%/9.9% and 13.2%/16.3%. Furthermore, our model achieves state-of-the-art performance on the Sintel and KITTI-15 benchmarks, ranking #1 among all two-frame methods on Sintel clean pass.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_16586v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>光流估计旨在找到两帧之间的2D密集运动场。由于模型结构和训练数据集的限制，现有的方法往往过于依赖局部线索，忽略了对象的完整性，导致运动估计碎片化。通过理论分析，我们发现预先训练的大视觉模型有助于光流估计，并且我们注意到最近著名的分段任意模型（SAM）表现出强大的分割完整对象的能力，适合于解决碎片问题。因此，我们提出了一种将冻结的SAM图像编码器嵌入FlowFormer中以增强对象感知的解决方案。为了解决在光流估计等非分割任务中深入利用SAM的挑战，我们提出了一种光流任务特定自适应方案，包括将SAM编码器与光流上下文编码器融合的上下文融合模块，以及通过学习任务特定嵌入将SAM特征适配于光流任务的上下文自适应模块。我们提出的SAMFlow模型在Sintel和KITTI-15训练集上达到了0.86/2.10的清洁/最终EPE和3.55/12.32的EPE/F1，分别超过Flowformer 8.5%/9.9%和13.2%/16.3%。此外，我们的模型在Sintl和KITTI-15基准上实现了最先进的性能，在Sintel清洁通道的所有两种框架方法中排名第一。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.16586v2" target="_blank">2307.16586v2</a>
                              </td>
                              <td>SAMFlow: Eliminating Any Fragmentation in Optical Flow with Segment Anything Model</td>
                              <td>Shili Zhou</td>
                              <td>2023-07-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_16586v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.16586v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_06794v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MMF-Track: Multi-modal Multi-level Fusion for 3D Single Object Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_06794v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_06794v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_06794v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>3D single object tracking plays a crucial role in computer vision. Mainstream methods mainly rely on point clouds to achieve geometry matching between target template and search area. However, textureless and incomplete point clouds make it difficult for single-modal trackers to distinguish objects with similar structures. To overcome the limitations of geometry matching, we propose a Multi-modal Multi-level Fusion Tracker (MMF-Track), which exploits the image texture and geometry characteristic of point clouds to track 3D target. Specifically, we first propose a Space Alignment Module (SAM) to align RGB images with point clouds in 3D space, which is the prerequisite for constructing inter-modal associations. Then, in feature interaction level, we design a Feature Interaction Module (FIM) based on dual-stream structure, which enhances intra-modal features in parallel and constructs inter-modal semantic associations. Meanwhile, in order to refine each modal feature, we introduce a Coarse-to-Fine Interaction Module (CFIM) to realize the hierarchical feature interaction at different scales. Finally, in similarity fusion level, we propose a Similarity Fusion Module (SFM) to aggregate geometry and texture clues from the target. Experiments show that our method achieves state-of-the-art performance on KITTI (39% Success and 42% Precision gains against previous multi-modal method) and is also competitive on NuScenes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_06794v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三维单目标跟踪在计算机视觉中起着至关重要的作用。主流的方法主要依靠点云来实现目标模板和搜索区域之间的几何匹配。然而，无纹理和不完整的点云使单模态跟踪器难以区分具有相似结构的对象。为了克服几何匹配的局限性，我们提出了一种多模态多级融合跟踪器（MMF Track），该跟踪器利用点云的图像纹理和几何特征来跟踪三维目标。具体来说，我们首先提出了一个空间对齐模块（SAM）来将RGB图像与3D空间中的点云对齐，这是构建模态间关联的先决条件。然后，在特征交互层面，我们设计了一个基于双流结构的特征交互模块，该模块并行增强模态内特征，构建模态间语义关联。同时，为了细化每个模态特征，我们引入了一个从粗到细的交互模块（CFIM）来实现不同尺度的层次特征交互。最后，在相似性融合层面，我们提出了一个相似性融合模块（SFM）来聚合来自目标的几何和纹理线索。实验表明，我们的方法在KITTI上实现了最先进的性能（与以前的多模态方法相比，成功率为39%，精度提高了42%），在NuScenes上也具有竞争力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.06794v2" target="_blank">2305.06794v2</a>
                              </td>
                              <td>MMF-Track: Multi-modal Multi-level Fusion for 3D Single Object Tracking</td>
                              <td>Zhiheng Li</td>
                              <td>2023-05-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_06794v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.06794v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_1808_00023v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Measure and Mismeasure of Fairness</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_1808_00023v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_1808_00023v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_1808_00023v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The field of fair machine learning aims to ensure that decisions guided by algorithms are equitable. Over the last decade, several formal, mathematical definitions of fairness have gained prominence. Here we first assemble and categorize these definitions into two broad families: (1) those that constrain the effects of decisions on disparities; and (2) those that constrain the effects of legally protected characteristics, like race and gender, on decisions. We then show, analytically and empirically, that both families of definitions typically result in strongly Pareto dominated decision policies. For example, in the case of college admissions, adhering to popular formal conceptions of fairness would simultaneously result in lower student-body diversity and a less academically prepared class, relative to what one could achieve by explicitly tailoring admissions policies to achieve desired outcomes. In this sense, requiring that these fairness definitions hold can, perversely, harm the very groups they were designed to protect. In contrast to axiomatic notions of fairness, we argue that the equitable design of algorithms requires grappling with their context-specific consequences, akin to the equitable design of policy. We conclude by listing several open challenges in fair machine learning and offering strategies to ensure algorithms are better aligned with policy goals.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_1808_00023v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>公平机器学习领域旨在确保由算法指导的决策是公平的。在过去的十年里，公平的几个正式的数学定义变得突出。在这里，我们首先将这些定义集合并分类为两大类：（1）那些限制决策对差异影响的定义；以及（2）限制种族和性别等受法律保护的特征对决策的影响的因素。然后，我们通过分析和实证表明，这两个定义家族通常会导致强烈的帕累托主导决策政策。例如，在大学招生的情况下，与通过明确调整招生政策以实现预期结果所能实现的目标相比，坚持流行的正式公平概念将同时导致学生群体多样性降低，课堂学术准备不足。从这个意义上说，要求这些公平定义成立，可能会反常地伤害到它们旨在保护的群体。与公理化的公平概念相反，我们认为算法的公平设计需要努力解决其特定环境的后果，类似于政策的公平设计。最后，我们列出了公平机器学习中的几个悬而未决的挑战，并提供了确保算法更好地与政策目标相一致的策略。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/1808.00023v3" target="_blank">1808.00023v3</a>
                              </td>
                              <td>The Measure and Mismeasure of Fairness</td>
                              <td>Sam Corbett-Davies</td>
                              <td>2018-07-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_1808_00023v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/1808.00023v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_06974v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A One Stop 3D Target Reconstruction and multilevel Segmentation Method</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_06974v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_06974v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_06974v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>3D object reconstruction and multilevel segmentation are fundamental to computer vision research. Existing algorithms usually perform 3D scene reconstruction and target objects segmentation independently, and the performance is not fully guaranteed due to the challenge of the 3D segmentation. Here we propose an open-source one stop 3D target reconstruction and multilevel segmentation framework (OSTRA), which performs segmentation on 2D images, tracks multiple instances with segmentation labels in the image sequence, and then reconstructs labelled 3D objects or multiple parts with Multi-View Stereo (MVS) or RGBD-based 3D reconstruction methods. We extend object tracking and 3D reconstruction algorithms to support continuous segmentation labels to leverage the advances in the 2D image segmentation, especially the Segment-Anything Model (SAM) which uses the pretrained neural network without additional training for new scenes, for 3D object segmentation. OSTRA supports most popular 3D object models including point cloud, mesh and voxel, and achieves high performance for semantic segmentation, instance segmentation and part segmentation on several 3D datasets. It even surpasses the manual segmentation in scenes with complex structures and occlusions. Our method opens up a new avenue for reconstructing 3D targets embedded with rich multi-scale segmentation information in complex scenes. OSTRA is available from https://github.com/ganlab/OSTRA.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_06974v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三维物体重建和多级分割是计算机视觉研究的基础。现有的算法通常独立地进行三维场景重建和目标对象分割，由于三维分割的挑战，性能不能得到充分保证。在这里，我们提出了一种开源的一站式三维目标重建和多级分割框架（OSTRA），该框架对二维图像进行分割，跟踪图像序列中带有分割标签的多个实例，然后使用基于多视图立体（MVS）或RGBD的三维重建方法重建标记的三维对象或多个部分。我们扩展了对象跟踪和3D重建算法，以支持连续分割标签，从而利用2D图像分割的进步，特别是Segment Anything Model（SAM），它使用预训练的神经网络，而无需对新场景进行额外训练，用于3D对象分割。OSTRA支持最流行的3D对象模型，包括点云、网格和体素，并在多个3D数据集上实现了高性能的语义分割、实例分割和零件分割。在具有复杂结构和遮挡的场景中，它甚至超过了手动分割。我们的方法为在复杂场景中重建嵌入丰富多尺度分割信息的三维目标开辟了一条新的途径。OSTRA可从https://github.com/ganlab/OSTRA.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.06974v1" target="_blank">2308.06974v1</a>
                              </td>
                              <td>A One Stop 3D Target Reconstruction and multilevel Segmentation Method</td>
                              <td>Jiexiong Xu</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_06974v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.06974v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_06929v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Predicting Listing Prices In Dynamic Short Term Rental Markets Using Machine Learning Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_06929v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_06929v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_06929v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Our research group wanted to take on the difficult task of predicting prices in a dynamic market. And short term rentals such as Airbnb listings seemed to be the perfect proving ground to do such a thing. Airbnb has revolutionized the travel industry by providing a platform for homeowners to rent out their properties to travelers. The pricing of Airbnb rentals is prone to high fluctuations, with prices changing frequently based on demand, seasonality, and other factors. Accurate prediction of Airbnb rental prices is crucial for hosts to optimize their revenue and for travelers to make informed booking decisions. In this project, we aim to predict the prices of Airbnb rentals using a machine learning modeling approach.   Our project expands on earlier research in the area of analyzing Airbnb rental prices by taking a methodical machine learning approach as well as incorporating sentiment analysis into our feature engineering. We intend to gain a deeper understanding on periodic changes of Airbnb rental prices. The primary objective of this study is to construct an accurate machine learning model for predicting Airbnb rental prices specifically in Austin, Texas. Our project's secondary objective is to identify the key factors that drive Airbnb rental prices and to investigate how these factors vary across different locations and property types.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_06929v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们的研究小组想承担在充满活力的市场中预测价格的艰巨任务。Airbnb房源等短期租赁似乎是做这件事的完美试验场。Airbnb为房主提供了一个将房产出租给旅行者的平台，从而彻底改变了旅游业。Airbnb租赁的定价容易出现高波动，价格会根据需求、季节性和其他因素频繁变化。准确预测Airbnb租赁价格对于房东优化收入和旅行者做出明智的预订决定至关重要。在这个项目中，我们的目标是使用机器学习建模方法来预测Airbnb租赁的价格。我们的项目通过采用有条不紊的机器学习方法，并将情绪分析纳入我们的功能工程，扩展了Airbnb租赁价格分析领域的早期研究。我们打算更深入地了解Airbnb租赁价格的周期性变化。本研究的主要目的是构建一个准确的机器学习模型，用于预测Airbnb在德克萨斯州奥斯汀的租赁价格。我们项目的次要目标是确定推动Airbnb租赁价格的关键因素，并调查这些因素在不同地点和房产类型之间的差异。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.06929v1" target="_blank">2308.06929v1</a>
                              </td>
                              <td>Predicting Listing Prices In Dynamic Short Term Rental Markets Using Machine Learning Models</td>
                              <td>Sam Chapman</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_06929v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.06929v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_02913v4_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Decentralized SGD and Average-direction SAM are Asymptotically Equivalent</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_02913v4_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_02913v4_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_02913v4_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Decentralized stochastic gradient descent (D-SGD) allows collaborative learning on massive devices simultaneously without the control of a central server. However, existing theories claim that decentralization invariably undermines generalization. In this paper, we challenge the conventional belief and present a completely new perspective for understanding decentralized learning. We prove that D-SGD implicitly minimizes the loss function of an average-direction Sharpness-aware minimization (SAM) algorithm under general non-convex non-$\beta$-smooth settings. This surprising asymptotic equivalence reveals an intrinsic regularization-optimization trade-off and three advantages of decentralization: (1) there exists a free uncertainty evaluation mechanism in D-SGD to improve posterior estimation; (2) D-SGD exhibits a gradient smoothing effect; and (3) the sharpness regularization effect of D-SGD does not decrease as total batch size increases, which justifies the potential generalization benefit of D-SGD over centralized SGD (C-SGD) in large-batch scenarios.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_02913v4_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分散随机梯度下降（D-SGD）允许在没有中央服务器控制的情况下同时在大规模设备上进行协作学习。然而，现有的理论声称，权力下放总是会破坏泛化。在本文中，我们挑战了传统的信念，并为理解去中心化学习提供了一个全新的视角。我们证明了在一般非凸非$\beta$-光滑设置下，D-SGD隐式地最小化了平均方向清晰度感知最小化（SAM）算法的损失函数。这种令人惊讶的渐近等价揭示了一种内在的正则化优化权衡和去中心化的三个优点：（1）D-SGD中存在一种自由的不确定性评估机制来改进后验估计；（2） D-SGD表现出梯度平滑效应；（3）D-SGD的清晰度正则化效应不会随着总批量的增加而降低，这证明了在大批量场景中，D-SGD相对于集中式SGD（C-SGD）的潜在泛化优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.02913v4" target="_blank">2306.02913v4</a>
                              </td>
                              <td>Decentralized SGD and Average-direction SAM are Asymptotically Equivalent</td>
                              <td>Tongtian Zhu</td>
                              <td>2023-06-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_02913v4_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.02913v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_06725v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLE Diffusion: Controllable Light Enhancement Diffusion Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_06725v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_06725v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_06725v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Low light enhancement has gained increasing importance with the rapid development of visual creation and editing. However, most existing enhancement algorithms are designed to homogeneously increase the brightness of images to a pre-defined extent, limiting the user experience. To address this issue, we propose Controllable Light Enhancement Diffusion Model, dubbed CLE Diffusion, a novel diffusion framework to provide users with rich controllability. Built with a conditional diffusion model, we introduce an illumination embedding to let users control their desired brightness level. Additionally, we incorporate the Segment-Anything Model (SAM) to enable user-friendly region controllability, where users can click on objects to specify the regions they wish to enhance. Extensive experiments demonstrate that CLE Diffusion achieves competitive performance regarding quantitative metrics, qualitative results, and versatile controllability. Project page: \url{https://yuyangyin.github.io/CLEDiffusion/}</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_06725v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着视觉创作和编辑的快速发展，微光增强越来越重要。然而，大多数现有的增强算法被设计为将图像的亮度均匀地增加到预定义的程度，从而限制了用户体验。为了解决这个问题，我们提出了可控光增强扩散模型，称为CLE扩散，这是一个新的扩散框架，为用户提供丰富的可控性。使用条件扩散模型构建，我们引入了照明嵌入，让用户控制他们想要的亮度水平。此外，我们结合了分段任何模型（SAM），以实现用户友好的区域可控性，用户可以点击对象来指定他们希望增强的区域。大量实验表明，CLE Diffusion在定量指标、定性结果和通用可控性方面取得了具有竞争力的性能。项目页面：\url{https://yuyangyin.github.io/CLEDiffusion/}</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.06725v1" target="_blank">2308.06725v1</a>
                              </td>
                              <td>CLE Diffusion: Controllable Light Enhancement Diffusion Model</td>
                              <td>Yuyang Yin</td>
                              <td>2023-08-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_06725v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.06725v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_06444v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TongueSAM: An Universal Tongue Segmentation Model Based on SAM with Zero-Shot</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_06444v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_06444v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_06444v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Tongue segmentation serves as the primary step in automated TCM tongue diagnosis, which plays a significant role in the diagnostic results. Currently, numerous deep learning based methods have achieved promising results. However, most of these methods exhibit mediocre performance on tongues different from the training set. To address this issue, this paper proposes a universal tongue segmentation model named TongueSAM based on SAM (Segment Anything Model). SAM is a large-scale pretrained interactive segmentation model known for its powerful zero-shot generalization capability. Applying SAM to tongue segmentation enables the segmentation of various types of tongue images with zero-shot. In this study, a Prompt Generator based on object detection is integrated into SAM to enable an end-to-end automated tongue segmentation method. Experiments demonstrate that TongueSAM achieves exceptional performance across various of tongue segmentation datasets, particularly under zero-shot. TongueSAM can be directly applied to other datasets without fine-tuning. As far as we know, this is the first application of large-scale pretrained model for tongue segmentation. The project and pretrained model of TongueSAM be publiced in :https://github.com/cshan-github/TongueSAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_06444v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>舌片分割是中医舌片自动诊断的首要步骤，对诊断结果起着重要作用。目前，许多基于深度学习的方法已经取得了可喜的成果。然而，这些方法中的大多数在不同于训练集的舌头上表现出平庸的表现。为了解决这一问题，本文提出了一种基于SAM（Segment Anything model）的通用舌头分割模型TongueSAM。SAM是一种大规模的预训练交互式分割模型，以其强大的零样本泛化能力而闻名。将SAM应用于舌头分割使得能够利用零样本对各种类型的舌头图像进行分割。在这项研究中，基于对象检测的提示生成器被集成到SAM中，以实现端到端的自动舌头分割方法。实验表明，TongueSAM在各种舌头分割数据集上都取得了优异的性能，尤其是在零样本下。TongueSAM可以直接应用于其他数据集，而无需进行微调。据我们所知，这是大规模预训练模型在舌头分割中的首次应用。TongueSAM的项目和预训练模型将在以下网站上公布：https://github.com/cshan-github/TongueSAM.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.06444v1" target="_blank">2308.06444v1</a>
                              </td>
                              <td>TongueSAM: An Universal Tongue Segmentation Model Based on SAM with Zero-Shot</td>
                              <td>Shan Cao</td>
                              <td>2023-08-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_06444v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.06444v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_05938v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FoodSAM: Any Food Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_05938v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_05938v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_05938v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we explore the zero-shot capability of the Segment Anything Model (SAM) for food image segmentation. To address the lack of class-specific information in SAM-generated masks, we propose a novel framework, called FoodSAM. This innovative approach integrates the coarse semantic mask with SAM-generated masks to enhance semantic segmentation quality. Besides, we recognize that the ingredients in food can be supposed as independent individuals, which motivated us to perform instance segmentation on food images. Furthermore, FoodSAM extends its zero-shot capability to encompass panoptic segmentation by incorporating an object detector, which renders FoodSAM to effectively capture non-food object information. Drawing inspiration from the recent success of promptable segmentation, we also extend FoodSAM to promptable segmentation, supporting various prompt variants. Consequently, FoodSAM emerges as an all-encompassing solution capable of segmenting food items at multiple levels of granularity. Remarkably, this pioneering framework stands as the first-ever work to achieve instance, panoptic, and promptable segmentation on food images. Extensive experiments demonstrate the feasibility and impressing performance of FoodSAM, validating SAM's potential as a prominent and influential tool within the domain of food image segmentation. We release our code at https://github.com/jamesjg/FoodSAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_05938v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们探索了分割任何事物模型（SAM）在食物图像分割中的零样本能力。为了解决SAM生成的掩码中缺乏类特定信息的问题，我们提出了一个新的框架，称为FoodSAM。这种创新的方法将粗语义掩码与SAM生成的掩码相结合，以提高语义分割质量。此外，我们认识到食物中的成分可以被认为是独立的个体，这促使我们对食物图像进行实例分割。此外，FoodSAM将其零样本功能扩展到包含全景分割，结合了对象检测器，使FoodSAM能够有效地捕捉非食物对象信息。从最近可提示分割的成功中汲取灵感，我们还将FoodSAM扩展到可提示分割，支持各种提示变体。因此，FoodSAM成为一种包罗万象的解决方案，能够在多个粒度级别上分割食品。值得注意的是，这个开创性的框架是第一个在食物图像上实现实例、全景和可提示分割的工作。大量实验证明了FoodSAM的可行性和令人印象深刻的性能，验证了SAM作为食品图像分割领域中一个突出和有影响力的工具的潜力。我们在发布代码https://github.com/jamesjg/FoodSAM.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.05938v1" target="_blank">2308.05938v1</a>
                              </td>
                              <td>FoodSAM: Any Food Segmentation</td>
                              <td>Xing Lan</td>
                              <td>2023-08-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_05938v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.05938v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_05916v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">An Exploration of Mars Colonization with Agent-Based Modeling</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_05916v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_05916v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_05916v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Establishing a human settlement on Mars is an incredibly complex engineering problem. The inhospitable nature of the Martian environment requires any habitat to be largely self-sustaining. Beyond mining a few basic minerals and water, the colonizers will be dependent on Earth resupply and replenishment of necessities via technological means, i.e., splitting Martian water into oxygen for breathing and hydrogen for fuel. Beyond the technical and engineering challenges, future colonists will also face psychological and human behavior challenges. Our goal is to better understand the behavioral and psychological interactions of future Martian colonists through an Agent-Based Modeling (ABM simulation) approach. We seek to identify areas of consideration for planning a colony as well as propose a minimum initial population size required to create a stable colony. Accounting for engineering and technological limitations, we draw on research regarding high performing teams in isolated and high stress environments (ex: submarines, Arctic exploration, ISS, war) to include the 4 basic personality types within the ABM. Interactions between agents with different psychological profiles are modeled at the individual level, while global events such as accidents or delays in Earth resupply affect the colony as a whole. From our multiple simulations and scenarios (up to 28 Earth years), we found that an initial population of 22 was the minimum required to maintain a viable colony size over the long run. We also found that the agreeable personality type was the one more likely to survive. We find, contrary to other literature, that the minimum number of people with all personality types that can lead to a sustainable settlement is in the tens and not hundreds.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_05916v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在火星上建立人类定居点是一个极其复杂的工程问题。火星环境的恶劣性质要求任何栖息地在很大程度上都能自我维持。除了开采一些基本的矿物和水外，殖民者还将依赖地球的补给和通过技术手段补充必需品，即将火星水分解为氧气用于呼吸，氢气用于燃料。除了技术和工程方面的挑战，未来的殖民者还将面临心理和人类行为方面的挑战。我们的目标是通过基于代理的建模（ABM模拟）方法更好地了解未来火星殖民者的行为和心理互动。我们试图确定规划殖民地的考虑范围，并提出创建稳定殖民地所需的最小初始人口规模。考虑到工程和技术的局限性，我们借鉴了关于在孤立和高压力环境中（例如：潜艇、北极探索、国际空间站、战争）的高绩效团队的研究，将ABM中的4种基本人格类型包括在内。具有不同心理特征的特工之间的互动是在个人层面上建模的，而全球事件，如地球补给的事故或延误，会影响整个殖民地。从我们的多种模拟和情景（长达28个地球年）中，我们发现，从长远来看，保持可行的殖民地规模所需的最低初始人口为22人。我们还发现，随和的性格类型更有可能存活下来。与其他文献相反，我们发现，能够实现可持续解决的所有性格类型的人的最低数量是几十人，而不是几百人。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.05916v1" target="_blank">2308.05916v1</a>
                              </td>
                              <td>An Exploration of Mars Colonization with Agent-Based Modeling</td>
                              <td>Edgar Arguello</td>
                              <td>2023-08-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_05916v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.05916v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_05426v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Adaptive Low Rank Adaptation of Segment Anything to Salient Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_05426v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_05426v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_05426v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Foundation models, such as OpenAI's GPT-3 and GPT-4, Meta's LLaMA, and Google's PaLM2, have revolutionized the field of artificial intelligence. A notable paradigm shift has been the advent of the Segment Anything Model (SAM), which has exhibited a remarkable capability to segment real-world objects, trained on 1 billion masks and 11 million images. Although SAM excels in general object segmentation, it lacks the intrinsic ability to detect salient objects, resulting in suboptimal performance in this domain. To address this challenge, we present the Segment Salient Object Model (SSOM), an innovative approach that adaptively fine-tunes SAM for salient object detection by harnessing the low-rank structure inherent in deep learning. Comprehensive qualitative and quantitative evaluations across five challenging RGB benchmark datasets demonstrate the superior performance of our approach, surpassing state-of-the-art methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_05426v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基础模型，如OpenAI的GPT-3和GPT-4、Meta的LLaMA和谷歌的PaLM2，已经彻底改变了人工智能领域。一个显著的范式转变是分段任何事物模型（SAM）的出现，该模型在10亿个面具和1100万张图像上训练，表现出了对真实世界物体进行分段的非凡能力。尽管SAM在一般对象分割方面表现出色，但它缺乏检测显著对象的内在能力，导致该领域的性能不理想。为了应对这一挑战，我们提出了分段显著对象模型（SSOM），这是一种创新的方法，通过利用深度学习中固有的低秩结构，自适应地微调SAM以进行显著对象检测。对五个具有挑战性的RGB基准数据集进行全面的定性和定量评估，证明了我们的方法的卓越性能，超过了最先进的方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.05426v1" target="_blank">2308.05426v1</a>
                              </td>
                              <td>Adaptive Low Rank Adaptation of Segment Anything to Salient Object Detection</td>
                              <td>Ruikai Cui</td>
                              <td>2023-08-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_05426v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.05426v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_04333v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards an AI to Win Ghana's National Science and Maths Quiz</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_04333v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_04333v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_04333v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Can an AI win Ghana's National Science and Maths Quiz (NSMQ)? That is the question we seek to answer in the NSMQ AI project, an open-source project that is building AI to compete live in the NSMQ and win. The NSMQ is an annual live science and mathematics competition for senior secondary school students in Ghana in which 3 teams of 2 students compete by answering questions across biology, chemistry, physics, and math in 5 rounds over 5 progressive stages until a winning team is crowned for that year. The NSMQ is an exciting live quiz competition with interesting technical challenges across speech-to-text, text-to-speech, question-answering, and human-computer interaction. In this ongoing work that began in January 2023, we give an overview of the project, describe each of the teams, progress made thus far, and the next steps toward our planned launch and debut of the AI in October for NSMQ 2023. An AI that conquers this grand challenge can have real-world impact on education such as enabling millions of students across Africa to have one-on-one learning support from this AI.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_04333v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人工智能能赢得加纳国家科学和数学测验吗？这是我们在NSMQ人工智能项目中寻求回答的问题，这是一个开源项目，旨在构建人工智能，以在NSMQ中竞争并获胜。NSMQ是一项面向加纳高中生的年度现场科学和数学比赛，由3支由2名学生组成的队伍在5个渐进阶段的5轮比赛中回答生物学、化学、物理和数学的问题，直到当年获胜。NSMQ是一项激动人心的现场智力竞赛，在语音到文本、文本到语音、问答和人机交互方面都有有趣的技术挑战。在2023年1月开始的这项正在进行的工作中，我们概述了该项目，描述了每个团队，迄今为止取得的进展，以及我们计划在10月为NSMQ 2023推出和首次亮相AI的下一步行动。战胜这一巨大挑战的人工智能可以对教育产生现实世界的影响，例如使非洲数百万学生能够从这种人工智能中获得一对一的学习支持。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.04333v1" target="_blank">2308.04333v1</a>
                              </td>
                              <td>Towards an AI to Win Ghana's National Science and Maths Quiz</td>
                              <td>George Boateng</td>
                              <td>2023-08-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_04333v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.04333v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07873v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Why Does Little Robustness Help? Understanding Adversarial Transferability From Surrogate Training</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07873v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07873v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07873v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Adversarial examples (AEs) for DNNs have been shown to be transferable: AEs that successfully fool white-box surrogate models can also deceive other black-box models with different architectures. Although a bunch of empirical studies have provided guidance on generating highly transferable AEs, many of these findings lack explanations and even lead to inconsistent advice. In this paper, we take a further step towards understanding adversarial transferability, with a particular focus on surrogate aspects. Starting from the intriguing little robustness phenomenon, where models adversarially trained with mildly perturbed adversarial samples can serve as better surrogates, we attribute it to a trade-off between two predominant factors: model smoothness and gradient similarity. Our investigations focus on their joint effects, rather than their separate correlations with transferability. Through a series of theoretical and empirical analyses, we conjecture that the data distribution shift in adversarial training explains the degradation of gradient similarity. Building on these insights, we explore the impacts of data augmentation and gradient regularization on transferability and identify that the trade-off generally exists in the various training mechanisms, thus building a comprehensive blueprint for the regulation mechanism behind transferability. Finally, we provide a general route for constructing better surrogates to boost transferability which optimizes both model smoothness and gradient similarity simultaneously, e.g., the combination of input gradient regularization and sharpness-aware minimization (SAM), validated by extensive experiments. In summary, we call for attention to the united impacts of these two factors for launching effective transfer attacks, rather than optimizing one while ignoring the other, and emphasize the crucial role of manipulating surrogate models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07873v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>DNN的对抗性例子（AE）已被证明是可转移的：成功欺骗白盒代理模型的AE也可以欺骗其他具有不同架构的黑盒模型。尽管一系列实证研究为产生高度可转移的AE提供了指导，但其中许多发现缺乏解释，甚至导致建议不一致。在本文中，我们朝着理解对抗性可转移性迈出了进一步的一步，特别关注代理方面。从有趣的小鲁棒性现象开始，用轻度扰动的对抗性样本进行对抗性训练的模型可以作为更好的替代品，我们将其归因于两个主要因素之间的权衡：模型平滑性和梯度相似性。我们的研究重点是它们的联合效应，而不是它们与可转移性的单独相关性。通过一系列理论和实证分析，我们推测对抗性训练中的数据分布变化解释了梯度相似性的退化。基于这些见解，我们探索了数据扩充和梯度正则化对可转移性的影响，并确定了这种权衡通常存在于各种训练机制中，从而为可转移性背后的调节机制构建了一个全面的蓝图。最后，我们提供了一种构建更好的替代物以提高可转移性的通用途径，该途径同时优化了模型的平滑性和梯度相似性，例如，输入梯度正则化和清晰度感知最小化（SAM）的组合，并通过大量实验进行了验证。总之，我们呼吁关注这两个因素对发起有效转移攻击的联合影响，而不是优化其中一个而忽略另一个，并强调操纵代理模型的关键作用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07873v3" target="_blank">2307.07873v3</a>
                              </td>
                              <td>Why Does Little Robustness Help? Understanding Adversarial Transferability From Surrogate Training</td>
                              <td>Yechao Zhang</td>
                              <td>2023-07-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07873v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07873v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_04218v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AquaSAM: Underwater Image Foreground Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_04218v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_04218v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_04218v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Segment Anything Model (SAM) has revolutionized natural image segmentation, nevertheless, its performance on underwater images is still restricted. This work presents AquaSAM, the first attempt to extend the success of SAM on underwater images with the purpose of creating a versatile method for the segmentation of various underwater targets. To achieve this, we begin by classifying and extracting various labels automatically in SUIM dataset. Subsequently, we develop a straightforward fine-tuning method to adapt SAM to general foreground underwater image segmentation. Through extensive experiments involving eight segmentation tasks like human divers, we demonstrate that AquaSAM outperforms the default SAM model especially at hard tasks like coral reefs. AquaSAM achieves an average Dice Similarity Coefficient (DSC) of 7.13 (%) improvement and an average of 8.27 (%) on mIoU improvement in underwater segmentation tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_04218v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分段任意模型（SAM）已经彻底改变了自然图像分割，然而，它在水下图像上的性能仍然受到限制。这项工作介绍了AquaSAM，这是首次尝试在水下图像上扩展SAM的成功，目的是创建一种用于分割各种水下目标的通用方法。为了实现这一点，我们首先在SUIM数据集中自动分类和提取各种标签。随后，我们开发了一种简单的微调方法，将SAM应用于一般前景水下图像分割。通过涉及人类潜水员等八个分割任务的广泛实验，我们证明AquaSAM优于默认的SAM模型，尤其是在珊瑚礁等困难任务中。AquaSAM在水下分割任务中实现了7.13（%）的平均骰子相似系数（DSC）改进和8.27（%）对mIoU的平均改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.04218v1" target="_blank">2308.04218v1</a>
                              </td>
                              <td>AquaSAM: Underwater Image Foreground Segmentation</td>
                              <td>Muduo Xu</td>
                              <td>2023-08-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_04218v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.04218v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07326v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AI Text-to-Behavior: A Study In Steerability</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07326v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07326v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07326v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The research explores the steerability of Large Language Models (LLMs), particularly OpenAI's ChatGPT iterations. By employing a behavioral psychology framework called OCEAN (Openness, Conscientiousness, Extroversion, Agreeableness, Neuroticism), we quantitatively gauged the model's responsiveness to tailored prompts. When asked to generate text mimicking an extroverted personality, OCEAN scored the language alignment to that behavioral trait. In our analysis, while "openness" presented linguistic ambiguity, "conscientiousness" and "neuroticism" were distinctly evoked in the OCEAN framework, with "extroversion" and "agreeableness" showcasing a notable overlap yet distinct separation from other traits. Our findings underscore GPT's versatility and ability to discern and adapt to nuanced instructions. Furthermore, historical figure simulations highlighted the LLM's capacity to internalize and project instructible personas, precisely replicating their philosophies and dialogic styles. However, the rapid advancements in LLM capabilities and the opaque nature of some training techniques make metric proposals degrade rapidly. Our research emphasizes a quantitative role to describe steerability in LLMs, presenting both its promise and areas for further refinement in aligning its progress to human intentions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07326v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>该研究探索了大型语言模型（LLM）的可操纵性，特别是OpenAI的ChatGPT迭代。通过使用一个名为OCEAN（开放性、认真性、外向性、合意性、神经质）的行为心理学框架，我们定量地衡量了模型对定制提示的反应。当被要求生成模仿外向性格的文本时，OCEAN对该行为特征的语言一致性进行了评分。在我们的分析中，虽然“开放性”表现出语言上的模糊性，但在OCEAN框架中，“尽责性”和“神经质”被明显唤起，“外向”和“宜人性”显示出显著的重叠，但与其他特征明显分离。我们的发现强调了GPT的多功能性以及辨别和适应细微指令的能力。此外，历史人物模拟突出了LLM内化和投射可指导人物角色的能力，精确地复制了他们的哲学和对话风格。然而，LLM能力的快速进步和一些训练技术的不透明性使得度量建议迅速退化。我们的研究强调了描述LLM可操纵性的定量作用，展示了其前景和进一步完善的领域，以使其进展符合人类意图。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07326v1" target="_blank">2308.07326v1</a>
                              </td>
                              <td>AI Text-to-Behavior: A Study In Steerability</td>
                              <td>David Noever</td>
                              <td>2023-08-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07326v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07326v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_03726v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AdaptiveSAM: Towards Efficient Tuning of SAM for Surgical Scene Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_03726v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_03726v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_03726v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segmentation is a fundamental problem in surgical scene analysis using artificial intelligence. However, the inherent data scarcity in this domain makes it challenging to adapt traditional segmentation techniques for this task. To tackle this issue, current research employs pretrained models and finetunes them on the given data. Even so, these require training deep networks with millions of parameters every time new data becomes available. A recently published foundation model, Segment-Anything (SAM), generalizes well to a large variety of natural images, hence tackling this challenge to a reasonable extent. However, SAM does not generalize well to the medical domain as is without utilizing a large amount of compute resources for fine-tuning and using task-specific prompts. Moreover, these prompts are in the form of bounding-boxes or foreground/background points that need to be annotated explicitly for every image, making this solution increasingly tedious with higher data size. In this work, we propose AdaptiveSAM - an adaptive modification of SAM that can adjust to new datasets quickly and efficiently, while enabling text-prompted segmentation. For finetuning AdaptiveSAM, we propose an approach called bias-tuning that requires a significantly smaller number of trainable parameters than SAM (less than 2\%). At the same time, AdaptiveSAM requires negligible expert intervention since it uses free-form text as prompt and can segment the object of interest with just the label name as prompt. Our experiments show that AdaptiveSAM outperforms current state-of-the-art methods on various medical imaging datasets including surgery, ultrasound and X-ray. Code is available at https://github.com/JayParanjape/biastuning</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_03726v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分割是使用人工智能进行手术场景分析的一个基本问题。然而，该领域固有的数据稀缺性使得将传统的分割技术应用于该任务具有挑战性。为了解决这个问题，目前的研究采用了预先训练的模型，并根据给定的数据对其进行微调。即便如此，每次新数据可用时，这些都需要用数百万个参数来训练深度网络。最近发表的一个基础模型Segment Anything（SAM）很好地概括了各种各样的自然图像，因此在合理的程度上解决了这一挑战。然而，在没有利用大量计算资源进行微调和使用特定任务提示的情况下，SAM并不能很好地推广到医学领域。此外，这些提示是边界框或前景/背景点的形式，需要为每个图像明确注释，这使得这种解决方案在数据大小更大的情况下变得越来越乏味。在这项工作中，我们提出了AdaptiveSAM——一种对SAM的自适应修改，可以快速有效地适应新的数据集，同时实现文本提示分割。对于微调AdaptiveSAM，我们提出了一种称为偏差调整的方法，该方法需要比SAM少得多的可训练参数（小于2\%）。同时，AdaptiveSAM需要忽略不计的专家干预，因为它使用自由形式的文本作为提示，并且可以只使用标签名称作为提示来分割感兴趣的对象。我们的实验表明，AdaptiveSAM在包括手术、超声和X射线在内的各种医学成像数据集上优于当前最先进的方法。代码可在https://github.com/JayParanjape/biastuning</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.03726v1" target="_blank">2308.03726v1</a>
                              </td>
                              <td>AdaptiveSAM: Towards Efficient Tuning of SAM for Surgical Scene Segmentation</td>
                              <td>Jay N. Paranjape</td>
                              <td>2023-08-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_03726v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.03726v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_00216v4_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">EMV-LIO: An Efficient Multiple Vision aided LiDAR-Inertial Odometry</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_00216v4_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_00216v4_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_00216v4_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>To deal with the degeneration caused by the incomplete constraints of single sensor, multi-sensor fusion strategies especially in LiDAR-vision-inertial fusion area have attracted much interest from both the industry and the research community in recent years. Considering that a monocular camera is vulnerable to the influence of ambient light from a certain direction and fails, which makes the system degrade into a LiDAR-inertial system, multiple cameras are introduced to expand the visual observation so as to improve the accuracy and robustness of the system. Besides, removing LiDAR's noise via range image, setting condition for nearest neighbor search, and replacing kd-Tree with ikd-Tree are also introduced to enhance the efficiency. Based on the above, we propose an Efficient Multiple vision aided LiDAR-inertial odometry system (EMV-LIO), and evaluate its performance on both open datasets and our custom datasets. Experiments show that the algorithm is helpful to improve the accuracy, robustness and efficiency of the whole system compared with LVI-SAM. Our implementation will be available upon acceptance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_00216v4_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>为了解决单传感器不完全约束引起的退化问题，多传感器融合策略，特别是在激光雷达视觉惯性融合领域，近年来引起了业界和研究界的广泛兴趣。考虑到单目相机容易受到来自某个方向的环境光的影响而发生故障，使系统退化为激光雷达惯性系统，因此引入了多个相机来扩展视觉观察，以提高系统的准确性和鲁棒性。此外，还引入了通过距离图像去除激光雷达噪声、设置最近邻搜索条件以及用ikd-Tree代替kd-Tree来提高效率。在此基础上，我们提出了一种高效的多视觉辅助激光雷达惯性里程计系统（EMV-LIO），并在开放数据集和自定义数据集上评估了其性能。实验表明，与LVI-SAM相比，该算法有助于提高整个系统的精度、鲁棒性和效率。我们的实施将在验收后提供。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.00216v4" target="_blank">2302.00216v4</a>
                              </td>
                              <td>EMV-LIO: An Efficient Multiple Vision aided LiDAR-Inertial Odometry</td>
                              <td>Bingqi Shen</td>
                              <td>2023-02-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_00216v4_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.00216v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_03296v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Studying Large Language Model Generalization with Influence Functions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_03296v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_03296v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_03296v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>When trying to gain better visibility into a machine learning model in order to understand and mitigate the associated risks, a potentially valuable source of evidence is: which training examples most contribute to a given behavior? Influence functions aim to answer a counterfactual: how would the model's parameters (and hence its outputs) change if a given sequence were added to the training set? While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP). We use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions up to LLMs with up to 52 billion parameters. In our experiments, EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster. We investigate two algorithmic techniques to reduce the cost of computing gradients of candidate training sequences: TF-IDF filtering and query batching. We use influence functions to investigate the generalization patterns of LLMs, including the sparsity of the influence patterns, increasing abstraction with scale, math and programming abilities, cross-lingual generalization, and role-playing behavior. Despite many apparently sophisticated forms of generalization, we identify a surprising limitation: influences decay to near-zero when the order of key phrases is flipped. Overall, influence functions give us a powerful new tool for studying the generalization properties of LLMs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_03296v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当试图更好地了解机器学习模型以了解和减轻相关风险时，一个潜在的有价值的证据来源是：哪些训练示例对给定行为的贡献最大？影响函数旨在回答一个反事实：如果将给定序列添加到训练集中，模型的参数（以及其输出）将如何变化？虽然影响函数已经为小型模型产生了见解，但由于计算Hessian向量积（IHVP）的困难，它们很难扩展到大型语言模型（LLM）。我们使用特征值校正的Kronecker因子近似曲率（EK-FAC）近似来缩放具有高达520亿个参数的LLM的影响函数。在我们的实验中，EK-FAC实现了与传统影响函数估计器类似的精度，尽管IHVP的计算速度快了几个数量级。我们研究了两种降低候选训练序列梯度计算成本的算法技术：TF-IDF滤波和查询批处理。我们使用影响函数来研究LLM的泛化模式，包括影响模式的稀疏性、随规模增加的抽象性、数学和编程能力、跨语言泛化以及角色扮演行为。尽管有许多明显复杂的概括形式，但我们发现了一个令人惊讶的局限性：当关键短语的顺序翻转时，影响会衰减到接近零。总之，影响函数为我们研究LLM的泛化性质提供了一个强大的新工具。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.03296v1" target="_blank">2308.03296v1</a>
                              </td>
                              <td>Studying Large Language Model Generalization with Influence Functions</td>
                              <td>Roger Grosse</td>
                              <td>2023-08-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_03296v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.03296v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_03236v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">G-Mix: A Generalized Mixup Learning Framework Towards Flat Minima</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_03236v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_03236v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_03236v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Deep neural networks (DNNs) have demonstrated promising results in various complex tasks. However, current DNNs encounter challenges with over-parameterization, especially when there is limited training data available. To enhance the generalization capability of DNNs, the Mixup technique has gained popularity. Nevertheless, it still produces suboptimal outcomes. Inspired by the successful Sharpness-Aware Minimization (SAM) approach, which establishes a connection between the sharpness of the training loss landscape and model generalization, we propose a new learning framework called Generalized-Mixup, which combines the strengths of Mixup and SAM for training DNN models. The theoretical analysis provided demonstrates how the developed G-Mix framework enhances generalization. Additionally, to further optimize DNN performance with the G-Mix framework, we introduce two novel algorithms: Binary G-Mix and Decomposed G-Mix. These algorithms partition the training data into two subsets based on the sharpness-sensitivity of each example to address the issue of "manifold intrusion" in Mixup. Both theoretical explanations and experimental results reveal that the proposed BG-Mix and DG-Mix algorithms further enhance model generalization across multiple datasets and models, achieving state-of-the-art performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_03236v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>深度神经网络（DNN）已经在各种复杂任务中显示出有希望的结果。然而，当前的DNN遇到了过度参数化的挑战，尤其是当可用的训练数据有限时。为了增强DNN的泛化能力，Mixup技术越来越受欢迎。尽管如此，它仍然产生了次优的结果。受成功的清晰度感知最小化（SAM）方法的启发，该方法在训练损失景观的清晰度和模型泛化之间建立了联系，我们提出了一个新的学习框架，称为广义混合，它结合了混合和SAM的优势来训练DNN模型。所提供的理论分析展示了所开发的G-Mix框架如何增强泛化能力。此外，为了使用G-Mix框架进一步优化DNN性能，我们引入了两种新算法：二进制G-Mix和分解G-Mix。这些算法根据每个例子的清晰度敏感性将训练数据划分为两个子集，以解决Mixup中的“流形入侵”问题。理论解释和实验结果都表明，所提出的BG Mix和DG Mix算法进一步增强了多个数据集和模型的模型泛化能力，实现了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.03236v1" target="_blank">2308.03236v1</a>
                              </td>
                              <td>G-Mix: A Generalized Mixup Learning Framework Towards Flat Minima</td>
                              <td>Xingyu Li</td>
                              <td>2023-08-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_03236v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.03236v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_02776v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Dual Degradation-Inspired Deep Unfolding Network for Low-Light Image Enhancement</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_02776v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_02776v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_02776v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Although low-light image enhancement has achieved great stride based on deep enhancement models, most of them mainly stress on enhancement performance via an elaborated black-box network and rarely explore the physical significance of enhancement models. Towards this issue, we propose a Dual degrAdation-inSpired deep Unfolding network, termed DASUNet, for low-light image enhancement. Specifically, we construct a dual degradation model (DDM) to explicitly simulate the deterioration mechanism of low-light images. It learns two distinct image priors via considering degradation specificity between luminance and chrominance spaces. To make the proposed scheme tractable, we design an alternating optimization solution to solve the proposed DDM. Further, the designed solution is unfolded into a specified deep network, imitating the iteration updating rules, to form DASUNet. Local and long-range information are obtained by prior modeling module (PMM), inheriting the advantages of convolution and Transformer, to enhance the representation capability of dual degradation priors. Additionally, a space aggregation module (SAM) is presented to boost the interaction of two degradation models. Extensive experiments on multiple popular low-light image datasets validate the effectiveness of DASUNet compared to canonical state-of-the-art low-light image enhancement methods. Our source code and pretrained model will be publicly available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_02776v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管微光图像增强在深度增强模型的基础上取得了长足的进步，但大多数模型主要通过精心设计的黑盒网络来强调增强性能，很少探索增强模型的物理意义。针对这个问题，我们提出了一种螺旋深度展开网络中的双降级，称为DASUNet，用于微光图像增强。具体来说，我们构建了一个双重退化模型（DDM）来明确地模拟微光图像的退化机制。它通过考虑亮度和色度空间之间的退化特异性来学习两个不同的图像先验。为了使所提出的方案易于处理，我们设计了一个交替优化解决方案来解决所提出的DDM。此外，将设计的解决方案展开到指定的深度网络中，模仿迭代更新规则，形成DASUNet。先验建模模块（PMM）继承了卷积和Transformer的优点，获得了局部和长程信息，增强了双退化先验的表示能力。此外，还提出了一个空间聚合模块（SAM）来促进两个退化模型的相互作用。在多个流行的微光图像数据集上进行的大量实验验证了DASUNet与经典的最先进的微光图像增强方法相比的有效性。我们的源代码和预训练模型将公开。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.02776v1" target="_blank">2308.02776v1</a>
                              </td>
                              <td>Dual Degradation-Inspired Deep Unfolding Network for Low-Light Image Enhancement</td>
                              <td>Huake Wang</td>
                              <td>2023-08-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_02776v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.02776v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_02356v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">T-UNet: Triplet UNet for Change Detection in High-Resolution Remote Sensing Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_02356v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_02356v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_02356v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Remote sensing image change detection aims to identify the differences between images acquired at different times in the same area. It is widely used in land management, environmental monitoring, disaster assessment and other fields. Currently, most change detection methods are based on Siamese network structure or early fusion structure. Siamese structure focuses on extracting object features at different times but lacks attention to change information, which leads to false alarms and missed detections. Early fusion (EF) structure focuses on extracting features after the fusion of images of different phases but ignores the significance of object features at different times for detecting change details, making it difficult to accurately discern the edges of changed objects. To address these issues and obtain more accurate results, we propose a novel network, Triplet UNet(T-UNet), based on a three-branch encoder, which is capable to simultaneously extract the object features and the change features between the pre- and post-time-phase images through triplet encoder. To effectively interact and fuse the features extracted from the three branches of triplet encoder, we propose a multi-branch spatial-spectral cross-attention module (MBSSCA). In the decoder stage, we introduce the channel attention mechanism (CAM) and spatial attention mechanism (SAM) to fully mine and integrate detailed textures information at the shallow layer and semantic localization information at the deep layer.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_02356v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>遥感图像变化检测旨在识别同一区域内不同时间采集的图像之间的差异。广泛应用于土地管理、环境监测、灾害评估等领域。目前，大多数变化检测方法都是基于暹罗网络结构或早期融合结构。暹罗结构专注于提取不同时间的物体特征，但缺乏对变化信息的关注，这导致误报和漏检。早期融合（EF）结构专注于在不同相位的图像融合后提取特征，但忽略了不同时间的对象特征对检测变化细节的重要性，使得难以准确识别变化对象的边缘。为了解决这些问题并获得更准确的结果，我们提出了一种基于三分支编码器的新型网络Triplet UNet（T-UNet），该网络能够通过三元组编码器同时提取目标特征以及前后相位图像之间的变化特征。为了有效地交互和融合从三元组编码器的三个分支中提取的特征，我们提出了一个多分支空间谱交叉注意模块（MBSCA）。在解码器阶段，我们引入了通道注意力机制（CAM）和空间注意力机制（SAM），以充分挖掘和整合浅层的详细纹理信息和深层的语义定位信息。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.02356v1" target="_blank">2308.02356v1</a>
                              </td>
                              <td>T-UNet: Triplet UNet for Change Detection in High-Resolution Remote Sensing Images</td>
                              <td>Huan Zhong</td>
                              <td>2023-08-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_02356v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.02356v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01727v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Local Large Language Models for Complex Structured Medical Tasks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01727v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01727v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01727v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper introduces an approach that combines the language reasoning capabilities of large language models (LLMs) with the benefits of local training to tackle complex, domain-specific tasks. Specifically, the authors demonstrate their approach by extracting structured condition codes from pathology reports. The proposed approach utilizes local LLMs, which can be fine-tuned to respond to specific generative instructions and provide structured outputs. The authors collected a dataset of over 150k uncurated surgical pathology reports, containing gross descriptions, final diagnoses, and condition codes. They trained different model architectures, including LLaMA, BERT and LongFormer and evaluated their performance. The results show that the LLaMA-based models significantly outperform BERT-style models across all evaluated metrics, even with extremely reduced precision. The LLaMA models performed especially well with large datasets, demonstrating their ability to handle complex, multi-label tasks. Overall, this work presents an effective approach for utilizing LLMs to perform domain-specific tasks using accessible hardware, with potential applications in the medical domain, where complex data extraction and classification are required.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01727v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了一种方法，该方法将大型语言模型（LLM）的语言推理能力与本地训练的优势相结合，以处理复杂的、特定领域的任务。具体来说，作者通过从病理报告中提取结构化的条件代码来展示他们的方法。所提出的方法利用了局部LLM，可以对其进行微调以响应特定的生成指令并提供结构化输出。作者收集了超过15万份未分级手术病理报告的数据集，其中包括总体描述、最终诊断和状况代码。他们训练了不同的模型体系结构，包括LLaMA、BERT和LongFormer，并评估了它们的性能。结果表明，在所有评估指标中，基于LLaMA的模型显著优于BERT风格的模型，即使精度极低。LLaMA模型在大型数据集中表现特别好，证明了它们处理复杂、多标签任务的能力。总的来说，这项工作为利用LLM使用可访问的硬件执行特定领域的任务提供了一种有效的方法，在需要复杂数据提取和分类的医疗领域具有潜在的应用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01727v1" target="_blank">2308.01727v1</a>
                              </td>
                              <td>Local Large Language Models for Complex Structured Medical Tasks</td>
                              <td>V. K. Cody Bumgardner</td>
                              <td>2023-08-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01727v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01727v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_00675v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_00675v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_00675v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_00675v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Today, large language models (LLMs) are taught to use new tools by providing a few demonstrations of the tool's usage. Unfortunately, demonstrations are hard to acquire, and can result in undesirable biased usage if the wrong demonstration is chosen. Even in the rare scenario that demonstrations are readily available, there is no principled selection protocol to determine how many and which ones to provide. As tasks grow more complex, the selection search grows combinatorially and invariably becomes intractable. Our work provides an alternative to demonstrations: tool documentation. We advocate the use of tool documentation, descriptions for the individual tool usage, over demonstrations. We substantiate our claim through three main empirical findings on 6 tasks across both vision and language modalities. First, on existing benchmarks, zero-shot prompts with only tool documentation are sufficient for eliciting proper tool usage, achieving performance on par with few-shot prompts. Second, on a newly collected realistic tool-use dataset with hundreds of available tool APIs, we show that tool documentation is significantly more valuable than demonstrations, with zero-shot documentation significantly outperforming few-shot without documentation. Third, we highlight the benefits of tool documentations by tackling image generation and video tracking using just-released unseen state-of-the-art models as tools. Finally, we highlight the possibility of using tool documentation to automatically enable new applications: by using nothing more than the documentation of GroundingDino, Stable Diffusion, XMem, and SAM, LLMs can re-invent the functionalities of the just-released Grounded-SAM and Track Anything models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_00675v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>如今，大型语言模型（LLM）通过提供一些工具用法的演示来学习使用新工具。不幸的是，演示很难获得，如果选择了错误的演示，可能会导致不希望的有偏见的使用。即使在极少数情况下，演示很容易获得，也没有原则性的选择协议来确定提供多少演示以及提供哪些演示。随着任务变得越来越复杂，选择搜索组合增长，并且总是变得棘手。我们的工作提供了一种替代演示的方法：工具文档。我们提倡使用工具文档，对单个工具使用的描述，而不是演示。我们通过对视觉和语言模式中的6项任务的三个主要实证发现来证实我们的说法。首先，在现有的基准测试中，只有工具文档的零样本提示就足以引发正确的工具使用，实现与很少的零热点提示相当的性能。其次，在一个新收集的具有数百个可用工具API的真实工具使用数据集上，我们表明工具文档比演示更有价值，零样本文档显著优于没有文档的少数快照。第三，我们强调了工具文档的好处，通过使用刚刚发布的未公开的最先进模型作为工具来处理图像生成和视频跟踪。最后，我们强调了使用工具文档自动启用新应用程序的可能性：通过只使用GroundingDino、Stable Diffusion、XMem和SAM的文档，LLM可以重新发明刚刚发布的Grounded SAM和Track Anything模型的功能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.00675v1" target="_blank">2308.00675v1</a>
                              </td>
                              <td>Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models</td>
                              <td>Cheng-Yu Hsieh</td>
                              <td>2023-08-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_00675v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.00675v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="CLIP"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2308_07863v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">StyleDiffusion: Controllable Disentangled Style Transfer via Diffusion Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07863v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07863v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07863v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Content and style (C-S) disentanglement is a fundamental problem and critical challenge of style transfer. Existing approaches based on explicit definitions (e.g., Gram matrix) or implicit learning (e.g., GANs) are neither interpretable nor easy to control, resulting in entangled representations and less satisfying results. In this paper, we propose a new C-S disentangled framework for style transfer without using previous assumptions. The key insight is to explicitly extract the content information and implicitly learn the complementary style information, yielding interpretable and controllable C-S disentanglement and style transfer. A simple yet effective CLIP-based style disentanglement loss coordinated with a style reconstruction prior is introduced to disentangle C-S in the CLIP image space. By further leveraging the powerful style removal and generative ability of diffusion models, our framework achieves superior results than state of the art and flexible C-S disentanglement and trade-off control. Our work provides new insights into the C-S disentanglement in style transfer and demonstrates the potential of diffusion models for learning well-disentangled C-S characteristics.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07863v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>内容与风格（C-S）的解开是风格转换的一个基本问题和关键挑战。基于显式定义（例如，Gram矩阵）或隐式学习（例如，GANs）的现有方法既不可解释，也不易于控制，导致纠缠表示和不太令人满意的结果。在本文中，我们在不使用先前假设的情况下，提出了一种新的风格转移的C-S解纠缠框架。关键的见解是显式地提取内容信息，隐式地学习互补的风格信息，从而产生可解释和可控的C-S解纠缠和风格转移。引入了一种简单而有效的基于CLIP的风格去纠缠损失与风格重建先验相协调，以在CLIP图像空间中去纠缠C-S。通过进一步利用扩散模型强大的风格去除和生成能力，我们的框架实现了比现有技术更好的结果，并实现了灵活的C-S解纠缠和权衡控制。我们的工作为风格转移中的C-S解纠缠提供了新的见解，并证明了扩散模型在学习解纠缠的C-S特征方面的潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07863v1" target="_blank">2308.07863v1</a>
                              </td>
                              <td>StyleDiffusion: Controllable Disentangled Style Transfer via Diffusion Models</td>
                              <td>Zhizhong Wang</td>
                              <td>2023-08-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07863v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07863v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_16198v4_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SuS-X: Training-Free Name-Only Transfer of Vision-Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_16198v4_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_16198v4_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_16198v4_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet effective way to train large-scale vision-language models. CLIP demonstrates impressive zero-shot classification and retrieval on diverse downstream tasks. However, to leverage its full potential, fine-tuning still appears to be necessary. Fine-tuning the entire CLIP model can be resource-intensive and unstable. Moreover, recent methods that aim to circumvent this need for fine-tuning still require access to images from the target distribution. In this paper, we pursue a different approach and explore the regime of training-free "name-only transfer" in which the only knowledge we possess about the downstream task comprises the names of downstream target categories. We propose a novel method, SuS-X, consisting of two key building blocks -- SuS and TIP-X, that requires neither intensive fine-tuning nor costly labelled data. SuS-X achieves state-of-the-art zero-shot classification results on 19 benchmark datasets. We further show the utility of TIP-X in the training-free few-shot setting, where we again achieve state-of-the-art results over strong training-free baselines. Code is available at https://github.com/vishaal27/SuS-X.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_16198v4_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对比语言图像预训练（CLIP）是一种简单而有效的训练大规模视觉语言模型的方法。CLIP在不同的下游任务上展示了令人印象深刻的零样本分类和检索。然而，为了充分发挥其潜力，微调似乎仍然是必要的。微调整个CLIP模型可能是资源密集型且不稳定的。此外，最近旨在规避这种微调需求的方法仍然需要访问来自目标分布的图像。在本文中，我们采用了一种不同的方法，并探索了无需训练的“仅名称转移”机制，在该机制中，我们所拥有的关于下游任务的唯一知识包括下游目标类别的名称。我们提出了一种新的方法，SuS-X，由两个关键构建块组成——SuS和TIP-X，既不需要密集的微调，也不需要昂贵的标记数据。SuS-X在19个基准数据集上实现了最先进的零样本分类结果。我们进一步展示了TIP-X在无训练少数镜头设置中的实用性，在那里，我们再次在强大的无训练基线上取得了最先进的结果。代码可在https://github.com/vishaal27/SuS-X.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.16198v4" target="_blank">2211.16198v4</a>
                              </td>
                              <td>SuS-X: Training-Free Name-Only Transfer of Vision-Language Models</td>
                              <td>Vishaal Udandarao</td>
                              <td>2022-11-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_16198v4_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.16198v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07737v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Identity-Consistent Aggregation for Video Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07737v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07737v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07737v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In Video Object Detection (VID), a common practice is to leverage the rich temporal contexts from the video to enhance the object representations in each frame. Existing methods treat the temporal contexts obtained from different objects indiscriminately and ignore their different identities. While intuitively, aggregating local views of the same object in different frames may facilitate a better understanding of the object. Thus, in this paper, we aim to enable the model to focus on the identity-consistent temporal contexts of each object to obtain more comprehensive object representations and handle the rapid object appearance variations such as occlusion, motion blur, etc. However, realizing this goal on top of existing VID models faces low-efficiency problems due to their redundant region proposals and nonparallel frame-wise prediction manner. To aid this, we propose ClipVID, a VID model equipped with Identity-Consistent Aggregation (ICA) layers specifically designed for mining fine-grained and identity-consistent temporal contexts. It effectively reduces the redundancies through the set prediction strategy, making the ICA layers very efficient and further allowing us to design an architecture that makes parallel clip-wise predictions for the whole video clip. Extensive experimental results demonstrate the superiority of our method: a state-of-the-art (SOTA) performance (84.7% mAP) on the ImageNet VID dataset while running at a speed about 7x faster (39.3 fps) than previous SOTAs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07737v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在视频对象检测（VID）中，一种常见的做法是利用视频中丰富的时间上下文来增强每帧中的对象表示。现有的方法不分青红皂白地处理从不同对象获得的时间上下文，而忽略了它们的不同身份。直观地，将同一对象的局部视图聚集在不同的帧中可能有助于更好地理解该对象。因此，在本文中，我们的目标是使模型能够关注每个对象的身份一致的时间上下文，以获得更全面的对象表示，并处理快速的对象外观变化，如遮挡、运动模糊等。然而，在现有VID模型的基础上实现这一目标，由于其冗余区域建议和非并行逐帧预测方式，面临着低效率问题。为了帮助实现这一点，我们提出了ClipVID，这是一个配备了身份一致聚合（ICA）层的VID模型，专门用于挖掘细粒度和身份一致的时间上下文。它通过集合预测策略有效地减少了冗余，使ICA层非常高效，并进一步允许我们设计一种对整个视频剪辑进行并行剪辑预测的架构。大量的实验结果证明了我们方法的优越性：在ImageNet VID数据集上，以比以前的SOTA快7倍（39.3 fps）的速度运行时，具有最先进的（SOTA）性能（84.7%mAP）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07737v1" target="_blank">2308.07737v1</a>
                              </td>
                              <td>Identity-Consistent Aggregation for Video Object Detection</td>
                              <td>Chaorui Deng</td>
                              <td>2023-08-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07737v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07737v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07648v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Prompt Switch: Efficient CLIP Adaptation for Text-Video Retrieval</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07648v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07648v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07648v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In text-video retrieval, recent works have benefited from the powerful learning capabilities of pre-trained text-image foundation models (e.g., CLIP) by adapting them to the video domain. A critical problem for them is how to effectively capture the rich semantics inside the video using the image encoder of CLIP. To tackle this, state-of-the-art methods adopt complex cross-modal modeling techniques to fuse the text information into video frame representations, which, however, incurs severe efficiency issues in large-scale retrieval systems as the video representations must be recomputed online for every text query. In this paper, we discard this problematic cross-modal fusion process and aim to learn semantically-enhanced representations purely from the video, so that the video representations can be computed offline and reused for different texts. Concretely, we first introduce a spatial-temporal "Prompt Cube" into the CLIP image encoder and iteratively switch it within the encoder layers to efficiently incorporate the global video semantics into frame representations. We then propose to apply an auxiliary video captioning objective to train the frame representations, which facilitates the learning of detailed video semantics by providing fine-grained guidance in the semantic space. With a naive temporal fusion strategy (i.e., mean-pooling) on the enhanced frame representations, we obtain state-of-the-art performances on three benchmark datasets, i.e., MSR-VTT, MSVD, and LSMDC.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07648v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在文本视频检索中，最近的工作得益于预训练的文本图像基础模型（例如，CLIP）的强大学习能力，使其适应视频领域。对他们来说，一个关键问题是如何使用CLIP的图像编码器有效地捕捉视频中丰富的语义。为了解决这一问题，最先进的方法采用复杂的跨模态建模技术将文本信息融合到视频帧表示中，然而，这在大规模检索系统中会带来严重的效率问题，因为必须为每个文本查询在线重新计算视频表示。在本文中，我们放弃了这种有问题的跨模态融合过程，目的是纯粹从视频中学习语义增强的表示，这样视频表示就可以离线计算并重复用于不同的文本。具体而言，我们首先将时空“提示立方体”引入CLIP图像编码器，并在编码器层内迭代切换，以将全局视频语义有效地合并到帧表示中。然后，我们提出应用辅助视频字幕目标来训练帧表示，这通过在语义空间中提供细粒度的指导来促进详细视频语义的学习。在增强的帧表示上使用朴素的时间融合策略（即均值池），我们在三个基准数据集（即MSR-VTT、MSVD和LSMDC）上获得了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07648v1" target="_blank">2308.07648v1</a>
                              </td>
                              <td>Prompt Switch: Efficient CLIP Adaptation for Text-Video Retrieval</td>
                              <td>Chaorui Deng</td>
                              <td>2023-08-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07648v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07648v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_10465v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Implicit Temporal Modeling with Learnable Alignment for Video Recognition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_10465v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_10465v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_10465v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Contrastive language-image pretraining (CLIP) has demonstrated remarkable success in various image tasks. However, how to extend CLIP with effective temporal modeling is still an open and crucial problem. Existing factorized or joint spatial-temporal modeling trades off between the efficiency and performance. While modeling temporal information within straight through tube is widely adopted in literature, we find that simple frame alignment already provides enough essence without temporal attention. To this end, in this paper, we proposed a novel Implicit Learnable Alignment (ILA) method, which minimizes the temporal modeling effort while achieving incredibly high performance. Specifically, for a frame pair, an interactive point is predicted in each frame, serving as a mutual information rich region. By enhancing the features around the interactive point, two frames are implicitly aligned. The aligned features are then pooled into a single token, which is leveraged in the subsequent spatial self-attention. Our method allows eliminating the costly or insufficient temporal self-attention in video. Extensive experiments on benchmarks demonstrate the superiority and generality of our module. Particularly, the proposed ILA achieves a top-1 accuracy of 88.7% on Kinetics-400 with much fewer FLOPs compared with Swin-L and ViViT-H. Code is released at https://github.com/Francis-Rings/ILA .</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_10465v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对比语言图像预训练（CLIP）在各种图像任务中取得了显著的成功。然而，如何通过有效的时间建模来扩展CLIP仍然是一个悬而未决的关键问题。现有的因子分解或联合时空建模在效率和性能之间进行权衡。虽然在直通管内对时间信息进行建模在文献中被广泛采用，但我们发现，简单的帧对齐已经提供了足够的本质，而不需要时间关注。为此，在本文中，我们提出了一种新的隐式可学习对齐（ILA）方法，该方法最大限度地减少了时间建模工作量，同时实现了令人难以置信的高性能。具体地，对于帧对，在每个帧中预测交互点，作为相互信息丰富的区域。通过增强交互点周围的特征，两个帧可以隐式对齐。然后将对齐的特征汇集到一个令牌中，在随后的空间自关注中利用该令牌。我们的方法可以消除视频中代价高昂或时间不充分的自我关注。在基准测试上进行的大量实验证明了我们模块的优越性和通用性。特别是，与Swin-L和ViViT-H相比，所提出的ILA在Kinetics-400上以少得多的FLOP实现了88.7%的前1精度。代码发布于https://github.com/Francis-Rings/ILA。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.10465v2" target="_blank">2304.10465v2</a>
                              </td>
                              <td>Implicit Temporal Modeling with Learnable Alignment for Video Recognition</td>
                              <td>Shuyuan Tu</td>
                              <td>2023-04-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_10465v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.10465v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_01489v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Improved Visual Fine-tuning with Natural Language Supervision</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_01489v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_01489v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_01489v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Fine-tuning a visual pre-trained model can leverage the semantic information from large-scale pre-training data and mitigate the over-fitting problem on downstream vision tasks with limited training examples. While the problem of catastrophic forgetting in pre-trained backbone has been extensively studied for fine-tuning, its potential bias from the corresponding pre-training task and data, attracts less attention. In this work, we investigate this problem by demonstrating that the obtained classifier after fine-tuning will be close to that induced by the pre-trained model. To reduce the bias in the classifier effectively, we introduce a reference distribution obtained from a fixed text classifier, which can help regularize the learned vision classifier. The proposed method, Text Supervised fine-tuning (TeS), is evaluated with diverse pre-trained vision models including ResNet and ViT, and text encoders including BERT and CLIP, on 11 downstream tasks. The consistent improvement with a clear margin over distinct scenarios confirms the effectiveness of our proposal. Code is available at \url{https://github.com/idstcv/TeS}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_01489v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>微调视觉预训练模型可以利用来自大规模预训练数据的语义信息，并在有限训练示例的情况下缓解下游视觉任务的过度拟合问题。虽然预训练骨干中的灾难性遗忘问题已经被广泛研究以进行微调，但其与相应的预训练任务和数据的潜在偏差却很少引起关注。在这项工作中，我们通过证明微调后获得的分类器将接近预训练模型诱导的分类器来研究这个问题。为了有效地减少分类器中的偏差，我们引入了从固定文本分类器中获得的参考分布，这可以帮助正则化学习的视觉分类器。所提出的方法，文本监督微调（TeS），在11个下游任务上使用包括ResNet和ViT在内的各种预先训练的视觉模型以及包括BERT和CLIP在内的文本编码器进行评估。在不同的情况下，持续的改进和明显的差距证实了我们的提案的有效性。代码位于\url{https://github.com/idstcv/TeS}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.01489v2" target="_blank">2304.01489v2</a>
                              </td>
                              <td>Improved Visual Fine-tuning with Natural Language Supervision</td>
                              <td>Junyang Wang</td>
                              <td>2023-04-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_01489v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.01489v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07539v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Visual and Textual Prior Guided Mask Assemble for Few-Shot Segmentation and Beyond</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07539v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07539v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07539v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Few-shot segmentation (FSS) aims to segment the novel classes with a few annotated images. Due to CLIP's advantages of aligning visual and textual information, the integration of CLIP can enhance the generalization ability of FSS model. However, even with the CLIP model, the existing CLIP-based FSS methods are still subject to the biased prediction towards base classes, which is caused by the class-specific feature level interactions. To solve this issue, we propose a visual and textual Prior Guided Mask Assemble Network (PGMA-Net). It employs a class-agnostic mask assembly process to alleviate the bias, and formulates diverse tasks into a unified manner by assembling the prior through affinity. Specifically, the class-relevant textual and visual features are first transformed to class-agnostic prior in the form of probability map. Then, a Prior-Guided Mask Assemble Module (PGMAM) including multiple General Assemble Units (GAUs) is introduced. It considers diverse and plug-and-play interactions, such as visual-textual, inter- and intra-image, training-free, and high-order ones. Lastly, to ensure the class-agnostic ability, a Hierarchical Decoder with Channel-Drop Mechanism (HDCDM) is proposed to flexibly exploit the assembled masks and low-level features, without relying on any class-specific information. It achieves new state-of-the-art results in the FSS task, with mIoU of $77.6$ on $\text{PASCAL-}5^i$ and $59.4$ on $\text{COCO-}20^i$ in 1-shot scenario. Beyond this, we show that without extra re-training, the proposed PGMA-Net can solve bbox-level and cross-domain FSS, co-segmentation, zero-shot segmentation (ZSS) tasks, leading an any-shot segmentation framework.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07539v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>少镜头分割（FSS）旨在用少量注释图像对新类别进行分割。由于CLIP在对齐视觉和文本信息方面的优势，CLIP的集成可以增强FSS模型的泛化能力。然而，即使使用CLIP模型，现有的基于CLIP的FSS方法仍然受到对基类的有偏预测的影响，这是由类特定的特征级交互引起的。为了解决这个问题，我们提出了一个视觉和文本的先验引导掩码组装网络（PGMA-Net）。它采用了类不可知的掩码组装过程来减轻偏见，并通过亲和性组装先验，将不同的任务统一起来。具体来说，首先将与类相关的文本和视觉特征以概率图的形式转换为类不可知先验。然后，介绍了一种包括多个通用装配单元（GAU）的先验引导掩模装配模块（PGMAM）。它考虑了多种即插即用的交互，如视觉文本、图像间和图像内、无训练和高阶交互。最后，为了确保类不可知性，提出了一种具有信道丢弃机制的分层解码器（HDCDM），以灵活地利用组装的掩码和低级特征，而不依赖于任何特定于类的信息。它在FSS任务中获得了最先进的新结果，在单次拍摄场景中，$\text｛PASCAL-｝5^i$的mIoU为77.6$，$\text{COCO-｝20^i$为59.4$。除此之外，我们表明，在没有额外重新训练的情况下，所提出的PGMA-Net可以解决边界层和跨域FSS、共同分割、零样本分割（ZSS）任务，从而实现任意热点分割框架。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07539v1" target="_blank">2308.07539v1</a>
                              </td>
                              <td>Visual and Textual Prior Guided Mask Assemble for Few-Shot Segmentation and Beyond</td>
                              <td>Chen Shuai</td>
                              <td>2023-08-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07539v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07539v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2210_02352v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fast Untethered Soft Robotic Crawler with Elastic Instability</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2210_02352v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2210_02352v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2210_02352v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>High-speed locomotion of animals gives them tremendous advantages in exploring, hunting, and escaping from predators in varying environments. Enlightened by the fast-running gait of mammals like cheetahs and wolves, we designed and fabricated a single-servo-driving untethered soft robot that is capable of galloping at a speed of 313 mm/s or 1.56 body length per second (BL/s), 5.2 times and 2.6 times faster than the reported fastest predecessors in mm/s and BL/s, respectively, in literature. An in-plane prestressed hair clip mechanism (HCM) made up of semi-rigid materials like plastic is used as the supporting chassis, the compliant spine, and the muscle force amplifier of the robot at the same time, enabling the robot to be rapid and strong. The influence of factors including actuation frequency, substrates, tethering/untethering, and symmetric/asymmetric actuation is explored with experiments. Based on previous work, this paper further demonstrated the potential of HCM in addressing the speed problem of soft robots.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2210_02352v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>动物的高速运动使它们在不同环境中探索、狩猎和逃离捕食者方面具有巨大优势。受猎豹和狼等哺乳动物快速奔跑步态的启发，我们设计并制造了一种单伺服驱动的无约束软机器人，该机器人能够以313毫米/秒或1.56体长/秒的速度奔跑，分别是文献中报道的最快的前代机器人的5.2倍和2.6倍。由塑料等半刚性材料制成的平面内预应力发夹机构（HCM）同时用作机器人的支撑底盘、柔顺脊椎和肌肉力量放大器，使机器人快速而坚固。通过实验探讨了驱动频率、衬底、系留/解开以及对称/不对称驱动等因素的影响。在前人工作的基础上，本文进一步论证了HCM在解决软机器人速度问题方面的潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2210.02352v3" target="_blank">2210.02352v3</a>
                              </td>
                              <td>Fast Untethered Soft Robotic Crawler with Elastic Instability</td>
                              <td>Zechen Xiong</td>
                              <td>2022-10-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2210_02352v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2210.02352v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2206_14867v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Pre-stressed Bi-stable Hair Clip Mechanism for Faster Swimming Robots</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2206_14867v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2206_14867v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2206_14867v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Structural instability is a hazard that leads to catastrophic failure and is generally avoided through special designs. A trend, however, has emerged over the past decades pointing to the harnessing of mechanisms with instability. Inspired by the snapping of a hair clip, we are finessing the unique characteristics of the lateral-torsional buckling of beams and the snap-through of pre-buckled dome-like thin-wall structures in a new field: the in-plane prestressed mechanism. Analyses reveal how the 2D-3D assembly of an in-plane prestressed actuator (IPA) is achieved and how the post-buckling energy landscape is pictured. Combining them with soft robotics, we show that the inclusion of a bistable IPA can enormously enhance the performance of an underwater fish robot as well as inspire a finger-like soft gripper.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2206_14867v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>结构不稳定是一种导致灾难性失效的危险，通常通过特殊设计来避免。然而，在过去几十年中出现了一种趋势，指出利用不稳定的机制。受发夹卡扣的启发，我们正在一个新的领域中对梁的侧向扭转屈曲和预屈曲圆顶状薄壁结构的卡扣特性进行精细化处理：平面内预应力机制。分析揭示了平面内预应力致动器（IPA）的2D-3D组装是如何实现的，以及屈曲后能量景观是如何描绘的。将它们与软机器人相结合，我们表明，双稳态IPA的加入可以极大地提高水下鱼类机器人的性能，并激发出手指状的软抓取器。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2206.14867v3" target="_blank">2206.14867v3</a>
                              </td>
                              <td>Pre-stressed Bi-stable Hair Clip Mechanism for Faster Swimming Robots</td>
                              <td>Zechen Xiong</td>
                              <td>2022-06-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2206_14867v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2206.14867v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07428v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07428v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07428v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07428v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Image reconstruction and captioning from brain activity evoked by visual stimuli allow researchers to further understand the connection between the human brain and the visual perception system. While deep generative models have recently been employed in this field, reconstructing realistic captions and images with both low-level details and high semantic fidelity is still a challenging problem. In this work, we propose UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity. For the first time, we unify image reconstruction and captioning from visual-evoked functional magnetic resonance imaging (fMRI) through a latent diffusion model termed Versatile Diffusion. Specifically, we transform fMRI voxels into text and image latent for low-level information and guide the backward diffusion process through fMRI-based image and text conditions derived from CLIP to generate realistic captions and images. UniBrain outperforms current methods both qualitatively and quantitatively in terms of image reconstruction and reports image captioning results for the first time on the Natural Scenes Dataset (NSD) dataset. Moreover, the ablation experiments and functional region-of-interest (ROI) analysis further exhibit the superiority of UniBrain and provide comprehensive insight for visual-evoked brain decoding.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07428v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉刺激引起的大脑活动的图像重建和字幕使研究人员能够进一步了解人脑与视觉感知系统之间的联系。尽管深度生成模型最近被应用于该领域，但重建具有低层次细节和高语义保真度的逼真字幕和图像仍然是一个具有挑战性的问题。在这项工作中，我们提出了UniBrain：从人脑活动中统一图像重建和捕获一体扩散模型。我们首次将视觉诱发功能磁共振成像（fMRI）的图像重建和字幕通过一个称为多功能扩散的潜在扩散模型统一起来。具体而言，我们将fMRI体素转换为文本和潜在的图像，以获得低级别信息，并通过从CLIP导出的基于fMRI的图像和文本条件来引导后向扩散过程，以生成逼真的字幕和图像。UniBrain在图像重建方面在质量和数量上都优于当前的方法，并首次在自然场景数据集（NSD）数据集上报告图像字幕结果。此外，消融实验和功能感兴趣区（ROI）分析进一步展示了UniBrain的优越性，为视觉诱发脑解码提供了全面的见解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07428v1" target="_blank">2308.07428v1</a>
                              </td>
                              <td>UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity</td>
                              <td>Weijian Mai</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07428v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07428v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07415v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Semantify: Simplifying the Control of 3D Morphable Models using CLIP</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07415v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07415v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07415v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present Semantify: a self-supervised method that utilizes the semantic power of CLIP language-vision foundation model to simplify the control of 3D morphable models. Given a parametric model, training data is created by randomly sampling the model's parameters, creating various shapes and rendering them. The similarity between the output images and a set of word descriptors is calculated in CLIP's latent space. Our key idea is first to choose a small set of semantically meaningful and disentangled descriptors that characterize the 3DMM, and then learn a non-linear mapping from scores across this set to the parametric coefficients of the given 3DMM. The non-linear mapping is defined by training a neural network without a human-in-the-loop. We present results on numerous 3DMMs: body shape models, face shape and expression models, as well as animal shapes. We demonstrate how our method defines a simple slider interface for intuitive modeling, and show how the mapping can be used to instantly fit a 3D parametric body shape to in-the-wild images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07415v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了Semantify：一种自监督方法，它利用CLIP语言视觉基础模型的语义能力来简化对3D可变形模型的控制。给定一个参数模型，训练数据是通过随机采样模型的参数、创建各种形状并对其进行渲染来创建的。在CLIP的潜在空间中计算输出图像和一组单词描述符之间的相似性。我们的关键思想是首先选择一小组语义上有意义且不纠缠的描述符来表征3DMM，然后从该集合的分数学习到给定3DMM的参数系数的非线性映射。非线性映射是通过在没有人参与的情况下训练神经网络来定义的。我们展示了许多3DMM的结果：身体形状模型、面部形状和表情模型，以及动物形状。我们展示了我们的方法如何定义一个简单的滑块界面来进行直观建模，并展示了如何使用映射来立即将3D参数化身体形状拟合到野生图像中。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07415v1" target="_blank">2308.07415v1</a>
                              </td>
                              <td>Semantify: Simplifying the Control of 3D Morphable Models using CLIP</td>
                              <td>Omer Gralnik</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07415v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07415v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_18232v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DIME-FM: DIstilling Multimodal and Efficient Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_18232v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_18232v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_18232v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Vision-Language Foundation Models (VLFM), such as CLIP, ALIGN and Florence, are trained on large-scale datasets of image-caption pairs and achieve superior transferability and robustness on downstream tasks, but they are difficult to use in many practical applications due to their large size, high latency and fixed architectures. Unfortunately, recent work shows training a small custom VLFM for resource-limited applications is currently very difficult using public and smaller-scale data. In this paper, we introduce a new distillation mechanism (DIME-FM) that allows us to transfer the knowledge contained in large VLFMs to smaller, customized foundation models using a relatively small amount of inexpensive, unpaired images and sentences. We transfer the knowledge from the pre-trained CLIP-ViTL/14 model to a ViT-B/32 model, with only 40M public images and 28.4M unpaired public sentences. The resulting model "Distill-ViT-B/32" rivals the CLIP-ViT-B/32 model pre-trained on its private WiT dataset (400M image-text pairs): Distill-ViT-B/32 achieves similar results in terms of zero-shot and linear-probing performance on both ImageNet and the ELEVATER (20 image classification tasks) benchmarks. It also displays comparable robustness when evaluated on five datasets with natural distribution shifts from ImageNet.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_18232v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型视觉语言基础模型（VLFM），如CLIP、ALIGN和Florence，在图像字幕对的大规模数据集上进行训练，并在下游任务上实现了卓越的可转移性和鲁棒性，但由于其大尺寸、高延迟和固定架构，很难在许多实际应用中使用。不幸的是，最近的工作表明，目前使用公共和较小规模的数据为资源有限的应用程序训练小型定制VLFM非常困难。在本文中，我们介绍了一种新的蒸馏机制（DIME-FM），它使我们能够使用相对少量的廉价、不成对的图像和句子，将大型VLFM中包含的知识转移到较小的定制基础模型中。我们将知识从预先训练的CLIP-ViTL/14模型转移到ViT-B/32模型，该模型只有4000万个公共图像和2840万个未配对的公共句子。由此产生的模型“Distille-ViT-B/32”与在其私人WiT数据集（400M图像-文本对）上预先训练的CLIP-ViT-B/32模型相媲美：Distille-ViT-B/32在ImageNet和ELEVATER（20个图像分类任务）基准上的零样本和线性制作性能方面取得了类似的结果。当在ImageNet的自然分布变化的五个数据集上进行评估时，它也显示出相当的稳健性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.18232v2" target="_blank">2303.18232v2</a>
                              </td>
                              <td>DIME-FM: DIstilling Multimodal and Efficient Foundation Models</td>
                              <td>Ximeng Sun</td>
                              <td>2023-03-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_18232v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.18232v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07200v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Neural Categorical Priors for Physics-Based Character Control</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07200v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07200v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07200v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in learning reusable motion priors have demonstrated their effectiveness in generating naturalistic behaviors. In this paper, we propose a new learning framework in this paradigm for controlling physics-based characters with significantly improved motion quality and diversity over existing state-of-the-art methods. The proposed method uses reinforcement learning (RL) to initially track and imitate life-like movements from unstructured motion clips using the discrete information bottleneck, as adopted in the Vector Quantized Variational AutoEncoder (VQ-VAE). This structure compresses the most relevant information from the motion clips into a compact yet informative latent space, i.e., a discrete space over vector quantized codes. By sampling codes in the space from a trained categorical prior distribution, high-quality life-like behaviors can be generated, similar to the usage of VQ-VAE in computer vision. Although this prior distribution can be trained with the supervision of the encoder's output, it follows the original motion clip distribution in the dataset and could lead to imbalanced behaviors in our setting. To address the issue, we further propose a technique named prior shifting to adjust the prior distribution using curiosity-driven RL. The outcome distribution is demonstrated to offer sufficient behavioral diversity and significantly facilitates upper-level policy learning for downstream tasks. We conduct comprehensive experiments using humanoid characters on two challenging downstream tasks, sword-shield striking and two-player boxing game. Our results demonstrate that the proposed framework is capable of controlling the character to perform considerably high-quality movements in terms of behavioral strategies, diversity, and realism. Videos, codes, and data are available at https://tencent-roboticsx.github.io/NCP/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07200v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近在学习可重复使用的运动先验方面的进展已经证明了它们在生成自然行为方面的有效性。在本文中，我们在这种范式中提出了一种新的学习框架，用于控制基于物理的角色，与现有的最先进的方法相比，该框架显著提高了运动质量和多样性。所提出的方法使用强化学习（RL），使用离散信息瓶颈，从非结构化运动片段中初始跟踪和模仿类似生命的运动，如矢量量化变分自动编码器（VQ-VAE）中所采用的。这种结构将来自运动剪辑的最相关信息压缩到紧凑但信息丰富的潜在空间，即矢量量化码上的离散空间。通过从训练的分类先验分布中对空间中的代码进行采样，可以生成高质量的类生命行为，类似于计算机视觉中VQ-VAE的使用。尽管这种先验分布可以在编码器输出的监督下进行训练，但它遵循数据集中的原始运动片段分布，并可能导致我们设置中的不平衡行为。为了解决这个问题，我们进一步提出了一种名为先验移位的技术，使用好奇心驱动的RL来调整先验分布。结果分布被证明提供了足够的行为多样性，并显著促进了下游任务的上层政策学习。我们使用人形角色在两项具有挑战性的下游任务，剑盾攻击和双人拳击游戏中进行了全面的实验。我们的结果表明，所提出的框架能够控制角色在行为策略、多样性和真实性方面进行相当高质量的动作。视频、代码和数据可在https://tencent-roboticsx.github.io/NCP/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07200v1" target="_blank">2308.07200v1</a>
                              </td>
                              <td>Neural Categorical Priors for Physics-Based Character Control</td>
                              <td>Qingxu Zhu</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07200v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07200v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_00135v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TeViS:Translating Text Synopses to Video Storyboards</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_00135v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_00135v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_00135v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A video storyboard is a roadmap for video creation which consists of shot-by-shot images to visualize key plots in a text synopsis. Creating video storyboards, however, remains challenging which not only requires cross-modal association between high-level texts and images but also demands long-term reasoning to make transitions smooth across shots. In this paper, we propose a new task called Text synopsis to Video Storyboard (TeViS) which aims to retrieve an ordered sequence of images as the video storyboard to visualize the text synopsis. We construct a MovieNet-TeViS dataset based on the public MovieNet dataset. It contains 10K text synopses each paired with keyframes manually selected from corresponding movies by considering both relevance and cinematic coherence. To benchmark the task, we present strong CLIP-based baselines and a novel VQ-Trans. VQ-Trans first encodes text synopsis and images into a joint embedding space and uses vector quantization (VQ) to improve the visual representation. Then, it auto-regressively generates a sequence of visual features for retrieval and ordering. Experimental results demonstrate that VQ-Trans significantly outperforms prior methods and the CLIP-based baselines. Nevertheless, there is still a large gap compared to human performance suggesting room for promising future work. The code and data are available at: \url{https://ruc-aimind.github.io/projects/TeViS/}</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_00135v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频故事板是视频创建的路线图，由逐镜头的图像组成，以在文本概要中可视化关键情节。然而，创建视频故事板仍然具有挑战性，这不仅需要高级文本和图像之间的跨模态关联，还需要长期推理，以使镜头之间的过渡平滑。在本文中，我们提出了一个新的任务，称为视频故事板的文本概要（TeViS），旨在检索有序的图像序列作为视频故事板，以可视化文本概要。我们在公共MovieNet数据集的基础上构建了一个MovieNet TeViS数据集。它包含10K个文本摘要，每个摘要与通过考虑相关性和电影连贯性从相应电影中手动选择的关键帧配对。为了对任务进行基准测试，我们提出了强大的基于CLIP的基线和一种新颖的VQ-Trans。VQ-Trans首先将文本概要和图像编码到联合嵌入空间中，并使用矢量量化（VQ）来改进视觉表示。然后，它自动回归生成一系列视觉特征，用于检索和排序。实验结果表明，VQ-Trans显著优于先前的方法和基于CLIP的基线。尽管如此，与人类的表现相比，仍有很大的差距，这表明未来的工作有希望。代码和数据位于：\url{https://ruc-aimind.github.io/projects/TeViS/}</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.00135v3" target="_blank">2301.00135v3</a>
                              </td>
                              <td>TeViS:Translating Text Synopses to Video Storyboards</td>
                              <td>Xu Gu</td>
                              <td>2022-12-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_00135v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.00135v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07078v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ICPC: Instance-Conditioned Prompting with Contrastive Learning for Semantic Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07078v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07078v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07078v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Modern supervised semantic segmentation methods are usually finetuned based on the supervised or self-supervised models pre-trained on ImageNet. Recent work shows that transferring the knowledge from CLIP to semantic segmentation via prompt learning can achieve promising performance. The performance boost comes from the feature enhancement with multimodal alignment, i.e., the dot product between vision and text embeddings. However, how to improve the multimodal alignment for better transfer performance in dense tasks remains underexplored. In this work, we focus on improving the quality of vision-text alignment from two aspects of prompting design and loss function, and present an instance-conditioned prompting with contrastive learning (ICPC) framework. First, compared with the static prompt designs, we reveal that dynamic prompting conditioned on image content can more efficiently utilize the text encoder for complex dense tasks. Second, we propose an align-guided contrastive loss to refine the alignment of vision and text embeddings. We further propose lightweight multi-scale alignment for better performance. Extensive experiments on three large-scale datasets (ADE20K, COCO-Stuff10k, and ADE20K-Full) demonstrate that ICPC brings consistent improvements across diverse backbones. Taking ResNet-50 as an example, ICPC outperforms the state-of-the-art counterpart by 1.71%, 1.05%, and 1.41% mIoU on the three datasets, respectively.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07078v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现代监督语义分割方法通常基于在ImageNet上预先训练的监督或自监督模型进行微调。最近的工作表明，通过即时学习将知识从CLIP转移到语义分割可以获得有希望的性能。性能提升来自于多模式对齐的特征增强，即视觉和文本嵌入之间的点积。然而，如何在密集任务中改进多模式对齐以获得更好的传输性能仍有待探索。在这项工作中，我们从提示设计和损失函数两个方面致力于提高视觉-文本对齐的质量，并提出了一个具有对比学习的实例条件提示（ICPC）框架。首先，与静态提示设计相比，我们发现以图像内容为条件的动态提示可以更有效地利用文本编码器执行复杂密集的任务。其次，我们提出了一种对齐引导的对比损失来改进视觉和文本嵌入的对齐。我们进一步提出了轻量级的多尺度对齐，以获得更好的性能。在三个大型数据集（ADE20K、COCO-Stuff10k和ADE20K-Full）上进行的大量实验表明，ICPC在不同的主干上带来了一致的改进。以ResNet-50为例，ICPC在三个数据集上分别比最先进的同类产品高1.71%、1.05%和1.41%mIoU。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07078v1" target="_blank">2308.07078v1</a>
                              </td>
                              <td>ICPC: Instance-Conditioned Prompting with Contrastive Learning for Semantic Segmentation</td>
                              <td>Chaohui Yu</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07078v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07078v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07026v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AdvCLIP: Downstream-agnostic Adversarial Examples in Multimodal Contrastive Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07026v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07026v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07026v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multimodal contrastive learning aims to train a general-purpose feature extractor, such as CLIP, on vast amounts of raw, unlabeled paired image-text data. This can greatly benefit various complex downstream tasks, including cross-modal image-text retrieval and image classification. Despite its promising prospect, the security issue of cross-modal pre-trained encoder has not been fully explored yet, especially when the pre-trained encoder is publicly available for commercial use.   In this work, we propose AdvCLIP, the first attack framework for generating downstream-agnostic adversarial examples based on cross-modal pre-trained encoders. AdvCLIP aims to construct a universal adversarial patch for a set of natural images that can fool all the downstream tasks inheriting the victim cross-modal pre-trained encoder. To address the challenges of heterogeneity between different modalities and unknown downstream tasks, we first build a topological graph structure to capture the relevant positions between target samples and their neighbors. Then, we design a topology-deviation based generative adversarial network to generate a universal adversarial patch. By adding the patch to images, we minimize their embeddings similarity to different modality and perturb the sample distribution in the feature space, achieving unviersal non-targeted attacks. Our results demonstrate the excellent attack performance of AdvCLIP on two types of downstream tasks across eight datasets. We also tailor three popular defenses to mitigate AdvCLIP, highlighting the need for new defense mechanisms to defend cross-modal pre-trained encoders.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07026v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多模式对比学习旨在对大量未标记的成对图像文本数据训练通用特征提取器，如CLIP。这可以极大地有利于各种复杂的下游任务，包括跨模态图像文本检索和图像分类。尽管跨模态预训练编码器前景广阔，但其安全问题尚未得到充分探讨，尤其是当预训练编码器可公开用于商业用途时。在这项工作中，我们提出了AdvCLIP，这是第一个基于跨模态预训练编码器生成下游不可知对抗性示例的攻击框架。AdvCLIP旨在为一组自然图像构建一个通用的对抗性补丁，该补丁可以欺骗继承受害者跨模态预训练编码器的所有下游任务。为了解决不同模态和未知下游任务之间的异质性挑战，我们首先构建了一个拓扑图结构来捕捉目标样本与其邻居之间的相关位置。然后，我们设计了一个基于拓扑偏差的生成对抗性网络来生成通用的对抗性补丁。通过将补丁添加到图像中，我们最小化了它们与不同模态的嵌入相似性，并扰乱了特征空间中的样本分布，实现了无针对性的攻击。我们的结果证明了AdvCLIP在八个数据集上对两种类型的下游任务的出色攻击性能。我们还定制了三种流行的防御措施来缓解AdvCLIP，强调需要新的防御机制来防御跨模态预训练编码器。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07026v1" target="_blank">2308.07026v1</a>
                              </td>
                              <td>AdvCLIP: Downstream-agnostic Adversarial Examples in Multimodal Contrastive Learning</td>
                              <td>Ziqi Zhou</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07026v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07026v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2208_00847v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MAFW: A Large-scale, Multi-modal, Compound Affective Database for Dynamic Facial Expression Recognition in the Wild</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2208_00847v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2208_00847v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2208_00847v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Dynamic facial expression recognition (FER) databases provide important data support for affective computing and applications. However, most FER databases are annotated with several basic mutually exclusive emotional categories and contain only one modality, e.g., videos. The monotonous labels and modality cannot accurately imitate human emotions and fulfill applications in the real world. In this paper, we propose MAFW, a large-scale multi-modal compound affective database with 10,045 video-audio clips in the wild. Each clip is annotated with a compound emotional category and a couple of sentences that describe the subjects' affective behaviors in the clip. For the compound emotion annotation, each clip is categorized into one or more of the 11 widely-used emotions, i.e., anger, disgust, fear, happiness, neutral, sadness, surprise, contempt, anxiety, helplessness, and disappointment. To ensure high quality of the labels, we filter out the unreliable annotations by an Expectation Maximization (EM) algorithm, and then obtain 11 single-label emotion categories and 32 multi-label emotion categories. To the best of our knowledge, MAFW is the first in-the-wild multi-modal database annotated with compound emotion annotations and emotion-related captions. Additionally, we also propose a novel Transformer-based expression snippet feature learning method to recognize the compound emotions leveraging the expression-change relations among different emotions and modalities. Extensive experiments on MAFW database show the advantages of the proposed method over other state-of-the-art methods for both uni- and multi-modal FER. Our MAFW database is publicly available from https://mafw-database.github.io/MAFW.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2208_00847v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>动态面部表情识别（FER）数据库为情感计算和应用提供了重要的数据支持。然而，大多数FER数据库都用几个基本的互斥情感类别进行注释，并且只包含一种模态，例如视频。单调的标签和情态不能准确地模仿人类的情感并在现实世界中实现应用。在本文中，我们提出了MAFW，这是一个大规模的多模态复合情感数据库，包含10045个野外视频音频片段。每个剪辑都用一个复合情感类别和几句描述剪辑中受试者情感行为的句子进行注释。对于复合情绪注释，每个片段被分类为11种广泛使用的情绪中的一种或多种，即愤怒、厌恶、恐惧、快乐、中性、悲伤、惊讶、蔑视、焦虑、无助和失望。为了保证标签的高质量，我们使用期望最大化（EM）算法过滤掉不可靠的注释，然后获得11个单标签情感类别和32个多标签情感类别。据我们所知，MAFW是第一个用复合情感注释和情感相关注释注释的野生多模态数据库。此外，我们还提出了一种新的基于Transformer的表情片段特征学习方法，利用不同情绪和模态之间的表情变化关系来识别复合情绪。在MAFW数据库上进行的大量实验表明，与其他最先进的单模态和多模态FER方法相比，所提出的方法具有优势。我们的MAFW数据库可从https://mafw-database.github.io/MAFW.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2208.00847v2" target="_blank">2208.00847v2</a>
                              </td>
                              <td>MAFW: A Large-scale, Multi-modal, Compound Affective Database for Dynamic Facial Expression Recognition in the Wild</td>
                              <td>Yuanyuan Liu</td>
                              <td>2022-08-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2208_00847v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2208.00847v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_04512v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Defense-Prefix for Preventing Typographic Attacks on CLIP</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_04512v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_04512v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_04512v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision-language pre-training models (VLPs) have exhibited revolutionary improvements in various vision-language tasks. In VLP, some adversarial attacks fool a model into false or absurd classifications. Previous studies addressed these attacks by fine-tuning the model or changing its architecture. However, these methods risk losing the original model's performance and are difficult to apply to downstream tasks. In particular, their applicability to other tasks has not been considered. In this study, we addressed the reduction of the impact of typographic attacks on CLIP without changing the model parameters. To achieve this, we expand the idea of ``prefix learning'' and introduce our simple yet effective method: Defense-Prefix (DP), which inserts the DP token before a class name to make words ``robust'' against typographic attacks. Our method can be easily applied to downstream tasks, such as object detection, because the proposed method is independent of the model parameters. Our method significantly improves the accuracy of classification tasks for typographic attack datasets, while maintaining the zero-shot capabilities of the model. In addition, we leverage our proposed method for object detection, demonstrating its high applicability and effectiveness. The codes and datasets are available at https://github.com/azuma164/Defense-Prefix.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_04512v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉语言预训练模型（VLP）在各种视觉语言任务中表现出革命性的改进。在VLP中，一些对抗性攻击将模型欺骗为错误或荒谬的分类。先前的研究通过微调模型或更改其架构来解决这些攻击。然而，这些方法有可能失去原始模型的性能，并且很难应用于下游任务。特别是，没有考虑它们对其他任务的适用性。在这项研究中，我们讨论了在不改变模型参数的情况下减少排版攻击对CLIP的影响。为了实现这一点，我们扩展了“前缀学习”的概念，并引入了我们简单而有效的方法：防御前缀（DP），它在类名之前插入DP令牌，使单词“健壮”以抵御排版攻击。我们的方法可以很容易地应用于下游任务，如目标检测，因为所提出的方法与模型参数无关。我们的方法显著提高了印刷攻击数据集分类任务的准确性，同时保持了模型的零样本能力。此外，我们利用我们提出的方法进行目标检测，证明了它的高度适用性和有效性。代码和数据集可在https://github.com/azuma164/Defense-Prefix.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.04512v2" target="_blank">2304.04512v2</a>
                              </td>
                              <td>Defense-Prefix for Preventing Typographic Attacks on CLIP</td>
                              <td>Hiroki Azuma</td>
                              <td>2023-04-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_04512v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.04512v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2203_07931v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DialogueNeRF: Towards Realistic Avatar Face-to-Face Conversation Video Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2203_07931v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2203_07931v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2203_07931v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Conversation is an essential component of virtual avatar activities in the metaverse. With the development of natural language processing, textual and vocal conversation generation has achieved a significant breakthrough. However, face-to-face conversations account for the vast majority of daily conversations, while most existing methods focused on single-person talking head generation. In this work, we take a step further and consider generating realistic face-to-face conversation videos. Conversation generation is more challenging than single-person talking head generation, since it not only requires generating photo-realistic individual talking heads but also demands the listener to respond to the speaker. In this paper, we propose a novel unified framework based on neural radiance field (NeRF) to address this task. Specifically, we model both the speaker and listener with a NeRF framework, with different conditions to control individual expressions. The speaker is driven by the audio signal, while the response of the listener depends on both visual and acoustic information. In this way, face-to-face conversation videos are generated between human avatars, with all the interlocutors modeled within the same network. Moreover, to facilitate future research on this task, we collect a new human conversation dataset containing 34 clips of videos. Quantitative and qualitative experiments evaluate our method in different aspects, e.g., image quality, pose sequence trend, and naturalness of the rendering videos. Experimental results demonstrate that the avatars in the resulting videos are able to perform a realistic conversation, and maintain individual styles. All the code, data, and models will be made publicly available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2203_07931v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对话是元宇宙中虚拟化身活动的重要组成部分。随着自然语言处理的发展，语篇和语音会话生成取得了重大突破。然而，面对面的对话占日常对话的绝大多数，而大多数现有的方法都侧重于单人对话。在这项工作中，我们更进一步，考虑生成逼真的面对面对话视频。会话生成比单人会话头生成更具挑战性，因为它不仅需要生成逼真的个人会话头，还需要听众对说话者做出回应。在本文中，我们提出了一种新的基于神经辐射场（NeRF）的统一框架来解决这一任务。具体来说，我们用NeRF框架对说话者和倾听者进行建模，并在不同的条件下控制个体表达。扬声器由音频信号驱动，而听众的反应取决于视觉和声学信息。通过这种方式，在人类化身之间生成面对面的对话视频，所有对话者都在同一网络中建模。此外，为了促进未来对这项任务的研究，我们收集了一个新的人类对话数据集，其中包含34个视频片段。定量和定性实验从不同方面评估了我们的方法，例如图像质量、姿势序列趋势和渲染视频的自然度。实验结果表明，视频中的化身能够进行逼真的对话，并保持个人风格。所有的代码、数据和模型都将公开。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2203.07931v2" target="_blank">2203.07931v2</a>
                              </td>
                              <td>DialogueNeRF: Towards Realistic Avatar Face-to-Face Conversation Video Generation</td>
                              <td>Yichao Yan</td>
                              <td>2022-03-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2203_07931v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2203.07931v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16741v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Foundation Model for Endoscopy Video Analysis via Large-scale Self-supervised Pre-train</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16741v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16741v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16741v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Foundation models have exhibited remarkable success in various applications, such as disease diagnosis and text report generation. To date, a foundation model for endoscopic video analysis is still lacking. In this paper, we propose Endo-FM, a foundation model specifically developed using massive endoscopic video data. First, we build a video transformer, which captures both local and global long-range dependencies across spatial and temporal dimensions. Second, we pre-train our transformer model using global and local views via a self-supervised manner, aiming to make it robust to spatial-temporal variations and discriminative across different scenes. To develop the foundation model, we construct a large-scale endoscopy video dataset by combining 9 publicly available datasets and a privately collected dataset from Baoshan Branch of Renji Hospital in Shanghai, China. Our dataset overall consists of over 33K video clips with up to 5 million frames, encompassing various protocols, target organs, and disease types. Our pre-trained Endo-FM can be easily adopted for a given downstream task via fine-tuning by serving as the backbone. With experiments on 3 different types of downstream tasks, including classification, segmentation, and detection, our Endo-FM surpasses the current state-of-the-art (SOTA) self-supervised pre-training and adapter-based transfer learning methods by a significant margin, such as VCL (3.1% F1, 4.8% Dice, and 5.5% F1 for classification, segmentation, and detection) and ST-Adapter (5.9% F1, 9.6% Dice, and 9.9% F1 for classification, segmentation, and detection). Code, datasets, and models are released at https://github.com/med-air/Endo-FM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16741v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基础模型在各种应用中取得了显著的成功，如疾病诊断和文本报告生成。到目前为止，内窥镜视频分析的基础模型仍然缺乏。在本文中，我们提出了Endo FM，这是一个专门使用大量内窥镜视频数据开发的基础模型。首先，我们构建了一个视频转换器，它可以捕获跨空间和时间维度的局部和全局长期依赖关系。其次，我们通过自监督的方式使用全局和局部视图预训练我们的变换器模型，旨在使其对时空变化具有鲁棒性，并在不同场景中具有鉴别性。为了开发基础模型，我们将中国上海仁济医院宝山分院的9个公开可用的数据集和一个私人收集的数据集相结合，构建了一个大规模的内镜视频数据集。我们的数据集总体上由超过33K个视频片段组成，高达500万帧，包括各种协议、靶器官和疾病类型。我们经过预训练的Endo FM可以作为骨干，通过微调，轻松用于特定的下游任务。通过对3种不同类型的下游任务（包括分类、分割和检测）进行实验，我们的Endo FM显著超过了当前最先进的（SOTA）自监督预训练和基于适配器的迁移学习方法，例如VCL（用于分类、分割和检测的3.1%F1、4.8%Dice和5.5%F1）和ST适配器（用于分类和检测的5.9%F1、9.6%Dice和9.9%F1）。代码、数据集和模型发布于https://github.com/med-air/Endo-FM.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16741v2" target="_blank">2306.16741v2</a>
                              </td>
                              <td>Foundation Model for Endoscopy Video Analysis via Large-scale Self-supervised Pre-train</td>
                              <td>Zhao Wang</td>
                              <td>2023-06-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16741v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16741v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_06342v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Mirror Diffusion Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_06342v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_06342v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_06342v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Diffusion models have successfully been applied to generative tasks in various continuous domains. However, applying diffusion to discrete categorical data remains a non-trivial task. Moreover, generation in continuous domains often requires clipping in practice, which motivates the need for a theoretical framework for adapting diffusion to constrained domains. Inspired by the mirror Langevin algorithm for the constrained sampling problem, in this theoretical report we propose Mirror Diffusion Models (MDMs). We demonstrate MDMs in the context of simplex diffusion and propose natural extensions to popular domains such as image and text generation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_06342v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>扩散模型已经成功地应用于各种连续领域的生成任务。然而，将扩散应用于离散分类数据仍然是一项不平凡的任务。此外，在连续域中的生成通常需要在实践中进行裁剪，这激发了对将扩散适应约束域的理论框架的需求。受约束采样问题的镜像Langevin算法的启发，在本理论报告中，我们提出了镜像扩散模型（MDMs）。我们在单纯形扩散的背景下演示了MDM，并提出了对流行领域（如图像和文本生成）的自然扩展。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.06342v1" target="_blank">2308.06342v1</a>
                              </td>
                              <td>Mirror Diffusion Models</td>
                              <td>Jaesung Tae</td>
                              <td>2023-08-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_06342v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.06342v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_06628v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_06628v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_06628v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_06628v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Continual learning (CL) can help pre-trained vision-language models efficiently adapt to new or under-trained data distributions without re-training. Nevertheless, during the continual training of the Contrastive Language-Image Pre-training (CLIP) model, we observe that the model's zero-shot transfer ability significantly degrades due to catastrophic forgetting. Existing CL methods can mitigate forgetting by replaying previous data. However, since the CLIP dataset is private, replay methods cannot access the pre-training dataset. In addition, replaying data of previously learned downstream tasks can enhance their performance but comes at the cost of sacrificing zero-shot performance. To address this challenge, we propose a novel method ZSCL to prevent zero-shot transfer degradation in the continual learning of vision-language models in both feature and parameter space. In the feature space, a reference dataset is introduced for distillation between the current and initial models. The reference dataset should have semantic diversity but no need to be labeled, seen in pre-training, or matched image-text pairs. In parameter space, we prevent a large parameter shift by averaging weights during the training. We propose a more challenging Multi-domain Task Incremental Learning (MTIL) benchmark to evaluate different methods, where tasks are from various domains instead of class-separated in a single dataset. Our method outperforms other methods in the traditional class-incremental learning setting and the MTIL by 9.7% average score. Our code locates at https://github.com/Thunderbeee/ZSCL.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_06628v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>持续学习（CL）可以帮助预先训练的视觉语言模型有效地适应新的或训练不足的数据分布，而无需重新训练。然而，在对比语言图像预训练（CLIP）模型的持续训练过程中，我们观察到由于灾难性遗忘，该模型的零样本传递能力显著降低。现有的CL方法可以通过重放以前的数据来减轻遗忘。然而，由于CLIP数据集是私有的，回放方法无法访问预训练数据集。此外，回放先前学习的下游任务的数据可以提高它们的性能，但这是以牺牲零样本性能为代价的。为了应对这一挑战，我们提出了一种新的方法ZSCL，以防止在特征和参数空间中视觉语言模型的连续学习中零样本迁移退化。在特征空间中，引入了一个参考数据集，用于在当前模型和初始模型之间进行提取。参考数据集应该具有语义多样性，但不需要标记、在预训练中看到或匹配的图像-文本对。在参数空间中，我们通过在训练过程中平均权重来防止大的参数偏移。我们提出了一个更具挑战性的多领域任务增量学习（MTIL）基准来评估不同的方法，其中任务来自不同的领域，而不是在单个数据集中分类。在传统的课堂增量学习环境和MTIL中，我们的方法的平均得分比其他方法高9.7%。我们的代码位于https://github.com/Thunderbeee/ZSCL.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.06628v2" target="_blank">2303.06628v2</a>
                              </td>
                              <td>Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models</td>
                              <td>Zangwei Zheng</td>
                              <td>2023-03-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_06628v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.06628v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_06038v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_06038v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_06038v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_06038v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Benefiting from prompt tuning, recent years have witnessed the promising performance of pre-trained vision-language models, e.g., CLIP, on versatile downstream tasks. In this paper, we focus on a particular setting of learning adaptive prompts on the fly for each test sample from an unseen new domain, which is known as test-time prompt tuning (TPT). Existing TPT methods typically rely on data augmentation and confidence selection. However, conventional data augmentation techniques, e.g., random resized crops, suffers from the lack of data diversity, while entropy-based confidence selection alone is not sufficient to guarantee prediction fidelity. To address these issues, we propose a novel TPT method, named DiffTPT, which leverages pre-trained diffusion models to generate diverse and informative new data. Specifically, we incorporate augmented data by both conventional method and pre-trained stable diffusion to exploit their respective merits, improving the models ability to adapt to unknown new test data. Moreover, to ensure the prediction fidelity of generated data, we introduce a cosine similarity-based filtration technique to select the generated data with higher similarity to the single test sample. Our experiments on test datasets with distribution shifts and unseen categories demonstrate that DiffTPT improves the zero-shot accuracy by an average of 5.13\% compared to the state-of-the-art TPT method. Our code and models will be publicly released.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_06038v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>得益于快速调整，近年来，预训练的视觉语言模型（如CLIP）在多功能下游任务中表现良好。在本文中，我们重点关注一个特定的设置，即为来自一个看不见的新领域的每个测试样本动态学习自适应提示，这被称为测试时间提示调整（TPT）。现有的TPT方法通常依赖于数据扩充和置信度选择。然而，传统的数据增强技术，例如随机调整大小的作物，缺乏数据多样性，而仅基于熵的置信度选择不足以保证预测的保真度。为了解决这些问题，我们提出了一种新的TPT方法，称为DiffTPT，它利用预先训练的扩散模型来生成多样化和信息丰富的新数据。具体而言，我们通过传统方法和预先训练的稳定扩散结合了增强数据，以利用它们各自的优点，提高模型适应未知新测试数据的能力。此外，为了确保生成数据的预测保真度，我们引入了一种基于余弦相似性的过滤技术来选择与单个测试样本具有更高相似性的生成数据。我们在具有分布偏移和不可见类别的测试数据集上的实验表明，与最先进的TPT方法相比，DiffTPT将零样本精度平均提高了5.13\%。我们的代码和模型将公开发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.06038v1" target="_blank">2308.06038v1</a>
                              </td>
                              <td>Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning</td>
                              <td>Chun-Mei Feng</td>
                              <td>2023-08-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_06038v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.06038v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_06035v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Evidence of Human-Like Visual-Linguistic Integration in Multimodal Large Language Models During Predictive Language Processing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_06035v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_06035v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_06035v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The advanced language processing abilities of large language models (LLMs) have stimulated debate over their capacity to replicate human-like cognitive processes. One differentiating factor between language processing in LLMs and humans is that language input is often grounded in more than one perceptual modality, whereas most LLMs process solely text-based information. Multimodal grounding allows humans to integrate - e.g. visual context with linguistic information and thereby place constraints on the space of upcoming words, reducing cognitive load and improving perception and comprehension. Recent multimodal LLMs (mLLMs) combine visual and linguistic embedding spaces with a transformer type attention mechanism for next-word prediction. To what extent does predictive language processing based on multimodal input align in mLLMs and humans? To answer this question, 200 human participants watched short audio-visual clips and estimated the predictability of an upcoming verb or noun. The same clips were processed by the mLLM CLIP, with predictability scores based on a comparison of image and text feature vectors. Eye-tracking was used to estimate what visual features participants attended to, and CLIP's visual attention weights were recorded. We find that human estimates of predictability align significantly with CLIP scores, but not for a unimodal LLM of comparable parameter size. Further, alignment vanished when CLIP's visual attention weights were perturbed, and when the same input was fed to a multimodal model without attention. Analysing attention patterns, we find a significant spatial overlap between CLIP's visual attention weights and human eye-tracking data. Results suggest that comparable processes of integrating multimodal information, guided by attention to relevant visual features, supports predictive language processing in mLLMs and humans.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_06035v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）的高级语言处理能力引发了关于其复制类人认知过程能力的争论。LLM和人类的语言处理之间的一个区别因素是，语言输入通常基于一种以上的感知模态，而大多数LLM只处理基于文本的信息。多模式基础使人类能够将视觉上下文与语言信息相结合，从而对即将出现的单词的空间施加限制，减少认知负荷，提高感知和理解能力。最近的多模式LLM（mLLMs）将视觉和语言嵌入空间与用于下一个单词预测的转换类型注意力机制相结合。基于多模式输入的预测语言处理在多大程度上与mLLM和人类一致？为了回答这个问题，200名人类参与者观看了视听短片，并估计了即将到来的动词或名词的可预测性。相同的剪辑由mLLM CLIP处理，可预测性得分基于图像和文本特征向量的比较。眼动追踪用于估计参与者关注的视觉特征，并记录CLIP的视觉注意力权重。我们发现，人类对可预测性的估计与CLIP评分显著一致，但对于具有可比参数大小的单峰LLM则不一致。此外，当CLIP的视觉注意力权重受到干扰时，以及当相同的输入被馈送到没有注意力的多模式模型时，对齐消失。通过分析注意力模式，我们发现CLIP的视觉注意力权重与人眼跟踪数据之间存在显著的空间重叠。结果表明，在对相关视觉特征的关注的指导下，整合多模式信息的可比过程支持mLLM和人类的预测语言处理。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.06035v1" target="_blank">2308.06035v1</a>
                              </td>
                              <td>Evidence of Human-Like Visual-Linguistic Integration in Multimodal Large Language Models During Predictive Language Processing</td>
                              <td>Viktor Kewenig</td>
                              <td>2023-08-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_06035v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.06035v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_05976v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Zero-shot Text-driven Physically Interpretable Face Editing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_05976v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_05976v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_05976v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper proposes a novel and physically interpretable method for face editing based on arbitrary text prompts. Different from previous GAN-inversion-based face editing methods that manipulate the latent space of GANs, or diffusion-based methods that model image manipulation as a reverse diffusion process, we regard the face editing process as imposing vector flow fields on face images, representing the offset of spatial coordinates and color for each image pixel. Under the above-proposed paradigm, we represent the vector flow field in two ways: 1) explicitly represent the flow vectors with rasterized tensors, and 2) implicitly parameterize the flow vectors as continuous, smooth, and resolution-agnostic neural fields, by leveraging the recent advances of implicit neural representations. The flow vectors are iteratively optimized under the guidance of the pre-trained Contrastive Language-Image Pretraining~(CLIP) model by maximizing the correlation between the edited image and the text prompt. We also propose a learning-based one-shot face editing framework, which is fast and adaptable to any text prompt input. Our method can also be flexibly extended to real-time video face editing. Compared with state-of-the-art text-driven face editing methods, our method can generate physically interpretable face editing results with high identity consistency and image quality. Our code will be made publicly available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_05976v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种基于任意文本提示的人脸编辑新方法，该方法具有物理可解释性。与以前基于GAN反转的人脸编辑方法（操纵GAN的潜在空间）或基于扩散的方法（将图像操纵建模为反向扩散过程）不同，我们将人脸编辑过程视为在人脸图像上施加矢量流场，表示每个图像像素的空间坐标和颜色的偏移。在上述提出的范式下，我们用两种方式表示矢量流场：1）用光栅化张量显式表示流矢量，2）通过利用隐式神经表示的最新进展，将流矢量隐式参数化为连续、平滑和分辨率不可知的神经场。在预先训练的对比语言图像预训练（CLIP）模型的指导下，通过最大化编辑后的图像和文本提示之间的相关性，迭代优化流向量。我们还提出了一种基于学习的一次性人脸编辑框架，该框架快速且适用于任何文本提示输入。我们的方法还可以灵活地扩展到实时视频人脸编辑。与最先进的文本驱动的人脸编辑方法相比，我们的方法可以生成具有高身份一致性和图像质量的物理可解释的人脸编辑结果。我们的代码将公开。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.05976v1" target="_blank">2308.05976v1</a>
                              </td>
                              <td>Zero-shot Text-driven Physically Interpretable Face Editing</td>
                              <td>Yapeng Meng</td>
                              <td>2023-08-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_05976v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.05976v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_08966v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Training Multimedia Event Extraction With Generated Images and Captions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_08966v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_08966v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_08966v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Contemporary news reporting increasingly features multimedia content, motivating research on multimedia event extraction. However, the task lacks annotated multimodal training data and artificially generated training data suffer from distribution shift from real-world data. In this paper, we propose Cross-modality Augmented Multimedia Event Learning (CAMEL), which successfully utilizes artificially generated multimodal training data and achieves state-of-the-art performance. We start with two labeled unimodal datasets in text and image respectively, and generate the missing modality using off-the-shelf image generators like Stable Diffusion and image captioners like BLIP. After that, we train the network on the resultant multimodal datasets. In order to learn robust features that are effective across domains, we devise an iterative and gradual training strategy. Substantial experiments show that CAMEL surpasses state-of-the-art (SOTA) baselines on the M2E2 benchmark. On multimedia events in particular, we outperform the prior SOTA by 4.2% F1 on event mention identification and by 9.8% F1 on argument identification, which indicates that CAMEL learns synergistic representations from the two modalities. Our work demonstrates a recipe to unleash the power of synthetic training data in structured prediction.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_08966v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当代新闻报道越来越多地以多媒体内容为特色，促使人们对多媒体事件提取进行研究。然而，该任务缺乏注释的多模式训练数据，并且人工生成的训练数据与真实世界的数据存在分布偏移。在本文中，我们提出了跨模态增强多媒体事件学习（CAMEL），它成功地利用了人工生成的多模态训练数据，并实现了最先进的性能。我们分别从文本和图像中的两个标记的单峰数据集开始，并使用现成的图像生成器（如Stable Diffusion）和图像字幕生成器（如BLIP）生成缺失的模态。之后，我们在得到的多模态数据集上训练网络。为了学习跨领域有效的健壮特征，我们设计了一种迭代和渐进的训练策略。大量实验表明，CAMEL在M2E2基准上超过了最先进的（SOTA）基线。特别是在多媒体事件上，我们在事件提及识别上优于先前的SOTA 4.2%F1，在论点识别上优于之前的SOTA 9.8%F1，这表明CAMEL从两种模式中学习协同表示。我们的工作展示了一种在结构化预测中释放合成训练数据力量的方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.08966v2" target="_blank">2306.08966v2</a>
                              </td>
                              <td>Training Multimedia Event Extraction With Generated Images and Captions</td>
                              <td>Zilin Du</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_08966v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.08966v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_05659v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AD-CLIP: Adapting Domains in Prompt Space Using CLIP</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_05659v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_05659v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_05659v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Although deep learning models have shown impressive performance on supervised learning tasks, they often struggle to generalize well when the training (source) and test (target) domains differ. Unsupervised domain adaptation (DA) has emerged as a popular solution to this problem. However, current DA techniques rely on visual backbones, which may lack semantic richness. Despite the potential of large-scale vision-language foundation models like CLIP, their effectiveness for DA has yet to be fully explored. To address this gap, we introduce AD-CLIP, a domain-agnostic prompt learning strategy for CLIP that aims to solve the DA problem in the prompt space. We leverage the frozen vision backbone of CLIP to extract both image style (domain) and content information, which we apply to learn prompt tokens. Our prompts are designed to be domain-invariant and class-generalizable, by conditioning prompt learning on image style and content features simultaneously. We use standard supervised contrastive learning in the source domain, while proposing an entropy minimization strategy to align domains in the embedding space given the target domain data. We also consider a scenario where only target domain samples are available during testing, without any source domain data, and propose a cross-domain style mapping network to hallucinate domain-agnostic tokens. Our extensive experiments on three benchmark DA datasets demonstrate the effectiveness of AD-CLIP compared to existing literature.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_05659v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管深度学习模型在监督学习任务上表现出了令人印象深刻的性能，但当训练（源）和测试（目标）领域不同时，它们往往难以很好地概括。无监督领域自适应（DA）已成为解决这一问题的一种流行方法。然而，目前的DA技术依赖于视觉主干，而视觉主干可能缺乏语义丰富性。尽管像CLIP这样的大规模视觉语言基础模型具有潜力，但它们对DA的有效性仍有待充分探索。为了解决这一差距，我们引入了AD-CLIP，这是CLIP的一种领域不可知的提示学习策略，旨在解决提示空间中的DA问题。我们利用CLIP的冻结视觉主干来提取图像风格（域）和内容信息，并将其应用于学习提示令牌。我们的提示被设计为域不变和类可推广，通过同时对图像风格和内容特征进行提示学习。我们在源域中使用标准监督对比学习，同时提出了一种熵最小化策略，在给定目标域数据的情况下，在嵌入空间中对齐域。我们还考虑了一种场景，其中在测试期间只有目标域样本可用，而没有任何源域数据，并提出了一种跨域风格的映射网络来幻觉域不可知的令牌。与现有文献相比，我们在三个基准DA数据集上的大量实验证明了AD-CLIP的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.05659v1" target="_blank">2308.05659v1</a>
                              </td>
                              <td>AD-CLIP: Adapting Domains in Prompt Space Using CLIP</td>
                              <td>Mainak Singha</td>
                              <td>2023-08-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_05659v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.05659v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2112_02399v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">VT-CLIP: Enhancing Vision-Language Models with Visual-guided Texts</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2112_02399v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2112_02399v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2112_02399v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Contrastive Language-Image Pre-training (CLIP) has drawn increasing attention recently for its transferable visual representation learning. However, due to the semantic gap within datasets, CLIP's pre-trained image-text alignment becomes sub-optimal on downstream tasks, which severely harms its transferring performance. To better adapt the cross-modality embedding space, we propose to enhance CLIP via Visual-guided Texts, named VT-CLIP. Specifically, we guide textual features of different categories to adaptively explore informative regions on the image and aggregate visual features by attention mechanisms. In this way, the texts become visual-guided, namely, more semantically correlated with downstream images, which greatly benefits the category-wise matching process. In few-shot settings, we evaluate our VT-CLIP on 11 well-known classification datasets to demonstrate its effectiveness.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2112_02399v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对比语言图像预训练（CLIP）以其可迁移的视觉表征学习而受到越来越多的关注。然而，由于数据集内的语义差距，CLIP的预训练图像-文本对齐在下游任务中变得次优，这严重损害了其传输性能。为了更好地适应跨模态嵌入空间，我们建议通过视觉引导的文本来增强CLIP，称为VT-CLIP。具体来说，我们引导不同类别的文本特征自适应地探索图像上的信息区域，并通过注意力机制聚合视觉特征。通过这种方式，文本变得具有视觉引导性，即与下游图像在语义上更加相关，这大大有利于分类匹配过程。在少数镜头设置中，我们在11个著名的分类数据集上评估了我们的VT-CLIP，以证明其有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2112.02399v3" target="_blank">2112.02399v3</a>
                              </td>
                              <td>VT-CLIP: Enhancing Vision-Language Models with Visual-guided Texts</td>
                              <td>Longtian Qiu</td>
                              <td>2021-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2112_02399v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2112.02399v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_05543v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Deep Richardson-Lucy Deconvolution for Low-Light Image Deblurring</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_05543v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_05543v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_05543v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Images taken under the low-light condition often contain blur and saturated pixels at the same time. Deblurring images with saturated pixels is quite challenging. Because of the limited dynamic range, the saturated pixels are usually clipped in the imaging process and thus cannot be modeled by the linear blur model. Previous methods use manually designed smooth functions to approximate the clipping procedure. Their deblurring processes often require empirically defined parameters, which may not be the optimal choices for different images. In this paper, we develop a data-driven approach to model the saturated pixels by a learned latent map. Based on the new model, the non-blind deblurring task can be formulated into a maximum a posterior (MAP) problem, which can be effectively solved by iteratively computing the latent map and the latent image. Specifically, the latent map is computed by learning from a map estimation network (MEN), and the latent image estimation process is implemented by a Richardson-Lucy (RL)-based updating scheme. To estimate high-quality deblurred images without amplified artifacts, we develop a prior estimation network (PEN) to obtain prior information, which is further integrated into the RL scheme. Experimental results demonstrate that the proposed method performs favorably against state-of-the-art algorithms both quantitatively and qualitatively on synthetic and real-world images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_05543v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在弱光条件下拍摄的图像通常同时包含模糊和饱和像素。用饱和像素去模糊图像是相当具有挑战性的。由于动态范围有限，饱和像素通常在成像过程中被剪裁，因此无法通过线性模糊模型进行建模。以前的方法使用手动设计的平滑函数来近似剪裁过程。他们的去模糊过程通常需要经验定义的参数，这可能不是不同图像的最佳选择。在本文中，我们开发了一种数据驱动的方法，通过学习的潜图对饱和像素进行建模。基于新模型，非盲去模糊任务可以公式化为最大后验（MAP）问题，通过迭代计算潜图和潜像可以有效地解决该问题。具体地，通过从地图估计网络（MEN）学习来计算潜在地图，并且通过基于Richardson-Lucy（RL）的更新方案来实现潜在图像估计处理。为了在没有放大伪影的情况下估计高质量的去模糊图像，我们开发了一个先验估计网络（PEN）来获得先验信息，该先验信息被进一步集成到RL方案中。实验结果表明，该方法在合成图像和真实世界图像上的定量和定性性能都优于最先进的算法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.05543v1" target="_blank">2308.05543v1</a>
                              </td>
                              <td>Deep Richardson-Lucy Deconvolution for Low-Light Image Deblurring</td>
                              <td>Liang Chen</td>
                              <td>2023-08-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_05543v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.05543v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_05355v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TCSloT: Text Guided 3D Context and Slope Aware Triple Network for Dental Implant Position Prediction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_05355v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_05355v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_05355v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In implant prosthesis treatment, the surgical guide of implant is used to ensure accurate implantation. However, such design heavily relies on the manual location of the implant position. When deep neural network has been proposed to assist the dentist in locating the implant position, most of them take a single slice as input, which do not fully explore 3D contextual information and ignoring the influence of implant slope. In this paper, we design a Text Guided 3D Context and Slope Aware Triple Network (TCSloT) which enables the perception of contextual information from multiple adjacent slices and awareness of variation of implant slopes. A Texture Variation Perception (TVP) module is correspondingly elaborated to process the multiple slices and capture the texture variation among slices and a Slope-Aware Loss (SAL) is proposed to dynamically assign varying weights for the regression head. Additionally, we design a conditional text guidance (CTG) module to integrate the text condition (i.e., left, middle and right) from the CLIP for assisting the implant position prediction. Extensive experiments on a dental implant dataset through five-fold cross-validation demonstrated that the proposed TCSloT achieves superior performance than existing methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_05355v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在植入假体的治疗中，使用植入物的手术引导器来确保准确植入。然而，这种设计在很大程度上依赖于植入物位置的手动定位。当深度神经网络被提出来帮助牙医定位植入物位置时，它们大多以单个切片作为输入，没有充分探索3D上下文信息，忽略了植入物斜率的影响。在本文中，我们设计了一个文本引导的3D上下文和斜率感知三重网络（TCSloT），该网络能够感知来自多个相邻切片的上下文信息并感知植入斜率的变化。相应地阐述了纹理变化感知（TVP）模块来处理多个切片并捕捉切片之间的纹理变化，并提出了斜率感知损失（SAL）来动态分配回归头的变化权重。此外，我们设计了一个条件文本引导（CTG）模块来整合CLIP的文本条件（即左、中、右），以帮助植入位置预测。通过五次交叉验证在牙科植入物数据集上进行的大量实验表明，所提出的TCSloT比现有方法具有更好的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.05355v1" target="_blank">2308.05355v1</a>
                              </td>
                              <td>TCSloT: Text Guided 3D Context and Slope Aware Triple Network for Dental Implant Position Prediction</td>
                              <td>Xinquan Yang</td>
                              <td>2023-08-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_05355v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.05355v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_07304v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLIP-Count: Towards Text-Guided Zero-Shot Object Counting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_07304v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_07304v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_07304v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in visual-language models have shown remarkable zero-shot text-image matching ability that is transferable to downstream tasks such as object detection and segmentation. Adapting these models for object counting, however, remains a formidable challenge. In this study, we first investigate transferring vision-language models (VLMs) for class-agnostic object counting. Specifically, we propose CLIP-Count, the first end-to-end pipeline that estimates density maps for open-vocabulary objects with text guidance in a zero-shot manner. To align the text embedding with dense visual features, we introduce a patch-text contrastive loss that guides the model to learn informative patch-level visual representations for dense prediction. Moreover, we design a hierarchical patch-text interaction module to propagate semantic information across different resolution levels of visual features. Benefiting from the full exploitation of the rich image-text alignment knowledge of pretrained VLMs, our method effectively generates high-quality density maps for objects-of-interest. Extensive experiments on FSC-147, CARPK, and ShanghaiTech crowd counting datasets demonstrate state-of-the-art accuracy and generalizability of the proposed method. Code is available: https://github.com/songrise/CLIP-Count.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_07304v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>可视化语言模型的最新进展显示出显著的零样本文本图像匹配能力，可转移到下游任务，如对象检测和分割。然而，将这些模型用于物体计数仍然是一项艰巨的挑战。在这项研究中，我们首先研究了用于类不可知对象计数的转移视觉语言模型（VLM）。具体来说，我们提出了CLIP-Count，这是第一个端到端管道，以零样本方式通过文本引导来估计开放词汇表对象的密度图。为了使文本嵌入与密集的视觉特征保持一致，我们引入了一种补丁文本对比损失，该损失指导模型学习密集预测的信息补丁级视觉表示。此外，我们设计了一个分层的补丁文本交互模块，以在不同分辨率的视觉特征之间传播语义信息。得益于充分利用预训练VLM丰富的图像-文本对齐知识，我们的方法有效地为感兴趣的对象生成了高质量的密度图。在FSC-147、CARPK和ShanghaiTech人群计数数据集上进行的大量实验表明，该方法具有最先进的准确性和可推广性。代码可用：https://github.com/songrise/CLIP-Count.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.07304v2" target="_blank">2305.07304v2</a>
                              </td>
                              <td>CLIP-Count: Towards Text-Guided Zero-Shot Object Counting</td>
                              <td>Ruixiang Jiang</td>
                              <td>2023-05-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_07304v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.07304v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_11029v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RemoteCLIP: A Vision Language Foundation Model for Remote Sensing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_11029v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_11029v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_11029v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>General-purpose foundation models have become increasingly important in the field of artificial intelligence. While self-supervised learning (SSL) and Masked Image Modeling (MIM) have led to promising results in building such foundation models for remote sensing, these models primarily learn low-level features, require annotated data for fine-tuning, and not applicable for retrieval and zero-shot applications due to the lack of language understanding. In response to these limitations, we propose RemoteCLIP, the first vision-language foundation model for remote sensing that aims to learn robust visual features with rich semantics, as well as aligned text embeddings for seamless downstream application. To address the scarcity of pre-training data, we leverage data scaling, converting heterogeneous annotations based on Box-to-Caption (B2C) and Mask-to-Box (M2B) conversion, and further incorporating UAV imagery, resulting a 12xlarger pretraining dataset. RemoteCLIP can be applied to a variety of downstream tasks, including zero-shot image classification, linear probing, k-NN classification, few-shot classification, image-text retrieval, and object counting. Evaluations on 16 datasets, including a newly introduced RemoteCount benchmark to test the object counting ability, show that RemoteCLIP consistently outperforms baseline foundation models across different model scales. Impressively, RemoteCLIP outperform previous SoTA by 9.14% mean recall on RSICD dataset and by 8.92% on RSICD dataset. For zero-shot classification, our RemoteCLIP outperform CLIP baseline by up to 6.39% average accuracy on 12 downstream datasets.Pretrained models is available at https://github.com/ChenDelong1999/RemoteCLIP .</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_11029v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>通用基础模型在人工智能领域变得越来越重要。虽然自监督学习（SSL）和掩蔽图像建模（MIM）在构建此类遥感基础模型方面取得了有希望的结果，但这些模型主要学习低级特征，需要注释数据进行微调，并且由于缺乏语言理解，不适用于检索和零样本应用。针对这些限制，我们提出了RemoteCLIP，这是第一个用于遥感的视觉语言基础模型，旨在学习具有丰富语义的健壮视觉特征，以及用于无缝下游应用的对齐文本嵌入。为了解决预训练数据的稀缺性，我们利用数据缩放，基于Box-To-Caption（B2C）和Mask-To-Box（M2B）转换转换异构注释，并进一步合并无人机图像，生成了一个12倍大的预训练数据集。RemoteCLIP可应用于各种下游任务，包括零样本图像分类、线性探测、k-NN分类、少拍摄分类、图像-文本检索和对象计数。对16个数据集的评估，包括新引入的RemoteCount基准测试对象计数能力，表明RemoteCLIP在不同的模型规模上始终优于基线基础模型。令人印象深刻的是，RemoteCLIP在RSICD数据集上的平均召回率比以前的SoTA高9.14%，在RSICD数据集上高8.92%。对于零样本分类，我们的RemoteCLIP在12个下游数据集上的平均准确率高达6.39%，优于CLIP基线https://github.com/ChenDelong1999/RemoteCLIP。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.11029v2" target="_blank">2306.11029v2</a>
                              </td>
                              <td>RemoteCLIP: A Vision Language Foundation Model for Remote Sensing</td>
                              <td>Fan Liu</td>
                              <td>2023-06-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_11029v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.11029v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2209_14408v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RALACs: Action Recognition in Autonomous Vehicles using Interaction Encoding and Optical Flow</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2209_14408v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2209_14408v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2209_14408v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>When applied to autonomous vehicle (AV) settings, action recognition can enhance an environment model's situational awareness. This is especially prevalent in scenarios where traditional geometric descriptions and heuristics in AVs are insufficient. However, action recognition has traditionally been studied for humans, and its limited adaptability to noisy, un-clipped, un-pampered, raw RGB data has limited its application in other fields. To push for the advancement and adoption of action recognition into AVs, this work proposes a novel two-stage action recognition system, termed RALACs. RALACs formulates the problem of action recognition for road scenes, and bridges the gap between it and the established field of human action recognition. This work shows how attention layers can be useful for encoding the relations across agents, and stresses how such a scheme can be class-agnostic. Furthermore, to address the dynamic nature of agents on the road, RALACs constructs a novel approach to adapting Region of Interest (ROI) Alignment to agent tracks for downstream action classification. Finally, our scheme also considers the problem of active agent detection, and utilizes a novel application of fusing optical flow maps to discern relevant agents in a road scene. We show that our proposed scheme can outperform the baseline on the ICCV2021 Road Challenge dataset and by deploying it on a real vehicle platform, we provide preliminary insight to the usefulness of action recognition in decision making.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2209_14408v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当应用于自动驾驶汽车（AV）设置时，动作识别可以增强环境模型的态势感知。这在AV中传统的几何描述和启发式方法不足的情况下尤其普遍。然而，动作识别传统上是为人类研究的，其对嘈杂、未修剪、未娇惯的原始RGB数据的有限适应性限制了其在其他领域的应用。为了推动动作识别在AV中的发展和采用，本工作提出了一种新的两阶段动作识别系统，称为RALAC。RALACs制定了道路场景的动作识别问题，并弥合了它与人类动作识别领域之间的差距。这项工作展示了注意力层如何对跨代理的关系进行编码，并强调了这种方案是如何与类无关的。此外，为了解决道路上代理人的动态性质，RARAC构建了一种新的方法，将感兴趣区域（ROI）对齐调整为代理人轨迹，用于下游行动分类。最后，我们的方案还考虑了主动代理检测的问题，并利用融合光流图的新应用来识别道路场景中的相关代理。我们表明，我们提出的方案可以在ICCV2021道路挑战数据集上优于基线，通过将其部署在实车平台上，我们对行动识别在决策中的有用性提供了初步见解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2209.14408v2" target="_blank">2209.14408v2</a>
                              </td>
                              <td>RALACs: Action Recognition in Autonomous Vehicles using Interaction Encoding and Optical Flow</td>
                              <td>Eddy Zhou</td>
                              <td>2022-09-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2209_14408v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2209.14408v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_05144v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Adapt and Align to Improve Zero-Shot Sketch-Based Image Retrieval</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_05144v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_05144v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_05144v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Zero-shot sketch-based image retrieval (ZS-SBIR) is challenging due to the cross-domain nature of sketches and photos, as well as the semantic gap between seen and unseen image distributions. Previous methods fine-tune pre-trained models with various side information and learning strategies to learn a compact feature space that is shared between the sketch and photo domains and bridges seen and unseen classes. However, these efforts are inadequate in adapting domains and transferring knowledge from seen to unseen classes. In this paper, we present an effective ``Adapt and Align'' approach to address the key challenges. Specifically, we insert simple and lightweight domain adapters to learn new abstract concepts of the sketch domain and improve cross-domain representation capabilities. Inspired by recent advances in image-text foundation models (e.g., CLIP) on zero-shot scenarios, we explicitly align the learned image embedding with a more semantic text embedding to achieve the desired knowledge transfer from seen to unseen classes. Extensive experiments on three benchmark datasets and two popular backbones demonstrate the superiority of our method in terms of retrieval accuracy and flexibility.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_05144v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>零样本基于草图的图像检索（ZS-SBIR）具有挑战性，这是由于草图和照片的跨域性质，以及可见和不可见图像分布之间的语义差距。以前的方法利用各种辅助信息和学习策略对预先训练的模型进行微调，以学习在草图和照片域以及可见和不可见类之间共享的紧凑特征空间。然而，这些努力在调整领域和将知识从看得见的课堂转移到看不见的课堂方面还不够。在本文中，我们提出了一种有效的“适应和协调”方法来应对关键挑战。具体来说，我们插入简单而轻量级的域适配器来学习草图域的新抽象概念，并提高跨域表示能力。受零样本场景下图像-文本基础模型（例如CLIP）最新进展的启发，我们明确地将学习的图像嵌入与更具语义的文本嵌入对齐，以实现从可见类到不可见类的期望知识转移。在三个基准数据集和两个流行主干上进行的大量实验证明了我们的方法在检索准确性和灵活性方面的优越性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.05144v3" target="_blank">2305.05144v3</a>
                              </td>
                              <td>Adapt and Align to Improve Zero-Shot Sketch-Based Image Retrieval</td>
                              <td>Shiyin Dong</td>
                              <td>2023-05-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_05144v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.05144v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_04828v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Seeing in Flowing: Adapting CLIP for Action Recognition with Motion Prompts Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_04828v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_04828v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_04828v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Contrastive Language-Image Pre-training (CLIP) has recently shown remarkable generalization on "zero-shot" training and has applied to many downstream tasks. We explore the adaptation of CLIP to achieve a more efficient and generalized action recognition method. We propose that the key lies in explicitly modeling the motion cues flowing in video frames. To that end, we design a two-stream motion modeling block to capture motion and spatial information at the same time. And then, the obtained motion cues are utilized to drive a dynamic prompts learner to generate motion-aware prompts, which contain much semantic information concerning human actions. In addition, we propose a multimodal communication block to achieve a collaborative learning and further improve the performance. We conduct extensive experiments on HMDB-51, UCF-101, and Kinetics-400 datasets. Our method outperforms most existing state-of-the-art methods by a significant margin on "few-shot" and "zero-shot" training. We also achieve competitive performance on "closed-set" training with extremely few trainable parameters and additional computational costs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_04828v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对比语言图像预训练（CLIP）最近在“零样本”训练方面表现出了显著的泛化，并已应用于许多下游任务。我们探索了CLIP的适应性，以实现一种更高效、更通用的动作识别方法。我们提出，关键在于对视频帧中流动的运动线索进行显式建模。为此，我们设计了一个双流运动建模块，以同时捕获运动和空间信息。然后，利用所获得的运动线索来驱动动态提示学习器生成运动感知提示，该提示包含大量关于人类动作的语义信息。此外，我们提出了一种多模式通信块，以实现协作学习并进一步提高性能。我们在HMDB-51、UCF-101和Kinetics-400数据集上进行了广泛的实验。我们的方法在“少热”和“零样本”训练方面显著优于大多数现有的最先进的方法。我们还在“闭集”训练中获得了有竞争力的性能，只需极少数可训练参数和额外的计算成本。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.04828v1" target="_blank">2308.04828v1</a>
                              </td>
                              <td>Seeing in Flowing: Adapting CLIP for Action Recognition with Motion Prompts Learning</td>
                              <td>Qiang Wang</td>
                              <td>2023-08-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_04828v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.04828v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_04617v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Improved Activation Clipping for Universal Backdoor Mitigation and Test-Time Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_04617v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_04617v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_04617v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Deep neural networks are vulnerable to backdoor attacks (Trojans), where an attacker poisons the training set with backdoor triggers so that the neural network learns to classify test-time triggers to the attacker's designated target class. Recent work shows that backdoor poisoning induces over-fitting (abnormally large activations) in the attacked model, which motivates a general, post-training clipping method for backdoor mitigation, i.e., with bounds on internal-layer activations learned using a small set of clean samples. We devise a new such approach, choosing the activation bounds to explicitly limit classification margins. This method gives superior performance against peer methods for CIFAR-10 image classification. We also show that this method has strong robustness against adaptive attacks, X2X attacks, and on different datasets. Finally, we demonstrate a method extension for test-time detection and correction based on the output differences between the original and activation-bounded networks. The code of our method is online available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_04617v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>深度神经网络容易受到后门攻击（特洛伊木马），攻击者用后门触发器毒害训练集，使神经网络学会将测试时间触发器分类到攻击者指定的目标类别。最近的工作表明，后门中毒会导致受攻击模型中的过度拟合（异常大的激活），这激发了一种用于后门缓解的通用训练后裁剪方法，即使用一小组干净样本学习内层激活的边界。我们设计了一种新的这种方法，选择激活边界来明确限制分类裕度。该方法在CIFAR-10图像分类中具有优于同行方法的性能。我们还表明，该方法对自适应攻击、X2X攻击和不同数据集具有较强的鲁棒性。最后，我们展示了一种基于原始和激活有界网络之间输出差异的测试时间检测和校正方法扩展。我们方法的代码在线提供。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.04617v1" target="_blank">2308.04617v1</a>
                              </td>
                              <td>Improved Activation Clipping for Universal Backdoor Mitigation and Test-Time Detection</td>
                              <td>Hang Wang</td>
                              <td>2023-08-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_04617v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.04617v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_04529v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Generating Modern Persian Carpet Map by Style-transfer</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_04529v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_04529v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_04529v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Today, the great performance of Deep Neural Networks(DNN) has been proven in various fields. One of its most attractive applications is to produce artistic designs. A carpet that is known as a piece of art is one of the most important items in a house, which has many enthusiasts all over the world. The first stage of producing a carpet is to prepare its map, which is a difficult, time-consuming, and expensive task. In this research work, our purpose is to use DNN for generating a Modern Persian Carpet Map. To reach this aim, three different DNN style transfer methods are proposed and compared against each other. In the proposed methods, the Style-Swap method is utilized to create the initial carpet map, and in the following, to generate more diverse designs, methods Clip-Styler, Gatys, and Style-Swap are used separately. In addition, some methods are examined and introduced for coloring the produced carpet maps. The designed maps are evaluated via the results of filled questionnaires where the outcomes of user evaluations confirm the popularity of generated carpet maps. Eventually, for the first time, intelligent methods are used in producing carpet maps, and it reduces human intervention. The proposed methods can successfully produce diverse carpet designs, and at a higher speed than traditional ways.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_04529v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>如今，深度神经网络（DNN）的出色性能已经在各个领域得到了证明。它最吸引人的应用之一是制作艺术设计。被称为艺术品的地毯是房子里最重要的物品之一，世界各地都有许多爱好者。制作地毯的第一阶段是准备地图，这是一项困难、耗时且昂贵的任务。在这项研究工作中，我们的目的是使用DNN生成现代波斯地毯地图。为了达到这个目的，提出了三种不同的DNN类型的转移方法，并进行了比较。在所提出的方法中，“样式交换”方法用于创建初始地毯贴图，而在以下方法中，为了生成更多样化的设计，“剪裁样式器”、“Gatys”和“样式切换”方法分别使用。此外，还介绍了一些绘制地毯图的着色方法。设计的地图通过填写问卷的结果进行评估，其中用户评估的结果证实了生成的地毯地图的受欢迎程度。最终，智能方法首次被用于制作地毯地图，减少了人为干预。所提出的方法可以成功地以比传统方法更高的速度产生不同的地毯设计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.04529v1" target="_blank">2308.04529v1</a>
                              </td>
                              <td>Generating Modern Persian Carpet Map by Style-transfer</td>
                              <td>Dorsa Rahmatian</td>
                              <td>2023-08-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_04529v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.04529v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09345v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Evaluating Data Attribution for Text-to-Image Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09345v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09345v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09345v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While large text-to-image models are able to synthesize "novel" images, these images are necessarily a reflection of the training data. The problem of data attribution in such models -- which of the images in the training set are most responsible for the appearance of a given generated image -- is a difficult yet important one. As an initial step toward this problem, we evaluate attribution through "customization" methods, which tune an existing large-scale model toward a given exemplar object or style. Our key insight is that this allows us to efficiently create synthetic images that are computationally influenced by the exemplar by construction. With our new dataset of such exemplar-influenced images, we are able to evaluate various data attribution algorithms and different possible feature spaces. Furthermore, by training on our dataset, we can tune standard models, such as DINO, CLIP, and ViT, toward the attribution problem. Even though the procedure is tuned towards small exemplar sets, we show generalization to larger sets. Finally, by taking into account the inherent uncertainty of the problem, we can assign soft attribution scores over a set of training images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09345v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>虽然大型文本到图像模型能够合成“新颖”的图像，但这些图像必然是训练数据的反映。这种模型中的数据归属问题——训练集中的哪些图像对给定生成图像的出现最负责任——是一个困难但重要的问题。作为解决这个问题的第一步，我们通过“定制”方法评估归因，该方法将现有的大规模模型调整为给定的示例对象或风格。我们的关键见解是，这使我们能够有效地创建合成图像，这些图像在计算上受到示例的影响。通过我们的新数据集，我们能够评估各种数据归因算法和不同的可能特征空间。此外，通过在数据集上进行训练，我们可以针对归因问题调整标准模型，如DINO、CLIP和ViT。尽管该过程是针对小样本集进行调整的，但我们显示了对大样本集的泛化。最后，通过考虑问题固有的不确定性，我们可以在一组训练图像上分配软归因分数。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09345v2" target="_blank">2306.09345v2</a>
                              </td>
                              <td>Evaluating Data Attribution for Text-to-Image Models</td>
                              <td>Sheng-Yu Wang</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09345v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09345v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_11661v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_11661v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_11661v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_11661v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have revolutionized visual representation learning by providing good performance on downstream datasets. VLMs are 0-shot adapted to a downstream dataset by designing prompts that are relevant to the dataset. Such prompt engineering makes use of domain expertise and a validation dataset. Meanwhile, recent developments in generative pretrained models like GPT-4 mean they can be used as advanced internet search tools. They can also be manipulated to provide visual information in any structure. In this work, we show that GPT-4 can be used to generate text that is visually descriptive and how this can be used to adapt CLIP to downstream tasks. We show considerable improvements in 0-shot transfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD (~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP's default prompt. We also design a simple few-shot adapter that learns to choose the best possible sentences to construct generalizable classifiers that outperform the recently proposed CoCoOP by ~2% on average and by over 4% on 4 specialized fine-grained datasets. The code, prompts, and auxiliary text dataset is available at https://github.com/mayug/VDT-Adapter.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_11661v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>像CLIP这样经过对比预训练的大型视觉语言模型（VLM）通过在下游数据集上提供良好的性能，彻底改变了视觉表示学习。VLM通过设计与数据集相关的提示来适应下游数据集。这种快速工程利用了领域专业知识和验证数据集。同时，GPT-4等生成性预训练模型的最新发展意味着它们可以用作高级互联网搜索工具。它们也可以被操纵以提供任何结构中的视觉信息。在这项工作中，我们展示了GPT-4可以用于生成视觉描述性的文本，以及如何使用它来调整CLIP以适应下游任务。与CLIP的默认提示相比，我们在专门的细粒度数据集（如EuroSAT（~7%）、DTD（-7%）、SUN397（~4.6%）和CUB（~3.3%））上显示出0镜头传输精度的显著提高。我们还设计了一个简单的少数镜头适配器，它可以学习选择尽可能好的句子来构建可推广的分类器，该分类器在4个专门的细粒度数据集上平均优于最近提出的CoCoOP约2%，超过4%。代码、提示和辅助文本数据集可在https://github.com/mayug/VDT-Adapter.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.11661v2" target="_blank">2307.11661v2</a>
                              </td>
                              <td>Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts</td>
                              <td>Mayug Maniparambil</td>
                              <td>2023-07-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_11661v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.11661v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_04249v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_04249v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_04249v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_04249v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Reconstructing visual stimuli from brain recordings has been a meaningful and challenging task. Especially, the achievement of precise and controllable image reconstruction bears great significance in propelling the progress and utilization of brain-computer interfaces. Despite the advancements in complex image reconstruction techniques, the challenge persists in achieving a cohesive alignment of both semantic (concepts and objects) and structure (position, orientation, and size) with the image stimuli. To address the aforementioned issue, we propose a two-stage image reconstruction model called MindDiffuser. In Stage 1, the VQ-VAE latent representations and the CLIP text embeddings decoded from fMRI are put into Stable Diffusion, which yields a preliminary image that contains semantic information. In Stage 2, we utilize the CLIP visual feature decoded from fMRI as supervisory information, and continually adjust the two feature vectors decoded in Stage 1 through backpropagation to align the structural information. The results of both qualitative and quantitative analyses demonstrate that our model has surpassed the current state-of-the-art models on Natural Scenes Dataset (NSD). The subsequent experimental findings corroborate the neurobiological plausibility of the model, as evidenced by the interpretability of the multimodal feature employed, which align with the corresponding brain responses.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_04249v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从大脑记录中重建视觉刺激是一项有意义且富有挑战性的任务。特别是实现精确可控的图像重建，对推动脑机接口的发展和利用具有重要意义。尽管复杂图像重建技术取得了进步，但在实现语义（概念和对象）和结构（位置、方向和大小）与图像刺激的一致性方面仍然存在挑战。为了解决上述问题，我们提出了一种称为MindDiffuser的两阶段图像重建模型。在阶段1中，从fMRI解码的VQ-VAE潜在表示和CLIP文本嵌入被放入稳定扩散中，这产生了包含语义信息的初步图像。在第2阶段，我们利用从fMRI解码的CLIP视觉特征作为监督信息，并通过反向传播不断调整在第1阶段解码的两个特征向量，以对齐结构信息。定性和定量分析的结果表明，我们的模型已经超过了目前最先进的自然场景数据集（NSD）模型。随后的实验结果证实了该模型的神经生物学合理性，所采用的多模式特征的可解释性证明了这一点，这些特征与相应的大脑反应一致。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.04249v1" target="_blank">2308.04249v1</a>
                              </td>
                              <td>MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion</td>
                              <td>Yizhuo Lu</td>
                              <td>2023-08-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_04249v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.04249v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_04118v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multimodal Color Recommendation in Vector Graphic Documents</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_04118v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_04118v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_04118v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Color selection plays a critical role in graphic document design and requires sufficient consideration of various contexts. However, recommending appropriate colors which harmonize with the other colors and textual contexts in documents is a challenging task, even for experienced designers. In this study, we propose a multimodal masked color model that integrates both color and textual contexts to provide text-aware color recommendation for graphic documents. Our proposed model comprises self-attention networks to capture the relationships between colors in multiple palettes, and cross-attention networks that incorporate both color and CLIP-based text representations. Our proposed method primarily focuses on color palette completion, which recommends colors based on the given colors and text. Additionally, it is applicable for another color recommendation task, full palette generation, which generates a complete color palette corresponding to the given text. Experimental results demonstrate that our proposed approach surpasses previous color palette completion methods on accuracy, color distribution, and user experience, as well as full palette generation methods concerning color diversity and similarity to the ground truth palettes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_04118v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>色彩选择在平面文档设计中起着至关重要的作用，需要充分考虑各种背景。然而，即使对于经验丰富的设计师来说，推荐合适的颜色来和文档中的其他颜色和文本上下文相协调也是一项具有挑战性的任务。在这项研究中，我们提出了一种多模式掩蔽颜色模型，该模型集成了颜色和文本上下文，为图形文档提供文本感知的颜色推荐。我们提出的模型包括用于捕捉多个调色板中颜色之间关系的自注意网络，以及包含基于颜色和CLIP的文本表示的交叉注意网络。我们提出的方法主要关注调色板完成，它根据给定的颜色和文本推荐颜色。此外，它适用于另一个颜色推荐任务，即完整调色板生成，该任务生成与给定文本相对应的完整调色板。实验结果表明，我们提出的方法在准确性、颜色分布和用户体验方面超过了以前的调色板完成方法，以及在颜色多样性和与真实调色板相似性方面的完整调色板生成方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.04118v1" target="_blank">2308.04118v1</a>
                              </td>
                              <td>Multimodal Color Recommendation in Vector Graphic Documents</td>
                              <td>Qianru Qiu</td>
                              <td>2023-08-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_04118v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.04118v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_04052v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Five-Dollar Model: Generating Game Maps and Sprites from Sentence Embeddings</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_04052v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_04052v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_04052v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The five-dollar model is a lightweight text-to-image generative architecture that generates low dimensional images from an encoded text prompt. This model can successfully generate accurate and aesthetically pleasing content in low dimensional domains, with limited amounts of training data. Despite the small size of both the model and datasets, the generated images are still able to maintain the encoded semantic meaning of the textual prompt. We apply this model to three small datasets: pixel art video game maps, video game sprite images, and down-scaled emoji images and apply novel augmentation strategies to improve the performance of our model on these limited datasets. We evaluate our models performance using cosine similarity score between text-image pairs generated by the CLIP VIT-B/32 model.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_04052v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>五美元模型是一种轻量级的文本到图像生成架构，它从编码的文本提示生成低维图像。该模型可以用有限的训练数据在低维领域成功地生成准确且美观的内容。尽管模型和数据集的大小都很小，但生成的图像仍然能够保持文本提示的编码语义。我们将该模型应用于三个小数据集：像素艺术视频游戏地图、视频游戏精灵图像和缩小的表情符号图像，并应用新的增强策略来提高我们的模型在这些有限数据集上的性能。我们使用CLIP VIT-B/32模型生成的文本图像对之间的余弦相似性得分来评估我们的模型性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.04052v1" target="_blank">2308.04052v1</a>
                              </td>
                              <td>The Five-Dollar Model: Generating Game Maps and Sprites from Sentence Embeddings</td>
                              <td>Timothy Merino</td>
                              <td>2023-08-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_04052v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.04052v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_03864v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Storyfier: Exploring Vocabulary Learning Support with Text Generation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_03864v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_03864v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_03864v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vocabulary learning support tools have widely exploited existing materials, e.g., stories or video clips, as contexts to help users memorize each target word. However, these tools could not provide a coherent context for any target words of learners' interests, and they seldom help practice word usage. In this paper, we work with teachers and students to iteratively develop Storyfier, which leverages text generation models to enable learners to read a generated story that covers any target words, conduct a story cloze test, and use these words to write a new story with adaptive AI assistance. Our within-subjects study (N=28) shows that learners generally favor the generated stories for connecting target words and writing assistance for easing their learning workload. However, in the read-cloze-write learning sessions, participants using Storyfier perform worse in recalling and using target words than learning with a baseline tool without our AI features. We discuss insights into supporting learning tasks with generative models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_03864v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>词汇学习支持工具广泛利用现有材料，如故事或视频剪辑，作为上下文来帮助用户记住每个目标单词。然而，这些工具无法为学习者感兴趣的任何目标词提供连贯的上下文，也很少有助于练习单词使用。在本文中，我们与教师和学生合作，迭代开发Storyfier，它利用文本生成模型，使学习者能够阅读生成的涵盖任何目标单词的故事，进行故事完形填空测试，并使用这些单词在自适应人工智能辅助下写一个新故事。我们的受试者内部研究（N=28）表明，学习者通常喜欢生成的故事来连接目标单词，并喜欢写作帮助来减轻他们的学习工作量。然而，在阅读完形填空-写作学习环节中，使用Storifer的参与者在回忆和使用目标词方面的表现比使用没有人工智能功能的基线工具学习更差。我们讨论了用生成模型支持学习任务的见解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.03864v1" target="_blank">2308.03864v1</a>
                              </td>
                              <td>Storyfier: Exploring Vocabulary Learning Support with Text Generation Models</td>
                              <td>Zhenhui Peng</td>
                              <td>2023-08-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_03864v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.03864v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_03821v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Distributionally Robust Classification on a Data Budget</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_03821v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_03821v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_03821v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Real world uses of deep learning require predictable model behavior under distribution shifts. Models such as CLIP show emergent natural distributional robustness comparable to humans, but may require hundreds of millions of training samples. Can we train robust learners in a domain where data is limited? To rigorously address this question, we introduce JANuS (Joint Annotations and Names Set), a collection of four new training datasets with images, labels, and corresponding captions, and perform a series of carefully controlled investigations of factors contributing to robustness in image classification, then compare those results to findings derived from a large-scale meta-analysis. Using this approach, we show that standard ResNet-50 trained with the cross-entropy loss on 2.4 million image samples can attain comparable robustness to a CLIP ResNet-50 trained on 400 million samples. To our knowledge, this is the first result showing (near) state-of-the-art distributional robustness on limited data budgets. Our dataset is available at \url{https://huggingface.co/datasets/penfever/JANuS_dataset}, and the code used to reproduce our experiments can be found at \url{https://github.com/penfever/vlhub/}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_03821v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>深度学习的实际应用需要在分布变化下的可预测模型行为。CLIP等模型显示出与人类相当的突发自然分布稳健性，但可能需要数亿个训练样本。我们能在数据有限的领域中训练健壮的学习者吗？为了严格解决这个问题，我们引入了JANuS（联合注释和名称集），这是一个由四个新的训练数据集组成的集合，包含图像、标签和相应的标题，并对有助于图像分类稳健性的因素进行了一系列精心控制的调查，然后将这些结果与大规模荟萃分析的结果进行比较。使用这种方法，我们表明，在240万个图像样本上用交叉熵损失训练的标准ResNet-50可以获得与在4亿个样本上训练的CLIP ResNet-50相当的鲁棒性。据我们所知，这是第一个在有限的数据预算上显示（接近）最先进的分布稳健性的结果。我们的数据集位于\url{https://huggingface.co/datasets/penfever/JANuS_dataset}，用于重现我们实验的代码可以在\url中找到{https://github.com/penfever/vlhub/}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.03821v1" target="_blank">2308.03821v1</a>
                              </td>
                              <td>Distributionally Robust Classification on a Data Budget</td>
                              <td>Benjamin Feuer</td>
                              <td>2023-08-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_03821v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.03821v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2210_13984v4_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Abductive Action Inference</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2210_13984v4_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2210_13984v4_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2210_13984v4_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Abductive reasoning aims to make the most likely inference for a given set of incomplete observations. In this paper, we introduce a novel research task known as "abductive action inference" which addresses the question of which actions were executed by a human to reach a specific state shown in a single snapshot. The research explores three key abductive inference problems: action set prediction, action sequence prediction, and abductive action verification. To tackle these challenging tasks, we investigate various models, including established ones such as Transformers, Graph Neural Networks, CLIP, BLIP, GPT3, end-to-end trained Slow-Fast, Resnet50-3D, and ViT models. Furthermore, the paper introduces several innovative models tailored for abductive action inference, including a relational graph neural network, a relational bilinear pooling model, a relational rule-based inference model, a relational GPT-3 prompt method, and a relational Transformer model. Notably, the newly proposed object-relational bilinear graph encoder-decoder (BiGED) model emerges as the most effective among all methods evaluated, demonstrating good proficiency in handling the intricacies of the Action Genome dataset. The contributions of this research offer significant progress toward comprehending the implications of human actions and making highly plausible inferences concerning the outcomes of these actions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2210_13984v4_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>外展推理旨在对给定的一组不完全观测做出最有可能的推断。在本文中，我们介绍了一项被称为“溯因动作推理”的新研究任务，该任务解决了人类执行哪些动作以达到单个快照中显示的特定状态的问题。该研究探讨了三个关键的溯因推理问题：动作集预测、动作序列预测和溯因动作验证。为了解决这些具有挑战性的任务，我们研究了各种模型，包括已建立的模型，如Transformers、Graph Neural Networks、CLIP、BLIP、GPT3、端到端训练的Slow-Fast、Resnet50-3D和ViT模型。此外，本文还介绍了几种为溯因动作推理量身定制的创新模型，包括关系图神经网络、关系双线性池模型、基于关系规则的推理模型、关系GPT-3提示方法和关系Transformer模型。值得注意的是，新提出的对象关系双线性图编码器-解码器（BiGED）模型是所有评估方法中最有效的，在处理动作基因组数据集的复杂性方面表现出了良好的熟练度。这项研究的贡献在理解人类行为的含义和对这些行为的结果做出高度可信的推断方面取得了重大进展。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2210.13984v4" target="_blank">2210.13984v4</a>
                              </td>
                              <td>Abductive Action Inference</td>
                              <td>Clement Tan</td>
                              <td>2022-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2210_13984v4_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2210.13984v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_03471v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Deepfake Detection: A Comparative Analysis</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_03471v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_03471v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_03471v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper present a comprehensive comparative analysis of supervised and self-supervised models for deepfake detection. We evaluate eight supervised deep learning architectures and two transformer-based models pre-trained using self-supervised strategies (DINO, CLIP) on four benchmarks (FakeAVCeleb, CelebDF-V2, DFDC, and FaceForensics++). Our analysis includes intra-dataset and inter-dataset evaluations, examining the best performing models, generalisation capabilities, and impact of augmentations. We also investigate the trade-off between model size and performance. Our main goal is to provide insights into the effectiveness of different deep learning architectures (transformers, CNNs), training strategies (supervised, self-supervised), and deepfake detection benchmarks. These insights can help guide the development of more accurate and reliable deepfake detection systems, which are crucial in mitigating the harmful impact of deepfakes on individuals and society.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_03471v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文对深度伪造检测的监督和自监督模型进行了全面的比较分析。我们在四个基准测试（FakeAVCeleb、CelebDF-V2、DFDC和FaceForensics++）上评估了八种有监督的深度学习架构和两个使用自监督策略（DINO、CLIP）预训练的基于转换器的模型。我们的分析包括数据集内和数据集间评估，检查性能最佳的模型、泛化能力和增强的影响。我们还研究了模型大小和性能之间的权衡。我们的主要目标是深入了解不同深度学习架构（transformer、CNNs）、训练策略（监督、自监督）和深度伪造检测基准的有效性。这些见解有助于指导开发更准确可靠的深度伪造检测系统，这对于减轻深度伪造对个人和社会的有害影响至关重要。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.03471v1" target="_blank">2308.03471v1</a>
                              </td>
                              <td>Deepfake Detection: A Comparative Analysis</td>
                              <td>Sohail Ahmed Khan</td>
                              <td>2023-08-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_03471v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.03471v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_11418v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FaceCLIPNeRF: Text-driven 3D Face Manipulation using Deformable Neural Radiance Fields</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_11418v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_11418v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_11418v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>As recent advances in Neural Radiance Fields (NeRF) have enabled high-fidelity 3D face reconstruction and novel view synthesis, its manipulation also became an essential task in 3D vision. However, existing manipulation methods require extensive human labor, such as a user-provided semantic mask and manual attribute search unsuitable for non-expert users. Instead, our approach is designed to require a single text to manipulate a face reconstructed with NeRF. To do so, we first train a scene manipulator, a latent code-conditional deformable NeRF, over a dynamic scene to control a face deformation using the latent code. However, representing a scene deformation with a single latent code is unfavorable for compositing local deformations observed in different instances. As so, our proposed Position-conditional Anchor Compositor (PAC) learns to represent a manipulated scene with spatially varying latent codes. Their renderings with the scene manipulator are then optimized to yield high cosine similarity to a target text in CLIP embedding space for text-driven manipulation. To the best of our knowledge, our approach is the first to address the text-driven manipulation of a face reconstructed with NeRF. Extensive results, comparisons, and ablation studies demonstrate the effectiveness of our approach.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_11418v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着神经辐射场（NeRF）的最新进展，实现了高保真三维人脸重建和新颖的视图合成，其操作也成为三维视觉中的一项重要任务。然而，现有的操作方法需要大量的人力，例如用户提供的语义掩码和不适合非专家用户的手动属性搜索。相反，我们的方法被设计为需要单个文本来操作用NeRF重建的人脸。为此，我们首先在动态场景上训练场景操纵器，即潜在代码条件可变形NeRF，以使用潜在代码控制面部变形。然而，用单个潜在代码表示场景变形对于合成在不同实例中观察到的局部变形是不利的。因此，我们提出的位置条件锚合成器（PAC）学习用空间变化的潜在代码来表示被操纵的场景。然后对它们与场景操纵器的渲染进行优化，以在CLIP嵌入空间中产生与目标文本的高余弦相似性，用于文本驱动的操纵。据我们所知，我们的方法是第一个解决用NeRF重建的人脸的文本驱动操作的方法。大量的结果、比较和消融研究证明了我们方法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>46</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.11418v2" target="_blank">2307.11418v2</a>
                              </td>
                              <td>FaceCLIPNeRF: Text-driven 3D Face Manipulation using Deformable Neural Radiance Fields</td>
                              <td>Sungwon Hwang</td>
                              <td>2023-07-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_11418v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.11418v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="DINO"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2211_12860v5_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DETRs with Collaborative Hybrid Assignments Training</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_12860v5_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_12860v5_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_12860v5_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we provide the observation that too few queries assigned as positive samples in DETR with one-to-one set matching leads to sparse supervision on the encoder's output which considerably hurt the discriminative feature learning of the encoder and vice visa for attention learning in the decoder. To alleviate this, we present a novel collaborative hybrid assignments training scheme, namely $\mathcal{C}$o-DETR, to learn more efficient and effective DETR-based detectors from versatile label assignment manners. This new training scheme can easily enhance the encoder's learning ability in end-to-end detectors by training the multiple parallel auxiliary heads supervised by one-to-many label assignments such as ATSS and Faster RCNN. In addition, we conduct extra customized positive queries by extracting the positive coordinates from these auxiliary heads to improve the training efficiency of positive samples in the decoder. In inference, these auxiliary heads are discarded and thus our method introduces no additional parameters and computational cost to the original detector while requiring no hand-crafted non-maximum suppression (NMS). We conduct extensive experiments to evaluate the effectiveness of the proposed approach on DETR variants, including DAB-DETR, Deformable-DETR, and DINO-Deformable-DETR. The state-of-the-art DINO-Deformable-DETR with Swin-L can be improved from 58.5% to 59.5% AP on COCO val. Surprisingly, incorporated with ViT-L backbone, we achieve 66.0% AP on COCO test-dev and 67.9% AP on LVIS val, outperforming previous methods by clear margins with much fewer model sizes. Codes are available at \url{https://github.com/Sense-X/Co-DETR}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_12860v5_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们观察到，在具有一对一集匹配的DETR中，被分配为正样本的查询太少，导致对编码器输出的稀疏监督，这大大损害了编码器的判别特征学习，反之亦然。为了缓解这种情况，我们提出了一种新的协作混合任务训练方案，即$\mathcal｛C｝$o-DETR，以从通用的标签分配方式中学习更高效、更有效的基于DETR的检测器。这种新的训练方案可以通过训练由一对多标签分配（如ATSS和Faster RCNN）监督的多个并行辅助头，轻松增强编码器在端到端检测器中的学习能力。此外，我们通过从这些辅助头中提取正坐标来进行额外定制的正查询，以提高解码器中正样本的训练效率。在推断中，这些辅助头被丢弃，因此我们的方法在不需要手工制作的非最大值抑制（NMS）的同时，没有给原始检测器引入额外的参数和计算成本。我们进行了广泛的实验来评估所提出的方法对DETR变体的有效性，包括DAB-DETR、可变形DETR和DINO可变形DETER。最先进的带Swin-L的DINO可变形DETR在COCO val上的AP可以从58.5%提高到59.5%。令人惊讶的是，与ViT-L主干相结合，我们在COCO测试开发上实现了66.0%的AP，在LVIS val上实现了67.9%的AP，以更小的模型尺寸明显优于以前的方法。代码位于\url{https://github.com/Sense-X/Co-DETR}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.12860v5" target="_blank">2211.12860v5</a>
                              </td>
                              <td>DETRs with Collaborative Hybrid Assignments Training</td>
                              <td>Zhuofan Zong</td>
                              <td>2022-11-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_12860v5_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.12860v5" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_04589v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Temporal DINO: A Self-supervised Video Strategy to Enhance Action Prediction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_04589v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_04589v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_04589v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The emerging field of action prediction plays a vital role in various computer vision applications such as autonomous driving, activity analysis and human-computer interaction. Despite significant advancements, accurately predicting future actions remains a challenging problem due to high dimensionality, complex dynamics and uncertainties inherent in video data. Traditional supervised approaches require large amounts of labelled data, which is expensive and time-consuming to obtain. This paper introduces a novel self-supervised video strategy for enhancing action prediction inspired by DINO (self-distillation with no labels). The Temporal-DINO approach employs two models; a 'student' processing past frames; and a 'teacher' processing both past and future frames, enabling a broader temporal context. During training, the teacher guides the student to learn future context by only observing past frames. The strategy is evaluated on ROAD dataset for the action prediction downstream task using 3D-ResNet, Transformer, and LSTM architectures. The experimental results showcase significant improvements in prediction performance across these architectures, with our method achieving an average enhancement of 9.9% Precision Points (PP), highlighting its effectiveness in enhancing the backbones' capabilities of capturing long-term dependencies. Furthermore, our approach demonstrates efficiency regarding the pretraining dataset size and the number of epochs required. This method overcomes limitations present in other approaches, including considering various backbone architectures, addressing multiple prediction horizons, reducing reliance on hand-crafted augmentations, and streamlining the pretraining process into a single stage. These findings highlight the potential of our approach in diverse video-based tasks such as activity recognition, motion planning, and scene understanding.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_04589v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>动作预测这一新兴领域在自动驾驶、活动分析和人机交互等各种计算机视觉应用中发挥着至关重要的作用。尽管取得了重大进展，但由于视频数据的高维性、复杂的动态性和不确定性，准确预测未来行动仍然是一个具有挑战性的问题。传统的监督方法需要大量的标记数据，获取这些数据既昂贵又耗时。本文介绍了一种受DINO（无标签自蒸馏）启发的用于增强动作预测的新的自监督视频策略。时态DINO方法采用两种模型：；处理过去帧的“学生”；“老师”同时处理过去和未来的框架，实现更广泛的时间背景。在训练过程中，教师只通过观察过去的框架来引导学生学习未来的语境。该策略在ROAD数据集上使用3D ResNet、Transformer和LSTM架构进行评估，用于行动预测下游任务。实验结果表明，在这些架构中，预测性能有了显著提高，我们的方法平均提高了9.9%的精度点（PP），突出了其在增强主干捕获长期依赖性能力方面的有效性。此外，我们的方法证明了在预训练数据集大小和所需时期数量方面的效率。该方法克服了其他方法中存在的局限性，包括考虑各种骨干架构，解决多个预测范围，减少对手工增强的依赖，以及将预训练过程简化为单个阶段。这些发现突出了我们的方法在各种基于视频的任务中的潜力，如活动识别、运动规划和场景理解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.04589v1" target="_blank">2308.04589v1</a>
                              </td>
                              <td>Temporal DINO: A Self-supervised Video Strategy to Enhance Action Prediction</td>
                              <td>Izzeddin Teeti</td>
                              <td>2023-08-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_04589v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.04589v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09345v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Evaluating Data Attribution for Text-to-Image Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09345v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09345v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09345v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While large text-to-image models are able to synthesize "novel" images, these images are necessarily a reflection of the training data. The problem of data attribution in such models -- which of the images in the training set are most responsible for the appearance of a given generated image -- is a difficult yet important one. As an initial step toward this problem, we evaluate attribution through "customization" methods, which tune an existing large-scale model toward a given exemplar object or style. Our key insight is that this allows us to efficiently create synthetic images that are computationally influenced by the exemplar by construction. With our new dataset of such exemplar-influenced images, we are able to evaluate various data attribution algorithms and different possible feature spaces. Furthermore, by training on our dataset, we can tune standard models, such as DINO, CLIP, and ViT, toward the attribution problem. Even though the procedure is tuned towards small exemplar sets, we show generalization to larger sets. Finally, by taking into account the inherent uncertainty of the problem, we can assign soft attribution scores over a set of training images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09345v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>虽然大型文本到图像模型能够合成“新颖”的图像，但这些图像必然是训练数据的反映。这种模型中的数据归属问题——训练集中的哪些图像对给定生成图像的出现最负责任——是一个困难但重要的问题。作为解决这个问题的第一步，我们通过“定制”方法评估归因，该方法将现有的大规模模型调整为给定的示例对象或风格。我们的关键见解是，这使我们能够有效地创建合成图像，这些图像在计算上受到示例的影响。通过我们的新数据集，我们能够评估各种数据归因算法和不同的可能特征空间。此外，通过在数据集上进行训练，我们可以针对归因问题调整标准模型，如DINO、CLIP和ViT。尽管该过程是针对小样本集进行调整的，但我们显示了对大样本集的泛化。最后，通过考虑问题固有的不确定性，我们可以在一组训练图像上分配软归因分数。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09345v2" target="_blank">2306.09345v2</a>
                              </td>
                              <td>Evaluating Data Attribution for Text-to-Image Models</td>
                              <td>Sheng-Yu Wang</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09345v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09345v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_03747v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Mask Frozen-DETR: High Quality Instance Segmentation with One GPU</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_03747v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_03747v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_03747v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we aim to study how to build a strong instance segmenter with minimal training time and GPUs, as opposed to the majority of current approaches that pursue more accurate instance segmenter by building more advanced frameworks at the cost of longer training time and higher GPU requirements. To achieve this, we introduce a simple and general framework, termed Mask Frozen-DETR, which can convert any existing DETR-based object detection model into a powerful instance segmentation model. Our method only requires training an additional lightweight mask network that predicts instance masks within the bounding boxes given by a frozen DETR-based object detector. Remarkably, our method outperforms the state-of-the-art instance segmentation method Mask DINO in terms of performance on the COCO test-dev split (55.3% vs. 54.7%) while being over 10X times faster to train. Furthermore, all of our experiments can be trained using only one Tesla V100 GPU with 16 GB of memory, demonstrating the significant efficiency of our proposed framework.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_03747v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们的目标是研究如何用最少的训练时间和GPU构建一个强大的实例分割器，而不是目前的大多数方法，这些方法通过构建更先进的框架来追求更准确的实例分割，而代价是更长的训练时间和更高的GPU要求。为了实现这一点，我们引入了一个简单通用的框架，称为Mask Frozen DETR，它可以将任何现有的基于DETR的对象检测模型转换为强大的实例分割模型。我们的方法只需要训练一个额外的轻量级掩码网络，该网络预测基于冻结DETR的对象检测器给出的边界框内的实例掩码。值得注意的是，我们的方法在COCO测试开发拆分方面的性能优于最先进的实例分割方法Mask DINO（55.3%对54.7%），同时训练速度快了10倍以上。此外，我们所有的实验都可以只使用一个具有16GB内存的特斯拉V100 GPU进行训练，这证明了我们提出的框架的显著效率。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.03747v1" target="_blank">2308.03747v1</a>
                              </td>
                              <td>Mask Frozen-DETR: High Quality Instance Segmentation with One GPU</td>
                              <td>Zhanhao Liang</td>
                              <td>2023-08-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_03747v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.03747v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_03471v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Deepfake Detection: A Comparative Analysis</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_03471v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_03471v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_03471v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper present a comprehensive comparative analysis of supervised and self-supervised models for deepfake detection. We evaluate eight supervised deep learning architectures and two transformer-based models pre-trained using self-supervised strategies (DINO, CLIP) on four benchmarks (FakeAVCeleb, CelebDF-V2, DFDC, and FaceForensics++). Our analysis includes intra-dataset and inter-dataset evaluations, examining the best performing models, generalisation capabilities, and impact of augmentations. We also investigate the trade-off between model size and performance. Our main goal is to provide insights into the effectiveness of different deep learning architectures (transformers, CNNs), training strategies (supervised, self-supervised), and deepfake detection benchmarks. These insights can help guide the development of more accurate and reliable deepfake detection systems, which are crucial in mitigating the harmful impact of deepfakes on individuals and society.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_03471v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文对深度伪造检测的监督和自监督模型进行了全面的比较分析。我们在四个基准测试（FakeAVCeleb、CelebDF-V2、DFDC和FaceForensics++）上评估了八种有监督的深度学习架构和两个使用自监督策略（DINO、CLIP）预训练的基于转换器的模型。我们的分析包括数据集内和数据集间评估，检查性能最佳的模型、泛化能力和增强的影响。我们还研究了模型大小和性能之间的权衡。我们的主要目标是深入了解不同深度学习架构（transformer、CNNs）、训练策略（监督、自监督）和深度伪造检测基准的有效性。这些见解有助于指导开发更准确可靠的深度伪造检测系统，这对于减轻深度伪造对个人和社会的有害影响至关重要。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.03471v1" target="_blank">2308.03471v1</a>
                              </td>
                              <td>Deepfake Detection: A Comparative Analysis</td>
                              <td>Sohail Ahmed Khan</td>
                              <td>2023-08-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_03471v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.03471v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_11067v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CNOS: A Strong Baseline for CAD-based Novel Object Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_11067v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_11067v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_11067v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a simple three-stage approach to segment unseen objects in RGB images using their CAD models. Leveraging recent powerful foundation models, DINOv2 and Segment Anything, we create descriptors and generate proposals, including binary masks for a given input RGB image. By matching proposals with reference descriptors created from CAD models, we achieve precise object ID assignment along with modal masks. We experimentally demonstrate that our method achieves state-of-the-art results in CAD-based novel object segmentation, surpassing existing approaches on the seven core datasets of the BOP challenge by 19.8% AP using the same BOP evaluation protocol. Our source code is available at https://github.com/nv-nguyen/cnos.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_11067v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种简单的三阶段方法，使用RGB图像中的CAD模型来分割看不见的对象。利用最近强大的基础模型DINOv2和Segment Anything，我们创建描述符并生成建议，包括给定输入RGB图像的二进制掩码。通过将方案与从CAD模型创建的参考描述符相匹配，我们实现了精确的对象ID分配以及模式掩码。我们通过实验证明，我们的方法在基于CAD的新对象分割中取得了最先进的结果，使用相同的BOP评估协议，在BOP挑战的七个核心数据集上超过了现有方法19.8%AP。我们的源代码可在https://github.com/nv-nguyen/cnos.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.11067v2" target="_blank">2307.11067v2</a>
                              </td>
                              <td>CNOS: A Strong Baseline for CAD-based Novel Object Segmentation</td>
                              <td>Van Nguyen Nguyen</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_11067v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.11067v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2006_01236v5_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Aperiodicity, Star-freeness, and First-order Logic Definability of Structured Context-Free Languages</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2006_01236v5_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2006_01236v5_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2006_01236v5_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A classic result in formal language theory is the equivalence among non-counting, or aperiodic, regular languages, and languages defined through star-free regular expressions, or first-order logic. Past attempts to extend this result beyond the realm of regular languages have met with difficulties: for instance it is known that star-free tree languages may violate the non-counting property and there are aperiodic tree languages that cannot be defined through first-order logic. We extend such classic equivalence results to a significant family of deterministic context-free languages, the operator-precedence languages (OPL), which strictly includes the widely investigated visibly pushdown, alias input-driven, family and other structured context-free languages. The OP model originated in the '60s for defining programming languages and is still used by high performance compilers; its rich algebraic properties have been investigated initially in connection with grammar learning and recently completed with further closure properties and with monadic second order logic definition. We introduce an extension of regular expressions, the OP-expressions (OPE) which define the OPLs and, under the star-free hypothesis, define first-order definable and non-counting OPLs. Then, we prove, through a fairly articulated grammar transformation, that aperiodic OPLs are first-order definable. Thus, the classic equivalence of star-freeness, aperiodicity, and first-order definability is established for the large and powerful class of OPLs. We argue that the same approach can be exploited to obtain analogous results for visibly pushdown languages too.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2006_01236v5_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>形式语言理论中的一个经典结果是非计数或非周期的正则语言与通过无星正则表达式或一阶逻辑定义的语言之间的等价。过去试图将这一结果扩展到正则语言之外的尝试遇到了困难：例如，已知无星树语言可能违反不计数性质，并且存在无法通过一阶逻辑定义的非周期树语言。我们将这种经典等价结果扩展到一个重要的确定上下文无关语言家族，即算子优先语言（OPL），它严格包括广泛研究的可见下推、别名输入驱动、家族和其他结构化上下文无关语言。OP模型起源于60年代，用于定义编程语言，目前仍被高性能编译器使用；它丰富的代数性质最初是在语法学习中研究的，最近又完成了进一步的闭包性质和一元二阶逻辑定义。我们引入了正则表达式的一个扩展，即OP表达式（OPE），它定义了OPL，并且在无星假设下，定义了一阶可定义和不计数的OPL。然后，我们通过一个相当清晰的语法转换证明了非周期OPL是一阶可定义的。因此，对于大而有力的OPL类，建立了星自由度、非周期性和一阶可定义性的经典等价性。我们认为，同样的方法也可以用于获得明显下推语言的类似结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2006.01236v5" target="_blank">2006.01236v5</a>
                              </td>
                              <td>Aperiodicity, Star-freeness, and First-order Logic Definability of Structured Context-Free Languages</td>
                              <td>Dino Mandrioli</td>
                              <td>2020-06-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2006_01236v5_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2006.01236v5" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10907v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Role of Entropy and Reconstruction in Multi-View Self-Supervised Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10907v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10907v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10907v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The mechanisms behind the success of multi-view self-supervised learning (MVSSL) are not yet fully understood. Contrastive MVSSL methods have been studied through the lens of InfoNCE, a lower bound of the Mutual Information (MI). However, the relation between other MVSSL methods and MI remains unclear. We consider a different lower bound on the MI consisting of an entropy and a reconstruction term (ER), and analyze the main MVSSL families through its lens. Through this ER bound, we show that clustering-based methods such as DeepCluster and SwAV maximize the MI. We also re-interpret the mechanisms of distillation-based approaches such as BYOL and DINO, showing that they explicitly maximize the reconstruction term and implicitly encourage a stable entropy, and we confirm this empirically. We show that replacing the objectives of common MVSSL methods with this ER bound achieves competitive performance, while making them stable when training with smaller batch sizes or smaller exponential moving average (EMA) coefficients.   Github repo: https://github.com/apple/ml-entropy-reconstruction.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10907v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多视角自我监督学习（MVSSL）成功背后的机制尚不完全清楚。通过互信息（MI）的下界InfoNCE的视角研究了MVSSL的对比方法。然而，其他MVSSL方法与MI之间的关系仍不清楚。我们考虑了由熵和重建项（ER）组成的MI的不同下界，并通过其透镜分析了主要的MVSSL族。通过这个ER界，我们证明了基于聚类的方法，如DeepCluster和SwAV，最大化了MI。我们还重新解释了基于蒸馏的方法（如BYOL和DINO）的机制，表明它们显式地最大化了重建项，隐式地鼓励了稳定的熵，我们从经验上证实了这一点。我们表明，用该ER界取代常见MVSSL方法的目标可以获得有竞争力的性能，同时在使用较小的批量或较小的指数移动平均（EMA）系数进行训练时使其稳定。Github回购：https://github.com/apple/ml-entropy-reconstruction.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10907v1" target="_blank">2307.10907v1</a>
                              </td>
                              <td>The Role of Entropy and Reconstruction in Multi-View Self-Supervised Learning</td>
                              <td>Borja Rodríguez-Gálvez</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10907v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10907v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03376v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Weakly-supervised Contrastive Learning for Unsupervised Object Discovery</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03376v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03376v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03376v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Unsupervised object discovery (UOD) refers to the task of discriminating the whole region of objects from the background within a scene without relying on labeled datasets, which benefits the task of bounding-box-level localization and pixel-level segmentation. This task is promising due to its ability to discover objects in a generic manner. We roughly categorise existing techniques into two main directions, namely the generative solutions based on image resynthesis, and the clustering methods based on self-supervised models. We have observed that the former heavily relies on the quality of image reconstruction, while the latter shows limitations in effectively modeling semantic correlations. To directly target at object discovery, we focus on the latter approach and propose a novel solution by incorporating weakly-supervised contrastive learning (WCL) to enhance semantic information exploration. We design a semantic-guided self-supervised learning model to extract high-level semantic features from images, which is achieved by fine-tuning the feature encoder of a self-supervised model, namely DINO, via WCL. Subsequently, we introduce Principal Component Analysis (PCA) to localize object regions. The principal projection direction, corresponding to the maximal eigenvalue, serves as an indicator of the object region(s). Extensive experiments on benchmark unsupervised object discovery datasets demonstrate the effectiveness of our proposed solution. The source code and experimental results are publicly available via our project page at https://github.com/npucvr/WSCUOD.git.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03376v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>无监督对象发现（UOD）是指在不依赖标记数据集的情况下，从场景内的背景中区分整个对象区域的任务，这有利于边界框级定位和像素级分割的任务。这项任务很有前景，因为它能够以通用的方式发现对象。我们将现有技术大致分为两个主要方向，即基于图像再合成的生成解决方案和基于自监督模型的聚类方法。我们观察到，前者在很大程度上依赖于图像重建的质量，而后者在有效建模语义相关性方面表现出局限性。为了直接针对对象发现，我们专注于后一种方法，并通过结合弱监督对比学习（WCL）来增强语义信息探索，提出了一种新的解决方案。我们设计了一个语义引导的自监督学习模型来从图像中提取高级语义特征，这是通过WCL微调自监督模型（即DINO）的特征编码器来实现的。随后，我们引入主成分分析（PCA）来定位对象区域。与最大特征值相对应的主投影方向用作对象区域的指示符。在基准无监督对象发现数据集上进行的大量实验证明了我们提出的解决方案的有效性。源代码和实验结果可通过我们的项目页面公开获取，网址为https://github.com/npucvr/WSCUOD.git.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03376v1" target="_blank">2307.03376v1</a>
                              </td>
                              <td>Weakly-supervised Contrastive Learning for Unsupervised Object Discovery</td>
                              <td>Yunqiu Lv</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03376v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03376v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_08069v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DETRs Beat YOLOs on Real-time Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_08069v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_08069v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_08069v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, end-to-end transformer-based detectors~(DETRs) have achieved remarkable performance. However, the issue of the high computational cost of DETRs has not been effectively addressed, limiting their practical application and preventing them from fully exploiting the benefits of no post-processing, such as non-maximum suppression (NMS). In this paper, we first analyze the influence of NMS in modern real-time object detectors on inference speed, and establish an end-to-end speed benchmark. To avoid the inference delay caused by NMS, we propose a Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge. Specifically, we design an efficient hybrid encoder to efficiently process multi-scale features by decoupling the intra-scale interaction and cross-scale fusion, and propose IoU-aware query selection to improve the initialization of object queries. In addition, our proposed detector supports flexibly adjustment of the inference speed by using different decoder layers without the need for retraining, which facilitates the practical application of real-time object detectors. Our RT-DETR-L achieves 53.0% AP on COCO val2017 and 114 FPS on T4 GPU, while RT-DETR-X achieves 54.8% AP and 74 FPS, outperforming all YOLO detectors of the same scale in both speed and accuracy. Furthermore, our RT-DETR-R50 achieves 53.1% AP and 108 FPS, outperforming DINO-Deformable-DETR-R50 by 2.2% AP in accuracy and by about 21 times in FPS. ource code and pre-trained models are available at https://github.com/lyuwenyu/RT-DETR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_08069v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，基于变压器的端到端检测器（DETR）取得了显著的性能。然而，DETR的高计算成本问题尚未得到有效解决，这限制了它们的实际应用，并使它们无法充分利用无后处理的好处，例如非最大值抑制（NMS）。本文首先分析了现代实时对象检测器中NMS对推理速度的影响，并建立了端到端速度基准。为了避免NMS引起的推理延迟，我们提出了一种实时检测TRansformer（RT-DETR），这是我们所知的第一个实时端到端对象检测器。具体而言，我们设计了一种高效的混合编码器，通过解耦尺度内交互和跨尺度融合来高效处理多尺度特征，并提出了IoU感知查询选择，以提高对象查询的初始化能力。此外，我们提出的检测器支持通过使用不同的解码器层来灵活调整推理速度，而不需要重新训练，这有助于实时对象检测器的实际应用。我们的RT-DETR-L在COCO val2017上实现了53.0%的AP，在T4 GPU上实现了114 FPS，而RT-DETR-X实现了54.8%的AP和74 FPS，在速度和精度方面都优于相同规模的所有YOLO检测器。此外，我们的RT-DETR-R50实现了53.1%的AP和108 FPS，在精度上比DINO-Deformable-DETR-R5高出2.2%的AP，在FPS上高出约21倍。源代码和预训练模型可在https://github.com/lyuwenyu/RT-DETR.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.08069v2" target="_blank">2304.08069v2</a>
                              </td>
                              <td>DETRs Beat YOLOs on Real-time Object Detection</td>
                              <td>Wenyu Lv</td>
                              <td>2023-04-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_08069v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.08069v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_06211v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_06211v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_06211v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_06211v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segment anything model (SAM) developed by Meta AI Research has recently attracted significant attention. Trained on a large segmentation dataset of over 1 billion masks, SAM is capable of segmenting any object on a certain image. In the original SAM work, the authors turned to zero-short transfer tasks (like edge detection) for evaluating the performance of SAM. Recently, numerous works have attempted to investigate the performance of SAM in various scenarios to recognize and segment objects. Moreover, numerous projects have emerged to show the versatility of SAM as a foundation model by combining it with other models, like Grounding DINO, Stable Diffusion, ChatGPT, etc. With the relevant papers and projects increasing exponentially, it is challenging for the readers to catch up with the development of SAM. To this end, this work conducts the first yet comprehensive survey on SAM. This is an ongoing project and we intend to update the manuscript on a regular basis. Therefore, readers are welcome to contact us if they complete new works related to SAM so that we can include them in our next version.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_06211v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Meta AI Research开发的分段任意模型（SAM）最近引起了人们的极大关注。SAM在超过10亿个掩码的大型分割数据集上进行训练，能够分割特定图像上的任何对象。在最初的SAM工作中，作者转向零短转移任务（如边缘检测）来评估SAM的性能。最近，许多工作试图研究SAM在各种场景中的性能，以识别和分割对象。此外，通过将SAM与其他模型相结合，如Grounding DINO、Stable Diffusion、ChatGPT等，已经出现了许多项目来展示SAM作为基础模型的多功能性。随着相关论文和项目呈指数级增长，读者很难跟上SAM的发展。为此，本工作首次对SAM进行了全面的调查。这是一个正在进行的项目，我们打算定期更新手稿。因此，如果读者完成了与SAM相关的新作品，欢迎与我们联系，以便我们将其纳入下一版本。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.06211v3" target="_blank">2306.06211v3</a>
                              </td>
                              <td>A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering</td>
                              <td>Chaoning Zhang</td>
                              <td>2023-05-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_06211v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.06211v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09165v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DEYOv2: Rank Feature with Greedy Matching for End-to-End Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09165v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09165v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09165v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a novel object detector called DEYOv2, an improved version of the first-generation DEYO (DETR with YOLO) model. DEYOv2, similar to its predecessor, DEYOv2 employs a progressive reasoning approach to accelerate model training and enhance performance. The study delves into the limitations of one-to-one matching in optimization and proposes solutions to effectively address the issue, such as Rank Feature and Greedy Matching. This approach enables the third stage of DEYOv2 to maximize information acquisition from the first and second stages without needing NMS, achieving end-to-end optimization. By combining dense queries, sparse queries, one-to-many matching, and one-to-one matching, DEYOv2 leverages the advantages of each method. It outperforms all existing query-based end-to-end detectors under the same settings. When using ResNet-50 as the backbone and multi-scale features on the COCO dataset, DEYOv2 achieves 51.1 AP and 51.8 AP in 12 and 24 epochs, respectively. Compared to the end-to-end model DINO, DEYOv2 provides significant performance gains of 2.1 AP and 1.4 AP in the two epoch settings. To the best of our knowledge, DEYOv2 is the first fully end-to-end object detector that combines the respective strengths of classical detectors and query-based detectors.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09165v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种称为DEYOv2的新型物体检测器，它是第一代DEYO（DETR with YOLO）模型的改进版本。与前代类似，DEYOv2采用渐进式推理方法来加速模型训练并提高性能。该研究深入探讨了一对一匹配在优化中的局限性，并提出了有效解决该问题的解决方案，如秩特征和贪婪匹配。这种方法使DEYOv2的第三阶段能够最大限度地从第一和第二阶段获取信息，而无需NMS，实现端到端优化。通过组合密集查询、稀疏查询、一对多匹配和一对一匹配，DEYOv2充分利用了每种方法的优势。在相同设置下，它的性能优于所有现有的基于查询的端到端检测器。当在COCO数据集上使用ResNet-50作为主干和多尺度特征时，DEYOv2在12个和24个时期分别实现了51.1个AP和51.8个AP。与端到端模型DINO相比，DEYOv2在两个历元设置中提供了2.1 AP和1.4 AP的显著性能提升。据我们所知，DEYOv2是第一个完全端到端的对象检测器，它结合了经典检测器和基于查询的检测器的各自优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09165v2" target="_blank">2306.09165v2</a>
                              </td>
                              <td>DEYOv2: Rank Feature with Greedy Matching for End-to-End Object Detection</td>
                              <td>Haodong Ouyang</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09165v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09165v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_15472v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Taming Detection Transformers for Medical Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_15472v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_15472v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_15472v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The accurate detection of suspicious regions in medical images is an error-prone and time-consuming process required by many routinely performed diagnostic procedures. To support clinicians during this difficult task, several automated solutions were proposed relying on complex methods with many hyperparameters. In this study, we investigate the feasibility of DEtection TRansformer (DETR) models for volumetric medical object detection. In contrast to previous works, these models directly predict a set of objects without relying on the design of anchors or manual heuristics such as non-maximum-suppression to detect objects. We show by conducting extensive experiments with three models, namely DETR, Conditional DETR, and DINO DETR on four data sets (CADA, RibFrac, KiTS19, and LIDC) that these set prediction models can perform on par with or even better than currently existing methods. DINO DETR, the best-performing model in our experiments demonstrates this by outperforming a strong anchor-based one-stage detector, Retina U-Net, on three out of four data sets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_15472v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>准确检测医学图像中的可疑区域是一个容易出错且耗时的过程，这是许多常规诊断程序所要求的。为了在这项艰巨的任务中为临床医生提供支持，提出了几种基于具有许多超参数的复杂方法的自动化解决方案。在这项研究中，我们研究了DEtection TRansformer（DETR）模型用于体积医疗对象检测的可行性。与之前的工作相比，这些模型直接预测一组对象，而不依赖于锚的设计或手动启发式（如非最大值抑制）来检测对象。我们通过在四个数据集（CADA、RibFrac、KiTS19和LIDC）上对三个模型（即DETR、Conditional DETR和DINO DETR）进行广泛的实验表明，这些集合预测模型的性能可以与当前现有的方法相当，甚至更好。DINO DETR，我们实验中性能最好的模型，通过在四分之三的数据集上优于基于强锚的一级检测器Retina U-Net，证明了这一点。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.15472v1" target="_blank">2306.15472v1</a>
                              </td>
                              <td>Taming Detection Transformers for Medical Object Detection</td>
                              <td>Marc K. Ickler</td>
                              <td>2023-06-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_15472v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.15472v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_13723v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Social AI and the Challenges of the Human-AI Ecosystem</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_13723v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_13723v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_13723v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The rise of large-scale socio-technical systems in which humans interact with artificial intelligence (AI) systems (including assistants and recommenders, in short AIs) multiplies the opportunity for the emergence of collective phenomena and tipping points, with unexpected, possibly unintended, consequences. For example, navigation systems' suggestions may create chaos if too many drivers are directed on the same route, and personalised recommendations on social media may amplify polarisation, filter bubbles, and radicalisation. On the other hand, we may learn how to foster the "wisdom of crowds" and collective action effects to face social and environmental challenges. In order to understand the impact of AI on socio-technical systems and design next-generation AIs that team with humans to help overcome societal problems rather than exacerbate them, we propose to build the foundations of Social AI at the intersection of Complex Systems, Network Science and AI. In this perspective paper, we discuss the main open questions in Social AI, outlining possible technical and scientific challenges and suggesting research avenues.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_13723v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人类与人工智能（AI）系统（包括助手和推荐人，简称AI）互动的大规模社会技术系统的兴起，增加了集体现象和临界点出现的机会，带来了意想不到的、可能是无意的后果。例如，如果太多司机在同一条路线上行驶，导航系统的建议可能会造成混乱，而社交媒体上的个性化建议可能会放大两极分化、过滤泡沫和激进化。另一方面，我们可以学习如何培养“群体智慧”和集体行动效应，以应对社会和环境挑战。为了理解人工智能对社会技术系统的影响，并设计下一代人工智能，与人类合作，帮助克服而不是加剧社会问题，我们建议在复杂系统、网络科学和人工智能的交叉点上建立社会人工智能的基础，概述可能的技术和科学挑战，并提出研究途径。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.13723v1" target="_blank">2306.13723v1</a>
                              </td>
                              <td>Social AI and the Challenges of the Human-AI Ecosystem</td>
                              <td>Dino Pedreschi</td>
                              <td>2023-06-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_13723v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.13723v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_13337v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Patch-Level Contrasting without Patch Correspondence for Accurate and Dense Contrastive Representation Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_13337v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_13337v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_13337v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose ADCLR: A ccurate and D ense Contrastive Representation Learning, a novel self-supervised learning framework for learning accurate and dense vision representation. To extract spatial-sensitive information, ADCLR introduces query patches for contrasting in addition with global contrasting. Compared with previous dense contrasting methods, ADCLR mainly enjoys three merits: i) achieving both global-discriminative and spatial-sensitive representation, ii) model-efficient (no extra parameters in addition to the global contrasting baseline), and iii) correspondence-free and thus simpler to implement. Our approach achieves new state-of-the-art performance for contrastive methods. On classification tasks, for ViT-S, ADCLR achieves 77.5% top-1 accuracy on ImageNet with linear probing, outperforming our baseline (DINO) without our devised techniques as plug-in, by 0.5%. For ViT-B, ADCLR achieves 79.8%, 84.0% accuracy on ImageNet by linear probing and finetune, outperforming iBOT by 0.3%, 0.2% accuracy. For dense tasks, on MS-COCO, ADCLR achieves significant improvements of 44.3% AP on object detection, 39.7% AP on instance segmentation, outperforming previous SOTA method SelfPatch by 2.2% and 1.2%, respectively. On ADE20K, ADCLR outperforms SelfPatch by 1.0% mIoU, 1.2% mAcc on the segme</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_13337v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的自监督学习框架ADCLR：准确度和密集度对比表示学习，用于学习准确和密集的视觉表示。为了提取空间敏感信息，ADCLR除了引入全局对比之外，还引入了用于对比的查询补丁。与以前的密集对比方法相比，ADCLR主要有三个优点：i）同时实现全局判别和空间敏感表示，ii）模型有效（除了全局对比基线之外没有额外的参数），以及iii）无对应，因此实现更简单。我们的方法为对比方法实现了最先进的性能。在分类任务方面，对于ViT-S，ADCLR在使用线性探测的ImageNet上实现了77.5%的前1级准确率，比没有我们设计的技术作为插件的基线（DINO）高出0.5%。对于ViT-B，ADCLR通过线性探测和微调在ImageNet上分别实现了79.8%和84.0%的准确率，分别比iBOT高出0.3%和0.2%的准确率。对于密集任务，在MS-COCO上，ADCLR在对象检测上实现了44.3%的AP，在实例分割上实现了39.7%的AP，分别比以前的SOTA方法SelfPatch提高了2.2%和1.2%。在ADE20K上，ADCLR比SelfPatch高出1.0%mIoU，在segme上高出1.2%mAcc</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.13337v1" target="_blank">2306.13337v1</a>
                              </td>
                              <td>Patch-Level Contrasting without Patch Correspondence for Accurate and Dense Contrastive Representation Learning</td>
                              <td>Shaofeng Zhang</td>
                              <td>2023-06-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_13337v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.13337v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09346v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rosetta Neurons: Mining the Common Units in a Model Zoo</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09346v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09346v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09346v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Do different neural networks, trained for various vision tasks, share some common representations? In this paper, we demonstrate the existence of common features we call "Rosetta Neurons" across a range of models with different architectures, different tasks (generative and discriminative), and different types of supervision (class-supervised, text-supervised, self-supervised). We present an algorithm for mining a dictionary of Rosetta Neurons across several popular vision models: Class Supervised-ResNet50, DINO-ResNet50, DINO-ViT, MAE, CLIP-ResNet50, BigGAN, StyleGAN-2, StyleGAN-XL. Our findings suggest that certain visual concepts and structures are inherently embedded in the natural world and can be learned by different models regardless of the specific task or architecture, and without the use of semantic labels. We can visualize shared concepts directly due to generative models included in our analysis. The Rosetta Neurons facilitate model-to-model translation enabling various inversion-based manipulations, including cross-class alignments, shifting, zooming, and more, without the need for specialized training.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09346v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>为各种视觉任务训练的不同神经网络是否共享一些共同的表示？在本文中，我们在一系列具有不同架构、不同任务（生成和判别）和不同类型监督（类监督、文本监督、自监督）的模型中证明了我们称之为“罗塞塔神经元”的共同特征的存在。我们提出了一种在几种流行的视觉模型中挖掘罗塞塔神经元字典的算法：Class Supervisored-ResNet50、DINO-ResNet50、DINO-ViT、MAE、CLIP-ResNet50，BigGAN、StyleGAN-2、StyleGAN-XL。我们的研究结果表明，某些视觉概念和结构固有地嵌入在自然世界中，无论具体任务或架构如何，都可以通过不同的模型学习，而无需使用语义标签。由于我们的分析中包含了生成模型，我们可以直接可视化共享概念。罗塞塔神经元促进了模型到模型的翻译，实现了各种基于反转的操作，包括跨类对齐、移位、缩放等，而无需专门训练。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09346v2" target="_blank">2306.09346v2</a>
                              </td>
                              <td>Rosetta Neurons: Mining the Common Units in a Model Zoo</td>
                              <td>Amil Dravid</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09346v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09346v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_06588v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DEYO: DETR with YOLO for Step-by-Step Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_06588v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_06588v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_06588v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Object detection is an important topic in computer vision, with post-processing, an essential part of the typical object detection pipeline, posing a significant bottleneck affecting the performance of traditional object detection models. The detection transformer (DETR), as the first end-to-end target detection model, discards the requirement of manual components like the anchor and non-maximum suppression (NMS), significantly simplifying the target detection process. However, compared with most traditional object detection models, DETR converges very slowly, and a query's meaning is obscure. Thus, inspired by the Step-by-Step concept, this paper proposes a new two-stage object detection model, named DETR with YOLO (DEYO), which relies on a progressive inference to solve the above problems. DEYO is a two-stage architecture comprising a classic target detection model and a DETR-like model as the first and second stages, respectively. Specifically, the first stage provides high-quality query and anchor feeding into the second stage, improving the performance and efficiency of the second stage compared to the original DETR model. Meanwhile, the second stage compensates for the performance degradation caused by the first stage detector's limitations. Extensive experiments demonstrate that DEYO attains 50.6 AP and 52.1 AP in 12 and 36 epochs, respectively, while utilizing ResNet-50 as the backbone and multi-scale features on the COCO dataset. Compared with DINO, an optimal DETR-like model, the developed DEYO model affords a significant performance improvement of 1.6 AP and 1.2 AP in two epoch settings.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_06588v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>目标检测是计算机视觉中的一个重要课题，后处理是典型目标检测流水线的重要组成部分，对传统目标检测模型的性能造成了严重的瓶颈。检测转换器（DETR）作为第一个端到端目标检测模型，摒弃了锚和非最大值抑制（NMS）等手动组件的要求，大大简化了目标检测过程。然而，与大多数传统的对象检测模型相比，DETR收敛非常慢，并且查询的含义是模糊的。因此，受分步概念的启发，本文提出了一种新的两阶段目标检测模型，称为DETR with YOLO（DEYO），该模型依靠渐进推理来解决上述问题。DEYO是两阶段架构，包括分别作为第一和第二阶段的经典目标检测模型和类DETR模型。具体而言，第一阶段向第二阶段提供高质量的查询和锚馈送，与原始DETR模型相比，提高了第二阶段的性能和效率。同时，第二级补偿由第一级检测器的限制引起的性能下降。大量实验表明，DEYO在12个和36个时期分别达到50.6个AP和52.1个AP，同时利用ResNet-50作为COCO数据集上的主干和多尺度特征。与DINO（一种类似DETR的最佳模型）相比，所开发的DEYO模型在两个历元设置中提供了1.6 AP和1.2 AP的显著性能改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.06588v3" target="_blank">2211.06588v3</a>
                              </td>
                              <td>DEYO: DETR with YOLO for Step-by-Step Object Detection</td>
                              <td>Haodong Ouyang</td>
                              <td>2022-11-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_06588v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.06588v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_07483v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Semi-supervised learning made simple with self-supervised clustering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_07483v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_07483v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_07483v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised learning models have been shown to learn rich visual representations without requiring human annotations. However, in many real-world scenarios, labels are partially available, motivating a recent line of work on semi-supervised methods inspired by self-supervised principles. In this paper, we propose a conceptually simple yet empirically powerful approach to turn clustering-based self-supervised methods such as SwAV or DINO into semi-supervised learners. More precisely, we introduce a multi-task framework merging a supervised objective using ground-truth labels and a self-supervised objective relying on clustering assignments with a single cross-entropy loss. This approach may be interpreted as imposing the cluster centroids to be class prototypes. Despite its simplicity, we provide empirical evidence that our approach is highly effective and achieves state-of-the-art performance on CIFAR100 and ImageNet.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_07483v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督学习模型已被证明可以在不需要人工注释的情况下学习丰富的视觉表示。然而，在许多现实世界的场景中，标签是部分可用的，这激发了受自我监督原则启发的半监督方法的最新工作。在本文中，我们提出了一种概念简单但经验强大的方法，将基于聚类的自监督方法（如SwAV或DINO）转变为半监督学习者。更准确地说，我们引入了一个多任务框架，将使用基本事实标签的监督目标和依赖于具有单个交叉熵损失的聚类分配的自监督目标合并在一起。这种方法可以被解释为将集群质心强加为类原型。尽管它很简单，但我们提供的经验证据表明，我们的方法非常有效，并在CIFAR100和ImageNet上实现了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.07483v1" target="_blank">2306.07483v1</a>
                              </td>
                              <td>Semi-supervised learning made simple with self-supervised clustering</td>
                              <td>Enrico Fini</td>
                              <td>2023-06-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_07483v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.07483v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_05382v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Automatic Image Blending Algorithm Based on SAM and DINO</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_05382v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_05382v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_05382v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The field of image blending has gained popularity in recent years for its ability to create visually stunning content. However, the current image blending algorithm has the following problems: 1) The manual creation of the image blending mask requires a lot of manpower and material resources; 2) The image blending algorithm cannot effectively solve the problems of brightness distortion and low resolution. To this end, we propose a new image blending method: it combines semantic object detection and segmentation with corresponding mask generation to automatically blend images, while a two-stage iterative algorithm based on our proposed new saturation loss and PAN algorithm to fix brightness distortion and low resolution issues. Results on publicly available datasets show that our method outperforms many classic image blending algorithms on various performance metrics such as PSNR and SSIM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_05382v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，图像融合领域因其能够创建视觉上令人惊叹的内容而广受欢迎。然而，目前的图像混合算法存在以下问题：1）手动创建图像混合掩模需要大量的人力和物力；2） 图像混合算法不能有效地解决亮度失真和分辨率低的问题。为此，我们提出了一种新的图像混合方法：它将语义对象检测和分割与相应的掩模生成相结合来自动混合图像，而基于我们提出的新的饱和度损失和PAN算法的两阶段迭代算法来解决亮度失真和低分辨率问题。在公开数据集上的结果表明，我们的方法在各种性能指标（如PSNR和SSIM）上优于许多经典的图像混合算法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.05382v2" target="_blank">2306.05382v2</a>
                              </td>
                              <td>Automatic Image Blending Algorithm Based on SAM and DINO</td>
                              <td>Haochen Xue</td>
                              <td>2023-06-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_05382v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.05382v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_06203v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FLSL: Feature-level Self-supervised Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_06203v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_06203v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_06203v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Current self-supervised learning (SSL) methods (e.g., SimCLR, DINO, VICReg, MOCOv3) target primarily on representations at instance level and do not generalize well to dense prediction tasks, such as object detection and segmentation. Towards aligning SSL with dense predictions, this paper demonstrates for the first time the underlying mean-shift clustering process of Vision Transformers (ViT), which aligns well with natural image semantics (e.g., a world of objects and stuffs). By employing transformer for joint embedding and clustering, we propose a two-level feature clustering SSL method, coined Feature-Level Self-supervised Learning (FLSL). We present the formal definition of the FLSL problem and construct the objectives from the mean-shift and k-means perspectives. We show that FLSL promotes remarkable semantic cluster representations and learns an embedding scheme amenable to intra-view and inter-view feature clustering. Experiments show that FLSL yields significant improvements in dense prediction tasks, achieving 44.9 (+2.8)% AP and 46.5% AP in object detection, as well as 40.8 (+2.3)% AP and 42.1% AP in instance segmentation on MS-COCO, using Mask R-CNN with ViT-S/16 and ViT-S/8 as backbone, respectively. FLSL consistently outperforms existing SSL methods across additional benchmarks, including UAV object detection on UAVDT, and video instance segmentation on DAVIS 2017. We conclude by presenting visualization and various ablation studies to better 20 understand the success of FLSL.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_06203v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当前的自监督学习（SSL）方法（例如，SimCLR、DINO、VICReg、MOCOv3）主要针对实例级别的表示，并且不能很好地推广到密集预测任务，例如对象检测和分割。为了使SSL与密集预测相一致，本文首次演示了视觉变换器（ViT）的基本均值偏移聚类过程，该过程与自然图像语义（例如，对象和填充物的世界）非常一致。通过使用transformer进行联合嵌入和聚类，我们提出了一种两级特征聚类SSL方法，称为特征级自监督学习（FLSL）。我们给出了FLSL问题的形式化定义，并从均值偏移和k-均值的角度构建了目标。我们表明，FLSL促进了显著的语义聚类表示，并学习了一种适用于视图内和视图间特征聚类的嵌入方案。实验表明，使用以ViT-S/16和ViT-S/8为骨干的Mask R-CNN，FLSL在密集预测任务中产生了显著的改进，在MS-COCO上分别实现了44.9（+2.8）%AP和46.5%AP，在实例分割中实现了40.8（+2.3）%AP，42.1%AP。FLSL在其他基准测试中始终优于现有的SSL方法，包括UAVDT上的无人机对象检测和DAVIS 2017上的视频实例分割。最后，我们介绍了可视化和各种消融研究，以更好地了解FLSL的成功。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.06203v1" target="_blank">2306.06203v1</a>
                              </td>
                              <td>FLSL: Feature-level Self-supervised Learning</td>
                              <td>Qing Su</td>
                              <td>2023-06-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_06203v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.06203v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2110_15444v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">10 Security and Privacy Problems in Large Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2110_15444v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2110_15444v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2110_15444v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Foundation models--such as GPT, CLIP, and DINO--have achieved revolutionary progress in the past several years and are commonly believed to be a promising approach for general-purpose AI. In particular, self-supervised learning is adopted to pre-train a foundation model using a large amount of unlabeled data. A pre-trained foundation model is like an ``operating system'' of the AI ecosystem. Specifically, a foundation model can be used as a feature extractor for many downstream tasks with little or no labeled training data. Existing studies on foundation models mainly focused on pre-training a better foundation model to improve its performance on downstream tasks in non-adversarial settings, leaving its security and privacy in adversarial settings largely unexplored. A security or privacy issue of a pre-trained foundation model leads to a single point of failure for the AI ecosystem. In this book chapter, we discuss 10 basic security and privacy problems for the pre-trained foundation models, including six confidentiality problems, three integrity problems, and one availability problem. For each problem, we discuss potential opportunities and challenges. We hope our book chapter will inspire future research on the security and privacy of foundation models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2110_15444v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基础模型，如GPT、CLIP和DINO，在过去几年中取得了革命性的进展，通常被认为是通用人工智能的一种很有前途的方法。特别是，采用自我监督学习来使用大量未标记数据预训练基础模型。预先训练的基础模型就像人工智能生态系统的“操作系统”。具体而言，基础模型可以用作许多下游任务的特征提取器，这些任务很少或没有标记的训练数据。现有的基础模型研究主要集中在预训练一个更好的基础模型，以提高其在非对抗性环境中下游任务的性能，而其在对抗性环境下的安全性和隐私性在很大程度上没有得到探索。预先训练的基础模型的安全或隐私问题会导致人工智能生态系统的单点故障。在本书的章节中，我们讨论了预训练的基础模型的10个基本安全和隐私问题，包括6个机密性问题、3个完整性问题和1个可用性问题。对于每个问题，我们都会讨论潜在的机遇和挑战。我们希望本书的章节将启发未来对基金会模型的安全性和隐私性的研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2110.15444v3" target="_blank">2110.15444v3</a>
                              </td>
                              <td>10 Security and Privacy Problems in Large Foundation Models</td>
                              <td>Jinyuan Jia</td>
                              <td>2021-10-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2110_15444v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2110.15444v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04675v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04675v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04675v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04675v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We systematically study a wide variety of image-based generative models spanning semantically-diverse datasets to understand and improve the feature extractors and metrics used to evaluate them. Using best practices in psychophysics, we measure human perception of image realism for generated samples by conducting the largest experiment evaluating generative models to date, and find that no existing metric strongly correlates with human evaluations. Comparing to 16 modern metrics for evaluating the overall performance, fidelity, diversity, and memorization of generative models, we find that the state-of-the-art perceptual realism of diffusion models as judged by humans is not reflected in commonly reported metrics such as FID. This discrepancy is not explained by diversity in generated samples, though one cause is over-reliance on Inception-V3. We address these flaws through a study of alternative self-supervised feature extractors, find that the semantic information encoded by individual networks strongly depends on their training procedure, and show that DINOv2-ViT-L/14 allows for much richer evaluation of generative models. Next, we investigate data memorization, and find that generative models do memorize training examples on simple, smaller datasets like CIFAR10, but not necessarily on more complex datasets like ImageNet. However, our experiments show that current metrics do not properly detect memorization; none in the literature is able to separate memorization from other phenomena such as underfitting or mode shrinkage. To facilitate further development of generative models and their evaluation we release all generated image datasets, human evaluation data, and a modular library to compute 16 common metrics for 8 different encoders at https://github.com/layer6ai-labs/dgm-eval.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04675v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们系统地研究了各种基于图像的生成模型，这些模型跨越语义不同的数据集，以理解和改进用于评估它们的特征提取器和度量。利用心理物理学中的最佳实践，我们通过对生成模型进行迄今为止最大规模的评估实验，测量了人类对生成样本的图像真实感的感知，并发现没有任何现有指标与人类评估密切相关。与用于评估生成模型的整体性能、保真度、多样性和记忆的16个现代指标相比，我们发现，人类判断的扩散模型的最先进的感知真实性没有反映在常见的指标中，如FID。这种差异并不能用生成样本的多样性来解释，尽管其中一个原因是过度依赖Inception-V3。我们通过对替代自监督特征提取器的研究来解决这些缺陷，发现单个网络编码的语义信息在很大程度上取决于它们的训练过程，并表明DINOv2-ViT-L/14允许对生成模型进行更丰富的评估。接下来，我们研究了数据记忆，发现生成模型确实在像CIFAR10这样的简单、较小的数据集上记忆训练示例，但不一定在像ImageNet这样的更复杂数据集上。然而，我们的实验表明，目前的指标并不能正确地检测记忆；文献中没有一个能够将记忆与其他现象（如填充不足或模式收缩）区分开来。为了促进生成模型及其评估的进一步发展，我们发布了所有生成的图像数据集、人类评估数据和模块库，以计算8个不同编码器的16个通用度量https://github.com/layer6ai-labs/dgm-eval.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04675v1" target="_blank">2306.04675v1</a>
                              </td>
                              <td>Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models</td>
                              <td>George Stein</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04675v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04675v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03881v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Emergent Correspondence from Image Diffusion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03881v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03881v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03881v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Finding correspondences between images is a fundamental problem in computer vision. In this paper, we show that correspondence emerges in image diffusion models without any explicit supervision. We propose a simple strategy to extract this implicit knowledge out of diffusion networks as image features, namely DIffusion FeaTures (DIFT), and use them to establish correspondences between real images. Without any additional fine-tuning or supervision on the task-specific data or annotations, DIFT is able to outperform both weakly-supervised methods and competitive off-the-shelf features in identifying semantic, geometric, and temporal correspondences. Particularly for semantic correspondence, DIFT from Stable Diffusion is able to outperform DINO and OpenCLIP by 19 and 14 accuracy points respectively on the challenging SPair-71k benchmark. It even outperforms the state-of-the-art supervised methods on 9 out of 18 categories while remaining on par for the overall performance. Project page: https://diffusionfeatures.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03881v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>寻找图像之间的对应关系是计算机视觉中的一个基本问题。在本文中，我们证明了在没有任何明确监督的情况下，图像扩散模型中会出现对应关系。我们提出了一种简单的策略来从扩散网络中提取这种隐含的知识作为图像特征，即diffusion features（DIFT），并使用它们来建立真实图像之间的对应关系。在没有对特定任务的数据或注释进行任何额外的微调或监督的情况下，DIFT能够在识别语义、几何和时间对应性方面优于弱监督方法和有竞争力的现成特征。特别是在语义对应方面，来自Stable Diffusion的DIFT能够在具有挑战性的SPair 71k基准上分别比DINO和OpenCLIP高出19和14个准确度点。它甚至在18个类别中的9个类别上优于最先进的监督方法，同时在总体性能上保持标准。项目页面：https://diffusionfeatures.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03881v1" target="_blank">2306.03881v1</a>
                              </td>
                              <td>Emergent Correspondence from Image Diffusion</td>
                              <td>Luming Tang</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03881v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03881v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04654v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DenseDINO: Boosting Dense Self-Supervised Learning with Token-Based Point-Level Consistency</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04654v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04654v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04654v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we propose a simple yet effective transformer framework for self-supervised learning called DenseDINO to learn dense visual representations. To exploit the spatial information that the dense prediction tasks require but neglected by the existing self-supervised transformers, we introduce point-level supervision across views in a novel token-based way. Specifically, DenseDINO introduces some extra input tokens called reference tokens to match the point-level features with the position prior. With the reference token, the model could maintain spatial consistency and deal with multi-object complex scene images, thus generalizing better on dense prediction tasks. Compared with the vanilla DINO, our approach obtains competitive performance when evaluated on classification in ImageNet and achieves a large margin (+7.2% mIoU) improvement in semantic segmentation on PascalVOC under the linear probing protocol for segmentation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04654v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了一个简单而有效的自监督学习转换器框架，称为DenseDINO，用于学习密集的视觉表示。为了利用密集预测任务所需但被现有的自监督变换器忽略的空间信息，我们以一种新颖的基于令牌的方式引入了跨视图的点级监督。具体来说，DenseDINO引入了一些称为参考标记的额外输入标记，以将点级特征与位置先验相匹配。有了参考标记，该模型可以保持空间一致性，处理多目标复杂场景图像，从而更好地推广到密集预测任务中。与普通的DINO相比，当在ImageNet中对分类进行评估时，我们的方法获得了有竞争力的性能，并且在用于分割的线性探测协议下，在PascalVOC上的语义分割方面实现了大幅度的改进（+7.2%mIoU）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04654v1" target="_blank">2306.04654v1</a>
                              </td>
                              <td>DenseDINO: Boosting Dense Self-Supervised Learning with Token-Based Point-Level Consistency</td>
                              <td>Yike Yuan</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04654v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04654v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_07598v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RHINO: Rotated DETR with Dynamic Denoising via Hungarian Matching for Oriented Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_07598v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_07598v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_07598v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With the publication of DINO, a variant of the Detection Transformer (DETR), Detection Transformers are breaking the record in the object detection benchmark with the merits of their end-to-end design and scalability. However, the extension of DETR to oriented object detection has not been thoroughly studied although more benefits from its end-to-end architecture are expected such as removing NMS and anchor-related costs. In this paper, we propose a first strong DINO-based baseline for oriented object detection. We found that straightforward employment of DETRs for oriented object detection does not guarantee non-duplicate prediction, and propose a simple cost to mitigate this. Furthermore, we introduce a $\textit{dynamic denoising}$ strategy that uses Hungarian matching to filter redundant noised queries and $\textit{query alignment}$ to preserve matching consistency between Transformer decoder layers. Our proposed model outperforms previous rotated DETRs and other counterparts, achieving state-of-the-art performance in DOTA-v1.0/v1.5/v2.0, and DIOR-R benchmarks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_07598v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着检测变压器（DETR）的变体DINO的发布，检测变压器凭借其端到端设计和可扩展性的优点打破了对象检测基准的记录。然而，DETR向面向对象检测的扩展尚未得到彻底研究，尽管预计其端到端架构会带来更多好处，例如消除NMS和锚相关成本。在本文中，我们提出了第一个基于强DINO的面向对象检测基线。我们发现，直接使用DETR进行定向对象检测并不能保证不重复预测，并提出了一种简单的成本来减轻这种情况。此外，我们引入了$\textit｛动态去噪｝$策略，该策略使用匈牙利匹配来过滤冗余的带噪查询，并使用$\textit{查询对齐｝$来保持Transformer解码器层之间的匹配一致性。我们提出的模型优于以前的旋转DETR和其他同类模型，在DOTA-1.0/v.5/v.20和DIOR-R基准测试中实现了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.07598v3" target="_blank">2305.07598v3</a>
                              </td>
                              <td>RHINO: Rotated DETR with Dynamic Denoising via Hungarian Matching for Oriented Object Detection</td>
                              <td>Hakjin Lee</td>
                              <td>2023-05-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_07598v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.07598v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2206_09959v5_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Global Context Vision Transformers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2206_09959v5_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2206_09959v5_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2206_09959v5_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose global context vision transformer (GC ViT), a novel architecture that enhances parameter and compute utilization for computer vision. Our method leverages global context self-attention modules, joint with standard local self-attention, to effectively and efficiently model both long and short-range spatial interactions, without the need for expensive operations such as computing attention masks or shifting local windows. In addition, we address the lack of the inductive bias in ViTs, and propose to leverage a modified fused inverted residual blocks in our architecture. Our proposed GC ViT achieves state-of-the-art results across image classification, object detection and semantic segmentation tasks. On ImageNet-1K dataset for classification, the variants of GC ViT with 51M, 90M and 201M parameters achieve 84.3%, 85.0% and 85.7% Top-1 accuracy, respectively, at 224 image resolution and without any pre-training, hence surpassing comparably-sized prior art such as CNN-based ConvNeXt and ViT-based MaxViT and Swin Transformer by a large margin. Pre-trained GC ViT backbones in downstream tasks of object detection, instance segmentation, and semantic segmentation using MS COCO and ADE20K datasets outperform prior work consistently. Specifically, GC ViT with a 4-scale DINO detection head achieves a box AP of 58.3 on MS COCO dataset.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2206_09959v5_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了全局上下文视觉转换器（GC-ViT），这是一种提高计算机视觉参数和计算利用率的新架构。我们的方法利用全局上下文自注意模块与标准的局部自注意相结合，有效地对长距离和短距离空间交互进行建模，而不需要计算注意力掩码或移动局部窗口等昂贵的操作。此外，我们解决了ViTs中缺乏电感偏置的问题，并建议在我们的架构中利用改进的融合反向残差块。我们提出的GC-ViT在图像分类、对象检测和语义分割任务中实现了最先进的结果。在用于分类的ImageNet-1K数据集上，具有51M、90M和201M参数的GC ViT变体在224图像分辨率和没有任何预训练的情况下分别达到84.3%、85.0%和85.7%的Top-1准确率，因此大大超过了类似大小的现有技术，如基于CNN的ConvNeXt和基于ViT的MaxViT和Swin Transformer。使用MS COCO和ADE20K数据集在对象检测、实例分割和语义分割的下游任务中预先训练的GC-ViT骨干始终优于先前的工作。具体而言，具有4级DINO检测头的GC ViT在MS COCO数据集上实现了58.3的框AP。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2206.09959v5" target="_blank">2206.09959v5</a>
                              </td>
                              <td>Global Context Vision Transformers</td>
                              <td>Ali Hatamizadeh</td>
                              <td>2022-06-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2206_09959v5_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2206.09959v5" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01398v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Evaluating The Robustness of Self-Supervised Representations to Background/Foreground Removal</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01398v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01398v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01398v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite impressive empirical advances of SSL in solving various tasks, the problem of understanding and characterizing SSL representations learned from input data remains relatively under-explored. We provide a comparative analysis of how the representations produced by SSL models differ when masking parts of the input. Specifically, we considered state-of-the-art SSL pretrained models, such as DINOv2, MAE, and SwaV, and analyzed changes at the representation levels across 4 Image Classification datasets. First, we generate variations of the datasets by applying foreground and background segmentation. Then, we conduct statistical analysis using Canonical Correlation Analysis (CCA) and Centered Kernel Alignment (CKA) to evaluate the robustness of the representations learned in SSL models. Empirically, we show that not all models lead to representations that separate foreground, background, and complete images. Furthermore, we test different masking strategies by occluding the center regions of the images to address cases where foreground and background are difficult. For example, the DTD dataset that focuses on texture rather specific objects.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01398v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管SSL在解决各种任务方面取得了令人印象深刻的经验进步，但理解和表征从输入数据中学习到的SSL表示的问题仍然相对不足。我们提供了SSL模型生成的表示在屏蔽部分输入时的差异的比较分析。具体而言，我们考虑了最先进的SSL预训练模型，如DINOv2、MAE和SwaV，并分析了4个图像分类数据集在表示级别上的变化。首先，我们通过应用前景和背景分割来生成数据集的变体。然后，我们使用标准相关分析（CCA）和中心核对齐（CKA）进行统计分析，以评估SSL模型中学习的表示的稳健性。从经验上讲，我们表明并非所有模型都能产生分离前景、背景和完整图像的表示。此外，我们通过遮挡图像的中心区域来测试不同的掩蔽策略，以解决前景和背景困难的情况。例如，专注于纹理而非特定对象的DTD数据集。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01398v1" target="_blank">2306.01398v1</a>
                              </td>
                              <td>Evaluating The Robustness of Self-Supervised Representations to Background/Foreground Removal</td>
                              <td>Xavier F. Cadet</td>
                              <td>2023-06-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01398v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01398v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2207_00449v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Dissecting Self-Supervised Learning Methods for Surgical Computer Vision</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2207_00449v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2207_00449v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2207_00449v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The field of surgical computer vision has undergone considerable breakthroughs in recent years with the rising popularity of deep neural network-based methods. However, standard fully-supervised approaches for training such models require vast amounts of annotated data, imposing a prohibitively high cost; especially in the clinical domain. Self-Supervised Learning (SSL) methods, which have begun to gain traction in the general computer vision community, represent a potential solution to these annotation costs, allowing to learn useful representations from only unlabeled data. Still, the effectiveness of SSL methods in more complex and impactful domains, such as medicine and surgery, remains limited and unexplored. In this work, we address this critical need by investigating four state-of-the-art SSL methods (MoCo v2, SimCLR, DINO, SwAV) in the context of surgical computer vision. We present an extensive analysis of the performance of these methods on the Cholec80 dataset for two fundamental and popular tasks in surgical context understanding, phase recognition and tool presence detection. We examine their parameterization, then their behavior with respect to training data quantities in semi-supervised settings. Correct transfer of these methods to surgery, as described and conducted in this work, leads to substantial performance gains over generic uses of SSL - up to 7.4% on phase recognition and 20% on tool presence detection - as well as state-of-the-art semi-supervised phase recognition approaches by up to 14%. Further results obtained on a highly diverse selection of surgical datasets exhibit strong generalization properties. The code is available at https://github.com/CAMMA-public/SelfSupSurg.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2207_00449v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，随着深度神经网络方法的日益普及，外科计算机视觉领域取得了相当大的突破。然而，训练此类模型的标准完全监督方法需要大量的注释数据，成本高得令人望而却步；尤其是在临床领域。自监督学习（SSL）方法已经开始在普通计算机视觉社区中获得吸引力，它代表了这些注释成本的潜在解决方案，允许仅从未标记的数据中学习有用的表示。尽管如此，SSL方法在更复杂和更有影响力的领域（如医学和外科）的有效性仍然有限，尚未探索。在这项工作中，我们通过在外科计算机视觉的背景下研究四种最先进的SSL方法（MoCov2、SimCLR、DINO、SwAV）来解决这一关键需求。我们在Cholec80数据集上对这些方法在外科上下文理解、相位识别和工具存在检测中的两项基本和流行任务的性能进行了广泛的分析。我们检查了它们的参数化，然后检查了它们在半监督设置中相对于训练数据量的行为。正如这项工作中所描述和进行的那样，将这些方法正确地转移到手术中，与SSL的一般用途相比，可以获得显著的性能提升——在相位识别方面高达7.4%，在工具存在检测方面高达20%——以及最先进的半监督相位识别方法，高达14%。在高度多样化的外科数据集选择上获得的进一步结果显示出强大的泛化特性。代码可在https://github.com/CAMMA-public/SelfSupSurg.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2207.00449v3" target="_blank">2207.00449v3</a>
                              </td>
                              <td>Dissecting Self-Supervised Learning Methods for Surgical Computer Vision</td>
                              <td>Sanat Ramesh</td>
                              <td>2022-07-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2207_00449v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2207.00449v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_07044v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SSL4EO-S12: A Large-Scale Multi-Modal, Multi-Temporal Dataset for Self-Supervised Learning in Earth Observation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_07044v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_07044v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_07044v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised pre-training bears potential to generate expressive representations without human annotation. Most pre-training in Earth observation (EO) are based on ImageNet or medium-size, labeled remote sensing (RS) datasets. We share an unlabeled RS dataset SSL4EO-S12 (Self-Supervised Learning for Earth Observation - Sentinel-1/2) to assemble a large-scale, global, multimodal, and multi-seasonal corpus of satellite imagery from the ESA Sentinel-1 \& -2 satellite missions. For EO applications we demonstrate SSL4EO-S12 to succeed in self-supervised pre-training for a set of methods: MoCo-v2, DINO, MAE, and data2vec. Resulting models yield downstream performance close to, or surpassing accuracy measures of supervised learning. In addition, pre-training on SSL4EO-S12 excels compared to existing datasets. We make openly available the dataset, related source code, and pre-trained models at https://github.com/zhu-xlab/SSL4EO-S12.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_07044v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自我监督的预训练有可能在没有人为注释的情况下生成表达表征。地球观测（EO）中的大多数预训练都是基于ImageNet或中型标记遥感（RS）数据集。我们共享一个未标记的RS数据集SSL4EO-S12（地球观测的自我监督学习-哨兵-1/2），以收集来自欧空局哨兵-1/2卫星任务的大规模、全球、多模式和多季节的卫星图像语料库。对于EO应用，我们展示了SSL4EO-S12在一组方法的自我监督预训练中的成功：MoCo-v2、DINO、MAE和data2vec。所得到的模型产生的下游性能接近或超过监督学习的准确性度量。此外，与现有数据集相比，SSL4EO-S12上的预训练表现出色。我们在https://github.com/zhu-xlab/SSL4EO-S12.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.07044v2" target="_blank">2211.07044v2</a>
                              </td>
                              <td>SSL4EO-S12: A Large-Scale Multi-Modal, Multi-Temporal Dataset for Self-Supervised Learning in Earth Observation</td>
                              <td>Yi Wang</td>
                              <td>2022-11-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_07044v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.07044v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_11922v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SupeRGB-D: Zero-shot Instance Segmentation in Cluttered Indoor Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_11922v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_11922v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_11922v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Object instance segmentation is a key challenge for indoor robots navigating cluttered environments with many small objects. Limitations in 3D sensing capabilities often make it difficult to detect every possible object. While deep learning approaches may be effective for this problem, manually annotating 3D data for supervised learning is time-consuming. In this work, we explore zero-shot instance segmentation (ZSIS) from RGB-D data to identify unseen objects in a semantic category-agnostic manner. We introduce a zero-shot split for Tabletop Objects Dataset (TOD-Z) to enable this study and present a method that uses annotated objects to learn the ``objectness'' of pixels and generalize to unseen object categories in cluttered indoor environments. Our method, SupeRGB-D, groups pixels into small patches based on geometric cues and learns to merge the patches in a deep agglomerative clustering fashion. SupeRGB-D outperforms existing baselines on unseen objects while achieving similar performance on seen objects. We further show competitive results on the real dataset OCID. With its lightweight design (0.4 MB memory requirement), our method is extremely suitable for mobile and robotic applications. Additional DINO features can increase performance with a higher memory requirement. The dataset split and code are available at https://github.com/evinpinar/supergb-d.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_11922v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对象实例分割是室内机器人在有许多小对象的杂乱环境中导航的一个关键挑战。3D传感能力的局限性往往使检测每一个可能的物体变得困难。虽然深度学习方法可能对这个问题有效，但手动注释3D数据进行监督学习是耗时的。在这项工作中，我们从RGB-D数据中探索了零样本实例分割（ZSIS），以语义分类的方式识别看不见的对象。我们为桌面对象数据集（TOD-Z）引入了零样本分割，以实现这项研究，并提出了一种方法，该方法使用注释对象来学习像素的“对象性”，并推广到杂乱的室内环境中看不见的对象类别。我们的方法SupeRGB-D基于几何线索将像素分组为小块，并学习以深度聚集聚类的方式合并小块。SupeRGB-D在看不见的对象上优于现有的基线，同时在看到的对象上实现了类似的性能。我们在真实数据集OCID上进一步展示了具有竞争力的结果。凭借其轻量级设计（需要0.4 MB内存），我们的方法非常适合移动和机器人应用。额外的DINO功能可以提高内存需求的性能。数据集拆分和代码可在https://github.com/evinpinar/supergb-d.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.11922v2" target="_blank">2212.11922v2</a>
                              </td>
                              <td>SupeRGB-D: Zero-shot Instance Segmentation in Cluttered Indoor Environments</td>
                              <td>Evin Pınar Örnek</td>
                              <td>2022-12-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_11922v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.11922v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_15347v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_15347v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_15347v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_15347v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Text-to-image diffusion models have made significant advances in generating and editing high-quality images. As a result, numerous approaches have explored the ability of diffusion model features to understand and process single images for downstream tasks, e.g., classification, semantic segmentation, and stylization. However, significantly less is known about what these features reveal across multiple, different images and objects. In this work, we exploit Stable Diffusion (SD) features for semantic and dense correspondence and discover that with simple post-processing, SD features can perform quantitatively similar to SOTA representations. Interestingly, the qualitative analysis reveals that SD features have very different properties compared to existing representation learning features, such as the recently released DINOv2: while DINOv2 provides sparse but accurate matches, SD features provide high-quality spatial information but sometimes inaccurate semantic matches. We demonstrate that a simple fusion of these two features works surprisingly well, and a zero-shot evaluation using nearest neighbors on these fused features provides a significant performance gain over state-of-the-art methods on benchmark datasets, e.g., SPair-71k, PF-Pascal, and TSS. We also show that these correspondences can enable interesting applications such as instance swapping in two images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_15347v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文本到图像的扩散模型在生成和编辑高质量图像方面取得了重大进展。因此，许多方法已经探索了扩散模型特征理解和处理下游任务的单个图像的能力，例如分类、语义分割和风格化。然而，人们对这些特征在多个不同的图像和对象中所揭示的内容知之甚少。在这项工作中，我们利用稳定扩散（SD）特征进行语义和密集对应，并发现通过简单的后处理，SD特征可以在数量上执行类似于SOTA的表示。有趣的是，定性分析表明，与现有的表示学习特征（如最近发布的DINOv2）相比，SD特征具有非常不同的特性：虽然DINOv2提供稀疏但准确的匹配，但SD特征提供了高质量的空间信息，但有时语义匹配不准确。我们证明，这两个特征的简单融合效果令人惊讶地好，并且在这些融合特征上使用最近邻居的零样本评估在基准数据集（例如，SPair-71k、PF-Pascal和TSS）上提供了比现有技术方法显著的性能增益。我们还展示了这些对应关系可以实现有趣的应用程序，例如两个图像中的实例交换。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.15347v1" target="_blank">2305.15347v1</a>
                              </td>
                              <td>A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence</td>
                              <td>Junyi Zhang</td>
                              <td>2023-05-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_15347v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.15347v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_14093v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D Open-vocabulary Segmentation with Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_14093v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_14093v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_14093v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature significantly as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting the open-vocabulary multimodal knowledge and object reasoning capability of pre-trained foundation models CLIP and DINO, without necessitating any fine-tuning. Specifically, we distill open-vocabulary visual and textual knowledge from CLIP into a neural radiance field (NeRF) which effectively lifts 2D features into view-consistent 3D segmentation. Furthermore, we introduce the Relevancy-Distribution Alignment loss and Feature-Distribution Alignment loss to respectively mitigate the ambiguities of CLIP features and distill precise object boundaries from DINO features, eliminating the need for segmentation annotations during training. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_14093v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三维场景的开放式词汇分割是人类感知的基本功能，因此也是计算机视觉研究的重要目标。然而，由于缺乏大规模和多样化的3D开放词汇分割数据集来训练健壮和可推广的模型，这项任务受到了严重阻碍。从预先训练的2D开放词汇分割模型中提取知识是有帮助的，但它严重损害了开放词汇的特征，因为2D模型大多是用紧密的词汇数据集进行微调的。我们通过利用预先训练的基础模型CLIP和DINO的开放词汇多模态知识和对象推理能力来应对3D开放词汇分割中的挑战，而不需要任何微调。具体来说，我们将CLIP中的开放词汇视觉和文本知识提取到神经辐射场（NeRF）中，该场有效地将2D特征提升到视图一致的3D分割中。此外，我们引入了相关性分布对齐损失和特征分布对齐损失，以分别减轻CLIP特征的模糊性，并从DINO特征中提取精确的对象边界，从而消除了训练过程中对分割注释的需要。大量实验表明，我们的方法甚至优于用分割注释训练的完全监督模型，这表明可以从2D图像和文本图像对中有效地学习3D开放词汇分割。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.14093v2" target="_blank">2305.14093v2</a>
                              </td>
                              <td>3D Open-vocabulary Segmentation with Foundation Models</td>
                              <td>Kunhao Liu</td>
                              <td>2023-05-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_14093v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.14093v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_12223v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">What Makes for Good Visual Tokenizers for Large Language Models?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_12223v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_12223v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_12223v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We empirically investigate proper pre-training methods to build good visual tokenizers, making Large Language Models (LLMs) powerful Multimodal Large Language Models (MLLMs). In our benchmark, which is curated to evaluate MLLMs visual semantic understanding and fine-grained perception capabilities, we discussed different visual tokenizers pre-trained with dominant methods (i.e., DeiT, CLIP, MAE, DINO), and observe that: i) Fully/weakly supervised models capture more semantics than self-supervised models, but the gap is narrowed by scaling up the pre-training dataset. ii) Self-supervised models are better at fine-grained perception, where patch-level supervision is particularly effective. iii) Tuning the visual tokenizer leads to the loss of semantics obtained from large-scale pretraining, which is unfavorable with relatively small-scale instruction-tuning dataset. Given the findings, we reviewed methods that attempted to unify semantics and fine-grained visual understanding, e.g., patch-level feature distillation with semantically-rich targets. We obtain an intriguing insight mask-based strategies that were once all the rage may not be applicable for obtaining good visual tokenizers. Based on this critical observation, we obtain a new MLLM equipped with a tailored Good Visual Tokenizer (GVT), which exhibits strong visual comprehension capability at multiple scales. In particular, without introducing extra parameters and task-specific fine-tuning, GVT achieves superior performance on visual question answering, image captioning, and other fine-grained visual understanding tasks such as object counting and multi-class identification.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_12223v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们实证研究了适当的预训练方法来构建良好的视觉标记器，使大型语言模型（LLM）成为强大的多模式大型语言模型。在我们的基准测试中，我们讨论了用主流方法（即DeiT、CLIP、MAE、DINO）预训练的不同视觉标记器，并观察到：i）全/弱监督模型比自监督模型捕获更多的语义，但通过扩大预训练数据集，差距缩小了。ii）自监督模型更擅长细粒度感知，其中补丁级别的监督尤其有效。iii）调整可视化标记器会导致从大规模预训练中获得的语义丢失，这对相对小规模的指令调整数据集是不利的。鉴于这些发现，我们回顾了试图统一语义和细粒度视觉理解的方法，例如，具有语义丰富目标的补丁级特征提取。我们获得了一种有趣的基于面具的洞察策略，这种策略曾经风靡一时，但可能不适用于获得良好的视觉标记器。基于这一关键观察，我们获得了一种新的MLLM，该MLLM配备了定制的良好视觉标记器（GVT），在多个尺度上表现出强大的视觉理解能力。特别是，在不引入额外参数和特定任务微调的情况下，GVT在视觉问答、图像字幕和其他细粒度视觉理解任务（如对象计数和多类识别）上实现了卓越的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.12223v2" target="_blank">2305.12223v2</a>
                              </td>
                              <td>What Makes for Good Visual Tokenizers for Large Language Models?</td>
                              <td>Guangzhi Wang</td>
                              <td>2023-05-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_12223v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.12223v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_13552v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Squared Neural Families: A New Class of Tractable Density Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_13552v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_13552v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_13552v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Flexible models for probability distributions are an essential ingredient in many machine learning tasks. We develop and investigate a new class of probability distributions, which we call a Squared Neural Family (SNEFY), formed by squaring the 2-norm of a neural network and normalising it with respect to a base measure. Following the reasoning similar to the well established connections between infinitely wide neural networks and Gaussian processes, we show that SNEFYs admit a closed form normalising constants in many cases of interest, thereby resulting in flexible yet fully tractable density models. SNEFYs strictly generalise classical exponential families, are closed under conditioning, and have tractable marginal distributions. Their utility is illustrated on a variety of density estimation and conditional density estimation tasks. Software available at https://github.com/RussellTsuchida/snefy.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_13552v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>概率分布的灵活模型是许多机器学习任务的重要组成部分。我们开发并研究了一类新的概率分布，我们称之为平方神经家族（SNEFY），它是通过对神经网络的2-范数进行平方并相对于基测度对其进行归一化而形成的。根据类似于无限宽神经网络和高斯过程之间已建立的良好连接的推理，我们表明，在许多感兴趣的情况下，SNEFY允许闭合形式的归一化常数，从而产生灵活但完全可处理的密度模型。SNEFY严格推广了经典指数族，在条件作用下是封闭的，并且具有可处理的边缘分布。它们在各种密度估计和条件密度估计任务中的效用得到了说明。软件可在https://github.com/RussellTsuchida/snefy.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.13552v1" target="_blank">2305.13552v1</a>
                              </td>
                              <td>Squared Neural Families: A New Class of Tractable Density Models</td>
                              <td>Russell Tsuchida</td>
                              <td>2023-05-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_13552v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.13552v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_13291v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Materialistic: Selecting Similar Materials in Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_13291v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_13291v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_13291v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Separating an image into meaningful underlying components is a crucial first step for both editing and understanding images. We present a method capable of selecting the regions of a photograph exhibiting the same material as an artist-chosen area. Our proposed approach is robust to shading, specular highlights, and cast shadows, enabling selection in real images. As we do not rely on semantic segmentation (different woods or metal should not be selected together), we formulate the problem as a similarity-based grouping problem based on a user-provided image location. In particular, we propose to leverage the unsupervised DINO features coupled with a proposed Cross-Similarity module and an MLP head to extract material similarities in an image. We train our model on a new synthetic image dataset, that we release. We show that our method generalizes well to real-world images. We carefully analyze our model's behavior on varying material properties and lighting. Additionally, we evaluate it against a hand-annotated benchmark of 50 real photographs. We further demonstrate our model on a set of applications, including material editing, in-video selection, and retrieval of object photographs with similar materials.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_13291v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>将图像分离为有意义的底层组件是编辑和理解图像的关键第一步。我们提出了一种方法，能够选择与艺术家选择的区域呈现相同材料的照片区域。我们提出的方法对明暗处理、镜面高光和投射阴影都很稳健，可以在真实图像中进行选择。由于我们不依赖于语义分割（不同的木材或金属不应该一起选择），我们将该问题表述为基于用户提供的图像位置的基于相似性的分组问题。特别地，我们建议利用无监督的DINO特征，结合所提出的交叉相似性模块和MLP头来提取图像中的材料相似性。我们在发布的一个新的合成图像数据集上训练我们的模型。我们证明了我们的方法可以很好地推广到真实世界的图像。我们仔细分析了模型在不同材质特性和照明条件下的行为。此外，我们根据50张真实照片的手绘基准对其进行了评估。我们在一系列应用程序上进一步展示了我们的模型，包括素材编辑、视频选择和检索具有类似素材的对象照片。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.13291v1" target="_blank">2305.13291v1</a>
                              </td>
                              <td>Materialistic: Selecting Similar Materials in Images</td>
                              <td>Prafull Sharma</td>
                              <td>2023-05-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_13291v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.13291v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </div>
  <div class="text-muted text-center py-2">
    <div class="container">
      <div class="row">
        <div class="col-md-12 my-4">
          <p class="mb-1">© 2023 Li Yingping - Powered by Arxiv API.</p>
        </div>
      </div>
    </div>
  </div>
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
    integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
    crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
    integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
    crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js"
    integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy"
    crossorigin="anonymous"></script>
  <pingendo onclick="window.open('https://pingendo.com/', '_blank')"
    style="cursor:pointer;position: fixed;bottom: 20px;right:20px;padding:4px;background-color: #00b0eb;border-radius: 8px; width:220px;display:flex;flex-direction:row;align-items:center;justify-content:center;font-size:14px;color:white">
    Made with Pingendo Free&nbsp;&nbsp;<img src="https://pingendo.com/site-assets/Pingendo_logo_big.png" class="d-block"
      alt="Pingendo logo" height="16"></pingendo>
</body>

</html>