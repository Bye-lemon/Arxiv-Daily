<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- PAGE settings -->
  <link rel="icon" href="https://templates.pingendo.com/assets/Pingendo_favicon.ico">
  <title>Arxiv Daily</title>
  <!-- CSS dependencies -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"
    type="text/css">
  <link rel="stylesheet" href="assets/wireframe.css">
</head>

<body class="bg-light">
  <div class="py-5">
    <div class="container">
      <div class="row">
        <div class="text-center col-md-7 mx-auto"> <i class="fa d-block fa-bullseye fa-5x mb-4 text-info"></i>
          <h2><b>Arxiv Daily - 2023-12-05</b></h2>
        </div>
      </div>
    </div>
  </div>
  <div class="py-2">
    <div class="container">
      <div class="row">
        <div class="col-md-12">
          <ul class="nav nav-tabs">
            
            <li class="nav-item"> <a href="" class="nav-link active"
                data-toggle="pill" data-target="#SLAM"><i class="fa fa-home"></i> SLAM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#SFM"><i class="fa fa-home"></i> SFM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#LLM"><i class="fa fa-home"></i> LLM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#SAM"><i class="fa fa-home"></i> SAM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#CLIP"><i class="fa fa-home"></i> CLIP</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#DINO"><i class="fa fa-home"></i> DINO</a>
            </li>
            
          </ul>
          <div class="tab-content mt-2">
            
            <div class="tab-pane fade active show" id="SLAM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2312_02141v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">iMatching: Imperative Correspondence Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02141v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02141v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02141v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Learning feature correspondence is a foundational task in computer vision, holding immense importance for downstream applications such as visual odometry and 3D reconstruction. Despite recent progress in data-driven models, feature correspondence learning is still limited by the lack of accurate per-pixel correspondence labels. To overcome this difficulty, we introduce a new self-supervised scheme, imperative learning (IL), for training feature correspondence. It enables correspondence learning on arbitrary uninterrupted videos without any camera pose or depth labels, heralding a new era for self-supervised correspondence learning. Specifically, we formulated the problem of correspondence learning as a bilevel optimization, which takes the reprojection error from bundle adjustment as a supervisory signal for the model. To avoid large memory and computation overhead, we leverage the stationary point to effectively back-propagate the implicit gradients through bundle adjustment. Through extensive experiments, we demonstrate superior performance on tasks including feature matching and pose estimation, in which we obtained an average of 30% accuracy gain over the state-of-the-art matching models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02141v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>学习特征对应关系是计算机视觉的一项基础任务，对视觉里程计和三维重建等下游应用具有极其重要的意义。尽管最近在数据驱动模型方面取得了进展，但由于缺乏准确的每像素对应标签，特征对应学习仍然受到限制。为了克服这一困难，我们引入了一种新的自监督方案，即命令式学习（IL），用于训练特征对应关系。它可以在没有任何相机姿势或深度标签的情况下，在任意不间断的视频上进行函授学习，预示着自我监督函授学习的新时代。具体来说，我们将对应学习问题公式化为双层优化，将束调整的重投影误差作为模型的监督信号。为了避免大的内存和计算开销，我们利用驻点通过束调整有效地反向传播隐式梯度。通过广泛的实验，我们在包括特征匹配和姿态估计在内的任务上表现出了卓越的性能，在这些任务中，我们获得了比最先进的匹配模型平均30%的精度增益。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02141v1" target="_blank">2312.02141v1</a>
                              </td>
                              <td>iMatching: Imperative Correspondence Learning</td>
                              <td>Zitong Zhan</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02141v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02141v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02126v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02126v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02126v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02126v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Dense simultaneous localization and mapping (SLAM) is pivotal for embodied scene understanding. Recent work has shown that 3D Gaussians enable high-quality reconstruction and real-time rendering of scenes using multiple posed cameras. In this light, we show for the first time that representing a scene by 3D Gaussians can enable dense SLAM using a single unposed monocular RGB-D camera. Our method, SplaTAM, addresses the limitations of prior radiance field-based representations, including fast rendering and optimization, the ability to determine if areas have been previously mapped, and structured map expansion by adding more Gaussians. We employ an online tracking and mapping pipeline while tailoring it to specifically use an underlying Gaussian representation and silhouette-guided optimization via differentiable rendering. Extensive experiments show that SplaTAM achieves up to 2X state-of-the-art performance in camera pose estimation, map construction, and novel-view synthesis, demonstrating its superiority over existing approaches, while allowing real-time rendering of a high-resolution dense 3D map.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02126v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>密集同时定位和映射（SLAM）对于具体场景的理解至关重要。最近的工作表明，3D高斯可以使用多个姿势的相机进行高质量的场景重建和实时渲染。在这种情况下，我们首次展示了通过3D高斯表示场景可以使用单个非聚焦单目RGB-D相机实现密集SLAM。我们的方法SplaTAM解决了先前基于辐射场的表示的局限性，包括快速渲染和优化、确定区域是否先前已映射的能力，以及通过添加更多高斯来进行结构化地图扩展。我们使用在线跟踪和映射管道，同时对其进行剪裁，以专门使用底层高斯表示和轮廓引导的优化，通过可微分渲染进行优化。大量实验表明，SplaTAM在相机姿态估计、地图构建和新颖视图合成方面实现了高达两倍的最先进性能，证明了其优于现有方法，同时允许实时渲染高分辨率密集3D地图。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02126v1" target="_blank">2312.02126v1</a>
                              </td>
                              <td>SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM</td>
                              <td>Nikhil Keetha</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02126v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02126v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07607v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SubT-MRS Dataset: Pushing SLAM Towards All-weather Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07607v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07607v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07607v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localization and mapping (SLAM) is a fundamental task for numerous applications such as autonomous navigation and exploration. Despite many SLAM datasets have been released, current SLAM solutions still struggle to have sustained and resilient performance. One major issue is the absence of high-quality datasets including diverse all-weather conditions and a reliable metric for assessing robustness. This limitation significantly restricts the scalability and generalizability of SLAM technologies, impacting their development, validation, and deployment. To address this problem, we present SubT-MRS, an extremely challenging real-world dataset designed to push SLAM towards all-weather environments to pursue the most robust SLAM performance. It contains multi-degraded environments including over 30 diverse scenes such as structureless corridors, varying lighting conditions, and perceptual obscurants like smoke and dust; multimodal sensors such as LiDAR, fisheye camera, IMU, and thermal camera; and multiple locomotions like aerial, legged, and wheeled robots. We develop accuracy and robustness evaluation tracks for SLAM and introduced novel robustness metrics. Comprehensive studies are performed, revealing new observations, challenges, and opportunities for future research.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07607v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和测绘（SLAM）是许多应用（如自主导航和勘探）的基本任务。尽管已经发布了许多SLAM数据集，但当前的SLAM解决方案仍难以获得持续和有弹性的性能。一个主要问题是缺乏高质量的数据集，包括不同的全天候条件和评估稳健性的可靠指标。这种限制极大地限制了SLAM技术的可扩展性和可推广性，影响了它们的开发、验证和部署。为了解决这个问题，我们提出了SubT-MRS，这是一个极具挑战性的真实世界数据集，旨在将SLAM推向全天候环境，以追求最稳健的SLAM性能。它包含了多种退化环境，包括30多个不同的场景，如无结构走廊、不同的照明条件和烟雾和灰尘等感知障碍物；多模式传感器，如激光雷达、鱼眼相机、IMU和热像仪；以及多种运动方式，如空中机器人、腿式机器人和轮式机器人。我们为SLAM开发了准确性和稳健性评估跟踪，并引入了新的稳健性度量。进行了全面的研究，揭示了新的观察结果、挑战和未来研究的机会。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07607v3" target="_blank">2307.07607v3</a>
                              </td>
                              <td>SubT-MRS Dataset: Pushing SLAM Towards All-weather Environments</td>
                              <td>Shibo Zhao</td>
                              <td>2023-07-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07607v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07607v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2210_09168v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Spatially scalable recursive estimation of Gaussian process terrain maps using local basis functions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2210_09168v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2210_09168v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2210_09168v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>When an agent, person, vehicle or robot is moving through an unknown environment without GNSS signals, online mapping of nonlinear terrains can be used to improve position estimates when the agent returns to a previously mapped area. Mapping algorithms using online Gaussian process (GP) regression are commonly integrated in algorithms for simultaneous localisation and mapping (SLAM). However, GP mapping algorithms have increasing computational demands as the mapped area expands relative to spatial field variations. This is due to the need for estimating an increasing amount of map parameters as the area of the map grows. Contrary to this, we propose a recursive GP mapping estimation algorithm which uses local basis functions in an information filter to achieve spatial scalability. Our proposed approximation employs a global grid of finite support basis functions but restricts computations to a localized subset around each prediction point. As our proposed algorithm is recursive, it can naturally be incorporated into existing algorithms that uses Gaussian process maps for SLAM. Incorporating our proposed algorithm into an extended Kalman filter (EKF) for magnetic field SLAM reduces the overall computational complexity of the algorithm. We show experimentally that our algorithm is faster than existing methods when the mapped area is large and the map is based on many measurements, both for recursive mapping tasks and for magnetic field SLAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2210_09168v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当代理、人、车辆或机器人在没有GNSS信号的情况下穿过未知环境时，当代理返回到先前映射的区域时，可以使用非线性地形的在线映射来改进位置估计。使用在线高斯过程（GP）回归的映射算法通常集成在用于同时定位和映射（SLAM）的算法中。然而，随着映射区域相对于空间场变化的扩展，GP映射算法具有越来越高的计算需求。这是由于随着地图面积的增长，需要估计越来越多的地图参数。与此相反，我们提出了一种递归GP映射估计算法，该算法在信息滤波器中使用局部基函数来实现空间可伸缩性。我们提出的近似使用有限支持基函数的全局网格，但将计算限制在每个预测点周围的局部子集。由于我们提出的算法是递归的，因此它可以自然地被纳入使用高斯过程图进行SLAM的现有算法中。将我们提出的算法结合到用于磁场SLAM的扩展卡尔曼滤波器（EKF）中降低了算法的总体计算复杂度。我们通过实验表明，当映射区域很大并且映射基于许多测量时，无论是对于递归映射任务还是对于磁场SLAM，我们的算法都比现有方法更快。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2210.09168v2" target="_blank">2210.09168v2</a>
                              </td>
                              <td>Spatially scalable recursive estimation of Gaussian process terrain maps using local basis functions</td>
                              <td>Frida Marie Viset</td>
                              <td>2022-10-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2210_09168v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2210.09168v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/fridaviset/predictiondependentbasisfunctions" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00204v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DNS SLAM: Dense Neural Semantic-Informed SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00204v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00204v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00204v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, coordinate-based neural implicit representations have shown promising results for the task of Simultaneous Localization and Mapping (SLAM). While achieving impressive performance on small synthetic scenes, these methods often suffer from oversmoothed reconstructions, especially for complex real-world scenes. In this work, we introduce DNS SLAM, a novel neural RGB-D semantic SLAM approach featuring a hybrid representation. Relying only on 2D semantic priors, we propose the first semantic neural SLAM method that trains class-wise scene representations while providing stable camera tracking at the same time. Our method integrates multi-view geometry constraints with image-based feature extraction to improve appearance details and to output color, density, and semantic class information, enabling many downstream applications. To further enable real-time tracking, we introduce a lightweight coarse scene representation which is trained in a self-supervised manner in latent space. Our experimental results achieve state-of-the-art performance on both synthetic data and real-world data tracking while maintaining a commendable operational speed on off-the-shelf hardware. Further, our method outputs class-wise decomposed reconstructions with better texture capturing appearance and geometric details.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00204v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，基于坐标的神经隐式表示在同时定位和映射（SLAM）任务中显示出了很好的结果。虽然这些方法在小的合成场景中获得了令人印象深刻的性能，但它们往往会遭受过度平滑的重建，尤其是在复杂的真实世界场景中。在这项工作中，我们介绍了DNS SLAM，这是一种新的神经RGB-D语义SLAM方法，具有混合表示。仅依靠2D语义先验，我们提出了第一种语义神经SLAM方法，该方法训练类场景表示，同时提供稳定的相机跟踪。我们的方法将多视图几何约束与基于图像的特征提取相结合，以改善外观细节并输出颜色、密度和语义类信息，从而实现许多下游应用。为了进一步实现实时跟踪，我们引入了一种轻量级的粗略场景表示，该表示在潜在空间中以自监督的方式进行训练。我们的实验结果在合成数据和真实世界的数据跟踪方面都实现了最先进的性能，同时在现成的硬件上保持了值得称赞的操作速度。此外，我们的方法输出具有更好的纹理捕捉外观和几何细节的类分解重建。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00204v1" target="_blank">2312.00204v1</a>
                              </td>
                              <td>DNS SLAM: Dense Neural Semantic-Informed SLAM</td>
                              <td>Kunyi Li</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00204v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00204v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_05504v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Detecting and Classifying Bio-Inspired Artificial Landmarks Using In-Air 3D Sonar</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_05504v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_05504v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_05504v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Various autonomous applications rely on recognizing specific known landmarks in their environment. For example, Simultaneous Localization And Mapping (SLAM) is an important technique that lays the foundation for many common tasks, such as navigation and long-term object tracking. This entails building a map on the go based on sensory inputs which are prone to accumulating errors. Recognizing landmarks in the environment plays a vital role in correcting these errors and further improving the accuracy of SLAM. The most popular choice of sensors for conducting SLAM today is optical sensors such as cameras or LiDAR sensors. These can use landmarks such as QR codes as a prerequisite. However, such sensors become unreliable in certain conditions, e.g., foggy, dusty, reflective, or glass-rich environments. Sonar has proven to be a viable alternative to manage such situations better. However, acoustic sensors also require a different type of landmark. In this paper, we put forward a method to detect the presence of bio-mimetic acoustic landmarks using support vector machines trained on the frequency bands of the reflecting acoustic echoes using an embedded real-time imaging sonar.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_05504v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>各种自主应用程序依赖于识别其环境中的特定已知地标。例如，同步定位和映射（SLAM）是一种重要的技术，它为许多常见任务（如导航和长期目标跟踪）奠定了基础。这需要根据容易累积错误的感官输入在旅途中构建地图。识别环境中的地标对于纠正这些错误和进一步提高SLAM的准确性起着至关重要的作用。如今，用于进行SLAM的最流行的传感器选择是光学传感器，例如相机或激光雷达传感器。这些可以使用诸如二维码之类的地标作为先决条件。然而，这种传感器在某些条件下变得不可靠，例如雾蒙蒙、多尘、反光或富含玻璃的环境。声纳已经被证明是一种可行的替代方案，可以更好地管理这种情况。然而，声学传感器也需要不同类型的地标。在本文中，我们提出了一种方法，使用嵌入式实时成像声纳在反射声回波的频带上训练的支持向量机来检测生物模拟声学标志的存在。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.05504v3" target="_blank">2308.05504v3</a>
                              </td>
                              <td>Detecting and Classifying Bio-Inspired Artificial Landmarks Using In-Air 3D Sonar</td>
                              <td>Maarten de Backer</td>
                              <td>2023-08-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_05504v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.05504v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_18189v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Event-based Visual Inertial Velometer</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_18189v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_18189v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_18189v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neuromorphic event-based cameras are bio-inspired visual sensors with asynchronous pixels and extremely high temporal resolution. Such favorable properties make them an excellent choice for solving state estimation tasks under aggressive ego motion. However, failures of camera pose tracking are frequently witnessed in state-of-the-art event-based visual odometry systems when the local map cannot be updated in time. One of the biggest roadblocks for this specific field is the absence of efficient and robust methods for data association without imposing any assumption on the environment. This problem seems, however, unlikely to be addressed as in standard vision due to the motion-dependent observability of event data. Therefore, we propose a mapping-free design for event-based visual-inertial state estimation in this paper. Instead of estimating the position of the event camera, we find that recovering the instantaneous linear velocity is more consistent with the differential working principle of event cameras. The proposed event-based visual-inertial velometer leverages a continuous-time formulation that incrementally fuses the heterogeneous measurements from a stereo event camera and an inertial measurement unit. Experiments on the synthetic dataset demonstrate that the proposed method can recover instantaneous linear velocity in metric scale with low latency.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_18189v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于神经形态事件的相机是受生物启发的视觉传感器，具有异步像素和极高的时间分辨率。这些有利的特性使它们成为解决攻击性自我运动下的状态估计任务的绝佳选择。然而，在最先进的基于事件的视觉里程计系统中，当局部地图不能及时更新时，经常会出现相机姿态跟踪的故障。这一特定领域的最大障碍之一是缺乏有效和稳健的数据关联方法，而不会对环境施加任何假设。然而，由于事件数据的运动相关可观察性，这个问题似乎不太可能像在标准视觉中那样得到解决。因此，我们在本文中提出了一种基于事件的视觉惯性状态估计的无映射设计。我们发现，恢复瞬时线速度更符合事件摄像机的差分工作原理，而不是估计事件摄像机的位置。所提出的基于事件的视觉惯性速度计利用了连续时间公式，该公式增量地融合了来自立体事件相机和惯性测量单元的异构测量。在合成数据集上的实验表明，该方法可以以低延迟恢复度量尺度下的瞬时线速度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.18189v1" target="_blank">2311.18189v1</a>
                              </td>
                              <td>Event-based Visual Inertial Velometer</td>
                              <td>Xiuyuan Lu</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_18189v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.18189v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00068v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GLiDR: Topologically Regularized Graph Generative Network for Sparse LiDAR Point Clouds</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00068v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00068v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00068v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Sparse LiDAR point clouds cause severe loss of detail of static structures and reduce the density of static points available for navigation. Reduced density can be detrimental to navigation under several scenarios. We observe that despite high sparsity, in most cases, the global topology of LiDAR outlining the static structures can be inferred. We utilize this property to obtain a backbone skeleton of a static LiDAR scan in the form of a single connected component that is a proxy to its global topology. We utilize the backbone to augment new points along static structures to overcome sparsity. Newly introduced points could correspond to existing static structures or to static points that were earlier obstructed by dynamic objects. To the best of our knowledge, we are the first to use this strategy for sparse LiDAR point clouds. Existing solutions close to our approach fail to identify and preserve the global static LiDAR topology and generate sub-optimal points. We propose GLiDR, a Graph Generative network that is topologically regularized using 0-dimensional Persistent Homology (PH) constraints. This enables GLiDR to introduce newer static points along a topologically consistent global static LiDAR backbone. GLiDR generates precise static points using 32x sparser dynamic scans and performs better than the baselines across three datasets. The newly introduced static points allow GLiDR to outperform LiDAR-based navigation using SLAM in several settings. GLiDR generates a valuable byproduct - an accurate binary segmentation mask of static and dynamic objects that is helpful for navigation planning and safety in constrained environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00068v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>稀疏的激光雷达点云会导致静态结构细节的严重损失，并降低可用于导航的静态点的密度。在几种情况下，密度降低可能对导航不利。我们观察到，尽管稀疏性很高，但在大多数情况下，可以推断出概述静态结构的激光雷达的全局拓扑结构。我们利用这一特性以单个连接组件的形式获得静态激光雷达扫描的主干骨架，该组件是其全局拓扑的代理。我们利用主干来沿着静态结构增加新的点，以克服稀疏性。新引入的点可以对应于现有的静态结构或先前被动态对象遮挡的静态点。据我们所知，我们是第一个将这种策略用于稀疏激光雷达点云的人。与我们的方法接近的现有解决方案无法识别和保持全局静态激光雷达拓扑，并生成次优点。我们提出了GLiDR，这是一个使用0维持久同调（PH）约束进行拓扑正则化的图生成网络。这使得GLiDR能够沿着拓扑一致的全球静态激光雷达主干引入较新的静态点。GLiDR使用32倍稀疏的动态扫描生成精确的静态点，并且在三个数据集上的性能优于基线。新引入的静态点允许GLiDR在多种设置中优于使用SLAM的基于激光雷达的导航。GLiDR生成了一个有价值的副产品-静态和动态对象的精确二进制分割掩码，有助于在受限环境中进行导航规划和安全。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00068v1" target="_blank">2312.00068v1</a>
                              </td>
                              <td>GLiDR: Topologically Regularized Graph Generative Network for Sparse LiDAR Point Clouds</td>
                              <td>Prashant Kumar</td>
                              <td>2023-11-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00068v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00068v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_17754v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Cinematic Behavior Transfer via NeRF-based Differentiable Filming</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_17754v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_17754v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_17754v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In the evolving landscape of digital media and video production, the precise manipulation and reproduction of visual elements like camera movements and character actions are highly desired. Existing SLAM methods face limitations in dynamic scenes and human pose estimation often focuses on 2D projections, neglecting 3D statuses. To address these issues, we first introduce a reverse filming behavior estimation technique. It optimizes camera trajectories by leveraging NeRF as a differentiable renderer and refining SMPL tracks. We then introduce a cinematic transfer pipeline that is able to transfer various shot types to a new 2D video or a 3D virtual environment. The incorporation of 3D engine workflow enables superior rendering and control abilities, which also achieves a higher rating in the user study.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_17754v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在数字媒体和视频制作不断发展的环境中，人们非常需要对相机动作和角色动作等视觉元素进行精确的操纵和再现。现有的SLAM方法在动态场景中面临限制，并且人体姿态估计通常侧重于2D投影，而忽略了3D状态。为了解决这些问题，我们首先介绍了一种反向拍摄行为估计技术。它通过利用NeRF作为可微分渲染器并细化SMPL轨迹来优化相机轨迹。然后，我们介绍了一种电影传输管道，该管道能够将各种镜头类型传输到新的2D视频或3D虚拟环境。3D引擎工作流的结合实现了卓越的渲染和控制能力，这也在用户研究中获得了更高的评价。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.17754v1" target="_blank">2311.17754v1</a>
                              </td>
                              <td>Cinematic Behavior Transfer via NeRF-based Differentiable Filming</td>
                              <td>Xuekun Jiang</td>
                              <td>2023-11-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_17754v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.17754v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_16728v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Photo-SLAM: Real-time Simultaneous Localization and Photorealistic Mapping for Monocular, Stereo, and RGB-D Cameras</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_16728v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_16728v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_16728v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The integration of neural rendering and the SLAM system recently showed promising results in joint localization and photorealistic view reconstruction. However, existing methods, fully relying on implicit representations, are so resource-hungry that they cannot run on portable devices, which deviates from the original intention of SLAM. In this paper, we present Photo-SLAM, a novel SLAM framework with a hyper primitives map. Specifically, we simultaneously exploit explicit geometric features for localization and learn implicit photometric features to represent the texture information of the observed environment. In addition to actively densifying hyper primitives based on geometric features, we further introduce a Gaussian-Pyramid-based training method to progressively learn multi-level features, enhancing photorealistic mapping performance. The extensive experiments with monocular, stereo, and RGB-D datasets prove that our proposed system Photo-SLAM significantly outperforms current state-of-the-art SLAM systems for online photorealistic mapping, e.g., PSNR is 30% higher and rendering speed is hundreds of times faster in the Replica dataset. Moreover, the Photo-SLAM can run at real-time speed using an embedded platform such as Jetson AGX Orin, showing the potential of robotics applications.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_16728v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经绘制和SLAM系统的集成最近在联合定位和真实感视图重建方面显示出有希望的结果。然而，现有的方法完全依赖于隐式表示，资源匮乏，无法在便携式设备上运行，这偏离了SLAM的初衷。在本文中，我们提出了Photo SLAM，这是一种具有超基元映射的新型SLAM框架。具体来说，我们同时利用显式几何特征进行定位，并学习隐式光度特征来表示观测环境的纹理信息。除了基于几何特征主动加密超基元外，我们还引入了一种基于高斯金字塔的训练方法来逐步学习多层次特征，提高了真实感映射的性能。对单眼、立体和RGB-D数据集的广泛实验证明，我们提出的系统Photo SLAM在在线真实感映射方面显著优于当前最先进的SLAM系统，例如，在Replica数据集中，PSNR高出30%，渲染速度快数百倍。此外，Photo SLAM可以使用Jetson AGX Orin等嵌入式平台以实时速度运行，显示了机器人应用的潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.16728v1" target="_blank">2311.16728v1</a>
                              </td>
                              <td>Photo-SLAM: Real-time Simultaneous Localization and Photorealistic Mapping for Monocular, Stereo, and RGB-D Cameras</td>
                              <td>Huajian Huang</td>
                              <td>2023-11-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_16728v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.16728v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_11310v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Twilight SLAM: Navigating Low-Light Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_11310v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_11310v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_11310v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a comparative study of low-light visual SLAM pipelines, specifically focusing on determining an efficient combination of the state-of-the-art low-light image enhancement algorithms with standard and contemporary Simultaneous Localization and Mapping (SLAM) frameworks by evaluating their performance in challenging low-light conditions. In this study, we investigate the performance of several different low-light SLAM pipelines for dark and/or poorly-lit datasets as opposed to just partially dim-lit datasets like other works in the literature. Our study takes an experimental approach to qualitatively and quantitatively compare the chosen combinations of modules to enhance the feature-based visual SLAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_11310v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文对微光视觉SLAM管道进行了比较研究，特别是通过评估其在具有挑战性的微光条件下的性能，确定最先进的微光图像增强算法与标准和现代同步定位和映射（SLAM）框架的有效组合。在这项研究中，我们研究了几种不同的弱光SLAM管道在黑暗和/或光线不足的数据集中的性能，而不是像文献中的其他工作一样仅在部分昏暗的数据集上。我们的研究采用实验方法对所选择的模块组合进行定性和定量比较，以增强基于特征的视觉SLAM。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.11310v3" target="_blank">2304.11310v3</a>
                              </td>
                              <td>Twilight SLAM: Navigating Low-Light Environments</td>
                              <td>Surya Pratap Singh</td>
                              <td>2023-04-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_11310v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.11310v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_14970v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">UWB Radar SLAM: an Anchorless Approach in Vision Denied Indoor Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_14970v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_14970v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_14970v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>LiDAR and cameras are frequently used as sensors for simultaneous localization and mapping (SLAM). However, these sensors are prone to failure under low visibility (e.g. smoke) or places with reflective surfaces (e.g. mirrors). On the other hand, electromagnetic waves exhibit better penetration properties when the wavelength increases, thus are not affected by low visibility. Hence, this paper presents ultra-wideband (UWB) radar as an alternative to the existing sensors. UWB is generally known to be used in anchor-tag SLAM systems. One or more anchors are installed in the environment and the tags are attached to the robots. Although this method performs well under low visibility, modifying the existing infrastructure is not always feasible. UWB has also been used in peer-to-peer ranging collaborative SLAM systems. However, this requires more than a single robot and does not include mapping in the mentioned environment like smoke. Therefore, the presented approach in this paper solely depends on the UWB transceivers mounted on-board. In addition, an extended Kalman filter (EKF) SLAM is used to solve the SLAM problem at the back-end. Experiments were conducted and demonstrated that the proposed UWB-based radar SLAM is able to map natural point landmarks inside an indoor environment while improving robot localization.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_14970v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>激光雷达和相机经常被用作同时定位和映射（SLAM）的传感器。然而，这些传感器在低能见度（如烟雾）或具有反射表面（如镜子）的地方容易发生故障。另一方面，当波长增加时，电磁波表现出更好的穿透特性，因此不受低能见度的影响。因此，本文提出了超宽带（UWB）雷达作为现有传感器的替代方案。UWB通常已知用于锚标签SLAM系统中。一个或多个锚被安装在环境中，并且标签被附接到机器人上。尽管这种方法在低可见性下表现良好，但修改现有基础设施并不总是可行的。UWB还被用于对等测距协作SLAM系统中。然而，这需要不止一个机器人，并且不包括在上述环境（如烟雾）中进行映射。因此，本文提出的方法仅依赖于安装在机载的UWB收发器。此外，在后端使用扩展卡尔曼滤波器（EKF）SLAM来解决SLAM问题。实验表明，所提出的基于UWB的雷达SLAM能够在改善机器人定位的同时，绘制室内环境中的自然点地标。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.14970v1" target="_blank">2311.14970v1</a>
                              </td>
                              <td>UWB Radar SLAM: an Anchorless Approach in Vision Denied Indoor Environments</td>
                              <td>H. A. G. C. Premachandra</td>
                              <td>2023-11-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_14970v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.14970v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_16748v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Novel Perception and Semantic Mapping Method for Robot Autonomy in Orchards</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_16748v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_16748v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_16748v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Agricultural robots must navigate challenging dynamic and semi-structured environments. Recently, environmental modeling using LiDAR-based SLAM has shown promise in providing highly accurate geometry. However, how this chaotic environmental information can be used to achieve effective robot automation in the agricultural sector remains unexplored. In this study, we propose a novel semantic mapping and navigation framework for achieving robotic autonomy in orchards. It consists of two main components: a semantic processing module and a navigation module. First, we present a novel 3D detection network architecture, 3D-ODN, which can accurately process object instance information from point clouds. Second, we develop a framework to construct the visibility map by incorporating semantic information and terrain analysis. By combining these two critical components, our framework is evaluated in a number of key horticultural production scenarios, including a robotic system for in-situ phenotyping and daily monitoring, and a selective harvesting system in apple orchards. The experimental results show that our method can ensure high accuracy in understanding the environment and enable reliable robot autonomy in agricultural environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_16748v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>农业机器人必须在充满挑战的动态和半结构化环境中导航。最近，使用基于激光雷达的SLAM的环境建模在提供高度精确的几何形状方面显示出了前景。然而，如何利用这些混乱的环境信息在农业部门实现有效的机器人自动化仍有待探索。在这项研究中，我们提出了一种新的语义映射和导航框架，用于实现果园中的机器人自主性。它由两个主要组件组成：语义处理模块和导航模块。首先，我们提出了一种新的三维检测网络架构3D-ODN，它可以准确地处理点云中的对象实例信息。其次，我们开发了一个框架，通过结合语义信息和地形分析来构建可见性地图。通过结合这两个关键组成部分，我们的框架在许多关键的园艺生产场景中进行了评估，包括用于原位表型和日常监测的机器人系统，以及苹果园的选择性收获系统。实验结果表明，我们的方法可以确保对环境的高精度理解，并使机器人在农业环境中实现可靠的自主性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.16748v3" target="_blank">2308.16748v3</a>
                              </td>
                              <td>A Novel Perception and Semantic Mapping Method for Robot Autonomy in Orchards</td>
                              <td>Yaoqiang Pan</td>
                              <td>2023-08-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_16748v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.16748v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_00168v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Pose-Graph Attentional Graph Neural Network for Lidar Place Recognition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_00168v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_00168v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_00168v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper proposes a pose-graph attentional graph neural network, called P-GAT, which compares (key)nodes between sequential and non-sequential sub-graphs for place recognition tasks as opposed to a common frame-to-frame retrieval problem formulation currently implemented in SOTA place recognition methods. P-GAT uses the maximum spatial and temporal information between neighbour cloud descriptors -- generated by an existing encoder -- utilising the concept of pose-graph SLAM. Leveraging intra- and inter-attention and graph neural network, P-GAT relates point clouds captured in nearby locations in Euclidean space and their embeddings in feature space. Experimental results on the large-scale publically available datasets demonstrate the effectiveness of our approach in scenes lacking distinct features and when training and testing environments have different distributions (domain adaptation). Further, an exhaustive comparison with the state-of-the-art shows improvements in performance gains. Code is available at https://github.com/csiro-robotics/P-GAT.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_00168v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种位姿图注意力图神经网络，称为P-GAT，它比较用于位置识别任务的顺序子图和非顺序子图之间的（关键）节点，而不是目前在SOTA位置识别方法中实现的常见的逐帧检索问题公式。P-GAT利用姿态图SLAM的概念，使用由现有编码器生成的相邻云描述符之间的最大空间和时间信息。利用内部和相互注意力以及图神经网络，P-GAT将欧几里得空间中附近位置捕获的点云及其在特征空间中的嵌入联系起来。在大规模公开可用数据集上的实验结果证明了我们的方法在缺乏明显特征的场景中以及在训练和测试环境具有不同分布（领域自适应）时的有效性。此外，与最先进的技术进行详尽的比较表明，性能提升有所提高。代码位于https://github.com/csiro-robotics/p-gat.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.00168v3" target="_blank">2309.00168v3</a>
                              </td>
                              <td>Pose-Graph Attentional Graph Neural Network for Lidar Place Recognition</td>
                              <td>Milad Ramezani</td>
                              <td>2023-08-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_00168v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.00168v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_01121v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Artifacts Mapping: Multi-Modal Semantic Mapping for Object Detection and 3D Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_01121v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_01121v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_01121v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Geometric navigation is nowadays a well-established field of robotics and the research focus is shifting towards higher-level scene understanding, such as Semantic Mapping. When a robot needs to interact with its environment, it must be able to comprehend the contextual information of its surroundings. This work focuses on classifying and localising objects within a map, which is under construction (SLAM) or already built. To further explore this direction, we propose a framework that can autonomously detect and localize predefined objects in a known environment using a multi-modal sensor fusion approach (combining RGB and depth data from an RGB-D camera and a lidar). The framework consists of three key elements: understanding the environment through RGB data, estimating depth through multi-modal sensor fusion, and managing artifacts (i.e., filtering and stabilizing measurements). The experiments show that the proposed framework can accurately detect 98% of the objects in the real sample environment, without post-processing, while 85% and 80% of the objects were mapped using the single RGBD camera or RGB + lidar setup respectively. The comparison with single-sensor (camera or lidar) experiments is performed to show that sensor fusion allows the robot to accurately detect near and far obstacles, which would have been noisy or imprecise in a purely visual or laser-based approach.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_01121v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>几何导航是当今机器人学的一个成熟领域，研究重点正转向更高层次的场景理解，如语义映射。当机器人需要与环境互动时，它必须能够理解周围环境的上下文信息。这项工作的重点是对正在建造或已经建造的地图中的对象进行分类和定位。为了进一步探索这一方向，我们提出了一个框架，该框架可以使用多模式传感器融合方法（结合来自RGB-D相机和激光雷达的RGB和深度数据）在已知环境中自主检测和定位预定义对象。该框架由三个关键元素组成：通过RGB数据了解环境，通过多模式传感器融合估计深度，以及管理伪影（即过滤和稳定测量）。实验表明，所提出的框架可以在不进行后处理的情况下准确检测真实样本环境中98%的物体，而85%和80%的物体分别使用单个RGBD相机或RGB+激光雷达装置进行映射。与单传感器（相机或激光雷达）实验的比较表明，传感器融合使机器人能够准确检测远近障碍物，而在纯视觉或基于激光的方法中，这些障碍物可能会有噪声或不精确。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.01121v2" target="_blank">2307.01121v2</a>
                              </td>
                              <td>Artifacts Mapping: Multi-Modal Semantic Mapping for Object Detection and 3D Localization</td>
                              <td>Federico Rollo</td>
                              <td>2023-07-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_01121v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.01121v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/federicorollo/artifacts_mapping" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_12580v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CoVOR-SLAM: Cooperative SLAM using Visual Odometry and Ranges for Multi-Robot Systems</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_12580v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_12580v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_12580v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A swarm of robots has advantages over a single robot, since it can explore larger areas much faster and is more robust to single-point failures. Accurate relative positioning is necessary to successfully carry out a collaborative mission without collisions. When Visual Simultaneous Localization and Mapping (VSLAM) is used to estimate the poses of each robot, inter-agent loop closing is widely applied to reduce the relative positioning errors. This technique can mitigate errors using the feature points commonly observed by different robots. However, it requires significant computing and communication capabilities to detect inter-agent loops, and to process the data transmitted by multiple agents. In this paper, we propose Collaborative SLAM using Visual Odometry and Range measurements (CoVOR-SLAM) to overcome this challenge. In the framework of CoVOR-SLAM, robots only need to exchange pose estimates, covariances (uncertainty) of the estimates, and range measurements between robots. Since CoVOR-SLAM does not require to associate visual features and map points observed by different agents, the computational and communication loads are significantly reduced. The required range measurements can be obtained using pilot signals of the communication system, without requiring complex additional infrastructure. We tested CoVOR-SLAM using real images as well as real ultra-wideband-based ranges obtained with two rovers. In addition, CoVOR-SLAM is evaluated with a larger scale multi-agent setup exploiting public image datasets and ranges generated using a realistic simulation. The results show that CoVOR-SLAM can accurately estimate the robots' poses, requiring much less computational power and communication capabilities than the inter-agent loop closing technique.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_12580v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>一组机器人比单个机器人有优势，因为它可以更快地探索更大的区域，并且对单点故障更具鲁棒性。准确的相对定位对于在没有碰撞的情况下成功执行协作任务是必要的。当使用视觉同步定位与映射（VSLAM）来估计每个机器人的姿态时，广泛应用代理间闭环来减少相对定位误差。该技术可以利用不同机器人通常观察到的特征点来减轻误差。然而，它需要显著的计算和通信能力来检测代理间环路，并处理多个代理传输的数据。在本文中，我们提出了使用视觉里程计和测距的协作SLAM（CoVOR SLAM）来克服这一挑战。在CoVOR-SLAM的框架中，机器人只需要交换姿态估计、估计的协方差（不确定性）和机器人之间的距离测量。由于CoVOR SLAM不需要将不同代理观测到的视觉特征和地图点相关联，因此显著减少了计算和通信负载。所需的距离测量可以使用通信系统的导频信号来获得，而不需要复杂的附加基础设施。我们使用真实图像以及使用两辆漫游车获得的基于真实超宽带的距离来测试CoVOR SLAM。此外，CoVOR SLAM使用更大规模的多智能体设置进行评估，该设置利用真实模拟生成的公共图像数据集和范围。结果表明，与代理间闭环技术相比，CoVOR-SLAM可以准确地估计机器人的姿态，所需的计算能力和通信能力要小得多。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.12580v1" target="_blank">2311.12580v1</a>
                              </td>
                              <td>CoVOR-SLAM: Cooperative SLAM using Visual Odometry and Ranges for Multi-Robot Systems</td>
                              <td>Young-Hee Lee</td>
                              <td>2023-11-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_12580v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.12580v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11700v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11700v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11700v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11700v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we introduce $\textbf{GS-SLAM}$ that first utilizes 3D Gaussian representation in the Simultaneous Localization and Mapping (SLAM) system. It facilitates a better balance between efficiency and accuracy. Compared to recent SLAM methods employing neural implicit representations, our method utilizes a real-time differentiable splatting rendering pipeline that offers significant speedup to map optimization and RGB-D re-rendering. Specifically, we propose an adaptive expansion strategy that adds new or deletes noisy 3D Gaussian in order to efficiently reconstruct new observed scene geometry and improve the mapping of previously observed areas. This strategy is essential to extend 3D Gaussian representation to reconstruct the whole scene rather than synthesize a static object in existing methods. Moreover, in the pose tracking process, an effective coarse-to-fine technique is designed to select reliable 3D Gaussian representations to optimize camera pose, resulting in runtime reduction and robust estimation. Our method achieves competitive performance compared with existing state-of-the-art real-time methods on the Replica, TUM-RGBD datasets. The source code will be released soon.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11700v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们介绍了$\textbf{GS-SLAM}$，它首先在同步定位和映射（SLAM）系统中使用3D高斯表示。它有助于更好地平衡效率和准确性。与最近使用神经隐式表示的SLAM方法相比，我们的方法使用了实时可微分的飞溅渲染管道，大大加快了地图优化和RGB-D重新渲染的速度。具体而言，我们提出了一种自适应扩展策略，该策略添加新的或删除有噪声的3D高斯，以有效地重建新的观测场景几何结构并改进先前观测区域的映射。该策略对于扩展3D高斯表示以重建整个场景而不是在现有方法中合成静态对象至关重要。此外，在姿态跟踪过程中，设计了一种有效的从粗到细的技术来选择可靠的3D高斯表示来优化相机姿态，从而减少了运行时间并实现了稳健的估计。与Replica、TUM-RGBD数据集上现有的最先进的实时方法相比，我们的方法实现了具有竞争力的性能。源代码将很快发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11700v2" target="_blank">2311.11700v2</a>
                              </td>
                              <td>GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting</td>
                              <td>Chi Yan</td>
                              <td>2023-11-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11700v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11700v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11013v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Implicit Event-RGBD Neural SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11013v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11013v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11013v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Implicit neural SLAM has achieved remarkable progress recently. Nevertheless, existing methods face significant challenges in non-ideal scenarios, such as motion blur or lighting variation, which often leads to issues like convergence failures, localization drifts, and distorted mapping. To address these challenges, we propose $\textbf{EN-SLAM}$, the first event-RGBD implicit neural SLAM framework, which effectively leverages the high rate and high dynamic range advantages of event data for tracking and mapping. Specifically, EN-SLAM proposes a differentiable CRF (Camera Response Function) rendering technique to generate distinct RGB and event camera data via a shared radiance field, which is optimized by learning a unified implicit representation with the captured event and RGBD supervision. Moreover, based on the temporal difference property of events, we propose a temporal aggregating optimization strategy for the event joint tracking and global bundle adjustment, capitalizing on the consecutive difference constraints of events, significantly enhancing tracking accuracy and robustness. Finally, we construct the simulated dataset $\textbf{DEV-Indoors}$ and real captured dataset $\textbf{DEV-Reals}$ containing 6 scenes, 17 sequences with practical motion blur and lighting changes for evaluations. Experimental results show that our method outperforms the SOTA methods in both tracking ATE and mapping ACC with a real-time $17$ FPS in various challenging environments. The code and dataset will be released soon.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11013v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>内隐神经SLAM近年来取得了显著的进展。然而，现有方法在非理想场景中面临着重大挑战，如运动模糊或照明变化，这通常会导致收敛失败、定位漂移和失真映射等问题。为了应对这些挑战，我们提出了$\textbf{EN-SLAM}$，这是第一个事件RGBD隐式神经SLAM框架，它有效地利用了事件数据的高速率和高动态范围优势进行跟踪和映射。具体而言，EN-SLAM提出了一种可微分CRF（相机响应函数）渲染技术，通过共享辐射场生成不同的RGB和事件相机数据，该技术通过学习具有捕获事件和RGBD监督的统一隐式表示进行优化。此外，基于事件的时间差特性，我们提出了一种用于事件联合跟踪和全局束调整的时间聚合优化策略，利用事件的连续差约束，显著提高了跟踪的准确性和鲁棒性。最后，我们构建了模拟数据集$\textbf｛DEV Indoors｝$和真实捕获的数据集$_textbf{DEV Reals｝$，其中包含6个场景、17个具有实际运动模糊和照明变化的序列，用于评估。实验结果表明，在各种具有挑战性的环境中，我们的方法在跟踪ATE和映射ACC方面都优于SOTA方法，实时FPS为$17$FPS。代码和数据集将很快发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11013v2" target="_blank">2311.11013v2</a>
                              </td>
                              <td>Implicit Event-RGBD Neural SLAM</td>
                              <td>Delin Qu</td>
                              <td>2023-11-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11013v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11013v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_12245v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Accurate Loop Closure Detection in Semantic SLAM with 3D Semantic Covisibility Graphs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_12245v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_12245v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_12245v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Loop closure is necessary for correcting errors accumulated in simultaneous localization and mapping (SLAM) in unknown environments. However, conventional loop closure methods based on low-level geometric or image features may cause high ambiguity by not distinguishing similar scenarios. Thus, incorrect loop closures can occur. Though semantic 2D image information is considered in some literature to detect loop closures, there is little work that compares 3D scenes as an integral part of a semantic SLAM system. This paper introduces an approach, called SmSLAM+LCD, integrated into a semantic SLAM system to combine high-level 3D semantic information and low-level feature information to conduct accurate loop closure detection and effective drift reduction. The effectiveness of our approach is demonstrated in testing results.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_12245v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>闭环对于校正未知环境中同时定位和映射（SLAM）中积累的误差是必要的。然而，基于低级几何或图像特征的传统闭环方法可能由于不区分类似场景而导致高度模糊。因此，可能会出现错误的循环闭合。尽管在一些文献中考虑了语义2D图像信息来检测环路闭合，但很少有工作将3D场景作为语义SLAM系统的组成部分进行比较。本文介绍了一种称为SmSLAM+LCD的方法，该方法集成到语义SLAM系统中，将高级3D语义信息和低级特征信息相结合，以进行准确的闭环检测和有效的漂移减少。测试结果证明了我们方法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.12245v1" target="_blank">2311.12245v1</a>
                              </td>
                              <td>Towards Accurate Loop Closure Detection in Semantic SLAM with 3D Semantic Covisibility Graphs</td>
                              <td>Zhentian Qian</td>
                              <td>2023-11-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_12245v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.12245v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_13236v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Autonomous Search of Semantic Objects in Unknown Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_13236v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_13236v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_13236v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper addresses the problem of enabling a robot to search for a semantic object, i.e., an object with a semantic label, in an unknown and GPS-denied environment. For the robot in the unknown environment to detect and find the target semantic object, it must perform simultaneous localization and mapping (SLAM) at both geometric and semantic levels using its onboard sensors while planning and executing its motion based on the ever-updated SLAM results. In other words, the robot must be able to conduct simultaneous localization, semantic mapping, motion planning, and execution in real-time in the presence of sensing and motion uncertainty. This is an open problem as it combines semantic SLAM based on perception and real-time motion planning and execution under uncertainty. Moreover, the goals of the robot motion change on the fly depending on whether and how the robot can detect the target object. We propose a novel approach to tackle the problem, leveraging semantic SLAM, Bayesian Networks, Markov Decision Process, and Real-Time Dynamic Programming. The results in simulation and real experiments demonstrate the effectiveness and efficiency of our approach.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_13236v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文解决了使机器人能够在未知和GPS拒绝的环境中搜索语义对象的问题，即具有语义标签的对象。为了使机器人在未知环境中检测和找到目标语义对象，它必须使用其机载传感器在几何和语义层面上同时执行定位和映射（SLAM），同时基于不断更新的SLAM结果规划和执行其运动。换句话说，机器人必须能够在存在传感和运动不确定性的情况下实时进行同步定位、语义映射、运动规划和执行。这是一个悬而未决的问题，因为它将基于感知的语义SLAM与不确定性下的实时运动规划和执行相结合。此外，机器人运动的目标在飞行中会根据机器人是否以及如何检测目标物体而变化。我们提出了一种新的方法来解决这个问题，利用语义SLAM、贝叶斯网络、马尔可夫决策过程和实时动态规划。仿真和实际实验结果证明了该方法的有效性和有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.13236v2" target="_blank">2302.13236v2</a>
                              </td>
                              <td>Autonomous Search of Semantic Objects in Unknown Environments</td>
                              <td>Zhentian Qian</td>
                              <td>2023-02-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_13236v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.13236v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_08142v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MAVIS: Multi-Camera Augmented Visual-Inertial SLAM using SE2(3) Based Exact IMU Pre-integration</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_08142v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_08142v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_08142v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present a novel optimization-based Visual-Inertial SLAM system designed for multiple partially overlapped camera systems, named MAVIS. Our framework fully exploits the benefits of wide field-of-view from multi-camera systems, and the metric scale measurements provided by an inertial measurement unit (IMU). We introduce an improved IMU pre-integration formulation based on the exponential function of an automorphism of SE_2(3), which can effectively enhance tracking performance under fast rotational motion and extended integration time. Furthermore, we extend conventional front-end tracking and back-end optimization module designed for monocular or stereo setup towards multi-camera systems, and introduce implementation details that contribute to the performance of our system in challenging scenarios. The practical validity of our approach is supported by our experiments on public datasets. Our MAVIS won the first place in all the vision-IMU tracks (single and multi-session SLAM) on Hilti SLAM Challenge 2023 with 1.7 times the score compared to the second place.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_08142v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的基于优化的视觉惯性SLAM系统，该系统设计用于多个部分重叠的相机系统，称为MAVIS。我们的框架充分利用了多摄像头系统的宽视场以及惯性测量单元（IMU）提供的公制尺度测量的优势。我们引入了一种基于SE_2（3）自同构的指数函数的改进IMU预积分公式，该公式可以有效地提高在快速旋转运动和延长积分时间下的跟踪性能。此外，我们将为单目或立体设置而设计的传统前端跟踪和后端优化模块扩展到多摄像头系统，并介绍了在具有挑战性的场景中有助于提高系统性能的实现细节。我们在公共数据集上的实验支持了我们方法的实际有效性。我们的MAVIS在2023年Hilti SLAM挑战赛上以1.7倍于第二名的成绩获得了所有视觉IMU曲目（单节和多节SLAM）的第一名。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.08142v3" target="_blank">2309.08142v3</a>
                              </td>
                              <td>MAVIS: Multi-Camera Augmented Visual-Inertial SLAM using SE2(3) Based Exact IMU Pre-integration</td>
                              <td>Yifu Wang</td>
                              <td>2023-09-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_08142v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.08142v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11260v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Radarize: Large-Scale Radar SLAM for Indoor Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11260v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11260v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11260v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present Radarize, a self-contained SLAM pipeline for indoor environments that uses only a low-cost commodity single-chip mmWave radar. Our radar-native approach leverages phenomena unique to radio frequencies, such as doppler shift-based odometry, to improve performance. We evaluate our method on a large-scale dataset of 146 trajectories spanning 4 campus buildings, totaling approximately 4680m of travel distance. Our results show that our method outperforms state-of-the-art radar-based approaches by approximately 5x in terms of odometry and 8x in terms of end-to-end SLAM, as measured by absolute trajectory error (ATE), without the need additional sensors such as IMUs or wheel odometry.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11260v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了Radarize，这是一种适用于室内环境的独立SLAM管道，仅使用低成本的商品单片毫米波雷达。我们的雷达原生方法利用射频特有的现象，如基于多普勒频移的里程计，来提高性能。我们在跨越4栋校园建筑的146条轨迹的大规模数据集上评估了我们的方法，总行程约4680米。我们的结果表明，在不需要IMU或车轮里程计等额外传感器的情况下，通过绝对轨迹误差（ATE）测量，我们的方法在里程计方面比最先进的基于雷达的方法好大约5倍，在端到端SLAM方面好8倍。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11260v1" target="_blank">2311.11260v1</a>
                              </td>
                              <td>Radarize: Large-Scale Radar SLAM for Indoor Environments</td>
                              <td>Emerson Sie</td>
                              <td>2023-11-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11260v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11260v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_1710_05502v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Non-iterative SLAM for Warehouse Robots Using Ground Textures</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_1710_05502v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_1710_05502v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_1710_05502v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present a novel visual SLAM method for the warehouse robot with a single downward-facing camera using ground textures. Traditional methods resort to feature matching or point registration for pose optimization, which easily suffers from repetitive features and poor texture quality. In this paper, we present a robust kernel cross-correlator for robust image-level registration. Compared with the existing methods that often use iterative solutions, our method, named non-iterative visual SLAM (NI-SLAM), has a closed-form solution with a complexity of $O(n\log n)$. This allows it to run very efficiently, yet still provide better accuracy and robustness than the state-of-the-art methods. In the experiments, we demonstrate that it achieves 78% improvement over the state-of-the-art systems for indoor and outdoor localization. We have successfully tested it on warehouse robots equipped with a single downward camera, showcasing its product-ready superiority in a real operating area.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_1710_05502v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的基于地面纹理的仓库机器人视觉SLAM方法，该方法具有单个面向下的摄像头。传统方法采用特征匹配或点配准进行姿态优化，容易出现特征重复、纹理质量差的问题。在本文中，我们提出了一种用于鲁棒图像级配准的鲁棒核互相关器。与现有的经常使用迭代解的方法相比，我们的方法被命名为非迭代视觉SLAM（NI-SLAM），具有复杂度为$O（n\logn）$的闭式解。这使得它能够非常高效地运行，但仍然比最先进的方法提供更好的准确性和稳健性。在实验中，我们证明它在室内和室外定位方面比最先进的系统提高了78%。我们已经在配备了一个向下摄像头的仓库机器人上成功测试了它，展示了它在实际操作领域的产品准备优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/1710.05502v3" target="_blank">1710.05502v3</a>
                              </td>
                              <td>Non-iterative SLAM for Warehouse Robots Using Ground Textures</td>
                              <td>Kuan Xu</td>
                              <td>2017-10-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_1710_05502v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/1710.05502v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/sair-lab/ni-slam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_05735v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RO-MAP: Real-Time Multi-Object Mapping with Neural Radiance Fields</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_05735v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_05735v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_05735v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Accurate perception of objects in the environment is important for improving the scene understanding capability of SLAM systems. In robotic and augmented reality applications, object maps with semantic and metric information show attractive advantages. In this paper, we present RO-MAP, a novel multi-object mapping pipeline that does not rely on 3D priors. Given only monocular input, we use neural radiance fields to represent objects and couple them with a lightweight object SLAM based on multi-view geometry, to simultaneously localize objects and implicitly learn their dense geometry. We create separate implicit models for each detected object and train them dynamically and in parallel as new observations are added. Experiments on synthetic and real-world datasets demonstrate that our method can generate semantic object map with shape reconstruction, and be competitive with offline methods while achieving real-time performance (25Hz). The code and dataset will be available at: https://github.com/XiaoHan-Git/RO-MAP</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_05735v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>准确感知环境中的物体对于提高SLAM系统的场景理解能力非常重要。在机器人和增强现实应用中，具有语义和度量信息的对象地图显示出诱人的优势。在本文中，我们提出了RO-MAP，这是一种新的不依赖于3D先验的多对象映射管道。仅在给定单目输入的情况下，我们使用神经辐射场来表示对象，并将它们与基于多视图几何的轻量级对象SLAM耦合，以同时定位对象并隐式学习其密集几何。我们为每个检测到的对象创建单独的隐式模型，并在添加新的观测值时对其进行动态并行训练。在合成和真实世界数据集上的实验表明，我们的方法可以通过形状重建生成语义对象图，并在实现实时性能（25Hz）的同时与离线方法具有竞争力。代码和数据集将在以下位置提供：https://github.com/xiaohan-git/ro-map</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.05735v2" target="_blank">2304.05735v2</a>
                              </td>
                              <td>RO-MAP: Real-Time Multi-Object Mapping with Neural Radiance Fields</td>
                              <td>Xiao Han</td>
                              <td>2023-04-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_05735v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.05735v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/xiaohan-git/ro-map" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11016v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SNI-SLAM: Semantic Neural Implicit SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11016v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11016v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11016v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose SNI-SLAM, a semantic SLAM system utilizing neural implicit representation, that simultaneously performs accurate semantic mapping, high-quality surface reconstruction, and robust camera tracking. In this system, we introduce hierarchical semantic representation to allow multi-level semantic comprehension for top-down structured semantic mapping of the scene. In addition, to fully utilize the correlation between multiple attributes of the environment, we integrate appearance, geometry and semantic features through cross-attention for feature collaboration. This strategy enables a more multifaceted understanding of the environment, thereby allowing SNI-SLAM to remain robust even when single attribute is defective. Then, we design an internal fusion-based decoder to obtain semantic, RGB, Truncated Signed Distance Field (TSDF) values from multi-level features for accurate decoding. Furthermore, we propose a feature loss to update the scene representation at the feature level. Compared with low-level losses such as RGB loss and depth loss, our feature loss is capable of guiding the network optimization on a higher-level. Our SNI-SLAM method demonstrates superior performance over all recent NeRF-based SLAM methods in terms of mapping and tracking accuracy on Replica and ScanNet datasets, while also showing excellent capabilities in accurate semantic segmentation and real-time semantic mapping.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11016v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了SNI-SLAM，这是一种利用神经隐式表示的语义SLAM系统，它同时执行准确的语义映射、高质量的表面重建和稳健的相机跟踪。在这个系统中，我们引入了分层语义表示，以允许对场景的自上而下的结构化语义映射进行多级语义理解。此外，为了充分利用环境的多个属性之间的相关性，我们通过交叉关注来整合外观、几何和语义特征，以进行特征协作。这种策略能够更全面地了解环境，从而使SNI-SLAM即使在单个属性有缺陷的情况下也能保持稳健。然后，我们设计了一个基于内部融合的解码器，从多层次特征中获得语义、RGB、截断有符号距离场（TSDF）值，用于精确解码。此外，我们提出了一种在特征级别更新场景表示的特征损失。与RGB损失和深度损失等低水平损失相比，我们的特征损失能够在更高水平上指导网络优化。我们的SNI-SLAM方法在Replica和ScanNet数据集上的映射和跟踪精度方面优于最近所有基于NeRF的SLAM方法，同时在准确的语义分割和实时语义映射方面也表现出出色的能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11016v1" target="_blank">2311.11016v1</a>
                              </td>
                              <td>SNI-SLAM: Semantic Neural Implicit SLAM</td>
                              <td>Siting Zhu</td>
                              <td>2023-11-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11016v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11016v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_16490v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Active SLAM Utility Function Exploiting Path Entropy</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_16490v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_16490v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_16490v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this article we present a utility function for Active SLAM (A-SLAM) which utilizes map entropy along with D-Optimality criterion metrices for weighting goal frontier candidates. We propose a utility function for frontier goal selection that exploits the occupancy grid map by utilizing the path entropy and favors unknown map locations for maximum area coverage while maintaining a low localization and mapping uncertainties. We quantify the efficiency of our method using various graph connectivity matrices and map efficiency indexes for an environment exploration task. Using simulation and experimental results against similar approaches we achieve an average of 32% more coverage using publicly available data sets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_16490v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了主动SLAM（a-SLAM）的效用函数，该函数利用映射熵和D-最优准则度量来加权目标前沿候选者。我们提出了一种用于边界目标选择的效用函数，该函数通过利用路径熵来利用占用网格图，并在保持低定位和映射不确定性的同时，支持未知地图位置以实现最大区域覆盖。我们使用环境探索任务的各种图连通性矩阵和地图效率指数来量化我们的方法的效率。通过对类似方法的模拟和实验结果，我们使用公开可用的数据集实现了平均32%的覆盖率。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.16490v2" target="_blank">2309.16490v2</a>
                              </td>
                              <td>Active SLAM Utility Function Exploiting Path Entropy</td>
                              <td>Muhammad Farhan Ahmed</td>
                              <td>2023-09-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_16490v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.16490v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_09525v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NGEL-SLAM: Neural Implicit Representation-based Global Consistent Low-Latency SLAM System</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_09525v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_09525v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_09525v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural implicit representations have emerged as a promising solution for providing dense geometry in Simultaneous Localization and Mapping (SLAM). However, existing methods in this direction fall short in terms of global consistency and low latency. This paper presents NGEL-SLAM to tackle the above challenges. To ensure global consistency, our system leverages a traditional feature-based tracking module that incorporates loop closure. Additionally, we maintain a global consistent map by representing the scene using multiple neural implicit fields, enabling quick adjustment to the loop closure. Moreover, our system allows for fast convergence through the use of octree-based implicit representations. The combination of rapid response to loop closure and fast convergence makes our system a truly low-latency system that achieves global consistency. Our system enables rendering high-fidelity RGB-D images, along with extracting dense and complete surfaces. Experiments on both synthetic and real-world datasets suggest that our system achieves state-of-the-art tracking and mapping accuracy while maintaining low latency.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_09525v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经隐式表示已成为在同时定位和映射（SLAM）中提供密集几何的一种有前途的解决方案。然而，这方面的现有方法在全局一致性和低延迟方面存在不足。本文提出NGEL-SLAM来应对上述挑战。为了确保全局一致性，我们的系统利用了传统的基于特征的跟踪模块，该模块包含循环闭合。此外，我们通过使用多个神经隐式场表示场景来保持全局一致性映射，从而能够快速调整循环闭合。此外，我们的系统允许通过使用基于八叉树的隐式表示来快速收敛。对环路闭合的快速响应和快速收敛相结合，使我们的系统成为一个真正的低延迟系统，实现全局一致性。我们的系统能够渲染高保真RGB-D图像，同时提取密集完整的表面。在合成数据集和真实世界数据集上的实验表明，我们的系统在保持低延迟的同时实现了最先进的跟踪和映射精度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.09525v1" target="_blank">2311.09525v1</a>
                              </td>
                              <td>NGEL-SLAM: Neural Implicit Representation-based Global Consistent Low-Latency SLAM System</td>
                              <td>Yunxuan Mao</td>
                              <td>2023-11-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_09525v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.09525v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_08013v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CP-SLAM: Collaborative Neural Point-based SLAM System</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_08013v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_08013v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_08013v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a collaborative implicit neural simultaneous localization and mapping (SLAM) system with RGB-D image sequences, which consists of complete front-end and back-end modules including odometry, loop detection, sub-map fusion, and global refinement. In order to enable all these modules in a unified framework, we propose a novel neural point based 3D scene representation in which each point maintains a learnable neural feature for scene encoding and is associated with a certain keyframe. Moreover, a distributed-to-centralized learning strategy is proposed for the collaborative implicit SLAM to improve consistency and cooperation. A novel global optimization framework is also proposed to improve the system accuracy like traditional bundle adjustment. Experiments on various datasets demonstrate the superiority of the proposed method in both camera tracking and mapping.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_08013v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种具有RGB-D图像序列的协作隐式神经同时定位和映射（SLAM）系统，该系统由完整的前端和后端模块组成，包括里程计、环路检测、子图融合和全局细化。为了在统一的框架中实现所有这些模块，我们提出了一种新的基于神经点的3D场景表示，其中每个点都保持用于场景编码的可学习神经特征，并与某个关键帧相关联。此外，针对协作隐式SLAM，提出了一种分布式到集中式的学习策略，以提高一致性和协作性。还提出了一种新的全局优化框架，以像传统的束平差一样提高系统精度。在各种数据集上的实验证明了该方法在相机跟踪和映射方面的优越性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.08013v1" target="_blank">2311.08013v1</a>
                              </td>
                              <td>CP-SLAM: Collaborative Neural Point-based SLAM System</td>
                              <td>Jiarui Hu</td>
                              <td>2023-11-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_08013v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.08013v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2206_03062v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Object Scan Context: Object-centric Spatial Descriptor for Place Recognition within 3D Point Cloud Map</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2206_03062v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2206_03062v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2206_03062v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The integration of a SLAM algorithm with place recognition technology empowers it with the ability to mitigate accumulated errors and to relocalize itself. However, existing methods for point cloud-based place recognition predominantly rely on the matching of descriptors, which are mostly lidar-centric. These methods suffer from two major drawbacks: first, they cannot perform place recognition when the distance between two point clouds is significant, and second, they can only calculate the rotation angle without considering the offset in the X and Y directions. To overcome these limitations, we propose a novel local descriptor that is constructed around the Main Object. By using a geometric method, we can accurately calculate the relative pose. We have provided a theoretical analysis to demonstrate that this method can overcome the aforementioned limitations. Furthermore, we conducted extensive experiments on KITTI Odometry and KITTI360, which indicate that our proposed method has significant advantages over state-of-the-art methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2206_03062v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>SLAM算法与位置识别技术的集成使其能够减少累积的错误并重新定位自己。然而，现有的基于点云的位置识别方法主要依赖于描述符的匹配，这些描述符大多以激光雷达为中心。这些方法有两个主要缺点：第一，当两点云之间的距离很大时，它们无法进行位置识别；第二，它们只能计算旋转角度，而不考虑X和Y方向上的偏移。为了克服这些限制，我们提出了一种新的局部描述符，它是围绕主对象构建的。通过使用几何方法，我们可以精确地计算相对姿态。我们提供了一个理论分析来证明这种方法可以克服上述限制。此外，我们在KITTI Odometry和KITTI360上进行了广泛的实验，这表明我们提出的方法比最先进的方法具有显著的优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2206.03062v3" target="_blank">2206.03062v3</a>
                              </td>
                              <td>Object Scan Context: Object-centric Spatial Descriptor for Place Recognition within 3D Point Cloud Map</td>
                              <td>Haodong Yuan</td>
                              <td>2022-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2206_03062v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2206.03062v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_06659v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3DFusion, A real-time 3D object reconstruction pipeline based on streamed instance segmented data</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_06659v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_06659v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_06659v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a real-time segmentation and reconstruction system that utilizes RGB-D images to generate accurate and detailed individual 3D models of objects within a captured scene. Leveraging state-of-the-art instance segmentation techniques, the system performs pixel-level segmentation on RGB-D data, effectively separating foreground objects from the background. The segmented objects are then reconstructed into distinct 3D models in a high-performance computation platform. The real-time 3D modelling can be applied across various domains, including augmented/virtual reality, interior design, urban planning, road assistance, security systems, and more. To achieve real-time performance, the paper proposes a method that effectively samples consecutive frames to reduce network load while ensuring reconstruction quality. Additionally, a multi-process SLAM pipeline is adopted for parallel 3D reconstruction, enabling efficient cutting of the clustering objects into individuals. This system employs the industry-leading framework YOLO for instance segmentation. To improve YOLO's performance and accuracy, modifications were made to resolve duplicated or false detection of similar objects, ensuring the reconstructed models align with the targets. Overall, this work establishes a robust real-time system with a significant enhancement for object segmentation and reconstruction in the indoor environment. It can potentially be extended to the outdoor scenario, opening up numerous opportunities for real-world applications.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_06659v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种实时分割和重建系统，该系统利用RGB-D图像来生成捕获场景中对象的精确和详细的单个3D模型。利用最先进的实例分割技术，该系统对RGB-D数据进行像素级分割，有效地将前景对象与背景分离。然后在高性能计算平台中将分割的对象重建为不同的3D模型。实时3D建模可以应用于各个领域，包括增强/虚拟现实、室内设计、城市规划、道路辅助、安全系统等。为了实现实时性，本文提出了一种在保证重建质量的同时，对连续帧进行有效采样以减少网络负载的方法。此外，采用多进程SLAM流水线进行并行三维重建，能够有效地将聚类对象切割成个体。该系统采用行业领先的YOLO框架进行细分。为了提高YOLO的性能和准确性，对其进行了修改，以解决类似物体的重复或错误检测，确保重建的模型与目标对准。总的来说，这项工作建立了一个强大的实时系统，大大增强了室内环境中的对象分割和重建。它有可能扩展到户外场景，为现实世界的应用开辟了许多机会。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.06659v1" target="_blank">2311.06659v1</a>
                              </td>
                              <td>3DFusion, A real-time 3D object reconstruction pipeline based on streamed instance segmented data</td>
                              <td>Xi Sun</td>
                              <td>2023-11-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_06659v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.06659v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_06149v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Dense Visual Odometry Using Genetic Algorithm</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_06149v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_06149v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_06149v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Our work aims to estimate the camera motion mounted on the head of a mobile robot or a moving object from RGB-D images in a static scene. The problem of motion estimation is transformed into a nonlinear least squares function. Methods for solving such problems are iterative. Various classic methods gave an iterative solution by linearizing this function. We can also use the metaheuristic optimization method to solve this problem and improve results. In this paper, a new algorithm is developed for visual odometry using a sequence of RGB-D images. This algorithm is based on a genetic algorithm. The proposed iterative genetic algorithm searches using particles to estimate the optimal motion and then compares it to the traditional methods. To evaluate our method, we use the root mean square error to compare it with the based energy method and another metaheuristic method. We prove the efficiency of our innovative algorithm on a large set of images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_06149v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们的工作旨在从静态场景中的RGB-D图像中估计安装在移动机器人或移动物体头部的相机运动。将运动估计问题转化为一个非线性最小二乘函数。解决此类问题的方法是迭代的。各种经典方法通过线性化该函数给出了迭代解。我们还可以使用元启发式优化方法来解决这个问题并提高结果。在本文中，利用RGB-D图像序列开发了一种新的视觉里程计算法。该算法基于遗传算法。所提出的迭代遗传算法使用粒子来搜索最优运动，然后将其与传统方法进行比较。为了评估我们的方法，我们使用均方根误差将其与基于能量的方法和另一种元启发式方法进行比较。我们在大量图像上证明了我们的创新算法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.06149v1" target="_blank">2311.06149v1</a>
                              </td>
                              <td>Dense Visual Odometry Using Genetic Algorithm</td>
                              <td>Slimane Djema</td>
                              <td>2023-11-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_06149v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.06149v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_05600v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FogROS2-Sky: Optimizing Latency and Cost for Multi-Cloud Robot Applications</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_05600v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_05600v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_05600v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper studies the cost-performance tradeoffs in cloud robotics with heterogeneous cloud service providers, which have complex pricing models and varying application requirements. We present FogROS2-Sky, a cost-efficient open source robotics platform that offloads unmodified ROS2 applications to multiple cloud providers and enables fine-grained cost analysis for ROS2 applications' communication with multiple cloud providers. As each provider offers different options for CPU, GPU, memory, and latency, it can be very difficult for users to decide which to choose. FogROS2-Sky includes an optimization algorithm, which either finds the best available hardware specification that fulfills the user's latency and cost constraints or reports that such a specification does not exist. We use FogROS2-Sky to perform time-cost analysis on three robotics applications: visual SLAM, grasp planning, and motion planning. We are able to sample different hardware setups at nearly half the cost while still create cost and latency functions suitable for the optimizer. We also evaluate the optimizer's efficacy for these applications with the Pareto frontier and show that the optimizer selects efficient hardware configurations to balance cost and latency. Videos and code are available on the website https://sites.google.com/view/fogros2-sky</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_05600v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文研究了云机器人与异构云服务提供商的性价比权衡，异构云服务供应商具有复杂的定价模型和不同的应用需求。我们介绍了FogROS2-Sky，这是一个具有成本效益的开源机器人平台，它将未修改的ROS2应用程序卸载到多个云提供商，并能够对ROS2应用与多个云供应商的通信进行细粒度成本分析。由于每个提供商提供不同的CPU、GPU、内存和延迟选项，用户很难决定选择哪一个。FogROS2-Sky包括一个优化算法，该算法要么找到满足用户延迟和成本限制的最佳可用硬件规范，要么报告不存在这样的规范。我们使用FogROS2-Sky对三种机器人应用程序进行时间成本分析：视觉SLAM、抓取规划和运动规划。我们能够以几乎一半的成本对不同的硬件设置进行采样，同时仍然创建适合优化器的成本和延迟函数。我们还用Pareto前沿评估了优化器对这些应用程序的功效，并表明优化器选择了有效的硬件配置来平衡成本和延迟。网站上提供了视频和代码https://sites.google.com/view/fogros2-sky</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.05600v1" target="_blank">2311.05600v1</a>
                              </td>
                              <td>FogROS2-Sky: Optimizing Latency and Cost for Multi-Cloud Robot Applications</td>
                              <td>Kaiyuan Chen</td>
                              <td>2023-11-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_05600v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.05600v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_02831v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SemanticTopoLoop: Semantic Loop Closure With 3D Topological Graph Based on Quadric-Level Object Map</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_02831v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_02831v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_02831v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Loop closure, as one of the crucial components in SLAM, plays an essential role in correcting the accumulated errors. Traditional appearance-based methods, such as bag-of-words models, are often limited by local 2D features and the volume of training data, making them less versatile and robust in real-world scenarios, leading to missed detections or false positives detections in loop closure. To address these issues, we first propose a object-level data association method based on multi-level verification, which can associate 2D semantic features of current frame with 3D objects landmarks of map. Next, taking advantage of these association relations, we introduce a semantic loop closure method based on quadric-level object map topology, which represents scenes through the topological graph of objects and achieves accurate loop closure at a wide field of view by comparing differences in the topological graphs. Finally, we integrate these two methods into a complete object-aware SLAM system. Qualitative experiments and ablation studies demonstrate the effectiveness and robustness of the proposed object-level data association algorithm. Quantitative experiments show that our semantic loop closure method outperforms existing state-of-the-art methods in terms of precision, recall and localization accuracy metrics.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_02831v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>闭环作为SLAM的关键组成部分之一，在纠正累积误差方面起着至关重要的作用。传统的基于外观的方法，如单词袋模型，往往受到局部2D特征和训练数据量的限制，使其在现实世界场景中的通用性和鲁棒性较差，导致环路闭合中的漏检测或误报检测。为了解决这些问题，我们首先提出了一种基于多级验证的对象级数据关联方法，该方法可以将当前帧的2D语义特征与地图的3D对象地标相关联。接下来，利用这些关联关系，我们介绍了一种基于二次对象映射拓扑的语义环闭合方法，该方法通过对象的拓扑图来表示场景，并通过比较拓扑图的差异来实现宽视场下的精确环闭合。最后，我们将这两种方法集成到一个完整的对象感知SLAM系统中。定性实验和消融研究证明了所提出的对象级数据关联算法的有效性和稳健性。定量实验表明，我们的语义环闭合方法在精度、召回率和定位精度指标方面优于现有的最先进方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.02831v3" target="_blank">2311.02831v3</a>
                              </td>
                              <td>SemanticTopoLoop: Semantic Loop Closure With 3D Topological Graph Based on Quadric-Level Object Map</td>
                              <td>Zhenzhong Cao</td>
                              <td>2023-11-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_02831v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.02831v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03722v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Inertial Guided Uncertainty Estimation of Feature Correspondence in Visual-Inertial Odometry/SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03722v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03722v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03722v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual odometry and Simultaneous Localization And Mapping (SLAM) has been studied as one of the most important tasks in the areas of computer vision and robotics, to contribute to autonomous navigation and augmented reality systems. In case of feature-based odometry/SLAM, a moving visual sensor observes a set of 3D points from different viewpoints, correspondences between the projected 2D points in each image are usually established by feature tracking and matching. However, since the corresponding point could be erroneous and noisy, reliable uncertainty estimation can improve the accuracy of odometry/SLAM methods. In addition, inertial measurement unit is utilized to aid the visual sensor in terms of Visual-Inertial fusion. In this paper, we propose a method to estimate the uncertainty of feature correspondence using an inertial guidance robust to image degradation caused by motion blur, illumination change and occlusion. Modeling a guidance distribution to sample possible correspondence, we fit the distribution to an energy function based on image error, yielding more robust uncertainty than conventional methods. We also demonstrate the feasibility of our approach by incorporating it into one of recent visual-inertial odometry/SLAM algorithms for public datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03722v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉里程计和同步定位与映射（SLAM）已被研究为计算机视觉和机器人领域最重要的任务之一，有助于实现自主导航和增强现实系统。在基于特征的里程计/SLAM的情况下，移动的视觉传感器从不同的视点观察一组3D点，通常通过特征跟踪和匹配来建立每个图像中投影的2D点之间的对应关系。然而，由于对应点可能是错误的和有噪声的，可靠的不确定性估计可以提高里程计/SLAM方法的准确性。此外，惯性测量单元用于帮助视觉传感器进行视觉惯性融合。在本文中，我们提出了一种使用惯性制导来估计特征对应的不确定性的方法，该惯性制导对运动模糊、照明变化和遮挡引起的图像退化具有鲁棒性。对引导分布进行建模，以采样可能的对应关系，我们将该分布拟合为基于图像误差的能量函数，产生比传统方法更稳健的不确定性。我们还通过将其纳入最近的一种用于公共数据集的视觉惯性里程计/SLAM算法来证明我们的方法的可行性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03722v1" target="_blank">2311.03722v1</a>
                              </td>
                              <td>Inertial Guided Uncertainty Estimation of Feature Correspondence in Visual-Inertial Odometry/SLAM</td>
                              <td>Seongwook Yoon</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03722v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03722v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03484v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Osprey: Multi-Session Autonomous Aerial Mapping with LiDAR-based SLAM and Next Best View Planning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03484v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03484v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03484v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Aerial mapping systems are important for many surveying applications (e.g., industrial inspection or agricultural monitoring). Semi-autonomous mapping with GPS-guided aerial platforms that fly preplanned missions is already widely available but fully autonomous systems can significantly improve efficiency. Autonomously mapping complex 3D structures requires a system that performs online mapping and mission planning. This paper presents Osprey, an autonomous aerial mapping system with state-of-the-art multi-session mapping capabilities. It enables a non-expert operator to specify a bounded target area that the aerial platform can then map autonomously, over multiple flights if necessary. Field experiments with Osprey demonstrate that this system can achieve greater map coverage of large industrial sites than manual surveys with a pilot-flown aerial platform or a terrestrial laser scanner (TLS). Three sites, with a total ground coverage of $7085$ m$^2$ and a maximum height of $27$ m, were mapped in separate missions using $112$ minutes of autonomous flight time. True colour maps were created from images captured by Osprey using pointcloud and NeRF reconstruction methods. These maps provide useful data for structural inspection tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03484v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>航空测绘系统对许多测量应用（如工业检查或农业监测）都很重要。使用GPS引导的空中平台执行预先计划的任务的半自主地图已经广泛可用，但完全自主的系统可以显著提高效率。自动绘制复杂的三维结构需要一个执行在线绘制和任务规划的系统。本文介绍了Osprey，一种具有最先进的多会话映射能力的自主航空测绘系统。它使非专家操作员能够指定一个有界目标区域，然后空中平台可以在必要时通过多次飞行自主绘制该区域的地图。Osprey的现场实验表明，与使用飞行员飞行的空中平台或地面激光扫描仪（TLS）进行手动测量相比，该系统可以实现更大的大型工业场地地图覆盖范围。三个地点的总地面覆盖面积为7085美元，最高高度为2700美元，在单独的任务中使用112美元的自主飞行时间绘制了地图。使用点云和NeRF重建方法，从Osprey拍摄的图像中创建了真实的彩色地图。这些地图为结构检查任务提供了有用的数据。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03484v1" target="_blank">2311.03484v1</a>
                              </td>
                              <td>Osprey: Multi-Session Autonomous Aerial Mapping with LiDAR-based SLAM and Next Best View Planning</td>
                              <td>Rowan Border</td>
                              <td>2023-11-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03484v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03484v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_02327v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ECMD: An Event-Centric Multisensory Driving Dataset for SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_02327v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_02327v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_02327v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Leveraging multiple sensors enhances complex environmental perception and increases resilience to varying luminance conditions and high-speed motion patterns, achieving precise localization and mapping. This paper proposes, ECMD, an event-centric multisensory dataset containing 81 sequences and covering over 200 km of various challenging driving scenarios including high-speed motion, repetitive scenarios, dynamic objects, etc. ECMD provides data from two sets of stereo event cameras with different resolutions (640*480, 346*260), stereo industrial cameras, an infrared camera, a top-installed mechanical LiDAR with two slanted LiDARs, two consumer-level GNSS receivers, and an onboard IMU. Meanwhile, the ground-truth of the vehicle was obtained using a centimeter-level high-accuracy GNSS-RTK/INS navigation system. All sensors are well-calibrated and temporally synchronized at the hardware level, with recording data simultaneously. We additionally evaluate several state-of-the-art SLAM algorithms for benchmarking visual and LiDAR SLAM and identifying their limitations. The dataset is available at https://arclab-hku.github.io/ecmd/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_02327v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>利用多个传感器增强了复杂的环境感知，并提高了对不同亮度条件和高速运动模式的弹性，实现了精确的定位和映射。本文提出了ECMD，这是一个以事件为中心的多传感器数据集，包含81个序列，覆盖了200多公里的各种具有挑战性的驾驶场景，包括高速运动、重复场景、动态物体等。ECMD提供了来自两组不同分辨率的立体事件相机（640*480、346*260）、立体工业相机、红外相机，一个顶部安装的机械激光雷达，带有两个倾斜的激光雷达、两个消费者级GNSS接收器和一个机载IMU。同时，使用厘米级高精度GNSS-RTK/INS导航系统获得了车辆的地面实况。所有传感器都经过了良好的校准，并在硬件级别上进行了时间同步，同时记录数据。我们还评估了几种最先进的SLAM算法，用于对视觉和激光雷达SLAM进行基准测试，并确定其局限性。数据集位于https://arclab-hku.github.io/ecmd/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.02327v1" target="_blank">2311.02327v1</a>
                              </td>
                              <td>ECMD: An Event-Centric Multisensory Driving Dataset for SLAM</td>
                              <td>Peiyu Chen</td>
                              <td>2023-11-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_02327v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.02327v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/arclab-hku/event_based_vo-vio-slam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_18917v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TiV-NeRF: Tracking and Mapping via Time-Varying Representation with Dynamic Neural Radiance Fields</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_18917v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_18917v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_18917v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Previous attempts to integrate Neural Radiance Fields (NeRF) into Simultaneous Localization and Mapping (SLAM) framework either rely on the assumption of static scenes or treat dynamic objects as outliers. However, most of real-world scenarios is dynamic. In this paper, we propose a time-varying representation to track and reconstruct the dynamic scenes. Our system simultaneously maintains two processes, tracking process and mapping process. For tracking process, the entire input images are uniformly sampled and training of the RGB images are self-supervised. For mapping process, we leverage know masks to differentiate dynamic objects and static backgrounds, and we apply distinct sampling strategies for two types of areas. The parameters optimization for both processes are made up by two stages, the first stage associates time with 3D positions to convert the deformation field to the canonical field. And the second associates time with 3D positions in canonical field to obtain colors and Signed Distance Function (SDF). Besides, We propose a novel keyframe selection strategy based on the overlapping rate. We evaluate our approach on two publicly available synthetic datasets and validate that our method is more effective compared to current state-of-the-art dynamic mapping methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_18917v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>先前将神经辐射场（NeRF）集成到同步定位和映射（SLAM）框架中的尝试要么依赖于静态场景的假设，要么将动态对象视为异常值。然而，现实世界中的大多数场景都是动态的。在本文中，我们提出了一种时变表示来跟踪和重建动态场景。我们的系统同时维护两个过程，跟踪过程和映射过程。对于跟踪过程，对整个输入图像进行均匀采样，并对RGB图像的训练进行自监督。对于映射过程，我们利用已知遮罩来区分动态对象和静态背景，并对两种类型的区域应用不同的采样策略。两个过程的参数优化由两个阶段组成，第一阶段将时间与3D位置相关联，以将变形场转换为规范场。第二种方法将时间与规范场中的三维位置相关联，以获得颜色和符号距离函数（SDF）。此外，我们还提出了一种新的基于重叠率的关键帧选择策略。我们在两个公开可用的合成数据集上评估了我们的方法，并验证了与当前最先进的动态映射方法相比，我们的方法更有效。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.18917v2" target="_blank">2310.18917v2</a>
                              </td>
                              <td>TiV-NeRF: Tracking and Mapping via Time-Varying Representation with Dynamic Neural Radiance Fields</td>
                              <td>Chengyao Duan</td>
                              <td>2023-10-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_18917v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.18917v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_00928v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Quatro++: Robust Global Registration Exploiting Ground Segmentation for Loop Closing in LiDAR SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_00928v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_00928v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_00928v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Global registration is a fundamental task that estimates the relative pose between two viewpoints of 3D point clouds. However, there are two issues that degrade the performance of global registration in LiDAR SLAM: one is the sparsity issue and the other is degeneracy. The sparsity issue is caused by the sparse characteristics of the 3D point cloud measurements in a mechanically spinning LiDAR sensor. The degeneracy issue sometimes occurs because the outlier-rejection methods reject too many correspondences, leaving less than three inliers. These two issues have become more severe as the pose discrepancy between the two viewpoints of 3D point clouds becomes greater. To tackle these problems, we propose a robust global registration framework, called \textit{Quatro++}. Extending our previous work that solely focused on the global registration itself, we address the robust global registration in terms of the loop closing in LiDAR SLAM. To this end, ground segmentation is exploited to achieve robust global registration. Through the experiments, we demonstrate that our proposed method shows a higher success rate than the state-of-the-art global registration methods, overcoming the sparsity and degeneracy issues. In addition, we show that ground segmentation significantly helps to increase the success rate for the ground vehicles. Finally, we apply our proposed method to the loop closing module in LiDAR SLAM and confirm that the quality of the loop constraints is improved, showing more precise mapping results. Therefore, the experimental evidence corroborated the suitability of our method as an initial alignment in the loop closing. Our code is available at https://quatro-plusplus.github.io.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_00928v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>全局配准是估计三维点云的两个视点之间的相对姿态的基本任务。然而，有两个问题降低了激光雷达SLAM的全局配准性能：一个是稀疏性问题，另一个是退化性问题。稀疏性问题是由机械旋转的激光雷达传感器中的3D点云测量的稀疏特性引起的。退化问题有时会发生，因为异常值拒绝方法拒绝了太多的对应关系，只留下不到三个内部。随着3D点云的两个视点之间的姿态差异变得更大，这两个问题变得更加严重。为了解决这些问题，我们提出了一个强大的全局注册框架，称为\textit｛Quatro++｝。扩展我们之前仅专注于全球注册本身的工作，我们在激光雷达SLAM中解决了闭环方面的稳健全球注册问题。为此，利用地面分割来实现稳健的全局配准。通过实验，我们证明了我们提出的方法比最先进的全局配准方法具有更高的成功率，克服了稀疏性和退化性问题。此外，我们还表明，地面分割显著有助于提高地面车辆的成功率。最后，我们将我们提出的方法应用于激光雷达SLAM中的闭环模块，并证实了环路约束的质量得到了提高，显示出更精确的映射结果。因此，实验证据证实了我们的方法作为闭环初始对准的适用性。我们的代码可在https://quatro-plusplus.github.io.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.00928v1" target="_blank">2311.00928v1</a>
                              </td>
                              <td>Quatro++: Robust Global Registration Exploiting Ground Segmentation for Loop Closing in LiDAR SLAM</td>
                              <td>Hyungtae Lim</td>
                              <td>2023-11-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_00928v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.00928v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_00276v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LiDAR-based SLAM for robotic mapping: state of the art and new frontiers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_00276v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_00276v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_00276v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent decades, the field of robotic mapping has witnessed widespread research and development in LiDAR (Light Detection And Ranging)-based simultaneous localization and mapping (SLAM) techniques. In this paper, we review the state-of-the-art in LiDAR-based SLAM and explore the remaining challenges that still require attention to satisfy the needs of contemporary applications. A distinctive aspect of this study lies in its literature survey, which specifically investigates the application of various types and configurations of LiDAR, setting it apart from prior reviews. Furthermore, several representative comparisons of LiDAR-based SLAM algorithms are presented, which can serve as a point of reference. Finally, the paper concludes with an insightful discussion on the emergence of new frontiers in the domain of LiDAR-based SLAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_00276v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近几十年来，基于激光雷达（光探测和测距）的同时定位和测绘（SLAM）技术在机器人测绘领域得到了广泛的研究和发展。在本文中，我们回顾了基于激光雷达的SLAM的最新技术，并探讨了满足当代应用需求仍需关注的剩余挑战。这项研究的一个独特之处在于其文献调查，该调查专门调查了各种类型和配置的激光雷达的应用，使其与先前的综述不同。此外，还对基于激光雷达的SLAM算法进行了一些有代表性的比较，可供参考。最后，本文对基于激光雷达的SLAM领域新前沿的出现进行了深入的讨论。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.00276v1" target="_blank">2311.00276v1</a>
                              </td>
                              <td>LiDAR-based SLAM for robotic mapping: state of the art and new frontiers</td>
                              <td>Xiangdi Yue</td>
                              <td>2023-11-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_00276v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.00276v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_02257v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Visual Perception System for Autonomous Driving</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_02257v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_02257v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_02257v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The recent surge in interest in autonomous driving stems from its rapidly developing capacity to enhance safety, efficiency, and convenience. A pivotal aspect of autonomous driving technology is its perceptual systems, where core algorithms have yielded more precise algorithms applicable to autonomous driving, including vision-based Simultaneous Localization and Mapping (SLAMs), object detection, and tracking algorithms. This work introduces a visual-based perception system for autonomous driving that integrates trajectory tracking and prediction of moving objects to prevent collisions, while addressing autonomous driving's localization and mapping requirements. The system leverages motion cues from pedestrians to monitor and forecast their movements and simultaneously maps the environment. This integrated approach resolves camera localization and the tracking of other moving objects in the scene, subsequently generating a sparse map to facilitate vehicle navigation. The performance, efficiency, and resilience of this approach are substantiated through comprehensive evaluations of both simulated and real-world datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_02257v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近人们对自动驾驶的兴趣激增，源于其快速发展的提高安全性、效率和便利性的能力。自动驾驶技术的一个关键方面是其感知系统，其中核心算法产生了适用于自动驾驶的更精确的算法，包括基于视觉的同步定位和映射（SLAM）、物体检测和跟踪算法。这项工作介绍了一种基于视觉的自动驾驶感知系统，该系统集成了运动物体的轨迹跟踪和预测，以防止碰撞，同时满足自动驾驶的定位和映射要求。该系统利用行人的运动提示来监测和预测他们的运动，同时绘制环境地图。这种集成方法解决了摄像机定位和场景中其他移动物体的跟踪问题，随后生成稀疏地图以便于车辆导航。通过对模拟和真实世界数据集的综合评估，证实了这种方法的性能、效率和弹性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.02257v2" target="_blank">2303.02257v2</a>
                              </td>
                              <td>Visual Perception System for Autonomous Driving</td>
                              <td>Qi Zhang</td>
                              <td>2023-03-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_02257v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.02257v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2206_05927v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LinK3D: Linear Keypoints Representation for 3D LiDAR Point Cloud</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2206_05927v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2206_05927v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2206_05927v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Feature extraction and matching are the basic parts of many robotic vision tasks, such as 2D or 3D object detection, recognition, and registration. As known, 2D feature extraction and matching have already been achieved great success. Unfortunately, in the field of 3D, the current methods fail to support the extensive application of 3D LiDAR sensors in robotic vision tasks, due to the poor descriptiveness and inefficiency. To address this limitation, we propose a novel 3D feature representation method: Linear Keypoints representation for 3D LiDAR point cloud, called LinK3D. The novelty of LinK3D lies in that it fully considers the characteristics (such as the sparsity, and complexity of scenes) of LiDAR point clouds, and represents the keypoint with its robust neighbor keypoints, which provide strong distinction in the description of the keypoint. The proposed LinK3D has been evaluated on two public datasets (i.e., KITTI, Steven VLP16), and the experimental results show that our method greatly outperforms the state-of-the-art in matching performance. More importantly, LinK3D shows excellent real-time performance, faster than the sensor frame rate at 10 Hz of a typical rotating LiDAR sensor. LinK3D only takes an average of 32 milliseconds to extract features from the point cloud collected by a 64-beam LiDAR, and takes merely about 8 milliseconds to match two LiDAR scans when executed in a notebook with an Intel Core i7 @2.2 GHz processor. Moreover, our method can be widely extended to various 3D vision applications. In this paper, we apply the proposed LinK3D to the LiDAR odometry and place recognition task of LiDAR SLAM. The experimental results show that our method can improve the efficiency and accuracy of LiDAR SLAM system.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2206_05927v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>特征提取和匹配是许多机器人视觉任务的基本部分，如2D或3D对象检测、识别和配准。众所周知，二维特征提取和匹配已经取得了巨大的成功。不幸的是，在3D领域，由于描述性差和效率低，目前的方法无法支持3D激光雷达传感器在机器人视觉任务中的广泛应用。为了解决这一限制，我们提出了一种新的3D特征表示方法：3D激光雷达点云的线性关键点表示，称为LinK3D。LinK3D的新颖之处在于，它充分考虑了激光雷达点云的特点（如场景的稀疏性和复杂性），并用其稳健的邻居关键点来表示关键点，这在关键点的描述中提供了很强的区分性。所提出的LinK3D已经在两个公共数据集（即KITTI、Steven VLP16）上进行了评估，实验结果表明，我们的方法在匹配性能上大大优于最先进的方法。更重要的是，LinK3D显示出出色的实时性能，比典型旋转激光雷达传感器在10Hz下的传感器帧速率更快。LinK3D从64束激光雷达收集的点云中提取特征平均只需32毫秒，在使用英特尔酷睿i7@2.2 GHz处理器的笔记本电脑中执行时，匹配两次激光雷达扫描仅需约8毫秒。此外，我们的方法可以广泛扩展到各种三维视觉应用中。在本文中，我们将所提出的LinK3D应用于激光雷达SLAM的测距和位置识别任务。实验结果表明，该方法可以提高激光雷达SLAM系统的效率和精度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2206.05927v2" target="_blank">2206.05927v2</a>
                              </td>
                              <td>LinK3D: Linear Keypoints Representation for 3D LiDAR Point Cloud</td>
                              <td>Yunge Cui</td>
                              <td>2022-06-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2206_05927v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2206.05927v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_19400v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Distributed multi-agent magnetic field norm SLAM with Gaussian processes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_19400v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_19400v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_19400v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Accurately estimating the positions of multi-agent systems in indoor environments is challenging due to the lack of Global Navigation Satelite System (GNSS) signals. Noisy measurements of position and orientation can cause the integrated position estimate to drift without bound. Previous research has proposed using magnetic field simultaneous localization and mapping (SLAM) to compensate for position drift in a single agent. Here, we propose two novel algorithms that allow multiple agents to apply magnetic field SLAM using their own and other agents measurements.   Our first algorithm is a centralized approach that uses all measurements collected by all agents in a single extended Kalman filter. This algorithm simultaneously estimates the agents position and orientation and the magnetic field norm in a central unit that can communicate with all agents at all times. In cases where a central unit is not available, and there are communication drop-outs between agents, our second algorithm is a distributed approach that can be employed.   We tested both algorithms by estimating the position of magnetometers carried by three people in an optical motion capture lab with simulated odometry and simulated communication dropouts between agents. We show that both algorithms are able to compensate for drift in a case where single-agent SLAM is not. We also discuss the conditions for the estimate from our distributed algorithm to converge to the estimate from the centralized algorithm, both theoretically and experimentally.   Our experiments show that, for a communication drop-out rate of 80 percent, our proposed distributed algorithm, on average, provides a more accurate position estimate than single-agent SLAM. Finally, we demonstrate the drift-compensating abilities of our centralized algorithm on a real-life pedestrian localization problem with multiple agents moving inside a building.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_19400v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于缺乏全球导航卫星系统（GNSS）信号，准确估计多智能体系统在室内环境中的位置具有挑战性。位置和方向的噪声测量可能导致积分位置估计漂移而不受约束。先前的研究已经提出使用磁场同时定位和映射（SLAM）来补偿单个智能体中的位置漂移。在这里，我们提出了两种新的算法，允许多个代理使用他们自己和其他代理的测量来应用磁场SLAM。我们的第一个算法是一种集中式方法，它使用所有代理在单个扩展卡尔曼滤波器中收集的所有测量值。该算法同时估计可以随时与所有代理通信的中央单元中的代理位置和方向以及磁场范数。在中央单元不可用，并且代理之间存在通信中断的情况下，我们的第二种算法是可以使用的分布式方法。我们在光学运动捕捉实验室中用模拟里程计和模拟特工之间的通信中断来估计三个人携带的磁力计的位置，从而测试了这两种算法。我们证明了在单代理SLAM不存在的情况下，这两种算法都能够补偿漂移。我们还从理论和实验上讨论了分布式算法的估计收敛到集中式算法的估计的条件。我们的实验表明，对于80%的通信丢失率，我们提出的分布式算法平均比单代理SLAM提供了更准确的位置估计。最后，我们展示了我们的集中式算法在多个智能体在建筑物内移动的真实行人定位问题上的漂移补偿能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.19400v1" target="_blank">2310.19400v1</a>
                              </td>
                              <td>Distributed multi-agent magnetic field norm SLAM with Gaussian processes</td>
                              <td>Frida Viset</td>
                              <td>2023-10-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_19400v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.19400v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_18697v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">KernelGPA: A Globally Optimal Solution to Deformable SLAM in Closed-form</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_18697v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_18697v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_18697v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We study the generalized Procrustes analysis (GPA), as a minimal formulation to the simultaneous localization and mapping (SLAM) problem. We propose KernelGPA, a novel global registration technique to solve SLAM in the deformable environment. We propose the concept of deformable transformation which encodes the entangled pose and deformation. We define deformable transformations using a kernel method, and show that both the deformable transformations and the environment map can be solved globally in closed-form, up to global scale ambiguities. We solve the scale ambiguities by an optimization formulation that maximizes rigidity. We demonstrate KernelGPA using the Gaussian kernel, and validate the superiority of KernelGPA with various datasets. Code and data are available at \url{https://bitbucket.org/FangBai/deformableprocrustes}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_18697v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们研究了广义Procrustes分析（GPA），作为同时定位和映射（SLAM）问题的一个极小公式。我们提出了一种新的全局配准技术KernelGPA来解决可变形环境中的SLAM问题。我们提出了可变形变换的概念，它对纠缠的姿态和变形进行编码。我们使用核方法定义了可变形变换，并表明可变形变换和环境图都可以以闭合形式全局求解，直到全局尺度的模糊度。我们通过使刚度最大化的优化公式来解决尺度模糊性。我们使用高斯核对KernelGPA进行了验证，并用各种数据集验证了KernelGPA的优越性。代码和数据位于\url{https://bitbucket.org/fangbai/deformableprocrustes}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.18697v1" target="_blank">2310.18697v1</a>
                              </td>
                              <td>KernelGPA: A Globally Optimal Solution to Deformable SLAM in Closed-form</td>
                              <td>Fang Bai</td>
                              <td>2023-10-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_18697v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.18697v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_17879v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Split Covariance Intersection Filter Based Visual Localization With Accurate AprilTag Map For Warehouse Robot Navigation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_17879v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_17879v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_17879v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Accurate and efficient localization with conveniently-established map is the fundamental requirement for mobile robot operation in warehouse environments. An accurate AprilTag map can be conveniently established with the help of LiDAR-based SLAM. It is true that a LiDAR-based system is usually not commercially competitive in contrast with a vision-based system, yet fortunately for warehouse applications, only a single LiDAR-based SLAM system is needed to establish an accurate AprilTag map, whereas a large amount of visual localization systems can share this established AprilTag map for their own operations. Therefore, the cost of a LiDAR-based SLAM system is actually shared by the large amount of visual localization systems, and turns to be acceptable and even negligible for practical warehouse applications. Once an accurate AprilTag map is available, visual localization is realized as recursive estimation that fuses AprilTag measurements (i.e. AprilTag detection results) and robot motion data. AprilTag measurements may be nonlinear partial measurements; this can be handled by the well-known extended Kalman filter (EKF) in the spirit of local linearization. AprilTag measurements tend to have temporal correlation as well; however, this cannot be reasonably handled by the EKF. The split covariance intersection filter (Split CIF) is adopted to handle temporal correlation among AprilTag measurements. The Split CIF (in the spirit of local linearization) can also handle AprilTag nonlinear partial measurements. The Split CIF based visual localization system incorporates a measurement adaptive mechanism to handle outliers in AprilTag measurements and adopts a dynamic initialization mechanism to address the kidnapping problem. A comparative study in real warehouse environments demonstrates the potential and advantage of the Split CIF based visual localization solution.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_17879v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用方便的地图进行准确高效的定位是移动机器人在仓库环境中操作的基本要求。借助基于激光雷达的SLAM，可以方便地建立准确的AprilTag地图。的确，与基于视觉的系统相比，基于激光雷达的系统通常在商业上没有竞争力，但幸运的是，对于仓库应用来说，只需要一个基于激光DAR的SLAM系统就可以建立准确的AprilTag地图，而大量的视觉定位系统可以共享这个建立的April Tag地图用于自己的操作。因此，基于激光雷达的SLAM系统的成本实际上由大量的视觉定位系统分担，并且对于实际的仓库应用来说是可接受的，甚至可以忽略不计。一旦准确的AprilTag地图可用，视觉定位就被实现为融合AprilTag测量（即AprilTag-检测结果）和机器人运动数据的递归估计。AprilTag测量可以是非线性部分测量；这可以通过众所周知的扩展卡尔曼滤波器（EKF）本着局部线性化的精神来处理。AprilTag测量也往往具有时间相关性；然而，EKF不能合理地处理这一问题。采用分割协方差交集滤波器（split CIF）来处理AprilTag测量之间的时间相关性。Split CIF（本着局部线性化的精神）也可以处理AprilTag非线性部分测量。基于分割CIF的视觉定位系统结合了测量自适应机制来处理AprilTag测量中的异常值，并采用动态初始化机制来解决绑架问题。在真实仓库环境中进行的比较研究表明了基于拆分CIF的视觉定位解决方案的潜力和优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.17879v1" target="_blank">2310.17879v1</a>
                              </td>
                              <td>Split Covariance Intersection Filter Based Visual Localization With Accurate AprilTag Map For Warehouse Robot Navigation</td>
                              <td>Susu Fang</td>
                              <td>2023-10-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_17879v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.17879v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15023v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SONIC: Sonar Image Correspondence using Pose Supervised Learning for Imaging Sonars</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15023v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15023v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15023v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we address the challenging problem of data association for underwater SLAM through a novel method for sonar image correspondence using learned features. We introduce SONIC (SONar Image Correspondence), a pose-supervised network designed to yield robust feature correspondence capable of withstanding viewpoint variations. The inherent complexity of the underwater environment stems from the dynamic and frequently limited visibility conditions, restricting vision to a few meters of often featureless expanses. This makes camera-based systems suboptimal in most open water application scenarios. Consequently, multibeam imaging sonars emerge as the preferred choice for perception sensors. However, they too are not without their limitations. While imaging sonars offer superior long-range visibility compared to cameras, their measurements can appear different from varying viewpoints. This inherent variability presents formidable challenges in data association, particularly for feature-based methods. Our method demonstrates significantly better performance in generating correspondences for sonar images which will pave the way for more accurate loop closure constraints and sonar-based place recognition. Code as well as simulated and real-world datasets will be made public to facilitate further development in the field.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15023v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们通过一种利用学习特征进行声纳图像对应的新方法，解决了水下SLAM的数据关联这一具有挑战性的问题。我们介绍了SONIC（SONar图像对应），这是一种姿态监督网络，旨在产生能够承受视点变化的鲁棒特征对应。水下环境固有的复杂性源于动态和经常有限的能见度条件，将视野限制在几米的范围内，通常没有特征。这使得基于摄像头的系统在大多数开放水域应用场景中都不理想。因此，多波束成像声纳成为感知传感器的首选。然而，它们也并非没有局限性。虽然与相机相比，成像声纳提供了优越的远程可见性，但它们的测量结果在不同的视角下可能会有所不同。这种固有的可变性给数据关联带来了巨大的挑战，尤其是对于基于特征的方法。我们的方法在为声纳图像生成对应关系方面表现出明显更好的性能，这将为更准确的闭环约束和基于声纳的位置识别铺平道路。代码以及模拟和真实世界的数据集将公开，以促进该领域的进一步发展。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15023v1" target="_blank">2310.15023v1</a>
                              </td>
                              <td>SONIC: Sonar Image Correspondence using Pose Supervised Learning for Imaging Sonars</td>
                              <td>Samiran Gode</td>
                              <td>2023-10-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15023v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15023v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_07241v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ConceptFusion: Open-set Multimodal 3D Mapping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_07241v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_07241v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_07241v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Building 3D maps of the environment is central to robot navigation, planning, and interaction with objects in a scene. Most existing approaches that integrate semantic concepts with 3D maps largely remain confined to the closed-set setting: they can only reason about a finite set of concepts, pre-defined at training time. Further, these maps can only be queried using class labels, or in recent work, using text prompts.   We address both these issues with ConceptFusion, a scene representation that is (1) fundamentally open-set, enabling reasoning beyond a closed set of concepts and (ii) inherently multimodal, enabling a diverse range of possible queries to the 3D map, from language, to images, to audio, to 3D geometry, all working in concert. ConceptFusion leverages the open-set capabilities of today's foundation models pre-trained on internet-scale data to reason about concepts across modalities such as natural language, images, and audio. We demonstrate that pixel-aligned open-set features can be fused into 3D maps via traditional SLAM and multi-view fusion approaches. This enables effective zero-shot spatial reasoning, not needing any additional training or finetuning, and retains long-tailed concepts better than supervised approaches, outperforming them by more than 40% margin on 3D IoU. We extensively evaluate ConceptFusion on a number of real-world datasets, simulated home environments, a real-world tabletop manipulation task, and an autonomous driving platform. We showcase new avenues for blending foundation models with 3D open-set multimodal mapping.   For more information, visit our project page https://concept-fusion.github.io or watch our 5-minute explainer video https://www.youtube.com/watch?v=rkXgws8fiDs</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_07241v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>构建环境的3D地图是机器人导航、规划以及与场景中对象交互的核心。大多数现有的将语义概念与3D地图集成的方法在很大程度上仍然局限于闭集设置：它们只能推理在训练时预定义的有限概念集。此外，这些映射只能使用类标签查询，或者在最近的工作中使用文本提示查询。我们使用ConceptFusion解决了这两个问题，它是一种场景表示，（1）基本上是开集的，能够超越封闭的概念集进行推理；（2）固有的多模式，能够对3D地图进行各种可能的查询，从语言到图像，从音频到3D几何，所有这些都协同工作。ConceptFusion利用当今在互联网规模数据上预先训练的基础模型的开放集功能，对自然语言、图像和音频等模态的概念进行推理。我们证明了像素对齐的开放集特征可以通过传统的SLAM和多视图融合方法融合到3D地图中。这实现了有效的零样本空间推理，不需要任何额外的训练或微调，并比监督方法更好地保留了长尾概念，在3D IoU上比它们高出40%以上。我们在许多真实世界的数据集、模拟家庭环境、真实世界的桌面操作任务和自动驾驶平台上广泛评估了ConceptFusion。我们展示了将基础模型与3D开放集多模式映射相结合的新途径。有关更多信息，请访问我们的项目页面https://concept-fusion.github.io或者观看我们的5分钟解说视频https://www.youtube.com/watch?v=rkxgws8fids</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.07241v3" target="_blank">2302.07241v3</a>
                              </td>
                              <td>ConceptFusion: Open-set Multimodal 3D Mapping</td>
                              <td>Krishna Murthy Jatavallabhula</td>
                              <td>2023-02-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_07241v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.07241v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/concept-fusion/concept-fusion" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="SFM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2308_15984v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Structure-from-Motion with Graph Attention Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_15984v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_15984v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_15984v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we tackle the problem of learning Structure-from-Motion (SfM) through the use of graph attention networks. SfM is a classic computer vision problem that is solved though iterative minimization of reprojection errors, referred to as Bundle Adjustment (BA), starting from a good initialization. In order to obtain a good enough initialization to BA, conventional methods rely on a sequence of sub-problems (such as pairwise pose estimation, pose averaging or triangulation) which provides an initial solution that can then be refined using BA. In this work we replace these sub-problems by learning a model that takes as input the 2D keypoints detected across multiple views, and outputs the corresponding camera poses and 3D keypoint coordinates. Our model takes advantage of graph neural networks to learn SfM-specific primitives, and we show that it can be used for fast inference of the reconstruction for new and unseen sequences. The experimental results show that the proposed model outperforms competing learning-based methods, and challenges COLMAP while having lower runtime.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_15984v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们通过使用图注意力网络来解决从运动中学习结构（SfM）的问题。SfM是一个经典的计算机视觉问题，通过迭代最小化重投影误差来解决，称为束调整（BA），从良好的初始化开始。为了获得对BA足够好的初始化，传统方法依赖于一系列子问题（如成对姿态估计、姿态平均或三角测量），这些子问题提供了一个初始解决方案，然后可以使用BA进行细化。在这项工作中，我们通过学习一个模型来替换这些子问题，该模型将在多个视图中检测到的2D关键点作为输入，并输出相应的相机姿态和3D关键点坐标。我们的模型利用图神经网络来学习SfM特定的基元，并表明它可以用于新的和看不见的序列的重建的快速推理。实验结果表明，所提出的模型优于竞争的基于学习的方法，并在具有较低运行时间的同时挑战了COLMAP。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.15984v2" target="_blank">2308.15984v2</a>
                              </td>
                              <td>Learning Structure-from-Motion with Graph Attention Networks</td>
                              <td>Lucas Brynte</td>
                              <td>2023-08-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_15984v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.15984v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_17245v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_17245v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_17245v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_17245v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advancements in real-time neural rendering using point-based techniques have paved the way for the widespread adoption of 3D representations. However, foundational approaches like 3D Gaussian Splatting come with a substantial storage overhead caused by growing the SfM points to millions, often demanding gigabyte-level disk space for a single unbounded scene, posing significant scalability challenges and hindering the splatting efficiency.   To address this challenge, we introduce LightGaussian, a novel method designed to transform 3D Gaussians into a more efficient and compact format. Drawing inspiration from the concept of Network Pruning, LightGaussian identifies Gaussians that are insignificant in contributing to the scene reconstruction and adopts a pruning and recovery process, effectively reducing redundancy in Gaussian counts while preserving visual effects. Additionally, LightGaussian employs distillation and pseudo-view augmentation to distill spherical harmonics to a lower degree, allowing knowledge transfer to more compact representations while maintaining reflectance. Furthermore, we propose a hybrid scheme, VecTree Quantization, to quantize all attributes, resulting in lower bitwidth representations with minimal accuracy losses.   In summary, LightGaussian achieves an averaged compression rate over 15x while boosting the FPS from 139 to 215, enabling an efficient representation of complex scenes on Mip-NeRF 360, Tank and Temple datasets.   Project website: https://lightgaussian.github.io/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_17245v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用基于点的技术进行实时神经渲染的最新进展为3D表示的广泛采用铺平了道路。然而，像3D高斯飞溅这样的基础方法会带来大量的存储开销，这是由于SfM点增长到数百万，通常需要千兆字节级别的磁盘空间才能用于单个无边界场景，这对可扩展性提出了重大挑战，并阻碍了飞溅效率。为了应对这一挑战，我们引入了LightGaussian，这是一种新的方法，旨在将3D高斯变换为更高效、更紧凑的格式。LightGaussian从网络修剪的概念中汲取灵感，识别出对场景重建贡献不大的高斯，并采用修剪和恢复过程，有效地减少了高斯计数的冗余，同时保留了视觉效果。此外，LightGaussian采用蒸馏和伪视图增强来提取较低程度的球面谐波，允许将知识转移到更紧凑的表示中，同时保持反射率。此外，我们提出了一种混合方案，VecTree量化，来量化所有属性，从而以最小的精度损失获得较低的位宽表示。总之，LightGaussian实现了超过15倍的平均压缩率，同时将FPS从139提高到215，从而能够在Mip-NeRF 360、Tank和Temple数据集上高效地表示复杂场景。项目网站：https://lightgaussian.github.io/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.17245v2" target="_blank">2311.17245v2</a>
                              </td>
                              <td>LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS</td>
                              <td>Zhiwen Fan</td>
                              <td>2023-11-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_17245v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.17245v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/VITA-Group/LightGaussian" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00451v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00451v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00451v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00451v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Novel view synthesis from limited observations remains an important and persistent task. However, high efficiency in existing NeRF-based few-shot view synthesis is often compromised to obtain an accurate 3D representation. To address this challenge, we propose a few-shot view synthesis framework based on 3D Gaussian Splatting that enables real-time and photo-realistic view synthesis with as few as three training views. The proposed method, dubbed FSGS, handles the extremely sparse initialized SfM points with a thoughtfully designed Gaussian Unpooling process. Our method iteratively distributes new Gaussians around the most representative locations, subsequently infilling local details in vacant areas. We also integrate a large-scale pre-trained monocular depth estimator within the Gaussians optimization process, leveraging online augmented views to guide the geometric optimization towards an optimal solution. Starting from sparse points observed from limited input viewpoints, our FSGS can accurately grow into unseen regions, comprehensively covering the scene and boosting the rendering quality of novel views. Overall, FSGS achieves state-of-the-art performance in both accuracy and rendering efficiency across diverse datasets, including LLFF, Mip-NeRF360, and Blender. Project website: https://zehaozhu.github.io/FSGS/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00451v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从有限的观测中合成新的观点仍然是一项重要而持久的任务。然而，为了获得准确的3D表示，现有的基于NeRF的少镜头视图合成中的高效率经常受到损害。为了应对这一挑战，我们提出了一种基于3D高斯散射的多镜头视图合成框架，该框架能够在只有三个训练视图的情况下进行实时和照片逼真的视图合成。所提出的方法被称为FSGS，通过精心设计的高斯去极化过程来处理极稀疏的初始化SfM点。我们的方法迭代地将新的高斯分布在最具代表性的位置周围，随后在空置区域填充局部细节。我们还在Gaussians优化过程中集成了一个大规模的预训练单目深度估计器，利用在线增强视图来引导几何优化走向最优解。从有限输入视点观察到的稀疏点开始，我们的FSGS可以准确地生长到看不见的区域，全面覆盖场景，提高新视图的渲染质量。总体而言，FSGS在各种数据集（包括LLFF、Mip-NeRF360和Blender）的准确性和渲染效率方面都达到了最先进的性能。项目网站：https://zehaozhu.github.io/fsgs/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00451v1" target="_blank">2312.00451v1</a>
                              </td>
                              <td>FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting</td>
                              <td>Zehao Zhu</td>
                              <td>2023-12-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00451v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00451v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_18801v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Distributed Global Structure-from-Motion with a Deep Front-End</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_18801v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_18801v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_18801v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While initial approaches to Structure-from-Motion (SfM) revolved around both global and incremental methods, most recent applications rely on incremental systems to estimate camera poses due to their superior robustness. Though there has been tremendous progress in SfM `front-ends' powered by deep models learned from data, the state-of-the-art (incremental) SfM pipelines still rely on classical SIFT features, developed in 2004. In this work, we investigate whether leveraging the developments in feature extraction and matching helps global SfM perform on par with the SOTA incremental SfM approach (COLMAP). To do so, we design a modular SfM framework that allows us to easily combine developments in different stages of the SfM pipeline. Our experiments show that while developments in deep-learning based two-view correspondence estimation do translate to improvements in point density for scenes reconstructed with global SfM, none of them outperform SIFT when comparing with incremental SfM results on a range of datasets. Our SfM system is designed from the ground up to leverage distributed computation, enabling us to parallelize computation on multiple machines and scale to large scenes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_18801v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>虽然最初的运动结构（SfM）方法围绕着全局和增量方法，但由于其优越的鲁棒性，最近的应用依赖于增量系统来估计相机姿态。尽管通过从数据中学习的深度模型在SfM“前端”方面取得了巨大进展，但最先进的（增量）SfM管道仍然依赖于2004年开发的经典SIFT特征。在这项工作中，我们研究了利用特征提取和匹配的发展是否有助于全局SfM与SOTA增量SfM方法（COLMAP）不相上下。为此，我们设计了一个模块化的SfM框架，使我们能够轻松地将SfM管道不同阶段的开发结合起来。我们的实验表明，虽然基于深度学习的两视图对应性估计的发展确实转化为用全局SfM重建的场景的点密度的提高，但与一系列数据集上的增量SfM结果相比，它们都没有优于SIFT。我们的SfM系统是从头开始设计的，以利用分布式计算，使我们能够在多台机器上并行计算并扩展到大型场景。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.18801v1" target="_blank">2311.18801v1</a>
                              </td>
                              <td>Distributed Global Structure-from-Motion with a Deep Front-End</td>
                              <td>Ayush Baid</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_18801v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.18801v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/borglab/gtsfm" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_11702v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SACReg: Scene-Agnostic Coordinate Regression for Visual Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_11702v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_11702v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_11702v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Scene coordinates regression (SCR), i.e., predicting 3D coordinates for every pixel of a given image, has recently shown promising potential. However, existing methods remain limited to small scenes memorized during training, and thus hardly scale to realistic datasets and scenarios. In this paper, we propose a generalized SCR model trained once to be deployed in new test scenes, regardless of their scale, without any finetuning. Instead of encoding the scene coordinates into the network weights, our model takes as input a database image with some sparse 2D pixel to 3D coordinate annotations, extracted from e.g. off-the-shelf Structure-from-Motion or RGB-D data, and a query image for which are predicted a dense 3D coordinate map and its confidence, based on cross-attention. At test time, we rely on existing off-the-shelf image retrieval systems and fuse the predictions from a shortlist of relevant database images w.r.t. the query. Afterwards camera pose is obtained using standard Perspective-n-Point (PnP). Starting from selfsupervised CroCo pretrained weights, we train our model on diverse datasets to ensure generalizabilty across various scenarios, and significantly outperform other scene regression approaches, including scene-specific models, on multiple visual localization benchmarks. Finally, we show that the database representation of images and their 2D-3D annotations can be highly compressed with negligible loss of localization performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_11702v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>场景坐标回归（SCR），即预测给定图像的每个像素的3D坐标，最近显示出了很有前途的潜力。然而，现有的方法仍然局限于训练期间记忆的小场景，因此很难扩展到真实的数据集和场景。在本文中，我们提出了一个经过一次训练的广义SCR模型，该模型将部署在新的测试场景中，无论其规模如何，而无需任何微调。我们的模型不是将场景坐标编码到网络权重中，而是将具有一些稀疏的2D像素到3D坐标注释的数据库图像作为输入，该数据库图像是从例如现成的运动结构或RGB-D数据中提取的，以及查询图像，基于交叉关注，对其预测密集的3D坐标图及其置信度。在测试时，我们依赖现有的现成图像检索系统，并将相关数据库图像的短名单中的预测与查询相融合。然后，使用标准透视n-Point（PnP）来获得相机姿势。从自监督CroCo预训练的权重开始，我们在不同的数据集上训练我们的模型，以确保在各种场景中的可推广性，并在多个视觉定位基准上显著优于其他场景回归方法，包括场景特定模型。最后，我们证明了图像的数据库表示及其2D-3D注释可以被高度压缩，而定位性能的损失可以忽略不计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.11702v3" target="_blank">2307.11702v3</a>
                              </td>
                              <td>SACReg: Scene-Agnostic Coordinate Regression for Visual Localization</td>
                              <td>Jerome Revaud</td>
                              <td>2023-07-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_11702v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.11702v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_03704v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Pose-Free Generalizable Rendering Transformer</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_03704v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_03704v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_03704v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In the field of novel-view synthesis, the necessity of knowing camera poses (e.g., via Structure from Motion) before rendering has been a common practice. However, the consistent acquisition of accurate camera poses remains elusive, and errors in pose extraction can adversely impact the view synthesis process. To address this challenge, we introduce PF-GRT, a new Pose-Free framework for Generalizable Rendering Transformer, eliminating the need for pre-computed camera poses and instead leveraging feature-matching learned directly from data. PF-GRT is parameterized using a local relative coordinate system, where one of the source images is set as the origin. An OmniView Transformer is designed for fusing multi-view cues under the pose-free setting, where unposed-view fusion and origin-centric aggregation are performed. The 3D point feature along target ray is sampled by projecting onto the selected origin plane. The final pixel intensities are modulated and decoded using another Transformer. PF-GRT demonstrates an impressive ability to generalize to new scenes that were not encountered during the training phase, without the need of pre-computing camera poses. Our experiments with zero-shot rendering on the LLFF, RealEstate-10k, Shiny, and Blender datasets reveal that it produces superior quality in generating photo-realistic images. Moreover, it demonstrates robustness against noise in test camera poses. Code is available at https://zhiwenfan.github.io/PF-GRT/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_03704v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在新视图合成领域，在渲染之前了解相机姿势（例如，通过运动结构）的必要性已经成为一种常见的做法。然而，准确的相机姿势的一致获取仍然难以捉摸，姿势提取中的错误可能会对视图合成过程产生不利影响。为了应对这一挑战，我们引入了PF-GRT，这是一种用于通用渲染转换器的新的无姿势框架，无需预先计算相机姿势，而是利用直接从数据中学习的特征匹配。PF-GRT使用局部相对坐标系进行参数化，其中一个源图像被设置为原点。OmniView Transformer设计用于在无姿势设置下融合多视图线索，其中执行未融合的视图融合和以原点为中心的聚合。通过投影到选定的原点平面上，对沿目标射线的三维点特征进行采样。使用另一个Transformer对最终像素强度进行调制和解码。PF-GRT展示了一种令人印象深刻的能力，可以推广到训练阶段没有遇到的新场景，而无需预先计算相机姿势。我们在LLFF、RealEstate-10k、Shiny和Blender数据集上进行的零样本渲染实验表明，它在生成照片真实感图像时产生了卓越的质量。此外，它还展示了在测试相机姿态时对噪声的鲁棒性。代码位于https://zhiwenfan.github.io/pf-grt/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.03704v2" target="_blank">2310.03704v2</a>
                              </td>
                              <td>Pose-Free Generalizable Rendering Transformer</td>
                              <td>Zhiwen Fan</td>
                              <td>2023-10-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_03704v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.03704v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zhiwenfan/DragView" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11808v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Robot Hand-Eye Calibration using Structure-from-Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11808v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11808v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11808v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we propose a new flexible method for hand-eye calibration. The vast majority of existing hand-eye calibration techniques requires a calibration rig which is used in conjunction with camera pose estimation methods. Instead, we combine structure-from-motion with known robot motions and we show that the solution can be obtained in linear form. The latter solves for both the hand-eye parameters and for the unknown scale factor inherent with structure-from-motion methods. The algebraic analysis that is made possible with such a linear formulation allows to investigate not only the well known case of general screw motions but also such singular motions as pure translations, pure rotations, and planar motions. In essence, the robot-mounted camera looks to an unknown rigid layout, tracks points over an image sequence and estimates the camera-to-robot relationship. Such a self calibration process is relevant for unmanned vehicles, robots working in remote places, and so forth. We conduct a large number of experiments which validate the quality of the method by comparing it with existing ones.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11808v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了一种新的灵活的手眼校准方法。绝大多数现有的手眼校准技术需要与相机姿态估计方法结合使用的校准装置。相反，我们将运动中的结构与已知的机器人运动相结合，证明了该解可以以线性形式获得。后者同时求解手眼参数和运动中结构方法固有的未知比例因子。用这种线性公式进行代数分析不仅可以研究一般螺杆运动的已知情况，还可以研究纯平移、纯旋转和平面运动等奇异运动。本质上，安装在机器人上的相机观察未知的刚性布局，跟踪图像序列上的点，并估计相机与机器人的关系。这种自校准过程与无人车、在偏远地区工作的机器人等相关。我们进行了大量的实验，通过与现有方法的比较验证了该方法的质量。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11808v2" target="_blank">2311.11808v2</a>
                              </td>
                              <td>Robot Hand-Eye Calibration using Structure-from-Motion</td>
                              <td>Nicolas Andreff</td>
                              <td>2023-11-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11808v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11808v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11171v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LOSTU: Fast, Scalable, and Uncertainty-Aware Triangulation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11171v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11171v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11171v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Triangulation algorithms often aim to minimize the reprojection ($L_2$) error, but this only provides the maximum likelihood estimate when there are no errors in the camera parameters or camera poses. Although recent advancements have yielded techniques to estimate camera parameters accounting for 3D point uncertainties, most structure from motion (SfM) pipelines still use older triangulation algorithms. This work leverages recent discoveries to provide a fast, scalable, and statistically optimal way to triangulate called LOSTU. Results show that LOSTU consistently produces lower 3D reconstruction errors than conventional $L_2$ triangulation methods -- often allowing LOSTU to successfully triangulate more points. Moreover, in addition to providing a better 3D reconstruction, LOSTU can be substantially faster than Levenberg-Marquardt (or similar) optimization schemes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11171v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三角测量算法通常旨在最小化重投影（$L_2$）误差，但这仅在相机参数或相机姿态没有误差时提供最大似然估计。尽管最近的进步已经产生了估计相机参数的技术，考虑到3D点的不确定性，但大多数运动结构（SfM）管道仍然使用旧的三角测量算法。这项工作利用最近的发现，提供了一种快速、可扩展和统计优化的三角测量方法，称为LOSTU。结果表明，与传统的$L_2$三角测量方法相比，LOSTU始终产生较低的三维重建误差——通常允许LOSTU成功地对更多的点进行三角测量。此外，除了提供更好的3D重建外，LOSTU可以比Levenberg-Marquardt（或类似）优化方案快得多。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11171v1" target="_blank">2311.11171v1</a>
                              </td>
                              <td>LOSTU: Fast, Scalable, and Uncertainty-Aware Triangulation</td>
                              <td>Sébastien Henry</td>
                              <td>2023-11-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11171v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11171v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_10582v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Human motion trajectory prediction using the Social Force Model for real-time and low computational cost applications</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_10582v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_10582v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_10582v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Human motion trajectory prediction is a very important functionality for human-robot collaboration, specifically in accompanying, guiding, or approaching tasks, but also in social robotics, self-driving vehicles, or security systems. In this paper, a novel trajectory prediction model, Social Force Generative Adversarial Network (SoFGAN), is proposed. SoFGAN uses a Generative Adversarial Network (GAN) and Social Force Model (SFM) to generate different plausible people trajectories reducing collisions in a scene. Furthermore, a Conditional Variational Autoencoder (CVAE) module is added to emphasize the destination learning. We show that our method is more accurate in making predictions in UCY or BIWI datasets than most of the current state-of-the-art models and also reduces collisions in comparison to other approaches. Through real-life experiments, we demonstrate that the model can be used in real-time without GPU's to perform good quality predictions with a low computational cost.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_10582v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人类运动轨迹预测是人机协作的一个非常重要的功能，特别是在伴随、引导或接近任务时，也在社交机器人、自动驾驶车辆或安全系统中。本文提出了一种新的轨迹预测模型——社会力量生成对抗网络（SoFGAN）。SoFGAN使用生成对抗网络（GAN）和社会力量模型（SFM）来生成不同的看似合理的人的轨迹，从而减少场景中的碰撞。此外，增加了条件变分自动编码器（CVAE）模块，以强调目的地学习。我们表明，与当前大多数最先进的模型相比，我们的方法在UCY或BIWI数据集中进行预测时更准确，并且与其他方法相比，还减少了碰撞。通过真实的实验，我们证明了该模型可以在没有GPU的情况下实时使用，以低计算成本执行高质量的预测。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.10582v1" target="_blank">2311.10582v1</a>
                              </td>
                              <td>Human motion trajectory prediction using the Social Force Model for real-time and low computational cost applications</td>
                              <td>Oscar Gil</td>
                              <td>2023-11-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_10582v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.10582v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09012v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Yes, we CANN: Constrained Approximate Nearest Neighbors for local feature-based visual localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09012v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09012v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09012v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large-scale visual localization systems continue to rely on 3D point clouds built from image collections using structure-from-motion. While the 3D points in these models are represented using local image features, directly matching a query image's local features against the point cloud is challenging due to the scale of the nearest-neighbor search problem. Many recent approaches to visual localization have thus proposed a hybrid method, where first a global (per image) embedding is used to retrieve a small subset of database images, and local features of the query are matched only against those. It seems to have become common belief that global embeddings are critical for said image-retrieval in visual localization, despite the significant downside of having to compute two feature types for each query image. In this paper, we take a step back from this assumption and propose Constrained Approximate Nearest Neighbors (CANN), a joint solution of k-nearest-neighbors across both the geometry and appearance space using only local features. We first derive the theoretical foundation for k-nearest-neighbor retrieval across multiple metrics and then showcase how CANN improves visual localization. Our experiments on public localization benchmarks demonstrate that our method significantly outperforms both state-of-the-art global feature-based retrieval and approaches using local feature aggregation schemes. Moreover, it is an order of magnitude faster in both index and query time than feature aggregation schemes for these datasets. Code will be released.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09012v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大规模视觉定位系统继续依赖于使用来自运动的结构从图像集合构建的3D点云。虽然这些模型中的3D点是使用局部图像特征来表示的，但由于最近邻搜索问题的规模，将查询图像的局部特征与点云直接匹配是具有挑战性的。因此，许多最近的视觉定位方法提出了一种混合方法，其中首先使用全局（每图像）嵌入来检索数据库图像的一小部分，并且仅针对查询的局部特征进行匹配。人们似乎已经普遍认为，全局嵌入对于视觉定位中的所述图像检索至关重要，尽管必须为每个查询图像计算两种特征类型是显著的缺点。在本文中，我们从这一假设后退一步，提出了约束近似最近邻（CANN），这是一种仅使用局部特征在几何和外观空间上的k个最近邻的联合解。我们首先推导了跨多个度量的k近邻检索的理论基础，然后展示了CANN如何改进视觉定位。我们在公共定位基准上的实验表明，我们的方法显著优于最先进的基于全局特征的检索和使用局部特征聚合方案的方法。此外，它在索引和查询时间上都比这些数据集的特征聚合方案快一个数量级。将发布代码。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09012v2" target="_blank">2306.09012v2</a>
                              </td>
                              <td>Yes, we CANN: Constrained Approximate Nearest Neighbors for local feature-based visual localization</td>
                              <td>Dror Aiger</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09012v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09012v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/google-research/google-research" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_03426v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Convex Minimization with Integer Minima in $\widetilde O(n^4)$ Time</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_03426v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_03426v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_03426v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Given a convex function $f$ on $\mathbb{R}^n$ with an integer minimizer, we show how to find an exact minimizer of $f$ using $O(n^2 \log n)$ calls to a separation oracle and $O(n^4 \log n)$ time. The previous best polynomial time algorithm for this problem given in [Jiang, SODA 2021, JACM 2022] achieves $O(n^2\log\log n/\log n)$ oracle complexity. However, the overall runtime of Jiang's algorithm is at least $\widetilde{\Omega}(n^8)$, due to expensive sub-routines such as the Lenstra-Lenstra-Lov\'asz (LLL) algorithm [Lenstra, Lenstra, Lov\'asz, Math. Ann. 1982] and random walk based cutting plane method [Bertsimas, Vempala, JACM 2004]. Our significant speedup is obtained by a nontrivial combination of a faster version of the LLL algorithm due to [Neumaier, Stehl\'e, ISSAC 2016] that gives similar guarantees, the volumetric center cutting plane method (CPM) by [Vaidya, FOCS 1989] and its fast implementation given in [Jiang, Lee, Song, Wong, STOC 2020].   For the special case of submodular function minimization (SFM), our result implies a strongly polynomial time algorithm for this problem using $O(n^3 \log n)$ calls to an evaluation oracle and $O(n^4 \log n)$ additional arithmetic operations. Both the oracle complexity and the number of arithmetic operations of our more general algorithm are better than the previous best-known runtime algorithms for this specific problem given in [Lee, Sidford, Wong, FOCS 2015] and [Dadush, V\'egh, Zambelli, SODA 2018, MOR 2021].</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_03426v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>给定$\mathbb｛R｝^n$上具有整数极小值的凸函数$f$，我们展示了如何使用对分离预言机的$O（n^2\logn）$调用和$O（n ^4\logn）$时间来找到$f$的精确极小值。[Jiang，SODA 2021，JACM 2022]中给出的该问题的先前最佳多项式时间算法实现了$O（n^2 \log\logn/\logn）$oracle复杂度。然而，由于昂贵的子程序，如Lenstra-Lenstra-Lov'asz（LLL）算法[Lenstra，Lenstra，Lov'asz，Math.Ann.1982]和基于随机行走的切割平面方法[Bertsimas，Vempala，JACM 2004]，姜算法的总体运行时间至少为$\widetilde｛\Omega｝（n^8）$。我们的显著加速是通过[Neumaier，Stehl\'e，ISSAC 2016]的LLL算法的更快版本、[Vaidya，FOCS 1989]的体积中心切割平面法（CPM）及其在[Jiang，Lee，Song，Wong，STOC 2020]中给出的快速实现的非平凡组合而获得的。对于子模函数最小化（SFM）的特殊情况，我们的结果暗示了该问题的强多项式时间算法，使用$O（n^3\logn）$对评估预言机的调用和$O（n ^4\logn。对于这一特定问题，我们更通用的算法的预言机复杂性和算术运算次数都优于[Lee，Sidford，Wong，FOCS 2015]和[Dadush，V\'egh，Zambelli，SODA 2018，MOR 2021]中给出的以前最著名的运行时算法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.03426v2" target="_blank">2304.03426v2</a>
                              </td>
                              <td>Convex Minimization with Integer Minima in $\widetilde O(n^4)$ Time</td>
                              <td>Haotian Jiang</td>
                              <td>2023-04-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_03426v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.03426v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_06137v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MonoProb: Self-Supervised Monocular Depth Estimation with Interpretable Uncertainty</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_06137v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_06137v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_06137v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised monocular depth estimation methods aim to be used in critical applications such as autonomous vehicles for environment analysis. To circumvent the potential imperfections of these approaches, a quantification of the prediction confidence is crucial to guide decision-making systems that rely on depth estimation. In this paper, we propose MonoProb, a new unsupervised monocular depth estimation method that returns an interpretable uncertainty, which means that the uncertainty reflects the expected error of the network in its depth predictions. We rethink the stereo or the structure-from-motion paradigms used to train unsupervised monocular depth models as a probabilistic problem. Within a single forward pass inference, this model provides a depth prediction and a measure of its confidence, without increasing the inference time. We then improve the performance on depth and uncertainty with a novel self-distillation loss for which a student is supervised by a pseudo ground truth that is a probability distribution on depth output by a teacher. To quantify the performance of our models we design new metrics that, unlike traditional ones, measure the absolute performance of uncertainty predictions. Our experiments highlight enhancements achieved by our method on standard depth and uncertainty metrics as well as on our tailored metrics. https://github.com/CEA-LIST/MonoProb</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_06137v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督单目深度估计方法旨在用于关键应用，如用于环境分析的自动驾驶汽车。为了避免这些方法的潜在缺陷，预测置信度的量化对于指导依赖深度估计的决策系统至关重要。在本文中，我们提出了MonoProb，这是一种新的无监督单目深度估计方法，它返回可解释的不确定性，这意味着不确定性反映了网络在深度预测中的预期误差。我们将用于训练无监督单目深度模型的运动范式中的立体或结构重新思考为一个概率问题。在单次前向推理中，该模型提供深度预测及其置信度的测量，而不增加推理时间。然后，我们通过一种新颖的自蒸馏损失来提高深度和不确定性方面的性能，对于这种损失，学生受到伪基本真理的监督，伪基本真理是教师输出的深度上的概率分布。为了量化我们模型的性能，我们设计了新的指标，与传统指标不同，这些指标衡量不确定性预测的绝对性能。我们的实验强调了我们的方法在标准深度和不确定性指标以及我们定制的指标上实现的增强。https://github.com/cea-list/monoprob</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.06137v1" target="_blank">2311.06137v1</a>
                              </td>
                              <td>MonoProb: Self-Supervised Monocular Depth Estimation with Interpretable Uncertainty</td>
                              <td>Rémi Marsal</td>
                              <td>2023-11-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_06137v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.06137v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/cea-list/monoprob" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_05323v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Spatial Attention-based Distribution Integration Network for Human Pose Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_05323v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_05323v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_05323v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, human pose estimation has made significant progress through the implementation of deep learning techniques. However, these techniques still face limitations when confronted with challenging scenarios, including occlusion, diverse appearances, variations in illumination, and overlap. To cope with such drawbacks, we present the Spatial Attention-based Distribution Integration Network (SADI-NET) to improve the accuracy of localization in such situations. Our network consists of three efficient models: the receptive fortified module (RFM), spatial fusion module (SFM), and distribution learning module (DLM). Building upon the classic HourglassNet architecture, we replace the basic block with our proposed RFM. The RFM incorporates a dilated residual block and attention mechanism to expand receptive fields while enhancing sensitivity to spatial information. In addition, the SFM incorporates multi-scale characteristics by employing both global and local attention mechanisms. Furthermore, the DLM, inspired by residual log-likelihood estimation (RLE), reconfigures a predicted heatmap using a trainable distribution weight. For the purpose of determining the efficacy of our model, we conducted extensive experiments on the MPII and LSP benchmarks. Particularly, our model obtained a remarkable $92.10\%$ percent accuracy on the MPII test dataset, demonstrating significant improvements over existing models and establishing state-of-the-art performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_05323v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，通过深度学习技术的实施，人体姿态估计取得了重大进展。然而，当面临具有挑战性的场景时，这些技术仍然面临限制，包括遮挡、不同的外观、照明的变化和重叠。为了解决这些缺点，我们提出了基于空间注意力的分布集成网络（SADI-NET）来提高这种情况下的定位精度。我们的网络由三个有效的模型组成：接受强化模块（RFM）、空间融合模块（SFM）和分布学习模块（DLM）。在经典HourglassNet架构的基础上，我们用我们提出的RFM取代了基本块。RFM结合了扩张的残差块和注意力机制，以扩大感受野，同时增强对空间信息的敏感性。此外，通过采用全局和局部注意力机制，SFM融合了多尺度特征。此外，受残差对数似然估计（RLE）的启发，DLM使用可训练分布权重重新配置预测热图。为了确定我们的模型的有效性，我们在MPII和LSP基准上进行了广泛的实验。特别是，我们的模型在MPII测试数据集上获得了92.10%$%的显著准确率，证明了与现有模型相比的显著改进，并建立了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.05323v1" target="_blank">2311.05323v1</a>
                              </td>
                              <td>Spatial Attention-based Distribution Integration Network for Human Pose Estimation</td>
                              <td>Sihan Gao</td>
                              <td>2023-11-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_05323v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.05323v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_04634v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">VET: Visual Error Tomography for Point Cloud Completion and High-Quality Neural Rendering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_04634v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_04634v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_04634v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In the last few years, deep neural networks opened the doors for big advances in novel view synthesis. Many of these approaches are based on a (coarse) proxy geometry obtained by structure from motion algorithms. Small deficiencies in this proxy can be fixed by neural rendering, but larger holes or missing parts, as they commonly appear for thin structures or for glossy regions, still lead to distracting artifacts and temporal instability. In this paper, we present a novel neural-rendering-based approach to detect and fix such deficiencies. As a proxy, we use a point cloud, which allows us to easily remove outlier geometry and to fill in missing geometry without complicated topological operations. Keys to our approach are (i) a differentiable, blending point-based renderer that can blend out redundant points, as well as (ii) the concept of Visual Error Tomography (VET), which allows us to lift 2D error maps to identify 3D-regions lacking geometry and to spawn novel points accordingly. Furthermore, (iii) by adding points as nested environment maps, our approach allows us to generate high-quality renderings of the surroundings in the same pipeline. In our results, we show that our approach can improve the quality of a point cloud obtained by structure from motion and thus increase novel view synthesis quality significantly. In contrast to point growing techniques, the approach can also fix large-scale holes and missing thin structures effectively. Rendering quality outperforms state-of-the-art methods and temporal stability is significantly improved, while rendering is possible at real-time frame rates.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_04634v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在过去的几年里，深度神经网络为新视图合成的巨大进步打开了大门。这些方法中的许多都是基于通过结构从运动算法获得的（粗略）代理几何结构。这种代理中的小缺陷可以通过神经渲染来修复，但较大的孔洞或缺失部分，通常出现在薄结构或光滑区域，仍然会导致分散注意力的伪影和时间不稳定。在本文中，我们提出了一种新的基于神经渲染的方法来检测和修复这些缺陷。作为代理，我们使用点云，这使我们能够轻松删除异常几何体并填充缺失的几何体，而无需复杂的拓扑操作。我们方法的关键是（i）一种可微分的、基于混合点的渲染器，它可以混合掉多余的点，以及（ii）视觉误差层析成像（VET）的概念，它允许我们提升2D误差图，以识别缺乏几何结构的3D区域，并相应地生成新的点。此外，（iii）通过添加点作为嵌套的环境贴图，我们的方法使我们能够在同一管道中生成高质量的周围环境渲染图。在我们的结果中，我们表明我们的方法可以提高由结构从运动中获得的点云的质量，从而显著提高新视图合成的质量。与点生长技术相比，该方法还可以有效地修复大规模孔洞和缺失的薄结构。渲染质量优于最先进的方法，时间稳定性显著提高，同时可以以实时帧速率进行渲染。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.04634v1" target="_blank">2311.04634v1</a>
                              </td>
                              <td>VET: Visual Error Tomography for Point Cloud Completion and High-Quality Neural Rendering</td>
                              <td>Linus Franke</td>
                              <td>2023-11-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_04634v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.04634v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/lfranke/vet" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03404v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RGB-D Mapping and Tracking in a Plenoxel Radiance Field</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03404v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03404v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03404v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The widespread adoption of Neural Radiance Fields (NeRFs) have ensured significant advances in the domain of novel view synthesis in recent years. These models capture a volumetric radiance field of a scene, creating highly convincing, dense, photorealistic models through the use of simple, differentiable rendering equations. Despite their popularity, these algorithms suffer from severe ambiguities in visual data inherent to the RGB sensor, which means that although images generated with view synthesis can visually appear very believable, the underlying 3D model will often be wrong. This considerably limits the usefulness of these models in practical applications like Robotics and Extended Reality (XR), where an accurate dense 3D reconstruction otherwise would be of significant value. In this paper, we present the vital differences between view synthesis models and 3D reconstruction models. We also comment on why a depth sensor is essential for modeling accurate geometry in general outward-facing scenes using the current paradigm of novel view synthesis methods. Focusing on the structure-from-motion task, we practically demonstrate this need by extending the Plenoxel radiance field model: Presenting an analytical differential approach for dense mapping and tracking with radiance fields based on RGB-D data without a neural network. Our method achieves state-of-the-art results in both mapping and tracking tasks, while also being faster than competing neural network-based approaches. The code is available at: https://github.com/ysus33/RGB-D_Plenoxel_Mapping_Tracking.git</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03404v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，神经辐射场（NeRF）的广泛采用确保了新视图合成领域的重大进展。这些模型捕捉场景的体积辐射场，通过使用简单的、可微分的渲染方程创建高度令人信服的、密集的照片真实感模型。尽管这些算法很受欢迎，但RGB传感器固有的视觉数据存在严重的模糊性，这意味着尽管通过视图合成生成的图像在视觉上看起来非常可信，但底层的3D模型往往是错误的。这大大限制了这些模型在机器人和扩展现实（XR）等实际应用中的有用性，否则，精确的密集3D重建将具有重要价值。在本文中，我们介绍了视图合成模型和三维重建模型之间的重要区别。我们还评论了为什么深度传感器对于使用当前新型视图合成方法的范式在一般面向外的场景中建模精确的几何体至关重要。专注于运动任务中的结构，我们通过扩展Plenoxel辐射场模型来实际证明这一需求：在没有神经网络的情况下，基于RGB-D数据，提出了一种用于辐射场密集映射和跟踪的解析微分方法。我们的方法在映射和跟踪任务中都取得了最先进的结果，同时也比竞争对手的基于神经网络的方法更快。代码位于：https://github.com/ysus33/rgb-d_plenoxel_mapping_tracking.git</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03404v2" target="_blank">2307.03404v2</a>
                              </td>
                              <td>RGB-D Mapping and Tracking in a Plenoxel Radiance Field</td>
                              <td>Andreas L. Teigen</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03404v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03404v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/ysus33/rgb-d_plenoxel_mapping_tracking" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_14364v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Quantitative Evaluation of Dense 3D Reconstruction of Sinus Anatomy from Monocular Endoscopic Video</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_14364v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_14364v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_14364v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Generating accurate 3D reconstructions from endoscopic video is a promising avenue for longitudinal radiation-free analysis of sinus anatomy and surgical outcomes. Several methods for monocular reconstruction have been proposed, yielding visually pleasant 3D anatomical structures by retrieving relative camera poses with structure-from-motion-type algorithms and fusion of monocular depth estimates. However, due to the complex properties of the underlying algorithms and endoscopic scenes, the reconstruction pipeline may perform poorly or fail unexpectedly. Further, acquiring medical data conveys additional challenges, presenting difficulties in quantitatively benchmarking these models, understanding failure cases, and identifying critical components that contribute to their precision. In this work, we perform a quantitative analysis of a self-supervised approach for sinus reconstruction using endoscopic sequences paired with optical tracking and high-resolution computed tomography acquired from nine ex-vivo specimens. Our results show that the generated reconstructions are in high agreement with the anatomy, yielding an average point-to-mesh error of 0.91 mm between reconstructions and CT segmentations. However, in a point-to-point matching scenario, relevant for endoscope tracking and navigation, we found average target registration errors of 6.58 mm. We identified that pose and depth estimation inaccuracies contribute equally to this error and that locally consistent sequences with shorter trajectories generate more accurate reconstructions. These results suggest that achieving global consistency between relative camera poses and estimated depths with the anatomy is essential. In doing so, we can ensure proper synergy between all components of the pipeline for improved reconstructions that will facilitate clinical application of this innovative technology.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_14364v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从内窥镜视频中生成准确的3D重建是对鼻窦解剖结构和手术结果进行纵向无辐射分析的一种很有前途的途径。已经提出了几种单目重建方法，通过从运动类型算法中检索具有结构的相对相机姿态并融合单目深度估计，产生视觉上令人愉快的3D解剖结构。然而，由于底层算法和内窥镜场景的复杂特性，重建管道可能表现不佳或意外失败。此外，获取医疗数据带来了额外的挑战，在定量基准测试这些模型、了解故障案例和确定有助于其准确性的关键组件方面存在困难。在这项工作中，我们对自监督鼻窦重建方法进行了定量分析，该方法使用内窥镜序列与从9个离体标本中采集的光学跟踪和高分辨率计算机断层扫描相结合。我们的结果表明，生成的重建与解剖结构高度一致，在重建和CT分割之间产生0.91mm的平均点到网格误差。然而，在与内窥镜跟踪和导航相关的点对点匹配场景中，我们发现平均目标配准误差为6.58 mm。我们发现，姿态和深度估计的不准确度对该误差的贡献相同，并且轨迹较短的局部一致序列会产生更准确的重建。这些结果表明，实现相对相机姿态和估计深度与解剖结构之间的全局一致性至关重要。通过这样做，我们可以确保管道的所有组成部分之间的适当协同作用，以改进重建，从而促进这项创新技术的临床应用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.14364v1" target="_blank">2310.14364v1</a>
                              </td>
                              <td>A Quantitative Evaluation of Dense 3D Reconstruction of Sinus Anatomy from Monocular Endoscopic Video</td>
                              <td>Jan Emily Mangulabnan</td>
                              <td>2023-10-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_14364v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.14364v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_13605v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FMRT: Learning Accurate Feature Matching with Reconciliatory Transformer</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_13605v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_13605v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_13605v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Local Feature Matching, an essential component of several computer vision tasks (e.g., structure from motion and visual localization), has been effectively settled by Transformer-based methods. However, these methods only integrate long-range context information among keypoints with a fixed receptive field, which constrains the network from reconciling the importance of features with different receptive fields to realize complete image perception, hence limiting the matching accuracy. In addition, these methods utilize a conventional handcrafted encoding approach to integrate the positional information of keypoints into the visual descriptors, which limits the capability of the network to extract reliable positional encoding message. In this study, we propose Feature Matching with Reconciliatory Transformer (FMRT), a novel Transformer-based detector-free method that reconciles different features with multiple receptive fields adaptively and utilizes parallel networks to realize reliable positional encoding. Specifically, FMRT proposes a dedicated Reconciliatory Transformer (RecFormer) that consists of a Global Perception Attention Layer (GPAL) to extract visual descriptors with different receptive fields and integrate global context information under various scales, Perception Weight Layer (PWL) to measure the importance of various receptive fields adaptively, and Local Perception Feed-forward Network (LPFFN) to extract deep aggregated multi-scale local feature representation. Extensive experiments demonstrate that FMRT yields extraordinary performance on multiple benchmarks, including pose estimation, visual localization, homography estimation, and image matching.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_13605v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>局部特征匹配是计算机视觉任务（如运动结构和视觉定位）的重要组成部分，已通过基于Transformer的方法得到有效解决。然而，这些方法只将关键点之间的长程上下文信息与固定的感受野相结合，这限制了网络协调特征与不同感受野的重要性以实现完整的图像感知，从而限制了匹配精度。此外，这些方法利用传统的手工编码方法将关键点的位置信息集成到视觉描述符中，这限制了网络提取可靠位置编码消息的能力。在这项研究中，我们提出了具有协调变换器的特征匹配（FMRT），这是一种新的基于变换器的无检测器方法，它自适应地协调不同特征与多个感受野，并利用并行网络实现可靠的位置编码。具体而言，FMRT提出了一种专用的协调转换器（RecFormer），该转换器由全局感知注意力层（GPAL）组成，用于提取具有不同感受野的视觉描述符并整合各种尺度下的全局上下文信息，感知权重层（PWL）用于自适应地测量各种感受野的重要性，以及局部感知前馈网络（LPFFN）来提取深度聚合的多尺度局部特征表示。大量实验表明，FMRT在多个基准上产生了非凡的性能，包括姿态估计、视觉定位、单应性估计和图像匹配。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.13605v1" target="_blank">2310.13605v1</a>
                              </td>
                              <td>FMRT: Learning Accurate Feature Matching with Reconciliatory Transformer</td>
                              <td>Xinyu Zhang</td>
                              <td>2023-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_13605v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.13605v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09109v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09109v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09109v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09109v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in neural reconstruction enable high-quality 3D object reconstruction from casually captured image collections. Current techniques mostly analyze their progress on relatively simple image collections where Structure-from-Motion (SfM) techniques can provide ground-truth (GT) camera poses. We note that SfM techniques tend to fail on in-the-wild image collections such as image search results with varying backgrounds and illuminations. To enable systematic research progress on 3D reconstruction from casual image captures, we propose NAVI: a new dataset of category-agnostic image collections of objects with high-quality 3D scans along with per-image 2D-3D alignments providing near-perfect GT camera parameters. These 2D-3D alignments allow us to extract accurate derivative annotations such as dense pixel correspondences, depth and segmentation maps. We demonstrate the use of NAVI image collections on different problem settings and show that NAVI enables more thorough evaluations that were not possible with existing datasets. We believe NAVI is beneficial for systematic research progress on 3D reconstruction and correspondence estimation. Project page: https://navidataset.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09109v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经重建的最新进展使得能够从随意捕获的图像集合中进行高质量的3D对象重建。当前的技术主要分析它们在相对简单的图像采集上的进展，其中运动结构（SfM）技术可以提供地面实况（GT）相机姿态。我们注意到，SfM技术在野生图像集合中往往失败，例如具有不同背景和照明的图像搜索结果。为了实现从随意图像捕获的3D重建的系统研究进展，我们提出了NAVI：一个具有高质量3D扫描的对象的类别不可知图像集合的新数据集，以及提供近乎完美的GT相机参数的每张图像2D-3D对齐。这些2D-3D比对允许我们提取精确的导数注释，例如密集像素对应、深度和分割图。我们展示了NAVI图像集合在不同问题设置中的使用，并表明NAVI能够实现现有数据集无法实现的更彻底的评估。我们认为NAVI有利于三维重建和对应关系估计的系统研究进展。项目页面：https://navidataset.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09109v2" target="_blank">2306.09109v2</a>
                              </td>
                              <td>NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations</td>
                              <td>Varun Jampani</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09109v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09109v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_05504v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Colmap-PCD: An Open-source Tool for Fine Image-to-point cloud Registration</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_05504v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_05504v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_05504v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>State-of-the-art techniques for monocular camera reconstruction predominantly rely on the Structure from Motion (SfM) pipeline. However, such methods often yield reconstruction outcomes that lack crucial scale information, and over time, accumulation of images leads to inevitable drift issues. In contrast, mapping methods based on LiDAR scans are popular in large-scale urban scene reconstruction due to their precise distance measurements, a capability fundamentally absent in visual-based approaches. Researchers have made attempts to utilize concurrent LiDAR and camera measurements in pursuit of precise scaling and color details within mapping outcomes. However, the outcomes are subject to extrinsic calibration and time synchronization precision. In this paper, we propose a novel cost-effective reconstruction pipeline that utilizes a pre-established LiDAR map as a fixed constraint to effectively address the inherent scale challenges present in monocular camera reconstruction. To our knowledge, our method is the first to register images onto the point cloud map without requiring synchronous capture of camera and LiDAR data, granting us the flexibility to manage reconstruction detail levels across various areas of interest. To facilitate further research in this domain, we have released Colmap-PCD${^{3}}$, an open-source tool leveraging the Colmap algorithm, that enables precise fine-scale registration of images to the point cloud map.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_05504v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>用于单目相机重建的现有技术主要依赖于运动结构（SfM）流水线。然而，这种方法往往会产生缺乏关键尺度信息的重建结果，随着时间的推移，图像的积累会导致不可避免的漂移问题。相比之下，基于激光雷达扫描的地图绘制方法由于其精确的距离测量而在大规模城市场景重建中很受欢迎，而这在基于视觉的方法中根本不具备。研究人员试图利用激光雷达和相机的同时测量，在地图绘制结果中追求精确的缩放和颜色细节。然而，结果受到外部校准和时间同步精度的影响。在本文中，我们提出了一种新的具有成本效益的重建管道，该管道利用预先建立的激光雷达图作为固定约束，以有效解决单目相机重建中存在的固有规模挑战。据我们所知，我们的方法是第一个将图像配准到点云图上，而不需要同步捕获相机和激光雷达数据，这使我们能够灵活地管理各个感兴趣区域的重建细节水平。为了促进该领域的进一步研究，我们发布了Colmap PCD$｛^｛3｝｝$，这是一款利用Colmap算法的开源工具，可以将图像精确地精细配准到点云地图上。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.05504v1" target="_blank">2310.05504v1</a>
                              </td>
                              <td>Colmap-PCD: An Open-source Tool for Fine Image-to-point cloud Registration</td>
                              <td>Chunge Bai</td>
                              <td>2023-10-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_05504v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.05504v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/xiaobaiiiiii/colmap-pcd" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_05134v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LocoNeRF: A NeRF-based Approach for Local Structure from Motion for Precise Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_05134v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_05134v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_05134v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual localization is a critical task in mobile robotics, and researchers are continuously developing new approaches to enhance its efficiency. In this article, we propose a novel approach to improve the accuracy of visual localization using Structure from Motion (SfM) techniques. We highlight the limitations of global SfM, which suffers from high latency, and the challenges of local SfM, which requires large image databases for accurate reconstruction. To address these issues, we propose utilizing Neural Radiance Fields (NeRF), as opposed to image databases, to cut down on the space required for storage. We suggest that sampling reference images around the prior query position can lead to further improvements. We evaluate the accuracy of our proposed method against ground truth obtained using LIDAR and Advanced Lidar Odometry and Mapping in Real-time (A-LOAM), and compare its storage usage against local SfM with COLMAP in the conducted experiments. Our proposed method achieves an accuracy of 0.068 meters compared to the ground truth, which is slightly lower than the most advanced method COLMAP, which has an accuracy of 0.022 meters. However, the size of the database required for COLMAP is 400 megabytes, whereas the size of our NeRF model is only 160 megabytes. Finally, we perform an ablation study to assess the impact of using reference images from the NeRF reconstruction.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_05134v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉定位是移动机器人的一项关键任务，研究人员正在不断开发新的方法来提高其效率。在本文中，我们提出了一种使用运动结构（SfM）技术提高视觉定位准确性的新方法。我们强调了全局SfM的局限性，它具有高延迟，以及局部SfM面临的挑战，后者需要大型图像数据库才能进行精确重建。为了解决这些问题，我们建议利用神经辐射场（NeRF），而不是图像数据库，来减少存储所需的空间。我们建议，对先前查询位置周围的参考图像进行采样可以带来进一步的改进。我们根据使用激光雷达和高级激光雷达实时测距和测绘（A-LOAM）获得的地面实况评估了我们提出的方法的准确性，并在所进行的实验中比较了其相对于局部SfM和COLMAP的存储使用情况。与地面实况相比，我们提出的方法实现了0.068米的精度，这略低于最先进的方法COLMAP，后者的精度为0.022米。然而，COLMAP所需的数据库大小为400兆字节，而我们的NeRF模型的大小仅为160兆字节。最后，我们进行了消融研究，以评估使用NeRF重建的参考图像的影响。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.05134v1" target="_blank">2310.05134v1</a>
                              </td>
                              <td>LocoNeRF: A NeRF-based Approach for Local Structure from Motion for Precise Localization</td>
                              <td>Artem Nenashev</td>
                              <td>2023-10-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_05134v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.05134v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2204_04145v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Constrained Bundle Adjustment for Structure From Motion Using Uncalibrated Multi-Camera Systems</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2204_04145v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2204_04145v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2204_04145v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Structure from motion using uncalibrated multi-camera systems is a challenging task. This paper proposes a bundle adjustment solution that implements a baseline constraint respecting that these cameras are static to each other. We assume these cameras are mounted on a mobile platform, uncalibrated, and coarsely synchronized. To this end, we propose the baseline constraint that is formulated for the scenario in which the cameras have overlapping views. The constraint is incorporated in the bundle adjustment solution to keep the relative motion of different cameras static. Experiments were conducted using video frames of two collocated GoPro cameras mounted on a vehicle with no system calibration. These two cameras were placed capturing overlapping contents. We performed our bundle adjustment using the proposed constraint and then produced 3D dense point clouds. Evaluations were performed by comparing these dense point clouds against LiDAR reference data. We showed that, as compared to traditional bundle adjustment, our proposed method achieved an improvement of 29.38%.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2204_04145v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用未校准的多摄像机系统从运动中构建结构是一项具有挑战性的任务。本文提出了一种束平差解决方案，该解决方案实现了一个基线约束，即这些相机彼此静止。我们假设这些相机安装在移动平台上，未经校准，并且粗略同步。为此，我们提出了针对相机具有重叠视图的场景制定的基线约束。该约束被纳入束调整解决方案中，以保持不同相机的相对运动静止。实验使用安装在车辆上的两个并置GoPro相机的视频帧进行，无需系统校准。这两台摄像机被放置在拍摄重叠内容的位置。我们使用所提出的约束进行了束调整，然后生成了3D密集点云。通过将这些密集点云与激光雷达参考数据进行比较来进行评估。我们表明，与传统的束平差相比，我们提出的方法实现了29.38%的改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2204.04145v2" target="_blank">2204.04145v2</a>
                              </td>
                              <td>Constrained Bundle Adjustment for Structure From Motion Using Uncalibrated Multi-Camera Systems</td>
                              <td>Debao Huang</td>
                              <td>2022-04-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2204_04145v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2204.04145v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_01092v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Leveraging Cutting Edge Deep Learning Based Image Matching for Reconstructing a Large Scene from Sparse Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_01092v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_01092v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_01092v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present the top ranked solution for the AISG-SLA Visual Localisation Challenge benchmark (IJCAI 2023), where the task is to estimate relative motion between images taken in sequence by a camera mounted on a car driving through an urban scene.   For matching images we use our recent deep learning based matcher RoMa. Matching image pairs sequentially and estimating relative motion from point correspondences sampled by RoMa already gives very competitive results -- third rank on the challenge benchmark.   To improve the estimations we extract keypoints in the images, match them using RoMa, and perform structure from motion reconstruction using COLMAP. We choose our recent DeDoDe keypoints for their high repeatability. Further, we address time jumps in the image sequence by matching specific non-consecutive image pairs based on image retrieval with DINOv2. These improvements yield a solution beating all competitors.   We further present a loose upper bound on the accuracy obtainable by the image retrieval approach by also matching hand-picked non-consecutive pairs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_01092v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们为AISG-SLA视觉定位挑战基准（IJCAI 2023）提供了排名最高的解决方案，其中的任务是估计安装在汽车上的摄像头在城市场景中依次拍摄的图像之间的相对运动。对于匹配图像，我们使用最近的基于深度学习的匹配器RoMa。按顺序匹配图像对，并根据RoMa采样的点对应关系估计相对运动，已经给出了非常有竞争力的结果——在挑战基准上排名第三。为了改进估计，我们提取图像中的关键点，使用RoMa进行匹配，并使用COLMAP从运动重建中执行结构。我们选择了最近的DeDoDe关键点，因为它们具有很高的可重复性。此外，我们通过将基于图像检索的特定非连续图像对与DINOv2进行匹配来解决图像序列中的时间跳跃问题。这些改进产生了一个击败所有竞争对手的解决方案。我们进一步提出了图像检索方法通过匹配手工挑选的非连续对所获得的精度的宽松上限。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.01092v1" target="_blank">2310.01092v1</a>
                              </td>
                              <td>Leveraging Cutting Edge Deep Learning Based Image Matching for Reconstructing a Large Scene from Sparse Images</td>
                              <td>Georg Bökman</td>
                              <td>2023-10-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_01092v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.01092v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_00783v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Propagating Semantic Labels in Video Data</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_00783v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_00783v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_00783v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Semantic Segmentation combines two sub-tasks: the identification of pixel-level image masks and the application of semantic labels to those masks. Recently, so-called Foundation Models have been introduced; general models trained on very large datasets which can be specialized and applied to more specific tasks. One such model, the Segment Anything Model (SAM), performs image segmentation. Semantic segmentation systems such as CLIPSeg and MaskRCNN are trained on datasets of paired segments and semantic labels. Manual labeling of custom data, however, is time-consuming. This work presents a method for performing segmentation for objects in video. Once an object has been found in a frame of video, the segment can then be propagated to future frames; thus reducing manual annotation effort. The method works by combining SAM with Structure from Motion (SfM). The video input to the system is first reconstructed into 3D geometry using SfM. A frame of video is then segmented using SAM. Segments identified by SAM are then projected onto the the reconstructed 3D geometry. In subsequent video frames, the labeled 3D geometry is reprojected into the new perspective, allowing SAM to be invoked fewer times. System performance is evaluated, including the contributions of the SAM and SfM components. Performance is evaluated over three main metrics: computation time, mask IOU with manual labels, and the number of tracking losses. Results demonstrate that the system has substantial computation time improvements over human performance for tracking objects over video frames, but suffers in performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_00783v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语义分割结合了两个子任务：像素级图像掩码的识别和对掩码应用语义标签。最近，引入了所谓的基础模型；在非常大的数据集上训练的通用模型，这些数据集可以专门化并应用于更具体的任务。一个这样的模型，分段任意模型（SAM），执行图像分割。像CLIPSeg和MaskRCNN这样的语义分割系统是在成对片段和语义标签的数据集上训练的。但是，手动标记自定义数据非常耗时。这项工作提出了一种对视频中的对象进行分割的方法。一旦在视频帧中找到对象，则可以将该片段传播到未来的帧；从而减少了手动注释的工作量。该方法将SAM与运动结构（SfM）相结合。首先使用SfM将输入到系统的视频重构为3D几何结构。然后使用SAM对视频帧进行分割。然后将SAM识别的片段投影到重建的3D几何体上。在随后的视频帧中，标记的3D几何体被重新投影到新的透视图中，从而减少SAM的调用次数。评估系统性能，包括SAM和SfM组件的贡献。性能通过三个主要指标进行评估：计算时间、带有手动标签的掩码IOU和跟踪丢失数量。结果表明，该系统在视频帧上跟踪对象的计算时间大大提高了人类的性能，但性能较差。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.00783v1" target="_blank">2310.00783v1</a>
                              </td>
                              <td>Propagating Semantic Labels in Video Data</td>
                              <td>David Balaban</td>
                              <td>2023-10-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_00783v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.00783v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_16632v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Sparse Submodular Function Minimization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_16632v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_16632v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_16632v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we study the problem of minimizing a submodular function $f : 2^V \rightarrow \mathbb{R}$ that is guaranteed to have a $k$-sparse minimizer. We give a deterministic algorithm that computes an additive $\epsilon$-approximate minimizer of such $f$ in $\widetilde{O}(\mathsf{poly}(k) \log(|f|/\epsilon))$ parallel depth using a polynomial number of queries to an evaluation oracle of $f$, where $|f| = \max_{S \subseteq V} |f(S)|$. Further, we give a randomized algorithm that computes an exact minimizer of $f$ with high probability using $\widetilde{O}(|V| \cdot \mathsf{poly}(k))$ queries and polynomial time. When $k = \widetilde{O}(1)$, our algorithms use either nearly-constant parallel depth or a nearly-linear number of evaluation oracle queries. All previous algorithms for this problem either use $\Omega(|V|)$ parallel depth or $\Omega(|V|^2)$ queries.   In contrast to state-of-the-art weakly-polynomial and strongly-polynomial time algorithms for SFM, our algorithms use first-order optimization methods, e.g., mirror descent and follow the regularized leader. We introduce what we call {\em sparse dual certificates}, which encode information on the structure of sparse minimizers, and both our parallel and sequential algorithms provide new algorithmic tools for allowing first-order optimization methods to efficiently compute them. Correspondingly, our algorithm does not invoke fast matrix multiplication or general linear system solvers and in this sense is more combinatorial than previous state-of-the-art methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_16632v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们研究了子模函数$f:2^V\rightarrow\mathbb｛R｝$的最小化问题，该子模函数保证具有$k$-稀疏极小值。我们给出了一个确定性算法，该算法使用对$f$的评估预言的多项式查询数来计算$\widetilde｛O｝（\mathsf｛poly｝（k）\log（|f|/\epsilon））$并行深度中的$f$近似极小值，其中$|f|=\max_｛S\substeq V｝|f（S）|$。此外，我们给出了一个随机算法，该算法使用$\widetilde{O}（|V|\cdot\mathsf{poly}（k））$查询和多项式时间以高概率计算$f$的精确极小值。当$k=\widetilde｛O｝（1）$时，我们的算法使用几乎恒定的并行深度或几乎线性数量的评估oracle查询。以前针对此问题的所有算法都使用$\Omega（|V|）$并行深度或$\Omega（|V|^2）$查询。与最先进的SFM弱多项式和强多项式时间算法相比，我们的算法使用一阶优化方法，例如镜像下降和遵循正则化前导。我们介绍了我们所称的｛\em稀疏双证书｝，它对关于稀疏最小化器结构的信息进行编码，并且我们的并行和顺序算法都提供了新的算法工具，允许一阶优化方法有效地计算它们。相应地，我们的算法不调用快速矩阵乘法或一般线性系统求解器，并且在这个意义上比以前最先进的方法更具组合性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.16632v1" target="_blank">2309.16632v1</a>
                              </td>
                              <td>Sparse Submodular Function Minimization</td>
                              <td>Andrei Graur</td>
                              <td>2023-09-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_16632v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.16632v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_13772v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Motion Segmentation from a Moving Monocular Camera</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_13772v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_13772v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_13772v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Identifying and segmenting moving objects from a moving monocular camera is difficult when there is unknown camera motion, different types of object motions and complex scene structures. To tackle these challenges, we take advantage of two popular branches of monocular motion segmentation approaches: point trajectory based and optical flow based methods, by synergistically fusing these two highly complementary motion cues at object level. By doing this, we are able to model various complex object motions in different scene structures at once, which has not been achieved by existing methods. We first obtain object-specific point trajectories and optical flow mask for each common object in the video, by leveraging the recent foundational models in object recognition, segmentation and tracking. We then construct two robust affinity matrices representing the pairwise object motion affinities throughout the whole video using epipolar geometry and the motion information provided by optical flow. Finally, co-regularized multi-view spectral clustering is used to fuse the two affinity matrices and obtain the final clustering. Our method shows state-of-the-art performance on the KT3DMoSeg dataset, which contains complex motions and scene structures. Being able to identify moving objects allows us to remove them for map building when using visual SLAM or SFM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_13772v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当存在未知的摄像机运动、不同类型的物体运动和复杂的场景结构时，从移动的单目摄像机中识别和分割移动物体是困难的。为了应对这些挑战，我们利用了单目运动分割方法的两个流行分支：基于点轨迹的方法和基于光流的方法，通过在对象级别协同融合这两个高度互补的运动线索。通过这样做，我们能够同时对不同场景结构中的各种复杂物体运动进行建模，这是现有方法无法实现的。我们首先利用对象识别、分割和跟踪方面的最新基础模型，获得视频中每个常见对象的特定对象点轨迹和光流掩模。然后，我们使用极线几何和光流提供的运动信息构建了两个稳健的仿射矩阵，表示整个视频中的成对对象运动仿射。最后，使用共正则化多视图谱聚类来融合两个亲和矩阵，得到最终的聚类结果。我们的方法在KT3DMoSeg数据集上显示了最先进的性能，该数据集包含复杂的运动和场景结构。当使用视觉SLAM或SFM时，能够识别移动物体使我们能够将其移除以用于地图构建。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.13772v1" target="_blank">2309.13772v1</a>
                              </td>
                              <td>Motion Segmentation from a Moving Monocular Camera</td>
                              <td>Yuxiang Huang</td>
                              <td>2023-09-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_13772v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.13772v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_12804v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Scalable Semantic 3D Mapping of Coral Reefs with Deep Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_12804v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_12804v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_12804v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Coral reefs are among the most diverse ecosystems on our planet, and are depended on by hundreds of millions of people. Unfortunately, most coral reefs are existentially threatened by global climate change and local anthropogenic pressures. To better understand the dynamics underlying deterioration of reefs, monitoring at high spatial and temporal resolution is key. However, conventional monitoring methods for quantifying coral cover and species abundance are limited in scale due to the extensive manual labor required. Although computer vision tools have been employed to aid in this process, in particular SfM photogrammetry for 3D mapping and deep neural networks for image segmentation, analysis of the data products creates a bottleneck, effectively limiting their scalability. This paper presents a new paradigm for mapping underwater environments from ego-motion video, unifying 3D mapping systems that use machine learning to adapt to challenging conditions under water, combined with a modern approach for semantic segmentation of images. The method is exemplified on coral reefs in the northern Gulf of Aqaba, Red Sea, demonstrating high-precision 3D semantic mapping at unprecedented scale with significantly reduced required labor costs: a 100 m video transect acquired within 5 minutes of diving with a cheap consumer-grade camera can be fully automatically analyzed within 5 minutes. Our approach significantly scales up coral reef monitoring by taking a leap towards fully automatic analysis of video transects. The method democratizes coral reef transects by reducing the labor, equipment, logistics, and computing cost. This can help to inform conservation policies more efficiently. The underlying computational method of learning-based Structure-from-Motion has broad implications for fast low-cost mapping of underwater environments other than coral reefs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_12804v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>珊瑚礁是地球上最多样化的生态系统之一，数亿人依赖珊瑚礁。不幸的是，大多数珊瑚礁都受到全球气候变化和当地人为压力的威胁。为了更好地了解珊瑚礁退化背后的动力学，以高空间和时间分辨率进行监测是关键。然而，由于需要大量的体力劳动，量化珊瑚覆盖率和物种丰度的传统监测方法在规模上受到限制。尽管计算机视觉工具已被用于帮助这一过程，特别是用于3D地图绘制的SfM摄影测量和用于图像分割的深度神经网络，但数据产品的分析造成了瓶颈，有效地限制了其可扩展性。本文提出了一种从自我运动视频映射水下环境的新范式，将使用机器学习来适应水下具有挑战性的条件的3D映射系统与图像语义分割的现代方法相结合。该方法以红海亚喀巴湾北部的珊瑚礁为例，展示了前所未有的高精度3D语义映射，大大降低了所需的劳动力成本：用廉价的消费级相机在潜水5分钟内获取的100米视频样带可以在5分钟内全自动分析。我们的方法通过向视频样带的全自动分析迈出了一大步，大大扩大了珊瑚礁监测的规模。该方法通过减少劳动力、设备、物流和计算成本，使珊瑚礁断面民主化。这有助于更有效地为保护政策提供信息。基于运动结构学习的基本计算方法对珊瑚礁以外的水下环境的快速低成本测绘具有广泛的意义。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.12804v1" target="_blank">2309.12804v1</a>
                              </td>
                              <td>Scalable Semantic 3D Mapping of Coral Reefs with Deep Learning</td>
                              <td>Jonathan Sauder</td>
                              <td>2023-09-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_12804v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.12804v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_11883v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On-the-Fly SfM: What you capture is What you get</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_11883v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_11883v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_11883v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Over the last decades, ample achievements have been made on Structure from motion (SfM). However, the vast majority of them basically work in an offline manner, i.e., images are firstly captured and then fed together into a SfM pipeline for obtaining poses and sparse point cloud. In this work, on the contrary, we present an on-the-fly SfM: running online SfM while image capturing, the newly taken On-the-Fly image is online estimated with the corresponding pose and points, i.e., what you capture is what you get. Specifically, our approach firstly employs a vocabulary tree that is unsupervised trained using learning-based global features for fast image retrieval of newly fly-in image. Then, a robust feature matching mechanism with least squares (LSM) is presented to improve image registration performance. Finally, via investigating the influence of newly fly-in image's connected neighboring images, an efficient hierarchical weighted local bundle adjustment (BA) is used for optimization. Extensive experimental results demonstrate that on-the-fly SfM can meet the goal of robustly registering the images while capturing in an online way.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_11883v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在过去的几十年里，运动结构（SfM）取得了丰硕的成果。然而，它们中的绝大多数基本上是以离线方式工作的，即首先捕获图像，然后将其一起输入到SfM管道中，以获得姿态和稀疏点云。相反，在这项工作中，我们提出了一个动态SfM：在图像捕捉的同时运行在线SfM，新拍摄的动态图像是用相应的姿势和点进行在线估计的，即，你捕捉到的就是你得到的。具体来说，我们的方法首先使用了一个词汇树，该词汇树使用基于学习的全局特征进行无监督训练，用于新飞行图像的快速图像检索。然后，提出了一种鲁棒的最小二乘特征匹配机制来提高图像配准性能。最后，通过研究新飞入图像的连接相邻图像的影响，使用有效的分层加权局部束平差（BA）进行优化。大量的实验结果表明，动态SfM可以实现在以在线方式拍摄的同时稳健地配准图像的目标。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.11883v1" target="_blank">2309.11883v1</a>
                              </td>
                              <td>On-the-Fly SfM: What you capture is What you get</td>
                              <td>Zongqian Zhan</td>
                              <td>2023-09-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_11883v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.11883v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_10748v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SHOWMe: Benchmarking Object-agnostic Hand-Object 3D Reconstruction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_10748v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_10748v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_10748v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent hand-object interaction datasets show limited real object variability and rely on fitting the MANO parametric model to obtain groundtruth hand shapes. To go beyond these limitations and spur further research, we introduce the SHOWMe dataset which consists of 96 videos, annotated with real and detailed hand-object 3D textured meshes. Following recent work, we consider a rigid hand-object scenario, in which the pose of the hand with respect to the object remains constant during the whole video sequence. This assumption allows us to register sub-millimetre-precise groundtruth 3D scans to the image sequences in SHOWMe. Although simpler, this hypothesis makes sense in terms of applications where the required accuracy and level of detail is important eg., object hand-over in human-robot collaboration, object scanning, or manipulation and contact point analysis. Importantly, the rigidity of the hand-object systems allows to tackle video-based 3D reconstruction of unknown hand-held objects using a 2-stage pipeline consisting of a rigid registration step followed by a multi-view reconstruction (MVR) part. We carefully evaluate a set of non-trivial baselines for these two stages and show that it is possible to achieve promising object-agnostic 3D hand-object reconstructions employing an SfM toolbox or a hand pose estimator to recover the rigid transforms and off-the-shelf MVR algorithms. However, these methods remain sensitive to the initial camera pose estimates which might be imprecise due to lack of textures on the objects or heavy occlusions of the hands, leaving room for improvements in the reconstruction. Code and dataset are available at https://europe.naverlabs.com/research/showme</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_10748v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近的手-物体交互数据集显示出有限的真实物体可变性，并依赖于拟合MANO参数模型来获得真实的手形。为了超越这些限制并推动进一步的研究，我们引入了SHOWMe数据集，该数据集由96个视频组成，用真实和详细的手对象3D纹理网格进行注释。根据最近的工作，我们考虑了一个刚性手对象场景，其中手相对于对象的姿势在整个视频序列中保持不变。这一假设使我们能够将亚毫米精度的地面实况3D扫描注册到SHOWMe中的图像序列中。尽管更简单，但这一假设在所需精度和细节水平很重要的应用中是有意义的，例如，人机协作中的对象移交、对象扫描或操作和接触点分析。重要的是，手对象系统的刚性允许使用由刚性配准步骤和多视图重建（MVR）部分组成的两阶段流水线来处理未知手持对象的基于视频的3D重建。我们仔细评估了这两个阶段的一组非平凡基线，并表明使用SfM工具箱或手部姿态估计器来恢复刚性变换和现成的MVR算法，可以实现有前景的对象不可知的3D手部对象重建。然而，这些方法对初始相机姿态估计仍然敏感，由于对象上缺乏纹理或手的严重遮挡，初始相机姿态评估可能不精确，这为重建留下了改进的空间。代码和数据集可在https://europe.naverlabs.com/research/showme</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.10748v1" target="_blank">2309.10748v1</a>
                              </td>
                              <td>SHOWMe: Benchmarking Object-agnostic Hand-Object 3D Reconstruction</td>
                              <td>Anilkumar Swamy</td>
                              <td>2023-09-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_10748v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.10748v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_10269v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Using an Uncrewed Surface Vehicle to Create a Volumetric Model of Non-Navigable Rivers and Other Shallow Bodies of Water</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_10269v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_10269v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_10269v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Non-navigable rivers and retention ponds play important roles in buffering communities from flooding, yet emergency planners often have no data as to the volume of water that they can carry before flooding the surrounding. This paper describes a practical approach for using an uncrewed marine surface vehicle (USV) to collect and merge bathymetric maps with digital surface maps of the banks of shallow bodies of water into a unified volumetric model. The below-waterline mesh is developed by applying the Poisson surface reconstruction algorithm to the sparse sonar depth readings of the underwater surface. Dense above-waterline meshes of the banks are created using commercial structure from motion (SfM) packages. Merging is challenging for many reasons, the most significant is gaps in sensor coverage, i.e., the USV cannot collect sonar depth data or visually see sandy beaches leading to a bank thus the two meshes may not intersect. The approach is demonstrated on a Hydronalix EMILY USV with a Humminbird single beam echosounder and Teledyne FLIR camera at Lake ESTI at the Texas A&M Engineering Extension Service Disaster City complex.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_10269v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>非通航河流和蓄水池在缓冲社区免受洪水侵袭方面发挥着重要作用，但应急规划者往往没有数据表明它们在淹没周围地区之前可以携带的水量。本文描述了一种实用的方法，即使用未折叠的海洋表面飞行器（USV）收集水深图和浅水堤的数字表面图，并将其合并为一个统一的体积模型。将泊松曲面重建算法应用于水下表面的稀疏声纳深度读数，开发了水线下网格。河岸的密集水线上网格是使用商业运动结构（SfM）包创建的。由于许多原因，合并是具有挑战性的，最重要的是传感器覆盖范围的差距，即USV无法收集声纳深度数据或视觉上看到通向堤岸的沙滩，因此两个网格可能不会相交。该方法在Hydronalix EMILY USV上进行了演示，该V带有Humminbird单波束回声测深仪和Teledyne FLIR相机，位于德克萨斯州农工工程扩展服务灾难城市综合体的ESTI湖。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.10269v1" target="_blank">2309.10269v1</a>
                              </td>
                              <td>Using an Uncrewed Surface Vehicle to Create a Volumetric Model of Non-Navigable Rivers and Other Shallow Bodies of Water</td>
                              <td>Jayesh Tripathi</td>
                              <td>2023-09-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_10269v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.10269v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_08927v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DynaMoN: Motion-Aware Fast And Robust Camera Localization for Dynamic NeRF</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_08927v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_08927v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_08927v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Dynamic reconstruction with neural radiance fields (NeRF) requires accurate camera poses. These are often hard to retrieve with existing structure-from-motion (SfM) pipelines as both camera and scene content can change. We propose DynaMoN that leverages simultaneous localization and mapping (SLAM) jointly with motion masking to handle dynamic scene content. Our robust SLAM-based tracking module significantly accelerates the training process of the dynamic NeRF while improving the quality of synthesized views at the same time. Extensive experimental validation on TUM RGB-D, BONN RGB-D Dynamic and the DyCheck's iPhone dataset, three real-world datasets, shows the advantages of DynaMoN both for camera pose estimation and novel view synthesis.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_08927v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>利用神经辐射场（NeRF）进行动态重建需要精确的相机姿态。这些通常很难用现有的运动结构（SfM）管道来检索，因为相机和场景内容都可能发生变化。我们提出了DynaMoN，它利用同步定位和映射（SLAM）与运动掩蔽相结合来处理动态场景内容。我们基于SLAM的稳健跟踪模块显著加快了动态NeRF的训练过程，同时提高了合成视图的质量。对TUM RGB-D、BONN RGB-D Dynamic和DyCheck的iPhone数据集这三个真实世界的数据集进行了广泛的实验验证，显示了DynaMoN在相机姿态估计和新颖视图合成方面的优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.08927v1" target="_blank">2309.08927v1</a>
                              </td>
                              <td>DynaMoN: Motion-Aware Fast And Robust Camera Localization for Dynamic NeRF</td>
                              <td>Mert Asim Karaoglu</td>
                              <td>2023-09-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_08927v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.08927v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_04643v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Parallel Submodular Function Minimization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_04643v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_04643v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_04643v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We consider the parallel complexity of submodular function minimization (SFM). We provide a pair of methods which obtain two new query versus depth trade-offs a submodular function defined on subsets of $n$ elements that has integer values between $-M$ and $M$. The first method has depth $2$ and query complexity $n^{O(M)}$ and the second method has depth $\widetilde{O}(n^{1/3} M^{2/3})$ and query complexity $O(\mathrm{poly}(n, M))$. Despite a line of work on improved parallel lower bounds for SFM, prior to our work the only known algorithms for parallel SFM either followed from more general methods for sequential SFM or highly-parallel minimization of convex $\ell_2$-Lipschitz functions. Interestingly, to obtain our second result we provide the first highly-parallel algorithm for minimizing $\ell_\infty$-Lipschitz function over the hypercube which obtains near-optimal depth for obtaining constant accuracy.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_04643v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们考虑了子模函数最小化（SFM）的并行复杂性。我们提供了一对方法，以获得两种新的查询与深度的权衡——在整数值在$-M$和$M$之间的$n$元素子集上定义的子模函数。第一种方法的深度为$2$，查询复杂度为$n^｛O（M）｝$，第二种方法的厚度为$\widetilde｛O｝（n^｛1/3｝M^｛2/3｝）$，查询复杂性为$O（\mathrm｛poly｝（n，M））$。尽管有一系列关于改进的并行SFM下界的工作，但在我们的工作之前，并行SFM的唯一已知算法要么遵循序列SFM的更通用方法，要么遵循凸$\ell_2$-Lipschitz函数的高度并行最小化。有趣的是，为了获得我们的第二个结果，我们提供了在超立方体上最小化$\ell_\infty$-Lipschitz函数的第一个高度并行算法，该算法获得了接近最优的深度，以获得恒定的精度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.04643v1" target="_blank">2309.04643v1</a>
                              </td>
                              <td>Parallel Submodular Function Minimization</td>
                              <td>Deeparnab Chakrabarty</td>
                              <td>2023-09-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_04643v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.04643v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_04147v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Robot Localization and Mapping Final Report -- Sequential Adversarial Learning for Self-Supervised Deep Visual Odometry</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_04147v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_04147v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_04147v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual odometry (VO) and SLAM have been using multi-view geometry via local structure from motion for decades. These methods have a slight disadvantage in challenging scenarios such as low-texture images, dynamic scenarios, etc. Meanwhile, use of deep neural networks to extract high level features is ubiquitous in computer vision. For VO, we can use these deep networks to extract depth and pose estimates using these high level features. The visual odometry task then can be modeled as an image generation task where the pose estimation is the by-product. This can also be achieved in a self-supervised manner, thereby eliminating the data (supervised) intensive nature of training deep neural networks. Although some works tried the similar approach [1], the depth and pose estimation in the previous works are vague sometimes resulting in accumulation of error (drift) along the trajectory. The goal of this work is to tackle these limitations of past approaches and to develop a method that can provide better depths and pose estimates. To address this, a couple of approaches are explored: 1) Modeling: Using optical flow and recurrent neural networks (RNN) in order to exploit spatio-temporal correlations which can provide more information to estimate depth. 2) Loss function: Generative adversarial network (GAN) [2] is deployed to improve the depth estimation (and thereby pose too), as shown in Figure 1. This additional loss term improves the realism in generated images and reduces artifacts.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_04147v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>几十年来，视觉里程计（VO）和SLAM一直通过运动的局部结构使用多视图几何。这些方法在低纹理图像、动态场景等具有挑战性的场景中稍有不足。同时，使用深度神经网络提取高级特征在计算机视觉中无处不在。对于VO，我们可以使用这些深度网络来提取使用这些高级特征的深度和姿态估计。视觉里程测量任务然后可以被建模为图像生成任务，其中姿态估计是副产品。这也可以以自监督的方式实现，从而消除训练深度神经网络的数据（监督）密集性质。尽管一些工作尝试了类似的方法[1]，但先前工作中的深度和姿态估计是模糊的，有时会导致沿轨迹的误差（漂移）累积。这项工作的目标是解决过去方法的这些局限性，并开发一种可以提供更好深度和姿态估计的方法。为了解决这一问题，我们探索了几种方法：1）建模：使用光流和递归神经网络（RNN）来利用时空相关性，这可以提供更多的信息来估计深度。2） 损失函数：如图1所示，部署生成对抗性网络（GAN）[2]来改进深度估计（从而也提高姿态）。这个额外的损失项提高了生成图像的真实性并减少了伪影。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.04147v1" target="_blank">2309.04147v1</a>
                              </td>
                              <td>Robot Localization and Mapping Final Report -- Sequential Adversarial Learning for Self-Supervised Deep Visual Odometry</td>
                              <td>Akankshya Kar</td>
                              <td>2023-09-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_04147v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.04147v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_02420v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Doppelgangers: Learning to Disambiguate Images of Similar Structures</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_02420v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_02420v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_02420v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We consider the visual disambiguation task of determining whether a pair of visually similar images depict the same or distinct 3D surfaces (e.g., the same or opposite sides of a symmetric building). Illusory image matches, where two images observe distinct but visually similar 3D surfaces, can be challenging for humans to differentiate, and can also lead 3D reconstruction algorithms to produce erroneous results. We propose a learning-based approach to visual disambiguation, formulating it as a binary classification task on image pairs. To that end, we introduce a new dataset for this problem, Doppelgangers, which includes image pairs of similar structures with ground truth labels. We also design a network architecture that takes the spatial distribution of local keypoints and matches as input, allowing for better reasoning about both local and global cues. Our evaluation shows that our method can distinguish illusory matches in difficult cases, and can be integrated into SfM pipelines to produce correct, disambiguated 3D reconstructions. See our project page for our code, datasets, and more results: http://doppelgangers-3d.github.io/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_02420v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们考虑视觉消歧任务，即确定一对视觉相似的图像是否描绘了相同或不同的3D表面（例如，对称建筑的相同或相反侧）。两幅图像观察到不同但在视觉上相似的3D表面的伪图像匹配，对人类来说可能很难区分，也可能导致3D重建算法产生错误的结果。我们提出了一种基于学习的视觉消歧方法，将其表述为图像对的二元分类任务。为此，我们为这个问题引入了一个新的数据集，即Doppelgangers，它包括具有基本事实标签的相似结构的图像对。我们还设计了一种网络架构，该架构将局部关键点和匹配的空间分布作为输入，从而能够更好地对局部和全局线索进行推理。我们的评估表明，我们的方法可以在困难的情况下区分虚幻的匹配，并可以集成到SfM管道中，以产生正确的、消除歧义的3D重建。有关我们的代码、数据集和更多结果，请参阅我们的项目页面：http://doppelgangers-3d.github.io/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.02420v1" target="_blank">2309.02420v1</a>
                              </td>
                              <td>Doppelgangers: Learning to Disambiguate Images of Similar Structures</td>
                              <td>Ruojin Cai</td>
                              <td>2023-09-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_02420v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.02420v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/RuojinCai/Doppelgangers" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_08479v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DeDoDe: Detect, Don't Describe -- Describe, Don't Detect for Local Feature Matching</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_08479v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_08479v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_08479v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Keypoint detection is a pivotal step in 3D reconstruction, whereby sets of (up to) K points are detected in each view of a scene. Crucially, the detected points need to be consistent between views, i.e., correspond to the same 3D point in the scene. One of the main challenges with keypoint detection is the formulation of the learning objective. Previous learning-based methods typically jointly learn descriptors with keypoints, and treat the keypoint detection as a binary classification task on mutual nearest neighbours. However, basing keypoint detection on descriptor nearest neighbours is a proxy task, which is not guaranteed to produce 3D-consistent keypoints. Furthermore, this ties the keypoints to a specific descriptor, complicating downstream usage. In this work, we instead learn keypoints directly from 3D consistency. To this end, we train the detector to detect tracks from large-scale SfM. As these points are often overly sparse, we derive a semi-supervised two-view detection objective to expand this set to a desired number of detections. To train a descriptor, we maximize the mutual nearest neighbour objective over the keypoints with a separate network. Results show that our approach, DeDoDe, achieves significant gains on multiple geometry benchmarks. Code is provided at https://github.com/Parskatt/DeDoDe</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_08479v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>关键点检测是3D重建中的关键步骤，通过该步骤可以在场景的每个视图中检测到（最多）K个点的集合。至关重要的是，检测到的点需要在视图之间保持一致，即对应于场景中的同一3D点。关键点检测的主要挑战之一是学习目标的制定。以前基于学习的方法通常将描述符与关键点联合学习，并将关键点检测视为对相互最近邻居的二元分类任务。然而，基于描述符最近邻居的关键点检测是一项代理任务，不能保证产生3D一致的关键点。此外，这将关键点与特定描述符联系在一起，使下游使用变得复杂。在这项工作中，我们直接从3D一致性中学习关键点。为此，我们训练检测器来检测大规模SfM中的轨道。由于这些点往往过于稀疏，我们推导出一个半监督的双视图检测目标，以将该集扩展到所需的检测数量。为了训练描述符，我们使用单独的网络在关键点上最大化相互最近邻目标。结果表明，我们的方法DeDoDe在多个几何基准上实现了显著的增益。代码提供于https://github.com/parskatt/dedode</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.08479v2" target="_blank">2308.08479v2</a>
                              </td>
                              <td>DeDoDe: Detect, Don't Describe -- Describe, Don't Detect for Local Feature Matching</td>
                              <td>Johan Edstedt</td>
                              <td>2023-08-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_08479v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.08479v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/parskatt/dedode" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_00526v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SQLdepth: Generalizable Self-Supervised Fine-Structured Monocular Depth Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_00526v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_00526v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_00526v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, self-supervised monocular depth estimation has gained popularity with numerous applications in autonomous driving and robotics. However, existing solutions primarily seek to estimate depth from immediate visual features, and struggle to recover fine-grained scene details with limited generalization. In this paper, we introduce SQLdepth, a novel approach that can effectively learn fine-grained scene structures from motion. In SQLdepth, we propose a novel Self Query Layer (SQL) to build a self-cost volume and infer depth from it, rather than inferring depth from feature maps. The self-cost volume implicitly captures the intrinsic geometry of the scene within a single frame. Each individual slice of the volume signifies the relative distances between points and objects within a latent space. Ultimately, this volume is compressed to the depth map via a novel decoding approach. Experimental results on KITTI and Cityscapes show that our method attains remarkable state-of-the-art performance (AbsRel = $0.082$ on KITTI, $0.052$ on KITTI with improved ground-truth and $0.106$ on Cityscapes), achieves $9.9\%$, $5.5\%$ and $4.5\%$ error reduction from the previous best. In addition, our approach showcases reduced training complexity, computational efficiency, improved generalization, and the ability to recover fine-grained scene details. Moreover, the self-supervised pre-trained and metric fine-tuned SQLdepth can surpass existing supervised methods by significant margins (AbsRel = $0.043$, $14\%$ error reduction). self-matching-oriented relative distance querying in SQL improves the robustness and zero-shot generalization capability of SQLdepth. Code and the pre-trained weights will be publicly available. Code is available at \href{https://github.com/hisfog/SQLdepth-Impl}{https://github.com/hisfog/SQLdepth-Impl}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_00526v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，自监督单目深度估计在自动驾驶和机器人技术中得到了广泛应用。然而，现有的解决方案主要寻求从即时视觉特征估计深度，并且难以在有限的泛化能力下恢复细粒度的场景细节。在本文中，我们介绍了SQLdepth，这是一种可以有效地从运动中学习细粒度场景结构的新方法。在SQLdepth中，我们提出了一种新颖的自查询层（SQL）来构建自成本体积并从中推断深度，而不是从特征图中推断深度。自成本体积隐含地捕捉单个帧内场景的固有几何体。体积的每个单独切片表示潜在空间内的点和对象之间的相对距离。最终，通过一种新颖的解码方法将该体积压缩到深度图中。在KITTI和Cityscapes上的实验结果表明，我们的方法获得了显著的最先进的性能（AbsRel在KITTI上为0.082$，在具有改进的地面实况的KITTI上为0.052$，而在Cityscape上为0.106$），比以前的最佳方法实现了9.9\%$、5.5\%$和4.5\%$的误差减少。此外，我们的方法展示了降低的训练复杂性、计算效率、改进的泛化能力以及恢复细粒度场景细节的能力。此外，自监督预训练和度量微调的SQLdepth可以以显著的优势超过现有的监督方法（AbsRel=0.043$，误差减少$14\%$）。SQL中面向自匹配的相对距离查询提高了SQLdepth的鲁棒性和零样本泛化能力。代码和预先训练的重量将公开。代码位于\ href{https://github.com/hisfog/sqldepth-impl}{https://github.com/hisfog/sqldepth-impl}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.00526v1" target="_blank">2309.00526v1</a>
                              </td>
                              <td>SQLdepth: Generalizable Self-Supervised Fine-Structured Monocular Depth Estimation</td>
                              <td>Youhong Wang</td>
                              <td>2023-09-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_00526v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.00526v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2208_00487v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">One Object at a Time: Accurate and Robust Structure From Motion for Robots</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2208_00487v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2208_00487v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2208_00487v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A gaze-fixating robot perceives distance to the fixated object and relative positions of surrounding objects immediately, accurately, and robustly. We show how fixation, which is the act of looking at one object while moving, exploits regularities in the geometry of 3D space to obtain this information. These regularities introduce rotation-translation couplings that are not commonly used in structure from motion. To validate, we use a Franka Emika Robot with an RGB camera. We a) find that error in distance estimate is less than 5 mm at a distance of 15 cm, and b) show how relative position can be used to find obstacles under challenging scenarios. We combine accurate distance estimates and obstacle information into a reactive robot behavior that is able to pick up objects of unknown size, while impeded by unforeseen obstacles. Project page: https://oxidification.com/p/one-object-at-a-time/ .</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2208_00487v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>注视机器人可以立即、准确、稳健地感知到被注视物体的距离和周围物体的相对位置。我们展示了固定，即在移动时看着一个物体的行为，是如何利用三维空间几何中的规律来获得这些信息的。这些规律引入了旋转-平移耦合，这在结构运动中并不常见。为了验证，我们使用了带有RGB相机的Franka Emika机器人。我们a）发现，在15厘米的距离上，距离估计的误差小于5毫米，b）展示了在具有挑战性的场景下如何使用相对位置来寻找障碍物。我们将准确的距离估计和障碍物信息结合到反应机器人行为中，该行为能够拾取未知大小的物体，同时受到不可预见的障碍物的阻碍。项目页面：https://oxidification.com/p/one-object-at-a-time/。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2208.00487v3" target="_blank">2208.00487v3</a>
                              </td>
                              <td>One Object at a Time: Accurate and Robust Structure From Motion for Robots</td>
                              <td>Aravind Battaje</td>
                              <td>2022-07-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2208_00487v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2208.00487v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_00385v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Dense Voxel 3D Reconstruction Using a Monocular Event Camera</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_00385v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_00385v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_00385v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Event cameras are sensors inspired by biological systems that specialize in capturing changes in brightness. These emerging cameras offer many advantages over conventional frame-based cameras, including high dynamic range, high frame rates, and extremely low power consumption. Due to these advantages, event cameras have increasingly been adapted in various fields, such as frame interpolation, semantic segmentation, odometry, and SLAM. However, their application in 3D reconstruction for VR applications is underexplored. Previous methods in this field mainly focused on 3D reconstruction through depth map estimation. Methods that produce dense 3D reconstruction generally require multiple cameras, while methods that utilize a single event camera can only produce a semi-dense result. Other single-camera methods that can produce dense 3D reconstruction rely on creating a pipeline that either incorporates the aforementioned methods or other existing Structure from Motion (SfM) or Multi-view Stereo (MVS) methods. In this paper, we propose a novel approach for solving dense 3D reconstruction using only a single event camera. To the best of our knowledge, our work is the first attempt in this regard. Our preliminary results demonstrate that the proposed method can produce visually distinguishable dense 3D reconstructions directly without requiring pipelines like those used by existing methods. Additionally, we have created a synthetic dataset with $39,739$ object scans using an event camera simulator. This dataset will help accelerate other relevant research in this field.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_00385v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>事件摄像机是受生物系统启发，专门捕捉亮度变化的传感器。与传统的基于帧的相机相比，这些新兴相机具有许多优势，包括高动态范围、高帧速率和极低功耗。由于这些优势，事件摄像机越来越多地应用于各个领域，如帧插值、语义分割、里程计和SLAM。然而，它们在VR应用的3D重建中的应用还没有得到充分的探索。该领域以前的方法主要集中在通过深度图估计进行三维重建。产生密集3D重建的方法通常需要多个相机，而利用单个事件相机的方法只能产生半密集的结果。可以产生密集3D重建的其他单相机方法依赖于创建管道，该管道结合了上述方法或其他现有的运动结构（SfM）或多视图立体（MVS）方法。在本文中，我们提出了一种仅使用单个事件相机来解决密集三维重建的新方法。据我们所知，我们的工作是这方面的第一次尝试。我们的初步结果表明，所提出的方法可以直接产生视觉上可区分的密集三维重建，而不需要像现有方法那样使用管道。此外，我们还使用事件相机模拟器创建了一个合成数据集，其中包含39739美元的对象扫描。该数据集将有助于加速该领域的其他相关研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.00385v1" target="_blank">2309.00385v1</a>
                              </td>
                              <td>Dense Voxel 3D Reconstruction Using a Monocular Event Camera</td>
                              <td>Haodong Chen</td>
                              <td>2023-09-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_00385v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.00385v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_10902v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CamP: Camera Preconditioning for Neural Radiance Fields</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_10902v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_10902v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_10902v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural Radiance Fields (NeRF) can be optimized to obtain high-fidelity 3D scene reconstructions of objects and large-scale scenes. However, NeRFs require accurate camera parameters as input -- inaccurate camera parameters result in blurry renderings. Extrinsic and intrinsic camera parameters are usually estimated using Structure-from-Motion (SfM) methods as a pre-processing step to NeRF, but these techniques rarely yield perfect estimates. Thus, prior works have proposed jointly optimizing camera parameters alongside a NeRF, but these methods are prone to local minima in challenging settings. In this work, we analyze how different camera parameterizations affect this joint optimization problem, and observe that standard parameterizations exhibit large differences in magnitude with respect to small perturbations, which can lead to an ill-conditioned optimization problem. We propose using a proxy problem to compute a whitening transform that eliminates the correlation between camera parameters and normalizes their effects, and we propose to use this transform as a preconditioner for the camera parameters during joint optimization. Our preconditioned camera optimization significantly improves reconstruction quality on scenes from the Mip-NeRF 360 dataset: we reduce error rates (RMSE) by 67% compared to state-of-the-art NeRF approaches that do not optimize for cameras like Zip-NeRF, and by 29% relative to state-of-the-art joint optimization approaches using the camera parameterization of SCNeRF. Our approach is easy to implement, does not significantly increase runtime, can be applied to a wide variety of camera parameterizations, and can straightforwardly be incorporated into other NeRF-like models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_10902v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>可以优化神经辐射场（NeRF）以获得物体和大规模场景的高保真3D场景重建。然而，NeRF需要精确的相机参数作为输入——不准确的相机参数会导致渲染模糊。通常使用运动结构（SfM）方法作为NeRF的预处理步骤来估计外部和内部相机参数，但这些技术很少产生完美的估计。因此，先前的工作已经提出与NeRF一起联合优化相机参数，但这些方法在具有挑战性的设置中容易出现局部最小值。在这项工作中，我们分析了不同的相机参数化如何影响这个联合优化问题，并观察到标准参数化相对于小扰动在大小上表现出很大的差异，这可能导致病态优化问题。我们建议使用代理问题来计算白化变换，该变换消除了相机参数之间的相关性并归一化了它们的效果，并且我们建议在联合优化期间使用该变换作为相机参数的预处理器。我们的预处理相机优化显著提高了Mip-NeRF 360数据集场景的重建质量：与不优化Zip-NeRF等相机的最先进NeRF方法相比，我们将错误率（RMSE）降低了67%，与使用SCNeRF相机参数化的最先进联合优化方法相比，降低了29%。我们的方法易于实现，不会显著增加运行时间，可以应用于各种相机参数化，并且可以直接集成到其他类似NeRF的模型中。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.10902v2" target="_blank">2308.10902v2</a>
                              </td>
                              <td>CamP: Camera Preconditioning for Neural Radiance Fields</td>
                              <td>Keunhong Park</td>
                              <td>2023-08-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_10902v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.10902v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_13903v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Disjoint Pose and Shape for 3D Face Reconstruction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_13903v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_13903v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_13903v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Existing methods for 3D face reconstruction from a few casually captured images employ deep learning based models along with a 3D Morphable Model(3DMM) as face geometry prior. Structure From Motion(SFM), followed by Multi-View Stereo (MVS), on the other hand, uses dozens of high-resolution images to reconstruct accurate 3D faces.However, it produces noisy and stretched-out results with only two views available. In this paper, taking inspiration from both these methods, we propose an end-to-end pipeline that disjointly solves for pose and shape to make the optimization stable and accurate. We use a face shape prior to estimate face pose and use stereo matching followed by a 3DMM to solve for the shape. The proposed method achieves end-to-end topological consistency, enables iterative face pose refinement procedure, and show remarkable improvement on both quantitative and qualitative results over existing state-of-the-art methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_13903v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现有的从一些随意捕捉的图像进行3D人脸重建的方法使用基于深度学习的模型以及3D变形模型（3DMM）作为人脸几何先验。另一方面，“运动结构”（SFM）和“多视图立体”（MVS）使用数十幅高分辨率图像来重建精确的3D人脸。然而，在只有两个视图可用的情况下，它会产生嘈杂和拉伸的结果。在本文中，我们从这两种方法中获得灵感，提出了一种端到端的流水线，该流水线对姿态和形状进行不相交求解，以使优化稳定准确。我们在估计人脸姿态之前使用人脸形状，并使用立体匹配和3DMM来求解形状。所提出的方法实现了端到端的拓扑一致性，实现了迭代人脸姿态精化过程，并在定量和定性结果上都比现有的最先进的方法有了显著的改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.13903v1" target="_blank">2308.13903v1</a>
                              </td>
                              <td>Disjoint Pose and Shape for 3D Face Reconstruction</td>
                              <td>Raja Kumar</td>
                              <td>2023-08-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_13903v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.13903v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_10003v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Efficient Multi-View Inverse Rendering Using a Hybrid Differentiable Rendering Method</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_10003v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_10003v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_10003v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recovering the shape and appearance of real-world objects from natural 2D images is a long-standing and challenging inverse rendering problem. In this paper, we introduce a novel hybrid differentiable rendering method to efficiently reconstruct the 3D geometry and reflectance of a scene from multi-view images captured by conventional hand-held cameras. Our method follows an analysis-by-synthesis approach and consists of two phases. In the initialization phase, we use traditional SfM and MVS methods to reconstruct a virtual scene roughly matching the real scene. Then in the optimization phase, we adopt a hybrid approach to refine the geometry and reflectance, where the geometry is first optimized using an approximate differentiable rendering method, and the reflectance is optimized afterward using a physically-based differentiable rendering method. Our hybrid approach combines the efficiency of approximate methods with the high-quality results of physically-based methods. Extensive experiments on synthetic and real data demonstrate that our method can produce reconstructions with similar or higher quality than state-of-the-art methods while being more efficient.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_10003v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从自然2D图像中恢复真实世界对象的形状和外观是一个长期存在且具有挑战性的反向渲染问题。在本文中，我们介绍了一种新的混合可微绘制方法，以从传统手持相机拍摄的多视图图像中有效地重建场景的3D几何结构和反射率。我们的方法遵循综合分析法，由两个阶段组成。在初始化阶段，我们使用传统的SfM和MVS方法来重建与真实场景大致匹配的虚拟场景。然后在优化阶段，我们采用混合方法来细化几何体和反射率，其中首先使用近似可微渲染方法优化几何体，然后使用基于物理的可微渲染法优化反射率。我们的混合方法将近似方法的效率与基于物理的方法的高质量结果相结合。对合成数据和真实数据的大量实验表明，我们的方法可以产生与最先进方法相似或更高质量的重建，同时更高效。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.10003v1" target="_blank">2308.10003v1</a>
                              </td>
                              <td>Efficient Multi-View Inverse Rendering Using a Hybrid Differentiable Rendering Method</td>
                              <td>Xiangyang Zhu</td>
                              <td>2023-08-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_10003v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.10003v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_15667v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_15667v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_15667v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_15667v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Camera pose estimation is a long-standing computer vision problem that to date often relies on classical methods, such as handcrafted keypoint matching, RANSAC and bundle adjustment. In this paper, we propose to formulate the Structure from Motion (SfM) problem inside a probabilistic diffusion framework, modelling the conditional distribution of camera poses given input images. This novel view of an old problem has several advantages. (i) The nature of the diffusion framework mirrors the iterative procedure of bundle adjustment. (ii) The formulation allows a seamless integration of geometric constraints from epipolar geometry. (iii) It excels in typically difficult scenarios such as sparse views with wide baselines. (iv) The method can predict intrinsics and extrinsics for an arbitrary amount of images. We demonstrate that our method PoseDiffusion significantly improves over the classic SfM pipelines and the learned approaches on two real-world datasets. Finally, it is observed that our method can generalize across datasets without further training. Project page: https://posediffusion.github.io/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_15667v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>相机姿态估计是一个长期存在的计算机视觉问题，迄今为止，它通常依赖于经典的方法，如手工关键点匹配、RANSAC和束调整。在本文中，我们建议在概率扩散框架内公式化运动结构（SfM）问题，对给定输入图像的相机姿态的条件分布进行建模。这种对老问题的新颖看法有几个优点。（i） 扩散框架的性质反映了束调整的迭代过程。（ii）该公式允许来自核极几何的几何约束的无缝集成。（iii）它在典型的困难场景中表现出色，例如具有宽基线的稀疏视图。（iv）该方法可以预测任意数量的图像的内在和外在。我们在两个真实世界的数据集上证明了我们的方法PoseDiffusion显著优于经典的SfM管道和学习的方法。最后，我们观察到，我们的方法可以在不需要进一步训练的情况下在数据集之间进行推广。项目页面：https://posediffusion.github.io/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.15667v3" target="_blank">2306.15667v3</a>
                              </td>
                              <td>PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment</td>
                              <td>Jianyuan Wang</td>
                              <td>2023-06-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_15667v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.15667v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_10705v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Unsupervised 3D Pose Estimation with Non-Rigid Structure-from-Motion Modeling</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_10705v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_10705v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_10705v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Most of the previous 3D human pose estimation work relied on the powerful memory capability of the network to obtain suitable 2D-3D mappings from the training data. Few works have studied the modeling of human posture deformation in motion. In this paper, we propose a new modeling method for human pose deformations and design an accompanying diffusion-based motion prior. Inspired by the field of non-rigid structure-from-motion, we divide the task of reconstructing 3D human skeletons in motion into the estimation of a 3D reference skeleton, and a frame-by-frame skeleton deformation. A mixed spatial-temporal NRSfMformer is used to simultaneously estimate the 3D reference skeleton and the skeleton deformation of each frame from 2D observations sequence, and then sum them to obtain the pose of each frame. Subsequently, a loss term based on the diffusion model is used to ensure that the pipeline learns the correct prior motion knowledge. Finally, we have evaluated our proposed method on mainstream datasets and obtained superior results outperforming the state-of-the-art.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_10705v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>以前的大多数3D人体姿态估计工作都依赖于网络强大的存储能力来从训练数据中获得合适的2D-3D映射。很少有工作研究人体运动中姿势变形的建模。在本文中，我们提出了一种新的人体姿态变形建模方法，并设计了一种基于扩散的运动先验。受运动中非刚性结构领域的启发，我们将重建运动中的三维人体骨骼的任务分为三维参考骨骼的估计和逐帧骨骼变形。使用混合时空NRSfMformer从2D观测序列中同时估计3D参考骨架和每个帧的骨架变形，然后将它们相加以获得每个帧的姿态。随后，使用基于扩散模型的损失项来确保管道学习正确的先验运动知识。最后，我们在主流数据集上评估了我们提出的方法，并获得了优于现有技术的优越结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.10705v1" target="_blank">2308.10705v1</a>
                              </td>
                              <td>Unsupervised 3D Pose Estimation with Non-Rigid Structure-from-Motion Modeling</td>
                              <td>Haorui Ji</td>
                              <td>2023-08-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_10705v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.10705v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2210_05020v4_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Spectral Sparsification for Communication-Efficient Collaborative Rotation and Translation Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2210_05020v4_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2210_05020v4_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2210_05020v4_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose fast and communication-efficient optimization algorithms for multi-robot rotation averaging and translation estimation problems that arise from collaborative simultaneous localization and mapping (SLAM), structure-from-motion (SfM), and camera network localization applications. Our methods are based on theoretical relations between the Hessians of the underlying Riemannian optimization problems and the Laplacians of suitably weighted graphs. We leverage these results to design a collaborative solver in which robots coordinate with a central server to perform approximate second-order optimization, by solving a Laplacian system at each iteration. Crucially, our algorithms permit robots to employ spectral sparsification to sparsify intermediate dense matrices before communication, and hence provide a mechanism to trade off accuracy with communication efficiency with provable guarantees. We perform rigorous theoretical analysis of our methods and prove that they enjoy (local) linear rate of convergence. Furthermore, we show that our methods can be combined with graduated non-convexity to achieve outlier-robust estimation. Extensive experiments on real-world SLAM and SfM scenarios demonstrate the superior convergence rate and communication efficiency of our methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2210_05020v4_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们为多机器人旋转平均和平移估计问题提出了快速且高效的优化算法，这些问题源于协同同步定位和映射（SLAM）、运动结构（SfM）和相机网络定位应用。我们的方法基于基本黎曼优化问题的Hessians和适当加权图的拉普拉斯算子之间的理论关系。我们利用这些结果来设计一个协作求解器，在该求解器中，机器人与中央服务器协调，通过在每次迭代中求解拉普拉斯系统来执行近似的二阶优化。至关重要的是，我们的算法允许机器人在通信前使用频谱稀疏化来稀疏中间密集矩阵，因此提供了一种在通信效率与精度之间进行权衡的机制，并提供了可证明的保证。我们对我们的方法进行了严格的理论分析，并证明它们具有（局部）线性收敛率。此外，我们还证明了我们的方法可以与分级非凸性相结合，以实现异常值的鲁棒估计。在真实世界的SLAM和SfM场景上进行的大量实验证明了我们的方法优越的收敛速度和通信效率。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2210.05020v4" target="_blank">2210.05020v4</a>
                              </td>
                              <td>Spectral Sparsification for Communication-Efficient Collaborative Rotation and Translation Estimation</td>
                              <td>Yulun Tian</td>
                              <td>2022-10-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2210_05020v4_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2210.05020v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01246v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Tirtha -- An Automated Platform to Crowdsource Images and Create 3D Models of Heritage Sites</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01246v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01246v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01246v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Digital preservation of Cultural Heritage (CH) sites is crucial to protect them against damage from natural disasters or human activities. Creating 3D models of CH sites has become a popular method of digital preservation thanks to advancements in computer vision and photogrammetry. However, the process is time-consuming, expensive, and typically requires specialized equipment and expertise, posing challenges in resource-limited developing countries. Additionally, the lack of an open repository for 3D models hinders research and public engagement with their heritage. To address these issues, we propose Tirtha, a web platform for crowdsourcing images of CH sites and creating their 3D models. Tirtha utilizes state-of-the-art Structure from Motion (SfM) and Multi-View Stereo (MVS) techniques. It is modular, extensible and cost-effective, allowing for the incorporation of new techniques as photogrammetry advances. Tirtha is accessible through a web interface at https://tirtha.niser.ac.in and can be deployed on-premise or in a cloud environment. In our case studies, we demonstrate the pipeline's effectiveness by creating 3D models of temples in Odisha, India, using crowdsourced images. These models are available for viewing, interaction, and download on the Tirtha website. Our work aims to provide a dataset of crowdsourced images and 3D reconstructions for research in computer vision, heritage conservation, and related domains. Overall, Tirtha is a step towards democratizing digital preservation, primarily in resource-limited developing countries.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01246v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文化遗产（CH）遗址的数字保护对于保护它们免受自然灾害或人类活动的破坏至关重要。由于计算机视觉和摄影测量的进步，创建CH遗址的3D模型已成为一种流行的数字保存方法。然而，这一过程耗时、昂贵，通常需要专门的设备和专业知识，这对资源有限的发展中国家构成了挑战。此外，缺乏开放的3D模型存储库阻碍了研究和公众对其遗产的参与。为了解决这些问题，我们提出了Tirtha，一个用于众包CH网站图像并创建其3D模型的网络平台。Tirtha采用了最先进的运动结构（SfM）和多视图立体（MVS）技术。它是模块化的、可扩展的和具有成本效益的，允许随着摄影测量的发展而结合新技术。Tirtha可通过web界面访问，网址为https://tirtha.niser.ac.in并且可以在内部部署或在云环境中部署。在我们的案例研究中，我们通过使用众包图像创建印度奥迪沙寺庙的3D模型来证明该管道的有效性。这些模型可在Tirtha网站上查看、交互和下载。我们的工作旨在为计算机视觉、遗产保护和相关领域的研究提供众包图像和3D重建的数据集。总的来说，Tirtha是朝着数字保护民主化迈出的一步，主要是在资源有限的发展中国家。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01246v2" target="_blank">2308.01246v2</a>
                              </td>
                              <td>Tirtha -- An Automated Platform to Crowdsource Images and Create 3D Models of Heritage Sites</td>
                              <td>Jyotirmaya Shivottam</td>
                              <td>2023-08-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01246v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01246v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/smlab-niser/tirtha-public" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_06794v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MMF-Track: Multi-modal Multi-level Fusion for 3D Single Object Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_06794v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_06794v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_06794v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>3D single object tracking plays a crucial role in computer vision. Mainstream methods mainly rely on point clouds to achieve geometry matching between target template and search area. However, textureless and incomplete point clouds make it difficult for single-modal trackers to distinguish objects with similar structures. To overcome the limitations of geometry matching, we propose a Multi-modal Multi-level Fusion Tracker (MMF-Track), which exploits the image texture and geometry characteristic of point clouds to track 3D target. Specifically, we first propose a Space Alignment Module (SAM) to align RGB images with point clouds in 3D space, which is the prerequisite for constructing inter-modal associations. Then, in feature interaction level, we design a Feature Interaction Module (FIM) based on dual-stream structure, which enhances intra-modal features in parallel and constructs inter-modal semantic associations. Meanwhile, in order to refine each modal feature, we introduce a Coarse-to-Fine Interaction Module (CFIM) to realize the hierarchical feature interaction at different scales. Finally, in similarity fusion level, we propose a Similarity Fusion Module (SFM) to aggregate geometry and texture clues from the target. Experiments show that our method achieves state-of-the-art performance on KITTI (39% Success and 42% Precision gains against previous multi-modal method) and is also competitive on NuScenes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_06794v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三维单目标跟踪在计算机视觉中起着至关重要的作用。主流方法主要依靠点云来实现目标模板与搜索区域的几何匹配。然而，无纹理和不完整的点云使单模态跟踪器难以区分具有相似结构的对象。为了克服几何匹配的局限性，我们提出了一种多模式多级融合跟踪器（MMF Track），该跟踪器利用点云的图像纹理和几何特征来跟踪三维目标。具体来说，我们首先提出了一种空间对齐模块（SAM）来将RGB图像与3D空间中的点云对齐，这是构建模态间关联的先决条件。然后，在特征交互层面，我们设计了一个基于双流结构的特征交互模块，该模块并行增强模态内特征，构建模态间语义关联。同时，为了细化每个模态特征，我们引入了一个从粗到细的交互模块（CFIM）来实现不同尺度的层次特征交互。最后，在相似性融合层面，我们提出了一个相似性融合模块（SFM）来聚合来自目标的几何和纹理线索。实验表明，我们的方法在KITTI上实现了最先进的性能（与以前的多模态方法相比，成功率为39%，精度提高了42%），在NuScenes上也具有竞争力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.06794v2" target="_blank">2305.06794v2</a>
                              </td>
                              <td>MMF-Track: Multi-modal Multi-level Fusion for 3D Single Object Tracking</td>
                              <td>Zhiheng Li</td>
                              <td>2023-05-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_06794v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.06794v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="LLM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2312_02147v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rejuvenating image-GPT as Strong Visual Representation Learners</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02147v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02147v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02147v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper enhances image-GPT (iGPT), one of the pioneering works that introduce autoregressive pretraining to predict next pixels for visual representation learning. Two simple yet essential changes are made. First, we shift the prediction target from raw pixels to semantic tokens, enabling a higher-level understanding of visual content. Second, we supplement the autoregressive modeling by instructing the model to predict not only the next tokens but also the visible tokens. This pipeline is particularly effective when semantic tokens are encoded by discriminatively trained models, such as CLIP. We introduce this novel approach as D-iGPT. Extensive experiments showcase that D-iGPT excels as a strong learner of visual representations: A notable achievement of D-iGPT is its compelling performance on the ImageNet-1K dataset -- by training on publicly available datasets, D-iGPT achieves 89.5\% top-1 accuracy with a vanilla ViT-Large model. This model also shows strong generalization on the downstream task and robustness on out-of-distribution samples. Code is avaiable at \href{https://github.com/OliverRensu/D-iGPT}{https://github.com/OliverRensu/D-iGPT}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02147v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文增强了图像GPT（iGPT），这是引入自回归预训练来预测视觉表示学习的下一个像素的开创性工作之一。进行了两个简单但重要的更改。首先，我们将预测目标从原始像素转移到语义标记，从而实现对视觉内容的更高层次理解。其次，我们通过指示模型不仅预测下一个令牌，而且预测可见令牌来补充自回归建模。当语义标记由经过区别训练的模型（如CLIP）编码时，这种管道尤其有效。我们将这种新方法称为D-iGPT。大量实验表明，D-iGPT在视觉表示方面表现出色：D-iGPT的一个显著成就是其在ImageNet-1K数据集上的令人信服的性能——通过在公开可用的数据集上进行训练，D-iGPT在普通ViT-Large模型中实现了89.5\%的前1级准确率。该模型对下游任务也表现出较强的泛化能力，对分布外样本也表现出鲁棒性。代码可在\href获得{https://github.com/oliverrensu/d-igpt}{https://github.com/oliverrensu/d-igpt}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02147v1" target="_blank">2312.02147v1</a>
                              </td>
                              <td>Rejuvenating image-GPT as Strong Visual Representation Learners</td>
                              <td>Sucheng Ren</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02147v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02147v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/oliverrensu/d-igpt" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02143v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Competition-Level Problems Are Effective Evaluators of LLMs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02143v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02143v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02143v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have demonstrated impressive reasoning capabilities, yet there is ongoing debate about these abilities and the potential data contamination problem recently. This paper aims to evaluate the reasoning capacities of LLMs, specifically in solving recent competition-level programming problems in Codeforces, which are expert-crafted and unique, requiring deep understanding and robust reasoning skills. We first provide a comprehensive evaluation of GPT-4's peiceived zero-shot performance on this task, considering various aspects such as problems' release time, difficulties, and types of errors encountered. Surprisingly, the peiceived performance of GPT-4 has experienced a cliff like decline in problems after September 2021 consistently across all the difficulties and types of problems, which shows the potential data contamination, as well as the challenges for any existing LLM to solve unseen complex reasoning problems. We further explore various approaches such as fine-tuning, Chain-of-Thought prompting and problem description simplification, unfortunately none of them is able to consistently mitigate the challenges. Through our work, we emphasis the importance of this excellent data source for assessing the genuine reasoning capabilities of LLMs, and foster the development of LLMs with stronger reasoning abilities and better generalization in the future.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02143v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）已经展示了令人印象深刻的推理能力，但最近关于这些能力和潜在的数据污染问题仍存在争议。本文旨在评估LLM的推理能力，特别是在解决Codeforces中最近的竞争级别编程问题方面的推理能力。Codeforces是专家精心设计的，具有独特性，需要深入的理解和强大的推理技能。我们首先对GPT-4在该任务上的零样本性能进行了全面评估，考虑了问题的发布时间、困难和遇到的错误类型等各个方面。令人惊讶的是，在2021年9月之后，GPT-4的预期性能在所有困难和类型的问题中都经历了悬崖般的问题下降，这表明了潜在的数据污染，以及任何现有LLM解决看不见的复杂推理问题的挑战。我们进一步探索了各种方法，如微调、思想链提示和问题描述简化，不幸的是，它们都无法持续缓解挑战。通过我们的工作，我们强调了这一优秀数据源对评估LLM真实推理能力的重要性，并促进LLM在未来发展出更强的推理能力和更好的泛化能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02143v1" target="_blank">2312.02143v1</a>
                              </td>
                              <td>Competition-Level Problems Are Effective Evaluators of LLMs</td>
                              <td>Yiming Huang</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02143v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02143v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02125v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TPPoet: Transformer-Based Persian Poem Generation using Minimal Data and Advanced Decoding Techniques</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02125v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02125v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02125v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in language models (LMs), have demonstrated significant efficacy in tasks related to the arts and humanities. While LMs have exhibited exceptional performance across a wide range of natural language processing tasks, there are notable challenges associated with their utilization on small datasets and their ability to replicate more creative human capacities. In this study, we aim to address these challenges by training a Persian classical poetry generation model using a transformer architecture on a specialized dataset with no pretraining. Additionally, we propose a novel decoding method to enhance coherence and meaningfulness in the generated poetry, effectively managing the tradeoff between diversity and quality. Furthermore, the results of our training approach and the proposed decoding method are evaluated through comprehensive set of automatic and human evaluations and showed its superior capability to generate coherent and meaningful poetry in compare to other decoding methods and an existing Persian large language model (LLM).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02125v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语言模型（LMs）的最新进展已经证明在与艺术和人文学科相关的任务中具有显著的功效。虽然LMs在广泛的自然语言处理任务中表现出了卓越的性能，但在小型数据集上的利用以及复制更具创造性的人类能力的能力方面存在着显著的挑战。在这项研究中，我们的目标是通过在没有预训练的专业数据集上使用转换器架构训练波斯古典诗歌生成模型来解决这些挑战。此外，我们提出了一种新的解码方法，以增强生成诗歌的连贯性和意义，有效地管理多样性和质量之间的权衡。此外，我们的训练方法和所提出的解码方法的结果通过一组全面的自动和人工评估进行了评估，并显示出与其他解码方法和现有的波斯大语言模型（LLM）相比，它在生成连贯和有意义的诗歌方面具有优越的能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02125v1" target="_blank">2312.02125v1</a>
                              </td>
                              <td>TPPoet: Transformer-Based Persian Poem Generation using Minimal Data and Advanced Decoding Techniques</td>
                              <td>Amir Panahandeh</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02125v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02125v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02120v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Magicoder: Source Code Is All You Need</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02120v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02120v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02120v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets to generate high-quality instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs by empowering them with a wealth of open-source references for the production of more diverse, realistic, and controllable data. The orthogonality of OSS-Instruct and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks, including Python text-to-code generation, multilingual coding, and data-science program completion. Notably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1). Overall, OSS-Instruct opens a new direction for low-bias and high-quality instruction tuning using abundant open-source references.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02120v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了Magicoder，这是一系列完全开源（代码、权重和数据）的代码大型语言模型（LLM），它在参数不超过7B的情况下显著缩小了与顶级代码模型的差距。Magicoder模型使用OSS instruction在75K合成指令数据上进行训练，这是一种新颖的方法，可以用开源代码片段启发LLM，为代码生成高质量的指令数据。我们的主要动机是通过为LLM提供丰富的开源参考，以产生更多样、更现实、更可控的数据，从而减轻LLM生成的合成数据的固有偏见。OSS指令和Evol指令等其他数据生成方法的正交性进一步使我们能够构建增强的MagicoderS。Magicoder和MagicoderS在一系列编码基准测试中，包括Python文本到代码生成、多语言编码和数据科学程序完成，都大大优于尺寸相似甚至更大的最先进的代码模型。值得注意的是，基于CodeLlama的MagicoderS-CL-7B甚至超过了HumanEval+上的著名ChatGPT（66.5对65.9英寸pass@1)。总体而言，OSS instruction利用丰富的开源参考资料为低偏差和高质量的指令调优开辟了一个新的方向。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02120v1" target="_blank">2312.02120v1</a>
                              </td>
                              <td>Magicoder: Source Code Is All You Need</td>
                              <td>Yuxiang Wei</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02120v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02120v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/ise-uiuc/magicoder" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02119v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Tree of Attacks: Jailbreaking Black-Box LLMs Automatically</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02119v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02119v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02119v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While Large Language Models (LLMs) display versatile functionality, they continue to generate harmful, biased, and toxic content, as demonstrated by the prevalence of human-designed jailbreaks. In this work, we present Tree of Attacks with Pruning (TAP), an automated method for generating jailbreaks that only requires black-box access to the target LLM. TAP utilizes an LLM to iteratively refine candidate (attack) prompts using tree-of-thoughts reasoning until one of the generated prompts jailbreaks the target. Crucially, before sending prompts to the target, TAP assesses them and prunes the ones unlikely to result in jailbreaks. Using tree-of-thought reasoning allows TAP to navigate a large search space of prompts and pruning reduces the total number of queries sent to the target. In empirical evaluations, we observe that TAP generates prompts that jailbreak state-of-the-art LLMs (including GPT4 and GPT4-Turbo) for more than 80% of the prompts using only a small number of queries. This significantly improves upon the previous state-of-the-art black-box method for generating jailbreaks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02119v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>虽然大型语言模型（LLM）显示出多功能性，但它们仍会继续生成有害、有偏见和有毒的内容，这一点可以从人类设计的越狱的普遍性中看出。在这项工作中，我们提出了修剪攻击树（TAP），这是一种自动生成越狱的方法，只需要黑盒访问目标LLM。TAP利用LLM使用思想树推理迭代地细化候选（攻击）提示，直到生成的提示之一越狱目标。至关重要的是，在向目标发送提示之前，TAP会对其进行评估，并删除不太可能导致越狱的提示。使用思想树推理可以使TAP在提示的大搜索空间中导航，修剪可以减少发送到目标的查询总数。在经验评估中，我们观察到TAP仅使用少量查询就生成了超过80%的提示，这些提示打破了最先进的LLM（包括GPT4和GPT4-Turbo）。这大大改进了以前最先进的黑盒生成越狱的方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02119v1" target="_blank">2312.02119v1</a>
                              </td>
                              <td>Tree of Attacks: Jailbreaking Black-Box LLMs Automatically</td>
                              <td>Anay Mehrotra</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02119v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02119v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/ricommunity/tap" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_07944v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AutoRepo: A general framework for multi-modal LLM-based automated construction reporting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_07944v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_07944v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_07944v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Ensuring the safety, quality, and timely completion of construction projects is paramount, with construction inspections serving as a vital instrument towards these goals. Nevertheless, the predominantly manual approach of present-day inspections frequently results in inefficiencies and inadequate information management. Such methods often fall short of providing holistic, exhaustive assessments, consequently engendering regulatory oversights and potential safety hazards. To address this issue, this paper presents a novel framework named AutoRepo for automated generation of construction inspection reports. The unmanned vehicles efficiently perform construction inspections and collect scene information, while the multimodal large language models (LLMs) are leveraged to automatically generate the inspection reports. The framework was applied and tested on a real-world construction site, demonstrating its potential to expedite the inspection process, significantly reduce resource allocation, and produce high-quality, regulatory standard-compliant inspection reports. This research thus underscores the immense potential of multimodal large language models in revolutionizing construction inspection practices, signaling a significant leap forward towards a more efficient and safer construction management paradigm.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_07944v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>确保建设项目的安全、质量和及时完工至关重要，施工检查是实现这些目标的重要工具。然而，目前的检查主要采用手工方法，经常导致效率低下和信息管理不足。这种方法往往无法提供全面、详尽的评估，从而导致监管疏忽和潜在的安全隐患。为了解决这个问题，本文提出了一个名为AutoRepo的新框架，用于自动生成施工检查报告。无人车高效地执行施工检查并收集现场信息，同时利用多模式大语言模型（LLM）自动生成检查报告。该框架在现实世界的建筑工地上进行了应用和测试，展示了其加快检查过程、大幅减少资源分配以及生成高质量、符合监管标准的检查报告的潜力。因此，这项研究强调了多模式大语言模型在彻底改变施工检查实践方面的巨大潜力，标志着朝着更高效、更安全的施工管理模式迈出了重大步伐。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.07944v2" target="_blank">2310.07944v2</a>
                              </td>
                              <td>AutoRepo: A general framework for multi-modal LLM-based automated construction reporting</td>
                              <td>Hongxu Pu</td>
                              <td>2023-10-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_07944v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.07944v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02091v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Physics simulation capabilities of LLMs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02091v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02091v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02091v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>[Abridged abstract] Large Language Models (LLMs) can solve some undergraduate-level to graduate-level physics textbook problems and are proficient at coding. Combining these two capabilities could one day enable AI systems to simulate and predict the physical world.   We present an evaluation of state-of-the-art (SOTA) LLMs on PhD-level to research-level computational physics problems. We condition LLM generation on the use of well-documented and widely-used packages to elicit coding capabilities in the physics and astrophysics domains. We contribute $\sim 50$ original and challenging problems in celestial mechanics (with REBOUND), stellar physics (with MESA), 1D fluid dynamics (with Dedalus) and non-linear dynamics (with SciPy). Since our problems do not admit unique solutions, we evaluate LLM performance on several soft metrics: counts of lines that contain different types of errors (coding, physics, necessity and sufficiency) as well as a more "educational" Pass-Fail metric focused on capturing the salient physical ingredients of the problem at hand.   As expected, today's SOTA LLM (GPT4) zero-shot fails most of our problems, although about 40\% of the solutions could plausibly get a passing grade. About $70-90 \%$ of the code lines produced are necessary, sufficient and correct (coding \& physics). Physics and coding errors are the most common, with some unnecessary or insufficient lines. We observe significant variations across problem class and difficulty. We identify several failure modes of GPT4 in the computational physics domain.   Our reconnaissance work provides a snapshot of current computational capabilities in classical physics and points to obvious improvement targets if AI systems are ever to reach a basic level of autonomy in physics simulation capabilities.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02091v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>[摘要]大型语言模型（LLM）可以解决一些本科生到研究生级别的物理教科书问题，并且精通编码。将这两种能力相结合，有一天可以使人工智能系统模拟和预测物理世界。我们对最先进的（SOTA）LLM进行了博士级评估，以研究计算物理问题。我们将LLM的生成条件设定为使用有据可查且广泛使用的包，以获得物理和天体物理学领域的编码能力。我们在天体力学（与REBOUND）、恒星物理学（与MESA）、1D流体动力学（与Dedalus）和非线性动力学（与SciPy）方面贡献了$\sim 50美元的原创和具有挑战性的问题。由于我们的问题不允许有唯一的解决方案，我们在几个软指标上评估LLM的性能：包含不同类型错误（编码、物理、必要性和充分性）的行数，以及更“教育性”的通过-失败指标，该指标侧重于捕捉手头问题的显著物理成分。正如预期的那样，今天的SOTA LLM（GPT4）零样本解决了我们的大多数问题，尽管大约40%的解决方案可能会获得合格的分数。大约$70-90\%$产生的代码行是必要的、足够的和正确的（编码和物理）。物理和编码错误是最常见的，有一些不必要或不充分的行。我们观察到问题类别和难度之间存在显著差异。我们在计算物理领域确定了GPT4的几种失效模式。我们的侦察工作提供了经典物理中当前计算能力的快照，并指出了如果人工智能系统要在物理模拟能力方面达到基本的自主权水平，就会有明显的改进目标。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02091v1" target="_blank">2312.02091v1</a>
                              </td>
                              <td>Physics simulation capabilities of LLMs</td>
                              <td>Mohamad Ali-Dib</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02091v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02091v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02073v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02073v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02073v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02073v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have demonstrated impressive capabilities in storing and recalling factual knowledge, but also in adapting to novel in-context information. Yet, the mechanisms underlying their in-context grounding remain unknown, especially in situations where in-context information contradicts factual knowledge embedded in the parameters. This is critical for retrieval-augmented generation methods, which enrich the context with up-to-date information, hoping that grounding can rectify the outdated parametric knowledge. In this study, we introduce Fakepedia, a counterfactual dataset designed to evaluate grounding abilities when the parametric knowledge clashes with the in-context information. We benchmark various LLMs with Fakepedia and discover that GPT-4-turbo has a strong preference for its parametric knowledge. Mistral-7B, on the contrary, is the model that most robustly chooses the grounded answer. Then, we conduct causal mediation analysis on LLM components when answering Fakepedia queries. We demonstrate that inspection of the computational graph alone can predict LLM grounding with 92.8% accuracy, especially because few MLPs in the Transformer can predict non-grounded behavior. Our results, together with existing findings about factual recall mechanisms, provide a coherent narrative of how grounding and factual recall mechanisms interact within LLMs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02073v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）在存储和回忆事实知识方面，以及在适应新的上下文信息方面，都表现出了令人印象深刻的能力。然而，它们基于上下文的机制仍然未知，尤其是在上下文信息与嵌入参数中的事实知识相矛盾的情况下。这对于检索增强生成方法至关重要，该方法用最新信息丰富上下文，希望基础能够纠正过时的参数知识。在这项研究中，我们介绍了Fakepedia，这是一个反事实数据集，旨在评估参数知识与上下文信息冲突时的基础能力。我们用Fakepedia对各种LLM进行了基准测试，发现GPT-4-turbo对其参数知识有强烈的偏好。相反，Mistral-7B是最稳健地选择有根据答案的模型。然后，我们在回答Fakepedia查询时对LLM组件进行因果中介分析。我们证明，仅检查计算图就可以预测LLM接地，准确率为92.8%，特别是因为变压器中很少有MLP可以预测非接地行为。我们的研究结果，以及关于事实回忆机制的现有研究结果，为基础和事实回忆机制如何在LLM中相互作用提供了一个连贯的叙述。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02073v1" target="_blank">2312.02073v1</a>
                              </td>
                              <td>A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia</td>
                              <td>Giovanni Monea</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02073v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02073v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02065v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Know Your Audience: Do LLMs Adapt to Different Age and Education Levels?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02065v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02065v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02065v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) offer a range of new possibilities, including adapting the text to different audiences and their reading needs. But how well do they adapt? We evaluate the readability of answers generated by four state-of-the-art LLMs (commercial and open-source) to science questions when prompted to target different age groups and education levels. To assess the adaptability of LLMs to diverse audiences, we compare the readability scores of the generated responses against the recommended comprehension level of each age and education group. We find large variations in the readability of the answers by different LLMs. Our results suggest LLM answers need to be better adapted to the intended audience demographics to be more comprehensible. They underline the importance of enhancing the adaptability of LLMs in education settings to cater to diverse age and education levels. Overall, current LLMs have set readability ranges and do not adapt well to different audiences, even when prompted. That limits their potential for educational purposes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02065v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）提供了一系列新的可能性，包括使文本适应不同的受众及其阅读需求。但它们适应得有多好？当被提示针对不同的年龄组和教育水平时，我们评估了四个最先进的LLM（商业和开源）对科学问题产生的答案的可读性。为了评估LLM对不同受众的适应性，我们将生成的回答的可读性得分与每个年龄和教育组的推荐理解水平进行比较。我们发现不同LLM在答案的可读性方面存在很大差异。我们的研究结果表明，LLM的答案需要更好地适应预期的受众人口统计，才能更容易理解。他们强调了提高LLM在教育环境中的适应性以适应不同年龄和教育水平的重要性。总的来说，当前的LLM设置了可读性范围，即使在提示时也不能很好地适应不同的受众。这限制了他们用于教育目的的潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02065v1" target="_blank">2312.02065v1</a>
                              </td>
                              <td>Know Your Audience: Do LLMs Adapt to Different Age and Education Levels?</td>
                              <td>Donya Rooein</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02065v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02065v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02051v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02051v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02051v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02051v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This work proposes TimeChat, a time-sensitive multimodal large language model specifically designed for long video understanding. Our model incorporates two key architectural contributions: (1) a timestamp-aware frame encoder that binds visual content with the timestamp of each frame, and (2) a sliding video Q-Former that produces a video token sequence of varying lengths to accommodate videos of various durations. Additionally, we construct an instruction-tuning dataset, encompassing 6 tasks and a total of 125K instances, to further enhance TimeChat's instruction-following performance. Experiment results across various video understanding tasks, such as dense captioning, temporal grounding, and highlight detection, demonstrate TimeChat's strong zero-shot temporal localization and reasoning capabilities. For example, it achieves +9.2 F1 score and +2.8 CIDEr on YouCook2, +5.8 HIT@1 on QVHighlights, and +27.5 R@1 (IoU=0.5) on Charades-STA, compared to state-of-the-art video large language models, holding the potential to serve as a versatile video assistant for long-form video comprehension tasks and satisfy realistic user requirements.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02051v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>这项工作提出了TimeChat，一种时间敏感的多模式大型语言模型，专门为长视频理解而设计。我们的模型包含了两个关键的体系结构贡献：（1）时间戳感知帧编码器，它将视觉内容与每帧的时间戳绑定在一起；（2）滑动视频Q-Former，它产生不同长度的视频令牌序列，以适应不同持续时间的视频。此外，我们构建了一个指令调优数据集，包括6个任务和总共125K个实例，以进一步提高TimeChat的指令跟随性能。各种视频理解任务的实验结果，如密集字幕、时间基础和高亮检测，证明了TimeChat强大的零样本时间定位和推理能力。例如，它在YouCook2上获得了+9.2的F1成绩和+2.8的CIDEr成绩，+5.8hit@1与最先进的视频大语言模型相比，QVHighlights和Charades STA上的+27.5 R@1（IoU=0.5）具有作为长格式视频理解任务的通用视频助手的潜力，并满足现实的用户需求。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02051v1" target="_blank">2312.02051v1</a>
                              </td>
                              <td>TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding</td>
                              <td>Shuhuai Ren</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02051v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02051v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_17465v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AgentAvatar: Disentangling Planning, Driving and Rendering for Photorealistic Avatar Agents</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_17465v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_17465v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_17465v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this study, our goal is to create interactive avatar agents that can autonomously plan and animate nuanced facial movements realistically, from both visual and behavioral perspectives. Given high-level inputs about the environment and agent profile, our framework harnesses LLMs to produce a series of detailed text descriptions of the avatar agents' facial motions. These descriptions are then processed by our task-agnostic driving engine into motion token sequences, which are subsequently converted into continuous motion embeddings that are further consumed by our standalone neural-based renderer to generate the final photorealistic avatar animations. These streamlined processes allow our framework to adapt to a variety of non-verbal avatar interactions, both monadic and dyadic. Our extensive study, which includes experiments on both newly compiled and existing datasets featuring two types of agents -- one capable of monadic interaction with the environment, and the other designed for dyadic conversation -- validates the effectiveness and versatility of our approach. To our knowledge, we advanced a leap step by combining LLMs and neural rendering for generalized non-verbal prediction and photo-realistic rendering of avatar agents.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_17465v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项研究中，我们的目标是创建交互式化身代理，该代理可以从视觉和行为角度自主规划和逼真地动画化细微的面部动作。给定关于环境和代理概况的高级输入，我们的框架利用LLM来生成化身代理面部运动的一系列详细文本描述。然后，我们的任务无关驱动引擎将这些描述处理为运动标记序列，随后将其转换为连续的运动嵌入，这些运动嵌入由我们的独立基于神经的渲染器进一步消耗，以生成最终的照片真实感化身动画。这些简化的过程使我们的框架能够适应各种非语言化身交互，包括单元和二元交互。我们的广泛研究，包括在新编译的和现有的数据集上进行的实验，其中包括两种类型的代理——一种能够与环境进行一元交互，另一种设计用于二元对话——验证了我们方法的有效性和多功能性。据我们所知，我们通过将LLM和神经渲染相结合，实现了化身代理的广义非语言预测和照片逼真渲染，迈出了一大步。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.17465v3" target="_blank">2311.17465v3</a>
                              </td>
                              <td>AgentAvatar: Disentangling Planning, Driving and Rendering for Photorealistic Avatar Agents</td>
                              <td>Duomin Wang</td>
                              <td>2023-11-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_17465v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.17465v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02010v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Learning a Generalist Model for Embodied Navigation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02010v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02010v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02010v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Building a generalist agent that can interact with the world is the intriguing target of AI systems, thus spurring the research for embodied navigation, where an agent is required to navigate according to instructions or respond to queries. Despite the major progress attained, previous works primarily focus on task-specific agents and lack generalizability to unseen scenarios. Recently, LLMs have presented remarkable capabilities across various fields, and provided a promising opportunity for embodied navigation. Drawing on this, we propose the first generalist model for embodied navigation, NaviLLM. It adapts LLMs to embodied navigation by introducing schema-based instruction. The schema-based instruction flexibly casts various tasks into generation problems, thereby unifying a wide range of tasks. This approach allows us to integrate diverse data sources from various datasets into the training, equipping NaviLLM with a wide range of capabilities required by embodied navigation. We conduct extensive experiments to evaluate the performance and generalizability of our model. The experimental results demonstrate that our unified model achieves state-of-the-art performance on CVDN, SOON, and ScanQA. Specifically, it surpasses the previous stats-of-the-art method by a significant margin of 29% in goal progress on CVDN. Moreover, our model also demonstrates strong generalizability and presents impressive results on unseen tasks, e.g., embodied question answering and 3D captioning.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02010v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>构建一个能够与世界互动的多面手代理是人工智能系统的有趣目标，从而推动了对嵌入式导航的研究，在嵌入式导航中，代理需要根据指令导航或响应查询。尽管取得了重大进展，但以前的工作主要集中在特定任务的代理上，缺乏对看不见的场景的可推广性。最近，LLM在各个领域都表现出了非凡的能力，并为嵌入式导航提供了一个充满希望的机会。基于此，我们提出了第一个用于嵌入式导航的广义模型NaviLLM。它通过引入基于模式的指令，使LLM适应具体导航。基于模式的指令将各种任务灵活地转换为生成问题，从而统一了广泛的任务。这种方法使我们能够将来自各种数据集的各种数据源集成到训练中，为NaviLLM提供嵌入式导航所需的广泛功能。我们进行了大量的实验来评估我们的模型的性能和可推广性。实验结果表明，我们的统一模型在CVDN、SOON和ScanQA上实现了最先进的性能。具体而言，它在CVDN的进球进度方面以29%的显著优势超过了以往的统计数据。此外，我们的模型还表现出很强的可推广性，并在看不见的任务上取得了令人印象深刻的结果，例如嵌入式问答和3D字幕。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02010v1" target="_blank">2312.02010v1</a>
                              </td>
                              <td>Towards Learning a Generalist Model for Embodied Navigation</td>
                              <td>Duo Zheng</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02010v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02010v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zd11024/NaviLLM" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_14743v4_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Baseline Analysis of Reward Models' Ability To Accurately Analyze Foundation Models Under Distribution Shift</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_14743v4_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_14743v4_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_14743v4_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Foundation models, specifically Large Language Models (LLM's), have lately gained wide-spread attention and adoption. Reinforcement Learning with Human Feedback (RLHF) involves training a reward model to capture desired behaviors, which is then used to align LLM's. These reward models are additionally used at inference-time to estimate LLM responses' adherence to those desired behaviors. However, there is little work measuring how robust these reward models are to distribution shifts. In this work, we evaluate how reward model performance - measured via accuracy and calibration (i.e. alignment between accuracy and confidence) - is affected by distribution shift. We show novel calibration patterns and accuracy drops due to OOD prompts and responses, and that the reward model is more sensitive to shifts in responses than prompts. Additionally, we adapt an OOD detection technique commonly used in classification to the reward model setting to detect these distribution shifts in prompts and responses.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_14743v4_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基础模型，特别是大型语言模型（LLM），最近得到了广泛的关注和采用。带人类反馈的强化学习（RLHF）包括训练奖励模型来捕捉期望的行为，然后用于调整LLM。这些奖励模型在推理时被额外用于估计LLM响应对那些期望行为的遵守程度。然而，很少有工作来衡量这些奖励模型对分布变化的稳健性。在这项工作中，我们评估了通过准确性和校准（即准确性和置信度之间的一致性）测量的奖励模型性能如何受到分布变化的影响。我们展示了由于OOD提示和响应而导致的新的校准模式和准确性下降，并且奖励模型对响应的变化比提示更敏感。此外，我们将分类中常用的OOD检测技术应用于奖励模型设置，以检测提示和响应中的这些分布变化。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.14743v4" target="_blank">2311.14743v4</a>
                              </td>
                              <td>A Baseline Analysis of Reward Models' Ability To Accurately Analyze Foundation Models Under Distribution Shift</td>
                              <td>Ben Pikus</td>
                              <td>2023-11-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_14743v4_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.14743v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02003v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02003v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02003v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02003v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs), such as GPT-3 and BERT, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of LLMs with security and privacy. Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs. Through a comprehensive literature review, the paper categorizes findings into "The Good" (beneficial LLM applications), "The Bad" (offensive applications), and "The Ugly" (vulnerabilities and their defenses). We have some interesting findings. For example, LLMs have proven to enhance code and data security, outperforming traditional methods. However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities. We have identified areas that require further research efforts. For example, research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality. Safe instruction tuning, a recent development, requires more exploration. We hope that our work can shed light on the LLMs' potential to both bolster and jeopardize cybersecurity.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02003v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM），如GPT-3和BERT，已经彻底改变了自然语言的理解和生成。他们拥有深入的语言理解能力、类人的文本生成能力、上下文意识和强大的解决问题的技能，这使他们在各个领域（如搜索引擎、客户支持、翻译）都非常宝贵。与此同时，LLM也在安全界获得了关注，揭示了安全漏洞，并展示了其在安全相关任务中的潜力。本文探讨了LLM与安全和隐私的交叉点。具体而言，我们调查LLM如何对安全和隐私产生积极影响，与使用LLM相关的潜在风险和威胁，以及LLM中的固有漏洞。通过全面的文献综述，本文将研究结果分为“好的”（有益的LLM应用程序）、“坏的”（攻击性应用程序）和“丑陋的”（漏洞及其防御）。我们有一些有趣的发现。例如，LLM已被证明可以增强代码和数据安全性，优于传统方法。然而，由于其类似人类的推理能力，它们也可以用于各种攻击（特别是用户级攻击）。我们已经确定了需要进一步研究的领域。例如，对模型和参数提取攻击的研究是有限的，而且往往是理论上的，受到LLM参数规模和机密性的阻碍。安全指令调整是最近的发展，需要更多的探索。我们希望我们的工作能够揭示LLM在支持和危害网络安全方面的潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02003v1" target="_blank">2312.02003v1</a>
                              </td>
                              <td>A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly</td>
                              <td>Yifan Yao</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02003v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02003v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01987v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Bootstrapping SparseFormers from Vision Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01987v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01987v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01987v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The recently proposed SparseFormer architecture provides an alternative approach to visual understanding by utilizing a significantly lower number of visual tokens via adjusting RoIs, greatly reducing computational costs while still achieving promising performance. However, training SparseFormers from scratch is still expensive, and scaling up the number of parameters can be challenging. In this paper, we propose to bootstrap SparseFormers from ViT-based vision foundation models in a simple and efficient way. Since the majority of SparseFormer blocks are the standard transformer ones, we can inherit weights from large-scale pre-trained vision transformers and freeze them as much as possible. Therefore, we only need to train the SparseFormer-specific lightweight focusing transformer to adjust token RoIs and fine-tune a few early pre-trained blocks to align the final token representation. In such a way, we can bootstrap SparseFormer architectures from various large-scale pre-trained models (e.g., IN-21K pre-trained AugRegs or CLIPs) using a rather smaller amount of training samples (e.g., IN-1K) and without labels or captions within just a few hours. As a result, the bootstrapped unimodal SparseFormer (from AugReg-ViT-L/16-384) can reach 84.9% accuracy on IN-1K with only 49 tokens, and the multimodal SparseFormer from CLIPs also demonstrates notable zero-shot performance with highly reduced computational cost without seeing any caption during the bootstrapping procedure. In addition, CLIP-bootstrapped SparseFormers, which align the output space with language without seeing a word, can serve as efficient vision encoders in multimodal large language models. Code will be publicly available at https://github.com/showlab/sparseformer</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01987v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近提出的SparseFormer架构提供了一种视觉理解的替代方法，通过调整ROI来利用数量显著减少的视觉标记，大大降低了计算成本，同时仍然实现了有希望的性能。然而，从头开始训练SparseFormers仍然是昂贵的，并且扩大参数的数量可能具有挑战性。在本文中，我们建议以一种简单有效的方式从基于ViT的视觉基础模型中引导SparseFormers。由于大多数SparseFormer块都是标准的transformer块，我们可以从大规模预训练的视觉transformer中继承权重，并尽可能多地冻结它们。因此，我们只需要训练SparseFormer特定的轻量级聚焦转换器来调整令牌ROI，并微调一些早期预训练的块来对齐最终的令牌表示。通过这种方式，我们可以在短短几个小时内使用少量的训练样本（例如，In-1K）从各种大规模预训练模型（例如，In-21K预训练AugRegs或CLIP）中引导SparseFormer架构，而不需要标签或字幕。因此，自举单模态SparseFormer（来自AugReg-ViT-L/16-384）在IN-1K上仅使用49个令牌就可以达到84.9%的准确率，而来自CLIP的多模态SparseFormer也表现出显著的零样本性能，大大降低了计算成本，而在自举过程中看不到任何说明。此外，CLIP自举的SparseFormers可以在不看单词的情况下将输出空间与语言对齐，可以在多模式大型语言模型中用作高效的视觉编码器。代码将在公开https://github.com/showlab/sparseformer</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01987v1" target="_blank">2312.01987v1</a>
                              </td>
                              <td>Bootstrapping SparseFormers from Vision Foundation Models</td>
                              <td>Ziteng Gao</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01987v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01987v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/showlab/sparseformer" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01957v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian Perspective</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01957v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01957v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01957v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper proposes an interpretation of RLAIF as Bayesian inference by introducing distilled Self-Critique (dSC), which refines the outputs of a LLM through a Gibbs sampler that is later distilled into a fine-tuned model. Only requiring synthetic data, dSC is exercised in experiments regarding safety, sentiment, and privacy control, showing it can be a viable and cheap alternative to align LLMs. Code released at \url{https://github.com/vicgalle/distilled-self-critique}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01957v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文通过引入提取自批评（dSC），将RLAIF解释为贝叶斯推理，该自批评通过吉布斯采样器对LLM的输出进行细化，该采样器随后被提取为微调模型。dSC只需要合成数据，就可以在安全、情感和隐私控制方面进行实验，这表明它是一种可行且廉价的LLM对齐替代方案。代码发布于\url{https://github.com/vicgalle/distilled-self-critique}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01957v1" target="_blank">2312.01957v1</a>
                              </td>
                              <td>Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian Perspective</td>
                              <td>Victor Gallego</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01957v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01957v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/vicgalle/distilled-self-critique" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01954v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Zero- and Few-Shots Knowledge Graph Triplet Extraction with Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01954v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01954v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01954v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this work, we tested the Triplet Extraction (TE) capabilities of a variety of Large Language Models (LLMs) of different sizes in the Zero- and Few-Shots settings. In detail, we proposed a pipeline that dynamically gathers contextual information from a Knowledge Base (KB), both in the form of context triplets and of (sentence, triplets) pairs as examples, and provides it to the LLM through a prompt. The additional context allowed the LLMs to be competitive with all the older fully trained baselines based on the Bidirectional Long Short-Term Memory (BiLSTM) Network architecture. We further conducted a detailed analysis of the quality of the gathered KB context, finding it to be strongly correlated with the final TE performance of the model. In contrast, the size of the model appeared to only logarithmically improve the TE capabilities of the LLMs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01954v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项工作中，我们在零镜头和少量镜头设置中测试了各种不同大小的大型语言模型（LLM）的三元组提取（TE）功能。详细地说，我们提出了一种从知识库（KB）动态收集上下文信息的管道，以上下文三元组和（句子，三元组）对为例，并通过提示将其提供给LLM。额外的上下文使LLM能够与基于双向长短期存储器（BiLSTM）网络架构的所有较旧的完全训练的基线竞争。我们进一步对收集的知识库上下文的质量进行了详细分析，发现它与模型的最终TE性能密切相关。相比之下，模型的大小似乎只是对数地提高了LLM的TE能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01954v1" target="_blank">2312.01954v1</a>
                              </td>
                              <td>Zero- and Few-Shots Knowledge Graph Triplet Extraction with Large Language Models</td>
                              <td>Andrea Papaluca</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01954v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01954v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_12509v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Joint Prompt Optimization of Stacked LLMs using Variational Inference</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_12509v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_12509v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_12509v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) can be seen as atomic units of computation mapping sequences to a distribution over sequences. Thus, they can be seen as stochastic language layers in a language network, where the learnable parameters are the natural language prompts at each layer. By stacking two such layers and feeding the output of one layer to the next, we obtain a Deep Language Network (DLN). We first show how to effectively perform prompt optimization for a 1-Layer language network (DLN-1). Then, we present an extension that applies to 2-layer DLNs (DLN-2), where two prompts must be learned. The key idea is to consider the output of the first layer as a latent variable, which requires inference, and prompts to be learned as the parameters of the generative distribution. We first test the effectiveness of DLN-1 in multiple reasoning and natural language understanding tasks. Then, we show that DLN-2 can reach higher performance than a single layer, showing promise that we might reach comparable performance to GPT-4, even when each LLM in the network is smaller and less powerful.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_12509v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）可以被视为将序列映射到序列上的分布的原子计算单元。因此，它们可以被视为语言网络中的随机语言层，其中可学习参数是每一层的自然语言提示。通过堆叠两个这样的层并将一层的输出馈送到下一层，我们获得了深度语言网络（DLN）。我们首先展示了如何有效地对一层语言网络（DLN-1）进行即时优化。然后，我们提出了一个适用于2层DLN（DLN-2）的扩展，其中必须学习两个提示。关键思想是将第一层的输出视为一个潜在变量，这需要推理，并提示将其作为生成分布的参数进行学习。我们首先测试了DLN-1在多重推理和自然语言理解任务中的有效性。然后，我们展示了DLN-2可以达到比单层更高的性能，这表明我们可能达到与GPT-4相当的性能，即使网络中的每个LLM更小、功能较弱。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.12509v2" target="_blank">2306.12509v2</a>
                              </td>
                              <td>Joint Prompt Optimization of Stacked LLMs using Variational Inference</td>
                              <td>Alessandro Sordoni</td>
                              <td>2023-06-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_12509v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.12509v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/microsoft/deep-language-networks" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01941v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Intrusion Detection System with Machine Learning and Multiple Datasets</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01941v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01941v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01941v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>As Artificial Intelligence (AI) technologies continue to gain traction in the modern-day world, they ultimately pose an immediate threat to current cybersecurity systems via exploitative methods. Prompt engineering is a relatively new field that explores various prompt designs that can hijack large language models (LLMs). If used by an unethical attacker, it can enable an AI system to offer malicious insights and code to them. In this paper, an enhanced intrusion detection system (IDS) that utilizes machine learning (ML) and hyperparameter tuning is explored, which can improve a model's performance in terms of accuracy and efficacy. Ultimately, this improved system can be used to combat the attacks made by unethical hackers. A standard IDS is solely configured with pre-configured rules and patterns; however, with the utilization of machine learning, implicit and different patterns can be generated through the models' hyperparameter settings and parameters. In addition, the IDS will be equipped with multiple datasets so that the accuracy of the models improves. We evaluate the performance of multiple ML models and their respective hyperparameter settings through various metrics to compare their results to other models and past research work. The results of the proposed multi-dataset integration method yielded an accuracy score of 99.9% when equipped with the XGBoost and random forest classifiers and RandomizedSearchCV hyperparameter technique.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01941v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着人工智能（AI）技术在现代世界的不断发展，它们最终通过剥削的方式对当前的网络安全系统构成了直接威胁。提示工程是一个相对较新的领域，它探索了各种可能劫持大型语言模型（LLM）的提示设计。如果被不道德的攻击者使用，它可以使人工智能系统向他们提供恶意见解和代码。在本文中，探索了一种利用机器学习（ML）和超参数调整的增强型入侵检测系统（IDS），该系统可以提高模型的准确性和有效性。最终，这种改进后的系统可以用来对抗不道德黑客的攻击。标准IDS仅使用预先配置的规则和模式进行配置；然而，利用机器学习，可以通过模型的超参数设置和参数生成隐含的和不同的模式。此外，IDS将配备多个数据集，以提高模型的准确性。我们通过各种指标评估多个ML模型的性能及其各自的超参数设置，将其结果与其他模型和过去的研究工作进行比较。当配备XGBoost和随机森林分类器以及RandomizedSearchCV超参数技术时，所提出的多数据集集成方法的结果产生了99.9%的准确度得分。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01941v1" target="_blank">2312.01941v1</a>
                              </td>
                              <td>Intrusion Detection System with Machine Learning and Multiple Datasets</td>
                              <td>Haiyan Xuan</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01941v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01941v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_16563v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Skill Reinforcement Learning and Planning for Open-World Long-Horizon Tasks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_16563v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_16563v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_16563v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We study building multi-task agents in open-world environments. Without human demonstrations, learning to accomplish long-horizon tasks in a large open-world environment with reinforcement learning (RL) is extremely inefficient. To tackle this challenge, we convert the multi-task learning problem into learning basic skills and planning over the skills. Using the popular open-world game Minecraft as the testbed, we propose three types of fine-grained basic skills, and use RL with intrinsic rewards to acquire skills. A novel Finding-skill that performs exploration to find diverse items provides better initialization for other skills, improving the sample efficiency for skill learning. In skill planning, we leverage the prior knowledge in Large Language Models to find the relationships between skills and build a skill graph. When the agent is solving a task, our skill search algorithm walks on the skill graph and generates the proper skill plans for the agent. In experiments, our method accomplishes 40 diverse Minecraft tasks, where many tasks require sequentially executing for more than 10 skills. Our method outperforms baselines by a large margin and is the most sample-efficient demonstration-free RL method to solve Minecraft Tech Tree tasks. The project's website and code can be found at https://sites.google.com/view/plan4mc.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_16563v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们研究在开放世界环境中构建多任务代理。如果没有人类的演示，在大型开放世界环境中使用强化学习（RL）来完成长期任务的学习效率极低。为了应对这一挑战，我们将多任务学习问题转化为学习基本技能和计划技能。以流行的开放世界游戏《我的世界》为试验台，我们提出了三种类型的细粒度基本技能，并使用具有内在奖励的RL来获取技能。一种新颖的查找技能，通过探索来查找不同的项目，可以更好地初始化其他技能，提高技能学习的样本效率。在技能规划中，我们利用大型语言模型中的先验知识来找到技能之间的关系，并构建技能图。当代理解决任务时，我们的技能搜索算法在技能图上行走，并为代理生成适当的技能计划。在实验中，我们的方法完成了40项不同的Minecraft任务，其中许多任务需要连续执行10多项技能。我们的方法在很大程度上优于基线，是解决Minecraft技术树任务的最具样本效率的无演示RL方法。该项目的网站和代码可以在https://sites.google.com/view/plan4mc.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.16563v2" target="_blank">2303.16563v2</a>
                              </td>
                              <td>Skill Reinforcement Learning and Planning for Open-World Long-Horizon Tasks</td>
                              <td>Haoqi Yuan</td>
                              <td>2023-03-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_16563v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.16563v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_09909v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_09909v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_09909v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_09909v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Driven by the large foundation models, the development of artificial intelligence has witnessed tremendous progress lately, leading to a surge of general interest from the public. In this study, we aim to assess the performance of OpenAI's newest model, GPT-4V(ision), specifically in the realm of multimodal medical diagnosis. Our evaluation encompasses 17 human body systems, including Central Nervous System, Head and Neck, Cardiac, Chest, Hematology, Hepatobiliary, Gastrointestinal, Urogenital, Gynecology, Obstetrics, Breast, Musculoskeletal, Spine, Vascular, Oncology, Trauma, Pediatrics, with images taken from 8 modalities used in daily clinic routine, e.g., X-ray, Computed Tomography (CT), Magnetic Resonance Imaging (MRI), Positron Emission Tomography (PET), Digital Subtraction Angiography (DSA), Mammography, Ultrasound, and Pathology. We probe the GPT-4V's ability on multiple clinical tasks with or without patent history provided, including imaging modality and anatomy recognition, disease diagnosis, report generation, disease localisation.   Our observation shows that, while GPT-4V demonstrates proficiency in distinguishing between medical image modalities and anatomy, it faces significant challenges in disease diagnosis and generating comprehensive reports. These findings underscore that while large multimodal models have made significant advancements in computer vision and natural language processing, it remains far from being used to effectively support real-world medical applications and clinical decision-making.   All images used in this report can be found in https://github.com/chaoyi-wu/GPT-4V_Medical_Evaluation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_09909v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在大型基础模型的推动下，人工智能的发展最近取得了巨大进展，引起了公众的普遍兴趣。在这项研究中，我们旨在评估OpenAI的最新模型GPT-4V（vision）的性能，特别是在多模式医学诊断领域的性能。我们的评估涵盖了17个人体系统，包括中枢神经系统、头颈部、心脏、胸部、血液学、肝胆、胃肠道、泌尿生殖、妇科、产科、乳房、肌肉骨骼、脊柱、血管、肿瘤、创伤、儿科，图像取自日常临床使用的8种模式，如X射线、计算机断层扫描（CT），磁共振成像（MRI）、正电子发射断层扫描（PET）、数字减影血管造影（DSA）、乳腺造影、超声和病理学。我们探讨了GPT-4V在提供或不提供专利史的情况下执行多项临床任务的能力，包括成像模式和解剖识别、疾病诊断、报告生成、疾病定位。我们的观察结果表明，尽管GPT-4V在区分医学图像模式和解剖学方面表现出了熟练的能力，但它在疾病诊断和生成综合报告方面面临着重大挑战。这些发现强调，尽管大型多模式模型在计算机视觉和自然语言处理方面取得了重大进展，但它远未被用于有效支持现实世界的医疗应用和临床决策。此报告中使用的所有图像都可以在中找到https://github.com/chaoyi-wu/gpt-4v_medical_evaluation.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.09909v3" target="_blank">2310.09909v3</a>
                              </td>
                              <td>Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis</td>
                              <td>Chaoyi Wu</td>
                              <td>2023-10-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_09909v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.09909v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/chaoyi-wu/gpt-4v_medical_evaluation" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01886v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01886v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01886v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01886v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large vision-language models (LVLMs) have demonstrated their incredible capability in image understanding and response generation. However, this rich visual interaction also makes LVLMs vulnerable to adversarial examples. In this paper, we formulate a novel and practical gray-box attack scenario that the adversary can only access the visual encoder of the victim LVLM, without the knowledge of its prompts (which are often proprietary for service providers and not publicly available) and its underlying large language model (LLM). This practical setting poses challenges to the cross-prompt and cross-model transferability of targeted adversarial attack, which aims to confuse the LVLM to output a response that is semantically similar to the attacker's chosen target text. To this end, we propose an instruction-tuned targeted attack (dubbed InstructTA) to deliver the targeted adversarial attack on LVLMs with high transferability. Initially, we utilize a public text-to-image generative model to "reverse" the target response into a target image, and employ GPT-4 to infer a reasonable instruction $\boldsymbol{p}^\prime$ from the target response. We then form a local surrogate model (sharing the same visual encoder with the victim LVLM) to extract instruction-aware features of an adversarial image example and the target image, and minimize the distance between these two features to optimize the adversarial example. To further improve the transferability, we augment the instruction $\boldsymbol{p}^\prime$ with instructions paraphrased from an LLM. Extensive experiments demonstrate the superiority of our proposed method in targeted attack performance and transferability.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01886v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型视觉语言模型（LVLMs）在图像理解和响应生成方面表现出了令人难以置信的能力。然而，这种丰富的视觉交互也使LVLM容易受到对抗性示例的影响。在本文中，我们提出了一种新颖实用的灰盒攻击场景，即对手只能访问受害者LVLM的视觉编码器，而不知道其提示（通常是服务提供商专有的，不公开）及其底层的大型语言模型（LLM）。这种实际设置对目标对抗性攻击的跨提示和跨模型可转移性提出了挑战，其目的是混淆LVLM以输出与攻击者选择的目标文本语义相似的响应。为此，我们提出了一种指令调整的目标攻击（称为InstructionTA），以对具有高可转移性的LVLMs进行有针对性的对抗性攻击。最初，我们使用公共文本到图像生成模型将目标响应“反转”为目标图像，并使用GPT-4从目标响应中推断出合理的指令$\boldsymbol｛p｝^\prime$。然后，我们形成局部代理模型（与受害者LVLM共享相同的视觉编码器），以提取对抗性图像示例和目标图像的指令感知特征，并最小化这两个特征之间的距离，以优化对抗性示例。为了进一步提高可转移性，我们用从LLM中转述的指令来扩充指令$\boldsymbol｛p｝^\prime$。大量实验证明了我们提出的方法在目标攻击性能和可转移性方面的优越性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01886v1" target="_blank">2312.01886v1</a>
                              </td>
                              <td>InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models</td>
                              <td>Xunguang Wang</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01886v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01886v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01242v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01242v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01242v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01242v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The recent success of Large Language Models (LLMs) signifies an impressive stride towards artificial general intelligence. They have shown a promising prospect in automatically completing tasks upon user instructions, functioning as brain-like coordinators. The associated risks will be revealed as we delegate an increasing number of tasks to machines for automated completion. A big question emerges: how can we make machines behave responsibly when helping humans automate tasks as personal copilots? In this paper, we explore this question in depth from the perspectives of feasibility, completeness and security. In specific, we present Responsible Task Automation (ResponsibleTA) as a fundamental framework to facilitate responsible collaboration between LLM-based coordinators and executors for task automation with three empowered capabilities: 1) predicting the feasibility of the commands for executors; 2) verifying the completeness of executors; 3) enhancing the security (e.g., the protection of users' privacy). We further propose and compare two paradigms for implementing the first two capabilities. One is to leverage the generic knowledge of LLMs themselves via prompt engineering while the other is to adopt domain-specific learnable models. Moreover, we introduce a local memory mechanism for achieving the third capability. We evaluate our proposed ResponsibleTA on UI task automation and hope it could bring more attentions to ensuring LLMs more responsible in diverse scenarios.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01242v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）最近的成功标志着通用人工智能迈出了令人印象深刻的一步。它们在根据用户指令自动完成任务方面显示出了很有希望的前景，起到了类似大脑的协调器的作用。随着我们将越来越多的任务委托给机器自动完成，相关风险将显现出来。一个大问题出现了：当我们作为个人副驾驶帮助人类自动化任务时，我们如何让机器表现得负责任？在本文中，我们从可行性、完整性和安全性的角度对这个问题进行了深入的探讨。具体而言，我们提出了负责任的任务自动化（ResponsibleTA）作为一个基本框架，以促进基于LLM的协调员和执行者之间的负责任协作，实现任务自动化，具有三种授权能力：1）预测执行者的命令的可行性；2） 核实执行人的完整性；3） 增强安全性（例如保护用户隐私）。我们进一步提出并比较了实现前两种能力的两种范式。一种是通过即时工程利用LLM本身的通用知识，另一种是采用特定领域的可学习模型。此外，我们引入了一种用于实现第三种能力的本地存储器机制。我们评估了我们提出的关于UI任务自动化的ResponsibleTA，并希望它能引起更多的关注，以确保LLM在不同的场景中更负责任。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01242v2" target="_blank">2306.01242v2</a>
                              </td>
                              <td>Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators</td>
                              <td>Zhizheng Zhang</td>
                              <td>2023-06-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01242v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01242v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01882v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Unleashing the Potential of Large Language Model: Zero-shot VQA for Flood Disaster Scenario</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01882v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01882v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01882v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual question answering (VQA) is a fundamental and essential AI task, and VQA-based disaster scenario understanding is a hot research topic. For instance, we can ask questions about a disaster image by the VQA model and the answer can help identify whether anyone or anything is affected by the disaster. However, previous VQA models for disaster damage assessment have some shortcomings, such as limited candidate answer space, monotonous question types, and limited answering capability of existing models. In this paper, we propose a zero-shot VQA model named Zero-shot VQA for Flood Disaster Damage Assessment (ZFDDA). It is a VQA model for damage assessment without pre-training. Also, with flood disaster as the main research object, we build a Freestyle Flood Disaster Image Question Answering dataset (FFD-IQA) to evaluate our VQA model. This new dataset expands the question types to include free-form, multiple-choice, and yes-no questions. At the same time, we expand the size of the previous dataset to contain a total of 2,058 images and 22,422 question-meta ground truth pairs. Most importantly, our model uses well-designed chain of thought (CoT) demonstrations to unlock the potential of the large language model, allowing zero-shot VQA to show better performance in disaster scenarios. The experimental results show that the accuracy in answering complex questions is greatly improved with CoT prompts. Our study provides a research basis for subsequent research of VQA for other disaster scenarios.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01882v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>可视化问答（VQA）是人工智能的一项基本任务，基于VQA的灾难场景理解是当前研究的热点。例如，我们可以通过VQA模型询问有关灾难图像的问题，答案可以帮助确定是否有人或任何东西受到灾难的影响。然而，以往用于灾害损失评估的VQA模型存在一些不足，如候选答案空间有限、问题类型单调、现有模型的回答能力有限。在本文中，我们提出了一个名为零样本VQA的零样本VQA模型，用于洪水灾害损失评估（ZFDDA）。这是一个VQA模型，用于在没有预先训练的情况下进行损伤评估。此外，以洪涝灾害为主要研究对象，构建了一个自由式洪涝灾害图像问答数据集（FFD-IQA）来评估我们的VQA模型。这个新的数据集扩展了问题类型，包括自由形式、多项选择题和是非问题。同时，我们扩展了先前数据集的大小，总共包含2058张图像和22422个问题元-基本真理对。最重要的是，我们的模型使用精心设计的思想链（CoT）演示来释放大型语言模型的潜力，使零样本VQA在灾难场景中表现出更好的性能。实验结果表明，CoT提示大大提高了回答复杂问题的准确性。我们的研究为后续VQA在其他灾害场景中的研究提供了研究基础。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01882v1" target="_blank">2312.01882v1</a>
                              </td>
                              <td>Unleashing the Potential of Large Language Model: Zero-shot VQA for Flood Disaster Scenario</td>
                              <td>Yimin Sun</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01882v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01882v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_10933v4_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_10933v4_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_10933v4_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_10933v4_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recommender systems play a vital role in various online services. However, the insulated nature of training and deploying separately within a specific domain limits their access to open-world knowledge. Recently, the emergence of large language models (LLMs) has shown promise in bridging this gap by encoding extensive world knowledge and demonstrating reasoning capability. Nevertheless, previous attempts to directly use LLMs as recommenders have not achieved satisfactory results. In this work, we propose an Open-World Knowledge Augmented Recommendation Framework with Large Language Models, dubbed KAR, to acquire two types of external knowledge from LLMs -- the reasoning knowledge on user preferences and the factual knowledge on items. We introduce factorization prompting to elicit accurate reasoning on user preferences. The generated reasoning and factual knowledge are effectively transformed and condensed into augmented vectors by a hybrid-expert adaptor in order to be compatible with the recommendation task. The obtained vectors can then be directly used to enhance the performance of any recommendation model. We also ensure efficient inference by preprocessing and prestoring the knowledge from the LLM. Extensive experiments show that KAR significantly outperforms the state-of-the-art baselines and is compatible with a wide range of recommendation algorithms. We deploy KAR to Huawei's news and music recommendation platforms and gain a 7\% and 1.7\% improvement in the online A/B test, respectively.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_10933v4_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>推荐系统在各种在线服务中发挥着至关重要的作用。然而，在特定领域内单独训练和部署的隔离性质限制了他们获得开放世界知识的机会。最近，大型语言模型（LLM）的出现显示出了通过编码广泛的世界知识和展示推理能力来弥合这一差距的前景。尽管如此，以前直接使用LLM作为推荐人的尝试并没有取得令人满意的结果。在这项工作中，我们提出了一个具有大型语言模型的开放世界知识增强推荐框架，称为KAR，以从LLM中获取两种类型的外部知识——关于用户偏好的推理知识和关于项目的事实知识。我们引入因子分解提示来引出关于用户偏好的准确推理。生成的推理和事实知识通过混合专家适配器有效地转换和浓缩为增广向量，以便与推荐任务兼容。然后可以直接使用所获得的向量来增强任何推荐模型的性能。我们还通过预处理和预存储LLM中的知识来确保有效的推理。大量实验表明，KAR显著优于最先进的基线，并与广泛的推荐算法兼容。我们将KAR部署到华为的新闻和音乐推荐平台，在在线a/B测试中分别获得了7%和1.7%的提升。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.10933v4" target="_blank">2306.10933v4</a>
                              </td>
                              <td>Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models</td>
                              <td>Yunjia Xi</td>
                              <td>2023-06-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_10933v4_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.10933v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/yunjiaxi/open-world-knowledge-augmented-recommendation" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01858v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Evaluating Dependencies in Fact Editing for Language Models: Specificity and Implication Awareness</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01858v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01858v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01858v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The potential of using a large language model (LLM) as a knowledge base (KB) has sparked significant interest. To manage the knowledge acquired by LLMs, we need to ensure that the editing of learned facts respects internal logical constraints, which are known as dependency of knowledge. Existing work on editing LLMs has partially addressed the issue of dependency, when the editing of a fact should apply to its lexical variations without disrupting irrelevant ones. However, they neglect the dependency between a fact and its logical implications. We propose an evaluation protocol with an accompanying question-answering dataset, DepEdit, that provides a comprehensive assessment of the editing process considering the above notions of dependency. Our protocol involves setting up a controlled environment in which we edit facts and monitor their impact on LLMs, along with their implications based on If-Then rules. Extensive experiments on DepEdit show that existing knowledge editing methods are sensitive to the surface form of knowledge, and that they have limited performance in inferring the implications of edited facts.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01858v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用大型语言模型（LLM）作为知识库（KB）的潜力引起了人们的极大兴趣。为了管理LLM获得的知识，我们需要确保对所学事实的编辑尊重内部逻辑约束，即所谓的知识依赖性。现有的LLM编辑工作部分解决了依赖性问题，即事实的编辑应适用于其词汇变体，而不干扰无关的变体。然而，他们忽略了一个事实与其逻辑含义之间的依赖性。我们提出了一个评估协议，以及附带的问答数据集DepEdit，该协议考虑到上述依赖性概念，对编辑过程进行了全面评估。我们的协议涉及建立一个受控的环境，在这个环境中，我们编辑事实并监控它们对LLM的影响，以及基于If Then规则的影响。在DepEdit上进行的大量实验表明，现有的知识编辑方法对知识的表面形式很敏感，并且在推断编辑事实的含义方面性能有限。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01858v1" target="_blank">2312.01858v1</a>
                              </td>
                              <td>Evaluating Dependencies in Fact Editing for Language Models: Specificity and Implication Awareness</td>
                              <td>Zichao Li</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01858v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01858v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01823v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exchange-of-Thought: Enhancing Large Language Model Capabilities through Cross-Model Communication</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01823v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01823v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01823v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) have recently made significant strides in complex reasoning tasks through the Chain-of-Thought technique. Despite this progress, their reasoning is often constrained by their intrinsic understanding, lacking external insights. To address this, we propose Exchange-of-Thought (EoT), a novel framework that enables cross-model communication during problem-solving. Drawing inspiration from network topology, EoT integrates four unique communication paradigms: Memory, Report, Relay, and Debate. This paper delves into the communication dynamics and volume associated with each paradigm. To counterbalance the risks of incorrect reasoning chains, we implement a robust confidence evaluation mechanism within these communications. Our experiments across diverse complex reasoning tasks demonstrate that EoT significantly surpasses established baselines, underscoring the value of external insights in enhancing LLM performance. Furthermore, we show that EoT achieves these superior results in a cost-effective manner, marking a promising advancement for efficient and collaborative AI problem-solving.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01823v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）最近通过思想链技术在复杂推理任务中取得了重大进展。尽管取得了这一进展，但他们的推理往往受到内在理解的制约，缺乏外部见解。为了解决这一问题，我们提出了思想交换（EoT），这是一种新的框架，可以在解决问题的过程中实现跨模型沟通。EoT从网络拓扑结构中汲取灵感，融合了四种独特的通信模式：记忆、报告、中继和辩论。本文深入探讨了与每种范式相关的传播动力学和传播量。为了抵消不正确推理链的风险，我们在这些通信中实现了一个强大的置信度评估机制。我们在各种复杂推理任务中的实验表明，EoT显著超过了既定的基线，强调了外部见解在提高LLM性能方面的价值。此外，我们还表明，EoT以一种具有成本效益的方式实现了这些卓越的结果，标志着高效和协作的人工智能问题解决取得了可喜的进展。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01823v1" target="_blank">2312.01823v1</a>
                              </td>
                              <td>Exchange-of-Thought: Enhancing Large Language Model Capabilities through Cross-Model Communication</td>
                              <td>Zhangyue Yin</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01823v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01823v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/yinzhangyue/eot" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01818v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Machine Morality through Experience and Interaction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01818v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01818v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01818v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Increasing interest in ensuring safety of next-generation Artificial Intelligence (AI) systems calls for novel approaches to embedding morality into autonomous agents. Traditionally, this has been done by imposing explicit top-down rules or hard constraints on systems, for example by filtering system outputs through pre-defined ethical rules. Recently, instead, entirely bottom-up methods for learning implicit preferences from human behavior have become increasingly popular, such as those for training and fine-tuning Large Language Models. In this paper, we provide a systematization of existing approaches to the problem of introducing morality in machines - modeled as a continuum, and argue that the majority of popular techniques lie at the extremes - either being fully hard-coded, or entirely learned, where no explicit statement of any moral principle is required. Given the relative strengths and weaknesses of each type of methodology, we argue that more hybrid solutions are needed to create adaptable and robust, yet more controllable and interpretable agents.   In particular, we present three case studies of recent works which use learning from experience (i.e., Reinforcement Learning) to explicitly provide moral principles to learning agents - either as intrinsic rewards, moral logical constraints or textual principles for language models. For example, using intrinsic rewards in Social Dilemma games, we demonstrate how it is possible to represent classical moral frameworks for agents. We also present an overview of the existing work in this area in order to provide empirical evidence for the potential of this hybrid approach. We then discuss strategies for evaluating the effectiveness of moral learning agents. Finally, we present open research questions and implications for the future of AI safety and ethics which are emerging from this framework.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01818v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人们对确保下一代人工智能（AI）系统的安全性越来越感兴趣，这就要求采用新的方法将道德嵌入到自主智能中。传统上，这是通过对系统施加明确的自上而下的规则或硬约束来实现的，例如通过预定义的道德规则过滤系统输出。相反，最近，从人类行为中学习隐含偏好的完全自下而上的方法变得越来越流行，例如那些用于训练和微调大型语言模型的方法。在这篇论文中，我们对在机器中引入道德的现有方法进行了系统化——建模为一个连续体，并认为大多数流行的技术都处于极端——要么是完全硬编码的，要么是完全学习的，不需要任何道德原则的明确声明。考虑到每种方法的相对优势和劣势，我们认为需要更多的混合解决方案来创建适应性强、鲁棒性强、更可控、更可解释的代理。特别是，我们对最近的作品进行了三个案例研究，这些作品使用从经验中学习（即强化学习）来明确地向学习主体提供道德原则——作为语言模型的内在奖励、道德逻辑约束或文本原则。例如，在社会困境游戏中使用内在奖励，我们展示了如何为代理人代表经典道德框架。我们还概述了这一领域的现有工作，以便为这种混合方法的潜力提供经验证据。然后，我们讨论了评估道德学习主体有效性的策略。最后，我们提出了从这个框架中出现的人工智能安全和伦理的未来研究问题和启示。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01818v1" target="_blank">2312.01818v1</a>
                              </td>
                              <td>Learning Machine Morality through Experience and Interaction</td>
                              <td>Elizaveta Tennant</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01818v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01818v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00372v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Event-driven Real-time Retrieval in Web Search</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00372v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00372v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00372v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Information retrieval in real-time search presents unique challenges distinct from those encountered in classical web search. These challenges are particularly pronounced due to the rapid change of user search intent, which is influenced by the occurrence and evolution of breaking news events, such as earthquakes, elections, and wars. Previous dense retrieval methods, which primarily focused on static semantic representation, lack the capacity to capture immediate search intent, leading to inferior performance in retrieving the most recent event-related documents in time-sensitive scenarios. To address this issue, this paper expands the query with event information that represents real-time search intent. The Event information is then integrated with the query through a cross-attention mechanism, resulting in a time-context query representation. We further enhance the model's capacity for event representation through multi-task training. Since publicly available datasets such as MS-MARCO do not contain any event information on the query side and have few time-sensitive queries, we design an automatic data collection and annotation pipeline to address this issue, which includes ModelZoo-based Coarse Annotation and LLM-driven Fine Annotation processes. In addition, we share the training tricks such as two-stage training and hard negative sampling. Finally, we conduct a set of offline experiments on a million-scale production dataset to evaluate our approach and deploy an A/B testing in a real online system to verify the performance. Extensive experimental results demonstrate that our proposed approach significantly outperforms existing state-of-the-art baseline methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00372v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>实时搜索中的信息检索提出了与经典网络搜索不同的独特挑战。这些挑战尤其明显，因为用户搜索意图的快速变化受到突发新闻事件（如地震、选举和战争）发生和演变的影响。以前的密集检索方法主要关注静态语义表示，缺乏捕捉即时搜索意图的能力，导致在时间敏感的场景中检索最新事件相关文档的性能较差。为了解决这个问题，本文用表示实时搜索意图的事件信息扩展了查询。然后，事件信息通过交叉关注机制与查询集成，从而生成时间上下文查询表示。我们通过多任务训练进一步增强了模型的事件表示能力。由于公开可用的数据集（如MS-MARCO）在查询端不包含任何事件信息，并且很少有时间敏感的查询，我们设计了一个自动数据收集和注释管道来解决这个问题，其中包括基于ModelZoo的粗略注释和LLM驱动的精细注释过程。此外，我们还分享了两阶段训练和硬负采样等训练技巧。最后，我们在百万规模的生产数据集上进行了一组离线实验，以评估我们的方法，并在真实的在线系统中部署a/B测试来验证性能。大量的实验结果表明，我们提出的方法显著优于现有的最先进的基线方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00372v2" target="_blank">2312.00372v2</a>
                              </td>
                              <td>Event-driven Real-time Retrieval in Web Search</td>
                              <td>Nan Yang</td>
                              <td>2023-12-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00372v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00372v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_16458v4_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_16458v4_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_16458v4_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_16458v4_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Pre-trained large language models have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks and to be appropriately specialized to particular domains. Here, we target bioinformatics due to the amount of specialized domain knowledge, algorithms, and data operations this discipline requires. We present BioCoder, a benchmark developed to evaluate large language models (LLMs) in generating bioinformatics-specific code. BioCoder spans a broad spectrum of the field and covers cross-file dependencies, class declarations, and global variables. It incorporates 1026 Python functions and 1243 Java methods extracted from GitHub, along with 253 examples from the Rosalind Project, all pertaining to bioinformatics. Using topic modeling we show that overall coverage of the included code is representative of the full spectrum of bioinformatics calculations. BioCoder incorporates a fuzz-testing framework for evaluation. We have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT5+, GPT-3.5, and GPT-4. Furthermore, we finetuned StarCoder, demonstrating how our dataset can effectively enhance the performance of LLMs on our benchmark (by >15% in terms of Pass@K in certain prompt configurations and always >3%). The results highlight two key aspects of successful models: (1) Successful models accommodate a long prompt (> ~2600 tokens) with full context, for functional dependencies. (2) They contain specific domain knowledge of bioinformatics, beyond just general coding knowledge. This is evident from the performance gain of GPT-3.5/4 compared to the smaller models on the benchmark (50% vs up to ~25%). Our dataset, benchmark, Docker images, and scripts required for testing are all available at https://github.com/gersteinlab/biocoder.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_16458v4_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>经过预训练的大型语言模型显著改进了代码生成。随着这些模型的扩展，越来越需要输出来处理更复杂的任务，并对特定领域进行适当的专业化。在这里，我们针对生物信息学，因为这门学科需要大量的专业领域知识、算法和数据操作。我们介绍了BioCoder，这是一个开发用于评估大型语言模型（LLM）生成生物信息学特定代码的基准。BioCoder涵盖了广泛的领域，涵盖了跨文件依赖关系、类声明和全局变量。它包含了从GitHub中提取的1026个Python函数和1243个Java方法，以及来自Rosalind项目的253个例子，所有这些都与生物信息学有关。使用主题建模，我们表明所包含代码的总体覆盖率代表了生物信息学计算的全谱。BioCoder结合了一个模糊测试框架进行评估。我们已经将其应用于评估许多模型，包括InCoder、CodeGen、CodeGen2、SantaCoder、StarCoder、StarCoder+、InstructCodeT5+、GPT-3.5和GPT-4。此外，我们对StarCoder进行了微调，展示了我们的数据集如何有效地提高LLM在基准测试中的性能（就pass@k在某些提示配置中并且总是>3%）。结果强调了成功模型的两个关键方面：（1）成功的模型为功能依赖性提供了具有完整上下文的长提示（>~2600个令牌）。（2） 它们包含生物信息学的特定领域知识，而不仅仅是一般的编码知识。这一点从GPT-3.5/4与基准测试中较小的模型相比的性能增益中可以明显看出（50%与高达~25%）。我们的数据集、基准测试、Docker映像和测试所需的脚本都可以在https://github.com/gersteinlab/biocoder.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.16458v4" target="_blank">2308.16458v4</a>
                              </td>
                              <td>BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge</td>
                              <td>Xiangru Tang</td>
                              <td>2023-08-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_16458v4_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.16458v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/gersteinlab/biocoder" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01801v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SPROUT: Authoring Programming Tutorials with Interactive Visualization of Large Language Model Generation Process</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01801v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01801v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01801v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The rapid development of large language models (LLMs), such as ChatGPT, has revolutionized the efficiency of creating programming tutorials. LLMs can be instructed with text prompts to generate comprehensive text descriptions of code snippets. However, the lack of transparency in the end-to-end generation process has hindered the understanding of model behavior and limited user control over the generated results. To tackle this challenge, we introduce a novel approach that breaks down the programming tutorial creation task into actionable steps. By employing the tree-of-thought method, LLMs engage in an exploratory process to generate diverse and faithful programming tutorials. We then present SPROUT, an authoring tool equipped with a series of interactive visualizations that empower users to have greater control and understanding of the programming tutorial creation process. A formal user study demonstrated the effectiveness of SPROUT, showing that our tool assists users to actively participate in the programming tutorial creation process, leading to more reliable and customizable results. By providing users with greater control and understanding, SPROUT enhances the user experience and improves the overall quality of programming tutorial. A free copy of this paper and all supplemental materials are available at https://osf.io/uez2t/?view_only=5102e958802341daa414707646428f86.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01801v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）的快速发展，如ChatGPT，已经彻底改变了创建编程教程的效率。LLM可以通过文本提示来生成代码片段的全面文本描述。然而，端到端生成过程缺乏透明度，阻碍了对模型行为的理解，并限制了用户对生成结果的控制。为了应对这一挑战，我们引入了一种新颖的方法，将编程教程创建任务分解为可操作的步骤。通过采用思想树方法，LLM参与一个探索过程，以生成多样化和忠实的编程教程。然后，我们展示了SPROUT，这是一个配备了一系列交互式可视化的创作工具，使用户能够更好地控制和理解编程教程的创建过程。一项正式的用户研究证明了SPROUT的有效性，表明我们的工具可以帮助用户积极参与编程教程的创建过程，从而获得更可靠和可定制的结果。通过为用户提供更好的控制和理解，SPROUT增强了用户体验，提高了编程教程的整体质量。本文和所有补充材料的免费副本可在https://osf.io/uez2t/?view_only=5102e958802341daa414707646428f86.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01801v1" target="_blank">2312.01801v1</a>
                              </td>
                              <td>SPROUT: Authoring Programming Tutorials with Interactive Visualization of Large Language Model Generation Process</td>
                              <td>Yihan Liu</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01801v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01801v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01797v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01797v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01797v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01797v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This research focuses on how Large Language Models (LLMs) can help with path planning for mobile embodied agents such as robots, in a human-in-the-loop and interactive manner. A novel framework named LLM A*, aims to leverage the commonsense of LLMs, and the utility-optimal A* is proposed to facilitate few-shot near-optimal path planning. Prompts are used to 1) provide LLMs with essential information like environment, cost, heuristics, etc.; 2) communicate human feedback to LLMs on intermediate planning results. This makes the whole path planning process a `white box' and human feedback guides LLM A* to converge quickly compared to other data-driven methods such as reinforcement learning-based (RL) path planning. In addition, it makes code-free path planning practical, henceforth promoting the inclusiveness of artificial intelligence techniques. Comparative analysis against A* and RL shows that LLM A* is more efficient in terms of search space and achieves an on-a-par path with A* and a better path than RL. The interactive nature of LLM A* also makes it a promising tool for deployment in collaborative human-robot tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01797v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>这项研究的重点是大型语言模型（LLM）如何以人在环和交互的方式帮助机器人等移动代理进行路径规划。一个名为LLM A*的新框架旨在利用LLM的常识，并提出了效用最优A*来促进少镜头近似最优路径规划。提示用于1）向LLM提供基本信息，如环境、成本、启发式等。；2） 就中间规划结果向LLM传达人工反馈。这使得整个路径规划过程成为“白盒”，与其他数据驱动的方法（如基于强化学习的路径规划）相比，人类反馈引导LLM a*快速收敛。此外，它使无代码路径规划变得实用，从而促进了人工智能技术的包容性。与A*和RL的比较分析表明，LLM A*在搜索空间方面更有效，并且实现了与A*并行的路径和比RL更好的路径。LLM A*的交互式特性也使其成为一种很有前途的人机协作任务部署工具。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01797v1" target="_blank">2312.01797v1</a>
                              </td>
                              <td>LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics</td>
                              <td>Hengjia Xiao</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01797v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01797v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_15786v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">YUAN 2.0: A Large Language Model with Localized Filtering-based Attention</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_15786v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_15786v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_15786v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this work, we develop and release Yuan 2.0, a series of large language models with parameters ranging from 2.1 billion to 102.6 billion. The Localized Filtering-based Attention (LFA) is introduced to incorporate prior knowledge of local dependencies of natural language into Attention. A data filtering and generating system is presented to build pre-training and fine-tuning dataset in high quality. A distributed training method with non-uniform pipeline parallel, data parallel, and optimizer parallel is proposed, which greatly reduces the bandwidth requirements of intra-node communication, and achieves good performance in large-scale distributed training. Yuan 2.0 models display impressive ability in code generation, math problem-solving, and chatting compared with existing models. The latest version of YUAN 2.0, including model weights and source code, is accessible at Github.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_15786v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项工作中，我们开发并发布了Yuan2.0，这是一系列参数从21亿到1026亿的大型语言模型。引入了基于局部过滤的注意力（LFA），将自然语言局部依赖性的先验知识引入到注意力中。为了建立高质量的预训练和微调数据集，提出了一种数据过滤和生成系统。提出了一种非均匀流水线并行、数据并行和优化器并行的分布式训练方法，大大降低了节点内通信的带宽要求，在大规模分布式训练中取得了良好的性能。与现有模型相比，Yuan 2.0模型在代码生成、数学问题解决和聊天方面表现出了令人印象深刻的能力。最新版本的YUAN 2.0，包括模型权重和源代码，可以在Github上访问。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.15786v2" target="_blank">2311.15786v2</a>
                              </td>
                              <td>YUAN 2.0: A Large Language Model with Localized Filtering-based Attention</td>
                              <td>Shaohua Wu</td>
                              <td>2023-11-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_15786v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.15786v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/ieit-yuan/yuan-2.0" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15896v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs with Multi-turn Health Conversations Polished by ChatGPT</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15896v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15896v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15896v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have performed well in providing general and extensive health suggestions in single-turn conversations, exemplified by systems such as ChatGPT, ChatGLM, ChatDoctor, DoctorGLM, and etc. However, the limited information provided by users during single turn results in inadequate personalization and targeting of the generated suggestions, which requires users to independently select the useful part. It is mainly caused by the missing ability to engage in multi-turn questioning. In real-world medical consultations, doctors usually employ a series of iterative inquiries to comprehend the patient's condition thoroughly, enabling them to provide effective and personalized suggestions subsequently, which can be defined as chain of questioning (CoQ) for LLMs. To improve the CoQ of LLMs, we propose BianQue, a ChatGLM-based LLM finetuned with the self-constructed health conversation dataset BianQueCorpus that is consist of multiple turns of questioning and health suggestions polished by ChatGPT. Experimental results demonstrate that the proposed BianQue can simultaneously balance the capabilities of both questioning and health suggestions, which will help promote the research and application of LLMs in the field of proactive health.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15896v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）在单回合对话中提供一般和广泛的健康建议方面表现良好，例如ChatGPT、ChatGLM、ChatDoctor、DoctorGLM等系统。然而，用户在单回合会话中提供的有限信息导致生成的建议的个性化和针对性不足，这需要用户独立地选择有用的部分。这主要是由于缺乏进行多回合提问的能力造成的。在现实世界的医疗咨询中，医生通常会采用一系列迭代询问来彻底了解患者的病情，使他们能够随后提供有效和个性化的建议，这可以被定义为LLM的提问链（CoQ）。为了提高LLM的CoQ，我们提出了BianQue，这是一种基于ChatGLM的LLM，使用自建的健康会话数据集BianQueCorpus进行了微调，该数据集由ChatGPT完善的多个提问和健康建议组成。实验结果表明，所提出的BianQue可以同时平衡提问和健康建议的能力，这将有助于促进LLM在主动健康领域的研究和应用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15896v2" target="_blank">2310.15896v2</a>
                              </td>
                              <td>BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs with Multi-turn Health Conversations Polished by ChatGPT</td>
                              <td>Yirong Chen</td>
                              <td>2023-10-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15896v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15896v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/scutcyr/bianque" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01714v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Retrieval-augmented Multi-modal Chain-of-Thoughts Reasoning for Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01714v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01714v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01714v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The advancement of Large Language Models(LLMs) has brought substantial attention to the Chain of Thought(CoT) approach, primarily due to its ability to enhance the capability of LLMs on tasks requiring complex reasoning. Moreover, the significance of CoT approaches extends to the application of LLMs for multi-modal tasks, such as multi-modal question answering. However, the selection of optimal CoT demonstration examples in multi-modal reasoning for LLMs remains less explored for LLMs due to the inherent complexity of multi-modal examples. In this paper, we introduce a novel approach that addresses this challenge by using retrieval mechanisms to dynamically and automatically select demonstration examples based on cross-modal similarities. This method aims to refine the CoT reasoning process in multi-modal scenarios via informing LLMs with more relevant and informative examples. Furthermore, we employ a stratified sampling method categorising demonstration examples into groups based on their types and retrieving examples from different groups respectively to promote the diversity of demonstration examples. Through a series of experiments, we demonstrate that our approach significantly improves the performance of LLMs, achieving state-of-the-art results in multi-modal reasoning tasks. Specifically, our methods demonstrate significant advancements on the ScienceQA dataset. While our method based on ChatGPT outperforms the Chameleon(ChatGPT) by 2.74% with an accuracy of 82.67%, the GPT4-based approach surpasses the Chameleon(GPT-4) by 0.89%, achieving 87.43% on accuracy under the same setting. Moreover, our best performing show a 6.05% increase over Chameleon for ChatGPT-based models and a 4.57% increase for GPT-4-based models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01714v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）的发展引起了人们对思想链（CoT）方法的极大关注，主要是因为它能够增强LLM在需要复杂推理的任务中的能力。此外，CoT方法的重要性扩展到LLM在多模式任务中的应用，如多模式问答。然而，由于多模态示例的固有复杂性，在LLM的多模态推理中选择最佳CoT演示示例的研究较少。在本文中，我们介绍了一种新的方法，通过使用检索机制根据跨模态相似性动态自动选择演示示例来解决这一挑战。该方法旨在通过向LLM提供更相关、更具信息性的示例来完善多模式场景中的CoT推理过程。此外，我们采用分层抽样方法，根据示范实例的类型将其分组，并分别从不同的组中检索实例，以促进示范实例的多样性。通过一系列实验，我们证明了我们的方法显著提高了LLM的性能，在多模态推理任务中取得了最先进的结果。具体来说，我们的方法在ScienceQA数据集上取得了重大进展。虽然我们基于ChatGPT的方法比变色龙（ChatGPT）高2.74%，准确率为82.67%，但基于GPT4的方法比Chameleon（GPT-4）高0.89%，在相同设置下实现了87.43%的准确率。此外，我们的最佳性能显示，基于ChatGPT的模型比Chameleon增长6.05%，基于GPT-4的模型增长4.57%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01714v1" target="_blank">2312.01714v1</a>
                              </td>
                              <td>Retrieval-augmented Multi-modal Chain-of-Thoughts Reasoning for Large Language Models</td>
                              <td>Bingshuai Liu</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01714v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01714v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01701v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01701v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01701v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01701v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have shown remarkable performance in natural language processing (NLP) tasks. To comprehend and execute diverse human instructions over image data, instruction-tuned large vision-language models (LVLMs) have been introduced. However, LVLMs may suffer from different types of object hallucinations. Nevertheless, LVLMs are evaluated for coarse-grained object hallucinations only (i.e., generated objects non-existent in the input image). The fine-grained object attributes and behaviors non-existent in the image may still be generated but not measured by the current evaluation methods. In this paper, we thus focus on reducing fine-grained hallucinations of LVLMs. We propose \textit{ReCaption}, a framework that consists of two components: rewriting captions using ChatGPT and fine-tuning the instruction-tuned LVLMs on the rewritten captions. We also propose a fine-grained probing-based evaluation method named \textit{Fine-Grained Object Hallucination Evaluation} (\textit{FGHE}). Our experiment results demonstrate that ReCaption effectively reduces fine-grained object hallucination for different LVLM options and improves their text generation quality. The code can be found at https://github.com/Anonymousanoy/FOHE.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01701v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）在自然语言处理（NLP）任务中表现出了显著的性能。为了理解和执行图像数据上的各种人类指令，引入了指令调优的大型视觉语言模型（LVLMs）。然而，LVLMs可能患有不同类型的物体幻觉。然而，LVLM仅针对粗粒度对象幻觉（即，输入图像中不存在的生成对象）进行评估。图像中不存在的细粒度对象属性和行为仍然可以生成，但不能通过当前的评估方法来测量。因此，在本文中，我们专注于减少LVLMs的细粒度幻觉。我们提出\textit｛ReCaption｝，这是一个由两个组件组成的框架：使用ChatGPT重写字幕和在重写的字幕上微调指令调整的LVLM。我们还提出了一种基于细粒度探测的评估方法，命名为\textit｛细粒度对象幻觉评估｝（\textit{FGHE｝）。我们的实验结果表明，ReCaption有效地减少了不同LVLM选项的细粒度对象幻觉，并提高了它们的文本生成质量。代码可在找到https://github.com/anonymousanoy/fohe.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01701v1" target="_blank">2312.01701v1</a>
                              </td>
                              <td>Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites</td>
                              <td>Lei Wang</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01701v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01701v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/anonymousanoy/fohe" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01700v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Data Management For Large Language Models: A Survey</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01700v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01700v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01700v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Data plays a fundamental role in the training of Large Language Models (LLMs). Effective data management, particularly in the formulation of a well-suited training dataset, holds significance for enhancing model performance and improving training efficiency during pretraining and supervised fine-tuning phases. Despite the considerable importance of data management, the current research community still falls short in providing a systematic analysis of the rationale behind management strategy selection, its consequential effects, methodologies for evaluating curated datasets, and the ongoing pursuit of improved strategies. Consequently, the exploration of data management has attracted more and more attention among the research community. This survey provides a comprehensive overview of current research in data management within both the pretraining and supervised fine-tuning stages of LLMs, covering various noteworthy aspects of data management strategy design: data quantity, data quality, domain/task composition, etc. Looking toward the future, we extrapolate existing challenges and outline promising directions for development in this field. Therefore, this survey serves as a guiding resource for practitioners aspiring to construct powerful LLMs through effective data management practices. The collection of the latest papers is available at https://github.com/ZigeW/data_management_LLM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01700v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>数据在大型语言模型（LLM）的训练中起着至关重要的作用。有效的数据管理，特别是在制定一个非常适合的训练数据集方面，对于增强模型性能和提高预训练和监督微调阶段的训练效率具有重要意义。尽管数据管理具有相当重要的意义，但当前的研究界仍未能对管理策略选择背后的原理、其后果、评估策划数据集的方法以及对改进策略的持续追求进行系统分析。因此，数据管理的探索越来越受到研究界的关注。本调查全面概述了LLM预训练和监督微调阶段的数据管理研究现状，涵盖了数据管理策略设计的各个值得注意的方面：数据量、数据质量、领域/任务组成等。展望未来，我们推断了现有的挑战，并概述了该领域有希望的发展方向。因此，这项调查为那些希望通过有效的数据管理实践构建强大LLM的从业者提供了指导资源。最新论文集可在https://github.com/zigew/data_management_llm.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01700v1" target="_blank">2312.01700v1</a>
                              </td>
                              <td>Data Management For Large Language Models: A Survey</td>
                              <td>Zige Wang</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01700v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01700v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_18940v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_18940v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_18940v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_18940v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Agents built with large language models (LLMs) have recently achieved great advancements. However, most of the efforts focus on single-agent or cooperative settings, leaving more general multi-agent environments underexplored. We propose a new framework powered by reinforcement learning (RL) to develop strategic language agents, i.e., LLM-based agents with strategic thinking ability, for a popular language game, Werewolf. Werewolf is a social deduction game with hidden roles that involves both cooperation and competition and emphasizes deceptive communication and diverse gameplay. Our agent tackles this game by first using LLMs to reason about potential deceptions and generate a set of strategically diverse actions. Then an RL policy, which selects an action from the candidates, is learned by population-based training to enhance the agents' decision-making ability. By combining LLMs with the RL policy, our agent produces a variety of emergent strategies, achieves the highest win rate against other LLM-based agents, and stays robust against adversarial human players in the Werewolf game.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_18940v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用大型语言模型（LLM）构建的代理最近取得了巨大的进步。然而，大多数工作都集中在单智能体或协作环境上，使得更通用的多智能体环境没有得到充分的探索。我们提出了一个新的框架，以强化学习（RL）为动力，为流行的语言游戏《狼人》开发战略语言代理，即具有战略思维能力的基于LLM的代理。《狼人》是一款隐藏角色的社会演绎游戏，既有合作，也有竞争，强调欺骗性的沟通和多样化的游戏性。我们的代理人通过首先使用LLM来推理潜在的欺骗行为，并产生一系列具有战略多样性的行动来处理这个游戏。然后，通过基于人群的训练来学习从候选者中选择行动的RL策略，以增强代理的决策能力。通过将LLM与RL策略相结合，我们的代理产生了各种紧急策略，在与其他基于LLM的代理的对抗中实现了最高的胜率，并在狼人游戏中与对抗性人类玩家保持稳健。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.18940v2" target="_blank">2310.18940v2</a>
                              </td>
                              <td>Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game</td>
                              <td>Zelai Xu</td>
                              <td>2023-10-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_18940v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.18940v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01678v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Jellyfish: A Large Language Model for Data Preprocessing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01678v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01678v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01678v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we present Jellyfish, an open-source LLM as a universal task solver for DP. Built on the Llama 2 13B model, Jellyfish is instruction-tuned with the datasets of several typical DP tasks including error detection, data imputation, schema matching, and entity matching, and delivers generalizability to other tasks. Remarkably, Jellyfish can operate on a local, single, and low-priced GPU with its 13 billion parameters, ensuring data security and enabling further tuning. Its proficiency in understanding natural language allows users to manually craft instructions for DP tasks. Unlike many existing methods that heavily rely on prior knowledge, Jellyfish acquires domain knowledge during its tuning process and integrates optional knowledge injection during inference. A distinctive feature of Jellyfish is its interpreter, which elucidates its output decisions. To construct Jellyfish, we develop a series of pre-tuning and DP-tuning techniques. Jellyfish is equipped with an instance serializer, which automatically translates raw data into model prompts, and a knowledge injector, which optionally introduces task- and dataset-specific knowledge to enhance DP performance. Our evaluation of Jellyfish, using a range of real datasets, shows its competitiveness compared to state-of-the-art methods and its strong generalizability to unseen tasks. Jellyfish's performance rivals that of GPT series models, and its interpreter offers enhanced reasoning capabilities compared to GPT-3.5. Furthermore, our evaluation highlights the effectiveness of the techniques employed in constructing Jellyfish. Our model is available at Hugging Face: https://huggingface.co/NECOUDBFM/Jellyfish .</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01678v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们介绍了Jellyfish，一种开源LLM，作为DP的通用任务求解器。Jellyfish建立在Llama 2 13B模型的基础上，利用几个典型DP任务的数据集进行指令调整，包括错误检测、数据插补、模式匹配和实体匹配，并为其他任务提供可推广性。值得注意的是，Jellyfish可以在具有130亿参数的本地、单个和低价GPU上运行，确保数据安全并实现进一步的调优。它在理解自然语言方面的熟练程度允许用户手动编写DP任务的指令。与许多严重依赖先验知识的现有方法不同，Jellyfish在调整过程中获取领域知识，并在推理过程中集成可选的知识注入。Jellyfish的一个显著特点是它的解释器，它阐明了它的输出决策。为了构建Jellyfish，我们开发了一系列预调谐和DP调谐技术。Jellyfish配备了一个实例序列化器和一个知识注入器，前者自动将原始数据转换为模型提示，后者可选地引入特定于任务和数据集的知识，以增强DP性能。我们使用一系列真实数据集对水母进行的评估显示，与最先进的方法相比，水母具有竞争力，并且对看不见的任务具有很强的可推广性。Jellyfish的性能可以与GPT系列模型相媲美，与GPT-3.5相比，其解释器提供了增强的推理能力。此外，我们的评估强调了水母建造技术的有效性。我们的模型可在Hugging Face上获得：https://huggingface.co/necoudbfm/jellyfish。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01678v1" target="_blank">2312.01678v1</a>
                              </td>
                              <td>Jellyfish: A Large Language Model for Data Preprocessing</td>
                              <td>Haochen Zhang</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01678v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01678v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_15452v5_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">When Do Program-of-Thoughts Work for Reasoning?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_15452v5_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_15452v5_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_15452v5_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In the realm of embodied artificial intelligence, the reasoning capabilities of Large Language Models (LLMs) play a pivotal role. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing and stratifying algorithm, and apply it to instruction generation for mathematical reasoning and code data filtering for code generation tasks. Extensive results demonstrates the effectiveness of our proposed approach. Code will be integrated into the EasyInstruct framework at https://github.com/zjunlp/EasyInstruct.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_15452v5_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在嵌入式人工智能领域，大型语言模型（LLM）的推理能力起着关键作用。尽管有一些有效的方法，如LLM的思维程序提示，它使用编程语言来处理复杂的推理任务，但代码数据对提高推理能力的具体影响仍有待探索。为了解决这一差距，我们提出了复杂度影响推理评分（CIRS），它结合了结构和逻辑属性，来衡量代码和推理能力之间的相关性。具体来说，我们使用抽象语法树对结构信息进行编码，并通过考虑难度和圈复杂度来计算逻辑复杂度。通过实证分析，我们发现并非所有复杂的代码数据都能被LLM学习或理解。优化的复杂度水平对于提高程序辅助提示的推理能力至关重要。然后，我们设计了一种自动合成和分层算法，并将其应用于数学推理的指令生成和代码生成任务的代码数据过滤。大量的结果证明了我们提出的方法的有效性。代码将集成到EasyInstruction框架中，网址为https://github.com/zjunlp/easyinstruct.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.15452v5" target="_blank">2308.15452v5</a>
                              </td>
                              <td>When Do Program-of-Thoughts Work for Reasoning?</td>
                              <td>Zhen Bi</td>
                              <td>2023-08-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_15452v5_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.15452v5" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zjunlp/easyinstruct" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_15500v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Function-constrained Program Synthesis</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_15500v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_15500v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_15500v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This work introduces (1) a technique that allows large language models (LLMs) to leverage user-provided code when solving programming tasks and (2) a method to iteratively generate modular sub-functions that can aid future code generation attempts when the initial code generated by the LLM is inadequate. Generating computer programs in general-purpose programming languages like Python poses a challenge for LLMs when instructed to use code provided in the prompt. Code-specific LLMs (e.g., GitHub Copilot, CodeLlama2) can generate code completions in real-time by drawing on all code available in a development environment. However, restricting code-specific LLMs to use only in-context code is not straightforward, as the model is not explicitly instructed to use the user-provided code and users cannot highlight precisely which snippets of code the model should incorporate into its context. Moreover, current systems lack effective recovery methods, forcing users to iteratively re-prompt the model with modified prompts until a sufficient solution is reached. Our method differs from traditional LLM-powered code-generation by constraining code-generation to an explicit function set and enabling recovery from failed attempts through automatically generated sub-functions. When the LLM cannot produce working code, we generate modular sub-functions to aid subsequent attempts at generating functional code. A by-product of our method is a library of reusable sub-functions that can solve related tasks, imitating a software team where efficiency scales with experience. We also introduce a new "half-shot" evaluation paradigm that provides tighter estimates of LLMs' coding abilities compared to traditional zero-shot evaluation. Our proposed evaluation method encourages models to output solutions in a structured format, decreasing syntax errors that can be mistaken for poor coding ability.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_15500v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>这项工作介绍了（1）一种允许大型语言模型（LLM）在解决编程任务时利用用户提供的代码的技术，以及（2）一种迭代生成模块化子函数的方法，该方法可以在LLM生成的初始代码不足时帮助未来的代码生成尝试。当被指示使用提示中提供的代码时，用Python等通用编程语言生成计算机程序对LLM来说是一个挑战。特定于代码的LLM（例如，GitHub Copilot、CodeLlama2）可以通过利用开发环境中可用的所有代码实时生成代码完成。然而，限制特定于代码的LLM仅在上下文代码中使用并不简单，因为没有明确指示模型使用用户提供的代码，并且用户无法准确地突出显示模型应将哪些代码片段纳入其上下文。此外，当前的系统缺乏有效的恢复方法，迫使用户使用修改后的提示反复重新提示模型，直到找到足够的解决方案。我们的方法不同于传统的LLM驱动的代码生成，它将代码生成约束为显式函数集，并通过自动生成的子函数实现从失败尝试中恢复。当LLM无法生成工作代码时，我们会生成模块化子函数，以帮助后续尝试生成功能代码。我们方法的一个副产品是一个可重复使用的子函数库，它可以解决相关任务，模仿一个效率随经验而变化的软件团队。我们还引入了一种新的“半程”评估范式，与传统的零样本评估相比，该范式对LLM的编码能力提供了更严格的估计。我们提出的评估方法鼓励模型以结构化格式输出解决方案，减少可能被误认为编码能力差的语法错误。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.15500v2" target="_blank">2311.15500v2</a>
                              </td>
                              <td>Function-constrained Program Synthesis</td>
                              <td>Patrick Hajali</td>
                              <td>2023-11-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_15500v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.15500v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01661v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ChatGPT as a Math Questioner? Evaluating ChatGPT on Generating Pre-university Math Questions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01661v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01661v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01661v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Mathematical questioning is crucial for assessing students problem-solving skills. Since manually creating such questions requires substantial effort, automatic methods have been explored. Existing state-of-the-art models rely on fine-tuning strategies and struggle to generate questions that heavily involve multiple steps of logical and arithmetic reasoning. Meanwhile, large language models(LLMs) such as ChatGPT have excelled in many NLP tasks involving logical and arithmetic reasoning. Nonetheless, their applications in generating educational questions are underutilized, especially in the field of mathematics. To bridge this gap, we take the first step to conduct an in-depth analysis of ChatGPT in generating pre-university math questions. Our analysis is categorized into two main settings: context-aware and context-unaware. In the context-aware setting, we evaluate ChatGPT on existing math question-answering benchmarks covering elementary, secondary, and ternary classes. In the context-unaware setting, we evaluate ChatGPT in generating math questions for each lesson from pre-university math curriculums that we crawl. Our crawling results in TopicMath, a comprehensive and novel collection of pre-university math curriculums collected from 121 math topics and 428 lessons from elementary, secondary, and tertiary classes. Through this analysis, we aim to provide insight into the potential of ChatGPT as a math questioner.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01661v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>数学提问对于评估学生解决问题的能力至关重要。由于手动创建这样的问题需要大量的努力，因此已经探索了自动方法。现有的最先进的模型依赖于微调策略，难以生成大量涉及逻辑和算术推理多个步骤的问题。同时，像ChatGPT这样的大型语言模型在许多涉及逻辑和算术推理的NLP任务中表现出色。尽管如此，它们在生成教育问题方面的应用没有得到充分利用，尤其是在数学领域。为了弥补这一差距，我们迈出了第一步，在生成大学前数学问题时对ChatGPT进行深入分析。我们的分析分为两种主要设置：上下文感知和上下文不感知。在上下文感知的环境中，我们根据涵盖小学、中学和三元类的现有数学问答基准来评估ChatGPT。在不了解上下文的环境中，我们评估ChatGPT为我们爬行的大学前数学课程中的每节课生成数学问题。我们在TopicMath中的爬行结果，这是一个全面而新颖的大学前数学课程集，收集了121个数学主题和428节小学、中学和大学课程。通过这一分析，我们旨在深入了解ChatGPT作为数学提问者的潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01661v1" target="_blank">2312.01661v1</a>
                              </td>
                              <td>ChatGPT as a Math Questioner? Evaluating ChatGPT on Generating Pre-university Math Questions</td>
                              <td>Phuoc Pham Van Long</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01661v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01661v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01656v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Contemporary Art of Image Search: Iterative User Intent Expansion via Vision-Language Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01656v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01656v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01656v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Image search is an essential and user-friendly method to explore vast galleries of digital images. However, existing image search methods heavily rely on proximity measurements like tag matching or image similarity, requiring precise user inputs for satisfactory results.To meet the growing demand for a contemporary image search engine that enables accurate comprehension of users' search intentions, we introduce an innovative user intent expansion framework. Our framework leverages visual-language models to parse and compose multi-modal user inputs to provide more accurate and satisfying results. It comprises two-stage processes: 1) a parsing stage that incorporates a language parsing module with large language models to enhance the comprehension of textual inputs, along with a visual parsing module that integrates an interactive segmentation module to swiftly identify detailed visual elements within images; and 2) a logic composition stage that combines multiple user search intents into a unified logic expression for more sophisticated operations in complex searching scenarios. Moreover, the intent expansion framework enables users to perform flexible contextualized interactions with the search results to further specify or adjust their detailed search intents iteratively. We implemented the framework into an image search system for NFT (non-fungible token) search and conducted a user study to evaluate its usability and novel properties. The results indicate that the proposed framework significantly improves users' image search experience. Particularly the parsing and contextualized interactions prove useful in allowing users to express their search intents more accurately and engage in a more enjoyable iterative search experience.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01656v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>图像搜索是探索大量数字图像库的一种基本且用户友好的方法。然而，现有的图像搜索方法在很大程度上依赖于邻近度测量，如标签匹配或图像相似性，需要精确的用户输入才能获得令人满意的结果。为了满足当代图像搜索引擎日益增长的需求，使其能够准确理解用户的搜索意图，我们引入了一个创新的用户意图扩展框架。我们的框架利用视觉语言模型来解析和组合多模式用户输入，以提供更准确和令人满意的结果。它包括两个阶段的过程：1）解析阶段，该阶段结合了具有大型语言模型的语言解析模块，以增强对文本输入的理解，以及视觉解析模块，该视觉解析模块集成了交互式分割模块，以快速识别图像中的详细视觉元素；以及2）逻辑组合阶段，其将多个用户搜索意图组合成统一的逻辑表达式，用于在复杂的搜索场景中进行更复杂的操作。此外，意图扩展框架使用户能够与搜索结果进行灵活的上下文交互，以进一步迭代地指定或调整他们的详细搜索意图。我们将该框架实现到用于NFT（不可替代代币）搜索的图像搜索系统中，并进行了用户研究，以评估其可用性和新颖性。结果表明，该框架显著提高了用户的图像搜索体验。特别是解析和上下文化的交互被证明是有用的，允许用户更准确地表达他们的搜索意图，并参与更愉快的迭代搜索体验。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01656v1" target="_blank">2312.01656v1</a>
                              </td>
                              <td>The Contemporary Art of Image Search: Iterative User Intent Expansion via Vision-Language Model</td>
                              <td>Yilin Ye</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01656v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01656v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01648v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Characterizing Large Language Model Geometry Solves Toxicity Detection and Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01648v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01648v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01648v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models~(LLMs) drive current AI breakthroughs despite very little being known about their internal representations, e.g., how to extract a few informative features to solve various downstream tasks. To provide a practical and principled answer, we propose to characterize LLMs from a geometric perspective. We obtain in closed form (i) the intrinsic dimension in which the Multi-Head Attention embeddings are constrained to exist and (ii) the partition and per-region affine mappings of the per-layer feedforward networks. Our results are informative, do not rely on approximations, and are actionable. First, we show that, motivated by our geometric interpretation, we can bypass Llama$2$'s RLHF by controlling its embedding's intrinsic dimension through informed prompt manipulation. Second, we derive $7$ interpretable spline features that can be extracted from any (pre-trained) LLM layer, providing a rich abstract representation of their inputs. Those features alone ($224$ for Mistral-7B and Llama$2$-7B) are sufficient to help solve toxicity detection, infer the domain of the prompt, and even tackle the Jigsaw challenge, which aims at characterizing the type of toxicity of various prompts. Our results demonstrate how, even in large-scale regimes, exact theoretical results can answer practical questions in language models. Code: \url{https://github.com/RandallBalestriero/SplineLLM}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01648v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）推动了当前人工智能的突破，尽管人们对其内部表示知之甚少，例如，如何提取一些信息特征来解决各种下游任务。为了提供一个实用且有原则的答案，我们建议从几何角度来描述LLM。我们以闭合形式获得（i）多头注意力嵌入被约束为存在的内在维度，以及（ii）每层前馈网络的划分和每区域仿射映射。我们的结果是有信息的，不依赖于近似值，并且是可操作的。首先，我们证明，在我们的几何解释的激励下，我们可以通过知情的即时操作来控制其嵌入的内在维度，从而绕过Llama$2$的RLHF。其次，我们推导出$7$的可解释样条特征，这些样条特征可以从任何（预训练的）LLM层中提取，提供其输入的丰富抽象表示。仅这些特征（Mistral-7B的224$和Llama的2$-7B）就足以帮助解决毒性检测、推断提示的领域，甚至解决Jigsaw挑战，该挑战旨在表征各种提示的毒性类型。我们的结果表明，即使在大规模的制度中，准确的理论结果也可以回答语言模型中的实际问题。代码：\url{https://github.com/randallbalestriero/splinellm}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01648v1" target="_blank">2312.01648v1</a>
                              </td>
                              <td>Characterizing Large Language Model Geometry Solves Toxicity Detection and Generation</td>
                              <td>Randall Balestriero</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01648v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01648v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/randallbalestriero/splinellm" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01639v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On the Effectiveness of Large Language Models in Domain-Specific Code Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01639v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01639v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01639v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) such as ChatGPT have shown remarkable capabilities in code generation. Despite their great success, their effectiveness within particular domains (e.g., web development) necessitates further evaluation. In this study, we conduct an empirical study of domain-specific code generation with LLMs. We demonstrate that LLMs exhibit sub-optimal performance in generating domain-specific code, due to their limited proficiency in utilizing domain-specific libraries. We further observe that incorporating API knowledge as prompts can empower LLMs to generate more professional code. Based on these findings, we further investigate how to efficiently incorporate API knowledge into the code generation process. We experiment with three strategies for incorporating domain knowledge, namely, external knowledge inquirer, chain-of-thought prompting, and chain-of-thought fine-tuning. We refer to these strategies as a new code generation approach called DomCoder. Experimental results show that all strategies of DomCoder lead to improvement in the effectiveness of domain-specific code generation under certain settings. The results also show that there is still ample room for further improvement, based on which we suggest possible future works.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01639v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>像ChatGPT这样的大型语言模型（LLM）在代码生成方面显示出了非凡的能力。尽管它们取得了巨大成功，但它们在特定领域（如网络开发）的有效性需要进一步评估。在这项研究中，我们对LLM的领域特定代码生成进行了实证研究。我们证明，LLM在生成领域特定代码方面表现出次优性能，因为它们在利用领域特定库方面的熟练程度有限。我们进一步观察到，结合API知识作为提示可以使LLM能够生成更专业的代码。基于这些发现，我们进一步研究了如何有效地将API知识融入到代码生成过程中。我们实验了三种整合领域知识的策略，即外部知识查询器、思维链提示和思维链微调。我们将这些策略称为一种称为DomCoder的新代码生成方法。实验结果表明，在一定的设置下，DomCoder的所有策略都提高了领域特定代码生成的有效性。研究结果还表明，仍有足够的改进空间，在此基础上，我们提出了未来可能的工作建议。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01639v1" target="_blank">2312.01639v1</a>
                              </td>
                              <td>On the Effectiveness of Large Language Models in Domain-Specific Code Generation</td>
                              <td>Meng Chen</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01639v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01639v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01629v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLAMP: Contrastive LAnguage Model Prompt-tuning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01629v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01629v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01629v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have emerged as powerful general-purpose interfaces for many machine learning problems. Recent work has adapted LLMs to generative visual tasks like image captioning, visual question answering, and visual chat, using a relatively small amount of instruction-tuning data. In this paper, we explore whether modern LLMs can also be adapted to classifying an image into a set of categories. First, we evaluate multimodal LLMs that are tuned for generative tasks on zero-shot image classification and find that their performance is far below that of specialized models like CLIP. We then propose an approach for light fine-tuning of LLMs using the same contrastive image-caption matching objective as CLIP. Our results show that LLMs can, indeed, achieve good image classification performance when adapted this way. Our approach beats state-of-the-art mLLMs by 13% and slightly outperforms contrastive learning with a custom text model, while also retaining the LLM's generative abilities. LLM initialization appears to particularly help classification in domains under-represented in the visual pre-training data.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01629v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）已经成为许多机器学习问题的强大通用接口。最近的工作使用相对少量的指令调整数据，使LLM适应生成视觉任务，如图像字幕、视觉问答和视觉聊天。在本文中，我们探讨了现代LLM是否也可以适用于将图像分类为一组类别。首先，我们评估了针对零样本图像分类的生成任务调整的多模式LLM，发现它们的性能远低于CLIP等专门模型。然后，我们提出了一种使用与CLIP相同的对比图像字幕匹配目标对LLM进行光微调的方法。我们的结果表明，当采用这种方式时，LLM确实可以实现良好的图像分类性能。我们的方法以13%的优势击败了最先进的mLLM，略优于使用自定义文本模型的对比学习，同时还保留了LLM的生成能力。LLM初始化似乎特别有助于在视觉预训练数据中表现不足的领域中进行分类。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01629v1" target="_blank">2312.01629v1</a>
                              </td>
                              <td>CLAMP: Contrastive LAnguage Model Prompt-tuning</td>
                              <td>Piotr Teterwak</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01629v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01629v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01619v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">How Many Validation Labels Do You Need? Exploring the Design Space of Label-Efficient Model Ranking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01619v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01619v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01619v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The paper introduces LEMR, a framework that reduces annotation costs for model selection tasks. Our approach leverages ensemble methods to generate pseudo-labels, employs uncertainty sampling for target acquisition, and utilizes a Z-score mechanism for iterative committee reelection to refine model ranks. We present a systematic study across various selection metrics, demonstrating that LEMR achieves comparable results to fully labeled datasets with a fraction of the labeling budget. Our findings indicate that LEMR not only economizes the labeling effort in weak supervision and semi-supervised learning settings but also effectively guides prompt selection for large language models. With extensive experiments across 23 tasks, we reveal that our framework can dramatically decrease the labeling cost without compromising the accuracy of model selection, thereby offering a cost-effective alternative to traditional practices.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01619v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了LEMR，这是一个降低模型选择任务注释成本的框架。我们的方法利用集成方法生成伪标签，采用不确定性采样进行目标获取，并利用Z评分机制进行迭代委员会连任，以细化模型等级。我们对各种选择指标进行了系统研究，证明LEMR以很小的标记预算实现了与完全标记数据集相当的结果。我们的研究结果表明，LEMR不仅在弱监督和半监督学习环境中节省了标记工作，而且有效地指导了大型语言模型的及时选择。通过对23项任务的广泛实验，我们发现我们的框架可以在不影响模型选择准确性的情况下显著降低标记成本，从而为传统实践提供了一种具有成本效益的替代方案。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>46</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01619v1" target="_blank">2312.01619v1</a>
                              </td>
                              <td>How Many Validation Labels Do You Need? Exploring the Design Space of Label-Efficient Model Ranking</td>
                              <td>Zhengyu Hu</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01619v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01619v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_13269v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_13269v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_13269v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_13269v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present chain-of-knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources. It results in more factual rationales and reduced hallucination in generation. Specifically, CoK consists of three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation. Given a knowledge-intensive question, CoK first prepares several preliminary rationales and answers while identifying the relevant knowledge domains. If there is no majority consensus among the answers from samples, CoK corrects the rationales step by step by adapting knowledge from the identified domains. These corrected rationales can plausibly serve as a better foundation for the final answer consolidation. Unlike prior studies that primarily use unstructured data, CoK also leverages structured knowledge sources such as Wikidata and tables that provide more reliable factual information. To access both unstructured and structured knowledge sources in the dynamic knowledge adapting stage, we propose an adaptive query generator that allows the generation of queries for various types of query languages, including SPARQL, SQL, and natural sentences. Moreover, to minimize error propagation between rationales, CoK corrects the rationales progressively using preceding corrected rationales to generate and correct subsequent rationales. Extensive experiments show that CoK consistently improves the performance of LLMs on knowledge-intensive tasks across different domains.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_13269v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了知识链（CoK），这是一种新的框架，通过动态地结合来自异构源的基础信息来增强大型语言模型（LLM）。它导致了更多的事实依据，并减少了一代人的幻觉。具体来说，CoK由三个阶段组成：推理准备、动态知识适应和答案巩固。对于知识密集型问题，CoK首先准备了几个初步的理由和答案，同时确定了相关的知识领域。如果样本中的答案没有达成多数共识，CoK会通过调整所识别领域的知识来逐步纠正理由。这些经过纠正的理由似乎可以为最终答案的巩固奠定更好的基础。与之前主要使用非结构化数据的研究不同，CoK还利用结构化知识源，如Wikidata和表格，提供更可靠的事实信息。为了在动态知识调整阶段访问非结构化和结构化知识源，我们提出了一种自适应查询生成器，该生成器允许为各种类型的查询语言生成查询，包括SPARQL、SQL和自然语句。此外，为了最大限度地减少理性之间的错误传播，CoK使用先前校正的理性逐步校正理性，以生成和校正后续理性。大量实验表明，CoK在不同领域的知识密集型任务中始终如一地提高了LLM的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>47</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.13269v3" target="_blank">2305.13269v3</a>
                              </td>
                              <td>Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources</td>
                              <td>Xingxuan Li</td>
                              <td>2023-05-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_13269v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.13269v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01598v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Good Questions Help Zero-Shot Image Reasoning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01598v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01598v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01598v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Aligning the recent large language models (LLMs) with computer vision models leads to large vision-language models (LVLMs), which have paved the way for zero-shot image reasoning tasks. However, LVLMs are usually trained on short high-level captions only referring to sparse focus regions in images. Such a ``tunnel vision'' limits LVLMs to exploring other relevant contexts in complex scenes. To address this challenge, we introduce Question-Driven Visual Exploration (QVix), a novel prompting strategy that enhances the exploratory capabilities of LVLMs in zero-shot reasoning tasks. QVix leverages LLMs' strong language prior to generate input-exploratory questions with more details than the original query, guiding LVLMs to explore visual content more comprehensively and uncover subtle or peripheral details. QVix enables a wider exploration of visual scenes, improving the LVLMs' reasoning accuracy and depth in tasks such as visual question answering and visual entailment. Our evaluations on various challenging zero-shot vision-language benchmarks, including ScienceQA and fine-grained visual classification, demonstrate that QVix significantly outperforms existing methods, highlighting its effectiveness in bridging the gap between complex visual data and LVLMs' exploratory abilities.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01598v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>将最近的大型语言模型（LLM）与计算机视觉模型相结合，产生了大型视觉语言模型（LVLM），这为零样本图像推理任务铺平了道路。然而，LVLMs通常在仅参考图像中稀疏焦点区域的短高级字幕上进行训练。这种“隧道视觉”限制了LVLM在复杂场景中探索其他相关上下文。为了应对这一挑战，我们引入了问题驱动的视觉探索（QVix），这是一种新颖的提示策略，可以增强LVLMs在零样本推理任务中的探索能力。QVix利用LLM的强大语言，在生成比原始查询更详细的输入探索性问题之前，引导LVLM更全面地探索视觉内容，并揭示微妙或外围的细节。QVix能够更广泛地探索视觉场景，提高LVLM在视觉问答和视觉暗示等任务中的推理准确性和深度。我们对各种具有挑战性的零样本视觉语言基准（包括ScienceQA和细粒度视觉分类）的评估表明，QVix显著优于现有方法，突出了其在弥合复杂视觉数据和LVLMs探索能力之间的差距方面的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>48</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01598v1" target="_blank">2312.01598v1</a>
                              </td>
                              <td>Good Questions Help Zero-Shot Image Reasoning</td>
                              <td>Kaiwen Yang</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01598v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01598v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/kai-wen-yang/QVix" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/kai-wen-yang/qvix" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="SAM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2312_01576v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Efficient Unsupervised Satellite Image-based Building Damage Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01576v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01576v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01576v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Existing Building Damage Detection (BDD) methods always require labour-intensive pixel-level annotations of buildings and their conditions, hence largely limiting their applications. In this paper, we investigate a challenging yet practical scenario of BDD, Unsupervised Building Damage Detection (U-BDD), where only unlabelled pre- and post-disaster satellite image pairs are provided. As a pilot study, we have first proposed an advanced U-BDD baseline that leverages pre-trained vision-language foundation models (i.e., Grounding DINO, SAM and CLIP) to address the U-BDD task. However, the apparent domain gap between satellite and generic images causes low confidence in the foundation models used to identify buildings and their damages. In response, we further present a novel self-supervised framework, U-BDD++, which improves upon the U-BDD baseline by addressing domain-specific issues associated with satellite imagery. Furthermore, the new Building Proposal Generation (BPG) module and the CLIP-enabled noisy Building Proposal Selection (CLIP-BPS) module in U-BDD++ ensure high-quality self-training. Extensive experiments on the widely used building damage assessment benchmark demonstrate the effectiveness of the proposed method for unsupervised building damage detection. The presented annotation-free and foundation model-based paradigm ensures an efficient learning phase. This study opens a new direction for real-world BDD and sets a strong baseline for future research.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01576v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现有的建筑物损坏检测（BDD）方法总是需要对建筑物及其条件进行劳动密集型像素级注释，因此在很大程度上限制了其应用。在本文中，我们研究了BDD的一个具有挑战性但实用的场景，即无监督的建筑物损坏检测（U-BDD），其中只提供未标记的灾前和灾后卫星图像对。作为一项试点研究，我们首先提出了一种先进的U-BDD基线，该基线利用预先训练的视觉语言基础模型（即基础DINO、SAM和CLIP）来解决U-BDD任务。然而，卫星图像和普通图像之间明显的领域差距导致用于识别建筑物及其损坏的基础模型的置信度较低。作为回应，我们进一步提出了一个新的自我监督框架U-BDD++，该框架通过解决与卫星图像相关的特定领域问题，改进了U-BDD基线。此外，U-BDD++中新的建筑方案生成（BPG）模块和启用CLIP的嘈杂建筑方案选择（CLIP-BPS）模块确保了高质量的自我培训。在广泛使用的建筑损伤评估基准上进行的大量实验证明了所提出的无监督建筑损伤检测方法的有效性。所提出的无注释和基于基础的范式确保了有效的学习阶段。这项研究为现实世界的BDD开辟了一个新的方向，并为未来的研究奠定了坚实的基础。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01576v1" target="_blank">2312.01576v1</a>
                              </td>
                              <td>Learning Efficient Unsupervised Satellite Image-based Building Damage Detection</td>
                              <td>Yiyun Zhang</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01576v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01576v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/fzmi/ubdd" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/fzmi/ubdd" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_01197v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Segment Anything Meets Point Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_01197v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_01197v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_01197v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Segment Anything Model (SAM) has established itself as a powerful zero-shot image segmentation model, enabled by efficient point-centric annotation and prompt-based models. While click and brush interactions are both well explored in interactive image segmentation, the existing methods on videos focus on mask annotation and propagation. This paper presents SAM-PT, a novel method for point-centric interactive video segmentation, empowered by SAM and long-term point tracking. SAM-PT leverages robust and sparse point selection and propagation techniques for mask generation. Compared to traditional object-centric mask propagation strategies, we uniquely use point propagation to exploit local structure information agnostic to object semantics. We highlight the merits of point-based tracking through direct evaluation on the zero-shot open-world Unidentified Video Objects (UVO) benchmark. Our experiments on popular video object segmentation and multi-object segmentation tracking benchmarks, including DAVIS, YouTube-VOS, and BDD100K, suggest that a point-based segmentation tracker yields better zero-shot performance and efficient interactions. We release our code that integrates different point trackers and video segmentation benchmarks at https://github.com/SysCV/sam-pt.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_01197v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segment Anything Model（SAM）已经成为一个强大的零样本图像分割模型，通过高效的点中心注释和基于提示的模型实现。虽然点击和刷式交互在交互式图像分割中都得到了很好的探索，但现有的视频方法侧重于掩码注释和传播。本文提出了一种新的以点为中心的交互式视频分割方法SAM-PT，该方法由SAM和长期点跟踪相结合。SAM-PT利用稳健和稀疏的点选择和传播技术来生成掩模。与传统的以对象为中心的掩码传播策略相比，我们独特地使用点传播来利用与对象语义无关的局部结构信息。我们通过对零样本开放世界未识别视频对象（UVO）基准的直接评估，强调了基于点的跟踪的优点。我们在流行的视频对象分割和多对象分割跟踪基准（包括DAVIS、YouTube-VOS和BDD100K）上的实验表明，基于点的分割跟踪器可以产生更好的零样本性能和有效的交互。我们在上发布了集成了不同点跟踪器和视频分割基准的代码https://github.com/syscv/sam-pt.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.01197v2" target="_blank">2307.01197v2</a>
                              </td>
                              <td>Segment Anything Meets Point Tracking</td>
                              <td>Frano Rajič</td>
                              <td>2023-07-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_01197v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.01197v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/syscv/sam-pt" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01531v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SANeRF-HQ: Segment Anything for NeRF in High Quality</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01531v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01531v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01531v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, the Segment Anything Model (SAM) has showcased remarkable capabilities of zero-shot segmentation, while NeRF (Neural Radiance Fields) has gained popularity as a method for various 3D problems beyond novel view synthesis. Though there exist initial attempts to incorporate these two methods into 3D segmentation, they face the challenge of accurately and consistently segmenting objects in complex scenarios. In this paper, we introduce the Segment Anything for NeRF in High Quality (SANeRF-HQ) to achieve high quality 3D segmentation of any object in a given scene. SANeRF-HQ utilizes SAM for open-world object segmentation guided by user-supplied prompts, while leveraging NeRF to aggregate information from different viewpoints. To overcome the aforementioned challenges, we employ density field and RGB similarity to enhance the accuracy of segmentation boundary during the aggregation. Emphasizing on segmentation accuracy, we evaluate our method quantitatively on multiple NeRF datasets where high-quality ground-truths are available or manually annotated. SANeRF-HQ shows a significant quality improvement over previous state-of-the-art methods in NeRF object segmentation, provides higher flexibility for object localization, and enables more consistent object segmentation across multiple views. Additional information can be found at https://lyclyc52.github.io/SANeRF-HQ/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01531v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，Segment Anything Model（SAM）展示了零样本分割的显著能力，而NeRF（Neural Radiance Fields）作为一种解决新视图合成之外的各种3D问题的方法而广受欢迎。尽管最初尝试将这两种方法结合到3D分割中，但它们面临着在复杂场景中准确、一致地分割对象的挑战。在本文中，我们介绍了高质量NeRF的任何分割（SANeRF-HQ），以实现给定场景中任何对象的高质量3D分割。SANeRF HQ利用SAM在用户提供的提示的指导下进行开放世界对象分割，同时利用NeRF从不同角度聚合信息。为了克服上述挑战，我们在聚合过程中使用密度场和RGB相似性来提高分割边界的准确性。强调分割的准确性，我们在多个NeRF数据集上定量评估我们的方法，在这些数据集上可以获得或手动注释高质量的基本事实。SANeRF HQ在NeRF对象分割方面比以前最先进的方法有了显著的质量改进，为对象定位提供了更高的灵活性，并实现了跨多个视图的更一致的对象分割。其他信息可在https://lyclyc52.github.io/sanerf-hq/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01531v1" target="_blank">2312.01531v1</a>
                              </td>
                              <td>SANeRF-HQ: Segment Anything for NeRF in High Quality</td>
                              <td>Yichen Liu</td>
                              <td>2023-12-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01531v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01531v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_17769v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Social Contract AI: Aligning AI Assistants with Implicit Group Norms</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_17769v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_17769v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_17769v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We explore the idea of aligning an AI assistant by inverting a model of users' (unknown) preferences from observed interactions. To validate our proposal, we run proof-of-concept simulations in the economic ultimatum game, formalizing user preferences as policies that guide the actions of simulated players. We find that the AI assistant accurately aligns its behavior to match standard policies from the economic literature (e.g., selfish, altruistic). However, the assistant's learned policies lack robustness and exhibit limited generalization in an out-of-distribution setting when confronted with a currency (e.g., grams of medicine) that was not included in the assistant's training distribution. Additionally, we find that when there is inconsistency in the relationship between language use and an unknown policy (e.g., an altruistic policy combined with rude language), the assistant's learning of the policy is slowed. Overall, our preliminary results suggest that developing simulation frameworks in which AI assistants need to infer preferences from diverse users can provide a valuable approach for studying practical alignment questions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_17769v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们探索了通过从观察到的交互中反转用户（未知）偏好的模型来调整人工智能助手的想法。为了验证我们的提议，我们在经济最后通牒游戏中进行了概念验证模拟，将用户偏好正式化为指导模拟玩家行动的政策。我们发现，人工智能助手准确地将其行为与经济文献中的标准政策（例如，自私、无私）相匹配。然而，当面对助理的培训分配中不包括的货币（如药品克数）时，助理的学习政策缺乏稳健性，在分配外的环境中表现出有限的通用性。此外，我们发现，当语言使用与未知政策（例如，利他主义政策与粗鲁语言相结合）之间的关系不一致时，助手对该政策的学习会减慢。总的来说，我们的初步结果表明，开发人工智能助手需要从不同用户那里推断偏好的模拟框架，可以为研究实际的对齐问题提供一种有价值的方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.17769v2" target="_blank">2310.17769v2</a>
                              </td>
                              <td>Social Contract AI: Aligning AI Assistants with Implicit Group Norms</td>
                              <td>Jan-Philipp Fränken</td>
                              <td>2023-10-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_17769v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.17769v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/janphilippfranken/scai" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_00749v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SEED: Domain-Specific Data Curation With Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_00749v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_00749v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_00749v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Data curation tasks that prepare data for analytics are critical for turning data into actionable insights. However, due to the diverse requirements of applications in different domains, generic off-the-shelf tools are typically insufficient. As a result, data scientists often have to develop domain-specific solutions tailored to both the dataset and the task, e.g. writing domain-specific code or training machine learning models on a sufficient number of annotated examples. This process is notoriously difficult and time-consuming. We present SEED, an LLM-as-compiler approach that automatically generates domain-specific data curation solutions via Large Language Models (LLMs). Once the user describes a task, input data, and expected output, the SEED compiler produces an executable pipeline composed of LLM-generated code, small model, and data access modules. SEED uses these generated modules to process most of the data records and dynamically decides when the LLM should step in to directly process some individual records, possibly using the data-access modules to retrieve relevant information from the data sources to assist the LLM in solving the task. To validate this new, revolutionary approach, we conducted experiments on 9 datasets spanning over 5 data curation tasks. The results show that SEED generates domain-specific solutions that significantly outperform their generic counterparts, often approaching the performance of the manually curated solutions that use thousands of labeled training examples. Moreover, in comparison to solutions that use the LLM on every data record, SEED achieves state-of-the-art or comparable few-shot performance, while significantly reducing the number of LLM calls.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_00749v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>为分析准备数据的数据管理任务对于将数据转化为可操作的见解至关重要。然而，由于不同领域中应用程序的不同需求，通用的现成工具通常是不够的。因此，数据科学家通常必须开发针对数据集和任务的特定领域解决方案，例如编写特定领域代码或在足够数量的注释示例上训练机器学习模型。这个过程是出了名的困难和耗时。我们介绍了SEED，这是一种LLM即编译器的方法，通过大型语言模型（LLM）自动生成特定于领域的数据管理解决方案。一旦用户描述了任务、输入数据和预期输出，SEED编译器就会生成一个由LLM生成的代码、小模型和数据访问模块组成的可执行管道。SEED使用这些生成的模块来处理大多数数据记录，并动态决定LLM何时介入直接处理一些单独的记录，可能使用数据访问模块从数据源检索相关信息，以帮助LLM解决任务。为了验证这种新的革命性方法，我们在5个数据管理任务的9个数据集上进行了实验。结果表明，SEED生成的领域特定解决方案明显优于其通用解决方案，通常接近使用数千个标记训练示例的手动策划解决方案的性能。此外，与在每个数据记录上使用LLM的解决方案相比，SEED实现了最先进或相当的少镜头性能，同时显著减少了LLM调用的数量。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.00749v2" target="_blank">2310.00749v2</a>
                              </td>
                              <td>SEED: Domain-Specific Data Curation With Large Language Models</td>
                              <td>Zui Chen</td>
                              <td>2023-10-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_00749v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.00749v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00869v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Segment and Caption Anything</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00869v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00869v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00869v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a method to efficiently equip the Segment Anything Model (SAM) with the ability to generate regional captions. SAM presents strong generalizability to segment anything while is short for semantic understanding. By introducing a lightweight query-based feature mixer, we align the region-specific features with the embedding space of language models for later caption generation. As the number of trainable parameters is small (typically in the order of tens of millions), it costs less computation, less memory usage, and less communication bandwidth, resulting in both fast and scalable training. To address the scarcity problem of regional caption data, we propose to first pre-train our model on objection detection and segmentation tasks. We call this step weak supervision pretraining since the pre-training data only contains category names instead of full-sentence descriptions. The weak supervision pretraining allows us to leverage many publicly available object detection and segmentation datasets. We conduct extensive experiments to demonstrate the superiority of our method and validate each design choice. This work serves as a stepping stone towards scaling up regional captioning data and sheds light on exploring efficient ways to augment SAM with regional semantics. The project page, along with the associated code, can be accessed via the following https://xk-huang.github.io/segment-caption-anything/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00869v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种方法来有效地为分段任意模型（SAM）配备生成区域字幕的能力。SAM表现出很强的可推广性来分割任何东西，而缺乏语义理解。通过引入一个轻量级的基于查询的特征混合器，我们将特定于区域的特征与语言模型的嵌入空间对齐，以便稍后生成字幕。由于可训练参数的数量很少（通常在数千万量级），因此计算成本更低，内存使用更少，通信带宽更少，从而实现快速且可扩展的训练。为了解决区域字幕数据的稀缺性问题，我们建议首先对我们的模型进行异议检测和分割任务的预训练。我们称这一步骤为弱监督预训练，因为预训练数据只包含类别名称，而不是完整的句子描述。弱监督预训练使我们能够利用许多公开可用的对象检测和分割数据集。我们进行了大量的实验来证明我们的方法的优越性，并验证了每一个设计选择。这项工作是扩大区域字幕数据的垫脚石，并有助于探索用区域语义增强SAM的有效方法。项目页面以及相关代码可以通过以下方式访问https://xk-huang.github.io/segment-caption-anything/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00869v1" target="_blank">2312.00869v1</a>
                              </td>
                              <td>Segment and Caption Anything</td>
                              <td>Xiaoke Huang</td>
                              <td>2023-12-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00869v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00869v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00863v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00863v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00863v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00863v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segment Anything Model (SAM) has emerged as a powerful tool for numerous vision applications. A key component that drives the impressive performance for zero-shot transfer and high versatility is a super large Transformer model trained on the extensive high-quality SA-1B dataset. While beneficial, the huge computation cost of SAM model has limited its applications to wider real-world applications. To address this limitation, we propose EfficientSAMs, light-weight SAM models that exhibits decent performance with largely reduced complexity. Our idea is based on leveraging masked image pretraining, SAMI, which learns to reconstruct features from SAM image encoder for effective visual representation learning. Further, we take SAMI-pretrained light-weight image encoders and mask decoder to build EfficientSAMs, and finetune the models on SA-1B for segment anything task. We perform evaluations on multiple vision tasks including image classification, object detection, instance segmentation, and semantic object detection, and find that our proposed pretraining method, SAMI, consistently outperforms other masked image pretraining methods. On segment anything task such as zero-shot instance segmentation, our EfficientSAMs with SAMI-pretrained lightweight image encoders perform favorably with a significant gain (e.g., ~4 AP on COCO/LVIS) over other fast SAM models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00863v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segment Anything Model（SAM）已成为众多视觉应用程序的强大工具。在广泛的高质量SA-1B数据集上训练的超大型变压器模型是推动零样本传输和高通用性的令人印象深刻的性能的关键组件。虽然SAM模型是有益的，但其巨大的计算成本限制了其在更广泛的现实世界应用中的应用。为了解决这一限制，我们提出了高效SAM，这是一种轻量级的SAM模型，它表现出良好的性能，并大大降低了复杂性。我们的想法是基于利用掩蔽图像预训练，SAMI，它学习从SAM图像编码器重建特征，以进行有效的视觉表示学习。此外，我们采用SAMI预训练的轻量级图像编码器和掩码解码器来构建高效SAM，并在SA-1B上对模型进行微调，以执行任何分割任务。我们对包括图像分类、对象检测、实例分割和语义对象检测在内的多个视觉任务进行了评估，发现我们提出的预训练方法SAMI始终优于其他掩蔽图像预训练方法。在分割任何任务（如零样本实例分割）上，与其他快速SAM模型相比，我们的带有SAMI预训练的轻量级图像编码器的高效SAM表现良好，具有显著的增益（例如，COCO/LVIS上的~4 AP）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00863v1" target="_blank">2312.00863v1</a>
                              </td>
                              <td>EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything</td>
                              <td>Yunyang Xiong</td>
                              <td>2023-12-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00863v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00863v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00732v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Gaussian Grouping: Segment and Edit Anything in 3D Scenes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00732v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00732v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00732v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The recent Gaussian Splatting achieves high-quality and real-time novel-view synthesis of the 3D scenes. However, it is solely concentrated on the appearance and geometry modeling, while lacking in fine-grained object-level scene understanding. To address this issue, we propose Gaussian Grouping, which extends Gaussian Splatting to jointly reconstruct and segment anything in open-world 3D scenes. We augment each Gaussian with a compact Identity Encoding, allowing the Gaussians to be grouped according to their object instance or stuff membership in the 3D scene. Instead of resorting to expensive 3D labels, we supervise the Identity Encodings during the differentiable rendering by leveraging the 2D mask predictions by SAM, along with introduced 3D spatial consistency regularization. Comparing to the implicit NeRF representation, we show that the discrete and grouped 3D Gaussians can reconstruct, segment and edit anything in 3D with high visual quality, fine granularity and efficiency. Based on Gaussian Grouping, we further propose a local Gaussian Editing scheme, which shows efficacy in versatile scene editing applications, including 3D object removal, inpainting, colorization and scene recomposition. Our code and models will be at https://github.com/lkeab/gaussian-grouping.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00732v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近的Gaussian Splatting实现了3D场景的高质量实时新颖视图合成。然而，它只专注于外观和几何建模，而缺乏细粒度的对象级场景理解。为了解决这个问题，我们提出了高斯分组，它扩展了高斯飞溅，以联合重建和分割开放世界3D场景中的任何内容。我们用紧凑的身份编码来增强每个高斯，允许高斯根据其在3D场景中的对象实例或物质成员身份进行分组。我们没有求助于昂贵的3D标签，而是通过利用SAM的2D掩模预测以及引入的3D空间一致性正则化，在可微分渲染期间监督身份编码。与隐式NeRF表示相比，我们表明离散和分组的3D高斯可以以高视觉质量、细粒度和效率重建、分割和编辑3D中的任何内容。在高斯分组的基础上，我们进一步提出了一种局部高斯编辑方案，该方案在多功能场景编辑应用中显示了有效性，包括3D对象去除、修复、着色和场景重组。我们的代码和模型将在https://github.com/lkeab/gaussian-grouping.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00732v1" target="_blank">2312.00732v1</a>
                              </td>
                              <td>Gaussian Grouping: Segment and Edit Anything in 3D Scenes</td>
                              <td>Mingqiao Ye</td>
                              <td>2023-12-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00732v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00732v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/lkeab/gaussian-grouping" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00312v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Segment Anything Model-guided Collaborative Learning Network for Scribble-supervised Polyp Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00312v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00312v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00312v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Polyp segmentation plays a vital role in accurately locating polyps at an early stage, which holds significant clinical importance for the prevention of colorectal cancer. Various polyp segmentation methods have been developed using fully-supervised deep learning techniques. However, pixel-wise annotation for polyp images by physicians during the diagnosis is both time-consuming and expensive. Moreover, visual foundation models such as the Segment Anything Model (SAM) have shown remarkable performance. Nevertheless, directly applying SAM to medical segmentation may not produce satisfactory results due to the inherent absence of medical knowledge. In this paper, we propose a novel SAM-guided Collaborative Learning Network (SAM-CLNet) for scribble-supervised polyp segmentation, enabling a collaborative learning process between our segmentation network and SAM to boost the model performance. Specifically, we first propose a Cross-level Enhancement and Aggregation Network (CEA-Net) for weakly-supervised polyp segmentation. Within CEA-Net, we propose a Cross-level Enhancement Module (CEM) that integrates the adjacent features to enhance the representation capabilities of different resolution features. Additionally, a Feature Aggregation Module (FAM) is employed to capture richer features across multiple levels. Moreover, we present a box-augmentation strategy that combines the segmentation maps generated by CEA-Net with scribble annotations to create more precise prompts. These prompts are then fed into SAM, generating segmentation SAM-guided masks, which can provide additional supervision to train CEA-Net effectively. Furthermore, we present an Image-level Filtering Mechanism to filter out unreliable SAM-guided masks. Extensive experimental results show that our SAM-CLNet outperforms state-of-the-art weakly-supervised segmentation methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00312v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>息肉分割在早期准确定位息肉中起着至关重要的作用，对癌症的预防具有重要的临床意义。已经使用完全监督的深度学习技术开发了各种息肉分割方法。然而，医生在诊断过程中对息肉图像进行逐像素注释既耗时又昂贵。此外，视觉基础模型，如分段任意模型（SAM），已经显示出显著的性能。然而，由于固有的医学知识的缺乏，直接将SAM应用于医学分割可能不会产生令人满意的结果。在本文中，我们提出了一种新的SAM引导的协作学习网络（SAM CLNet），用于涂鸦监督息肉分割，使我们的分割网络和SAM之间的协作学习过程能够提高模型性能。具体来说，我们首先提出了一种用于弱监督息肉分割的跨级别增强和聚合网络（CEA-Net）。在CEA-Net中，我们提出了一种跨级别增强模块（CEM），该模块集成了相邻的特征，以增强不同分辨率特征的表示能力。此外，还采用了特征聚合模块（FAM）来跨多个级别捕获更丰富的特征。此外，我们提出了一种方框扩充策略，该策略将CEA-Net生成的分割图与涂鸦注释相结合，以创建更精确的提示。然后将这些提示输入SAM，生成分段SAM引导的掩码，这可以提供额外的监督来有效地训练CEA-Net。此外，我们提出了一种图像级过滤机制来过滤不可靠的SAM引导掩模。大量的实验结果表明，我们的SAM CLNet优于最先进的弱监督分割方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00312v1" target="_blank">2312.00312v1</a>
                              </td>
                              <td>Segment Anything Model-guided Collaborative Learning Network for Scribble-supervised Polyp Segmentation</td>
                              <td>Yiming Zhao</td>
                              <td>2023-12-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00312v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00312v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03775v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Matched Pair Calibration for Ranking Fairness</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03775v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03775v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03775v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a test of fairness in score-based ranking systems called matched pair calibration. Our approach constructs a set of matched item pairs with minimal confounding differences between subgroups before computing an appropriate measure of ranking error over the set. The matching step ensures that we compare subgroup outcomes between identically scored items so that measured performance differences directly imply unfairness in subgroup-level exposures. We show how our approach generalizes the fairness intuitions of calibration from a binary classification setting to ranking and connect our approach to other proposals for ranking fairness measures. Moreover, our strategy shows how the logic of marginal outcome tests extends to cases where the analyst has access to model scores. Lastly, we provide an example of applying matched pair calibration to a real-word ranking data set to demonstrate its efficacy in detecting ranking bias.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03775v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种基于分数的排名系统中的公平性测试，称为匹配对校准。我们的方法构建了一组匹配的项目对，在计算该组的适当排名误差之前，子组之间的混杂差异最小。匹配步骤确保我们比较相同评分项目之间的亚组结果，以便测量的表现差异直接意味着亚组水平暴露的不公平。我们展示了我们的方法如何将校准的公平直觉从二元分类设置推广到排名，并将我们的方法与其他排名公平措施的建议联系起来。此外，我们的策略显示了边际结果测试的逻辑如何扩展到分析师可以访问模型分数的情况。最后，我们提供了一个将匹配对校准应用于真实单词排名数据集的例子，以证明其在检测排名偏差方面的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03775v3" target="_blank">2306.03775v3</a>
                              </td>
                              <td>Matched Pair Calibration for Ranking Fairness</td>
                              <td>Hannah Korevaar</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03775v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03775v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_01442v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Deep Double Descent for Time Series Forecasting: Avoiding Undertrained Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_01442v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_01442v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_01442v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Deep learning models, particularly Transformers, have achieved impressive results in various domains, including time series forecasting. While existing time series literature primarily focuses on model architecture modifications and data augmentation techniques, this paper explores the training schema of deep learning models for time series; how models are trained regardless of their architecture. We perform extensive experiments to investigate the occurrence of deep double descent in several Transformer models trained on public time series data sets. We demonstrate epoch-wise deep double descent and that overfitting can be reverted using more epochs. Leveraging these findings, we achieve state-of-the-art results for long sequence time series forecasting in nearly 70% of the 72 benchmarks tested. This suggests that many models in the literature may possess untapped potential. Additionally, we introduce a taxonomy for classifying training schema modifications, covering data augmentation, model inputs, model targets, time series per model, and computational budget.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_01442v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>深度学习模型，尤其是Transformers，在包括时间序列预测在内的各个领域都取得了令人印象深刻的成果。虽然现有的时间序列文献主要集中在模型架构修改和数据扩充技术上，但本文探索了时间序列的深度学习模型的训练模式；如何训练模型，而不管其体系结构如何。我们进行了广泛的实验，以研究在公共时间序列数据集上训练的几个Transformer模型中深度双下降的发生。我们证明了历元深度双下降，并且可以使用更多的历元来恢复过度拟合。利用这些发现，我们在测试的72个基准中的近70%中实现了最先进的长序列时间序列预测结果。这表明文献中的许多模型可能具有尚未开发的潜力。此外，我们还介绍了一种分类法，用于对训练模式修改进行分类，包括数据扩充、模型输入、模型目标、每个模型的时间序列和计算预算。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.01442v3" target="_blank">2311.01442v3</a>
                              </td>
                              <td>Deep Double Descent for Time Series Forecasting: Avoiding Undertrained Models</td>
                              <td>Valentino Assandri</td>
                              <td>2023-11-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_01442v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.01442v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_17707v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAMPro3D: Locating SAM Prompts in 3D for Zero-Shot Scene Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_17707v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_17707v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_17707v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce SAMPro3D for zero-shot 3D indoor scene segmentation. Given the 3D point cloud and multiple posed 2D frames of 3D scenes, our approach segments 3D scenes by applying the pretrained Segment Anything Model (SAM) to 2D frames. Our key idea involves locating 3D points in scenes as natural 3D prompts to align their projected pixel prompts across frames, ensuring frame-consistency in both pixel prompts and their SAM-predicted masks. Moreover, we suggest filtering out low-quality 3D prompts based on feedback from all 2D frames, for enhancing segmentation quality. We also propose to consolidate different 3D prompts if they are segmenting the same object, bringing a more comprehensive segmentation. Notably, our method does not require any additional training on domain-specific data, enabling us to preserve the zero-shot power of SAM. Extensive qualitative and quantitative results show that our method consistently achieves higher quality and more diverse segmentation than previous zero-shot or fully supervised approaches, and in many cases even surpasses human-level annotations. The project page can be accessed at https://mutianxu.github.io/sampro3d/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_17707v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了SAMPro3D用于零样本三维室内场景分割。给定3D场景的3D点云和多个姿势的2D帧，我们的方法通过将预训练的分段任何模型（SAM）应用于2D帧来分割3D场景。我们的关键思想包括将场景中的3D点定位为自然的3D提示，以使其投影的像素提示跨帧对齐，确保像素提示和SAM预测遮罩的帧一致性。此外，我们建议基于所有2D帧的反馈过滤掉低质量的3D提示，以提高分割质量。我们还建议，如果不同的3D提示正在分割同一对象，则可以合并它们，从而实现更全面的分割。值得注意的是，我们的方法不需要对特定领域的数据进行任何额外的训练，使我们能够保持SAM的零样本能力。广泛的定性和定量结果表明，与以前的零样本或全监督方法相比，我们的算法始终实现更高质量和更多样的分割，在许多情况下甚至超过了人类层面的注释。项目页面可访问https://mutianxu.github.io/sampro3d/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.17707v1" target="_blank">2311.17707v1</a>
                              </td>
                              <td>SAMPro3D: Locating SAM Prompts in 3D for Zero-Shot Scene Segmentation</td>
                              <td>Mutian Xu</td>
                              <td>2023-11-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_17707v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.17707v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16269v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation based on Visual Foundation Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16269v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16269v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16269v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Leveraging the extensive training data from SA-1B, the Segment Anything Model (SAM) demonstrates remarkable generalization and zero-shot capabilities. However, as a category-agnostic instance segmentation method, SAM heavily relies on prior manual guidance, including points, boxes, and coarse-grained masks. Furthermore, its performance in remote sensing image segmentation tasks remains largely unexplored and unproven. In this paper, we aim to develop an automated instance segmentation approach for remote sensing images, based on the foundational SAM model and incorporating semantic category information. Drawing inspiration from prompt learning, we propose a method to learn the generation of appropriate prompts for SAM. This enables SAM to produce semantically discernible segmentation results for remote sensing images, a concept we have termed RSPrompter. We also propose several ongoing derivatives for instance segmentation tasks, drawing on recent advancements within the SAM community, and compare their performance with RSPrompter. Extensive experimental results, derived from the WHU building, NWPU VHR-10, and SSDD datasets, validate the effectiveness of our proposed method. The code for our method is publicly available at kychen.me/RSPrompter.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16269v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>利用SA-1B的大量训练数据，Segment Anything Model（SAM）展示了显著的泛化和零样本能力。然而，作为一种类别不可知的实例分割方法，SAM在很大程度上依赖于先前的手动指导，包括点、框和粗粒度掩码。此外，它在遥感图像分割任务中的性能在很大程度上仍未被探索和证实。在本文中，我们的目标是开发一种基于基本SAM模型并结合语义类别信息的遥感图像自动实例分割方法。从提示学习中获得灵感，我们提出了一种学习SAM适当提示生成的方法。这使SAM能够为遥感图像产生语义上可辨别的分割结果，我们称之为RSPrompter。我们还提出了一些正在进行的衍生工具，例如分割任务，利用SAM社区的最新进展，并将其与RSpromoter的性能进行比较。从WHU大楼、NWPU VHR-10和SSDD数据集获得的大量实验结果验证了我们提出的方法的有效性。我们方法的代码可在kychen.me/RSPrompter上公开获取。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16269v2" target="_blank">2306.16269v2</a>
                              </td>
                              <td>RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation based on Visual Foundation Model</td>
                              <td>Keyan Chen</td>
                              <td>2023-06-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16269v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16269v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/KyanChen/RSPrompter" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_17539v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Effects of Overparameterization on Sharpness-aware Minimization: An Empirical and Theoretical Analysis</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_17539v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_17539v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_17539v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Training an overparameterized neural network can yield minimizers of the same level of training loss and yet different generalization capabilities. With evidence that indicates a correlation between sharpness of minima and their generalization errors, increasing efforts have been made to develop an optimization method to explicitly find flat minima as more generalizable solutions. This sharpness-aware minimization (SAM) strategy, however, has not been studied much yet as to how overparameterization can actually affect its behavior. In this work, we analyze SAM under varying degrees of overparameterization and present both empirical and theoretical results that suggest a critical influence of overparameterization on SAM. Specifically, we first use standard techniques in optimization to prove that SAM can achieve a linear convergence rate under overparameterization in a stochastic setting. We also show that the linearly stable minima found by SAM are indeed flatter and have more uniformly distributed Hessian moments compared to those of SGD. These results are corroborated with our experiments that reveal a consistent trend that the generalization improvement made by SAM continues to increase as the model becomes more overparameterized. We further present that sparsity can open up an avenue for effective overparameterization in practice.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_17539v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>训练过参数化的神经网络可以产生相同水平的训练损失和不同泛化能力的最小化器。有证据表明，极小值的锐度与其推广误差之间存在相关性，因此，人们越来越努力开发一种优化方法，将平坦极小值明确地作为更具推广性的解决方案。然而，对于过度参数化如何实际影响其行为，这种敏锐度感知最小化（SAM）策略还没有得到太多研究。在这项工作中，我们分析了不同程度的过参数化下的SAM，并给出了经验和理论结果，这些结果表明过参数化对SAM的关键影响。具体而言，我们首先使用优化中的标准技术来证明在随机环境中，在过参数化的情况下，SAM可以实现线性收敛速度。我们还表明，与SGD相比，SAM发现的线性稳定极小值确实更平坦，并且具有更均匀分布的Hessian矩。这些结果与我们的实验相证实，这些实验揭示了一个一致的趋势，即随着模型变得更加参数化，SAM所做的泛化改进继续增加。我们进一步提出，稀疏性可以在实践中为有效的过参数化开辟一条途径。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.17539v1" target="_blank">2311.17539v1</a>
                              </td>
                              <td>The Effects of Overparameterization on Sharpness-aware Minimization: An Empirical and Theoretical Analysis</td>
                              <td>Sungbin Shin</td>
                              <td>2023-11-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_17539v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.17539v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_17960v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Guided Prompting in SAM for Weakly Supervised Cell Segmentation in Histopathological Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_17960v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_17960v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_17960v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Cell segmentation in histopathological images plays a crucial role in understanding, diagnosing, and treating many diseases. However, data annotation for this is expensive since there can be a large number of cells per image, and expert pathologists are needed for labelling images. Instead, our paper focuses on using weak supervision -- annotation from related tasks -- to induce a segmenter. Recent foundation models, such as Segment Anything (SAM), can use prompts to leverage additional supervision during inference. SAM has performed remarkably well in natural image segmentation tasks; however, its applicability to cell segmentation has not been explored.   In response, we investigate guiding the prompting procedure in SAM for weakly supervised cell segmentation when only bounding box supervision is available. We develop two workflows: (1) an object detector's output as a test-time prompt to SAM (D-SAM), and (2) SAM as pseudo mask generator over training data to train a standalone segmentation model (SAM-S). On finding that both workflows have some complementary strengths, we develop an integer programming-based approach to reconcile the two sets of segmentation masks, achieving yet higher performance. We experiment on three publicly available cell segmentation datasets namely, ConSep, MoNuSeg, and TNBC, and find that all SAM-based solutions hugely outperform existing weakly supervised image segmentation models, obtaining 9-15 pt Dice gains.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_17960v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>组织病理学图像中的细胞分割在理解、诊断和治疗许多疾病方面起着至关重要的作用。然而，这方面的数据注释是昂贵的，因为每个图像可能有大量的细胞，并且需要专业的病理学家来标记图像。相反，我们的论文侧重于使用弱监督——相关任务的注释——来诱导分割器。最近的基础模型，如Segment Anything（SAM），可以在推理过程中使用提示来利用额外的监督。SAM在自然图像分割任务中表现非常好；然而，它在细胞分割中的适用性尚未得到探索。作为回应，我们研究了当只有边界框监督可用时，在SAM中指导弱监督细胞分割的提示过程。我们开发了两个工作流程：（1）对象检测器的输出作为SAM的测试时间提示（D-SAM），以及（2）SAM作为训练数据上的伪掩码生成器来训练独立分割模型（SAM-s）。在发现两个工作流都有一些互补的优势后，我们开发了一种基于整数编程的方法来协调两组分割掩码，从而实现更高的性能。我们在三个公开可用的细胞分割数据集上进行了实验，即ConSep、MoNuSeg和TNBC，发现所有基于SAM的解决方案都大大优于现有的弱监督图像分割模型，获得了9-15 pt的Dice增益。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.17960v1" target="_blank">2311.17960v1</a>
                              </td>
                              <td>Guided Prompting in SAM for Weakly Supervised Cell Segmentation in Histopathological Images</td>
                              <td>Aayush Kumar Tyagi</td>
                              <td>2023-11-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_17960v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.17960v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_16754v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Full-scene Domain Generalization in Multi-agent Collaborative Bird's Eye View Segmentation for Connected and Autonomous Driving</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_16754v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_16754v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_16754v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Collaborative perception has recently gained significant attention in autonomous driving, improving perception quality by enabling the exchange of additional information among vehicles. However, deploying collaborative perception systems can lead to domain shifts due to diverse environmental conditions and data heterogeneity among connected and autonomous vehicles (CAVs). To address these challenges, we propose a unified domain generalization framework applicable in both training and inference stages of collaborative perception. In the training phase, we introduce an Amplitude Augmentation (AmpAug) method to augment low-frequency image variations, broadening the model's ability to learn across various domains. We also employ a meta-consistency training scheme to simulate domain shifts, optimizing the model with a carefully designed consistency loss to encourage domain-invariant representations. In the inference phase, we introduce an intra-system domain alignment mechanism to reduce or potentially eliminate the domain discrepancy among CAVs prior to inference. Comprehensive experiments substantiate the effectiveness of our method in comparison with the existing state-of-the-art works. Code will be released at https://github.com/DG-CAVs/DG-CoPerception.git.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_16754v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>协作感知最近在自动驾驶中获得了极大的关注，通过实现车辆之间的附加信息交换来提高感知质量。然而，由于联网和自动驾驶汽车（CAV）之间的不同环境条件和数据异构性，部署协同感知系统可能会导致领域转变。为了应对这些挑战，我们提出了一个统一的领域泛化框架，适用于协同感知的训练和推理阶段。在训练阶段，我们引入了一种幅度增强（AmpAug）方法来增强低频图像的变化，从而拓宽了模型在各个领域的学习能力。我们还使用元一致性训练方案来模拟域移动，用精心设计的一致性损失来优化模型，以鼓励域不变表示。在推理阶段，我们引入了一种系统内域对齐机制，以在推理之前减少或潜在地消除CAV之间的域差异。与现有的最先进的工作相比，综合实验证明了我们的方法的有效性。代码将在发布https://github.com/dg-cavs/dg-coperception.git.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.16754v1" target="_blank">2311.16754v1</a>
                              </td>
                              <td>Towards Full-scene Domain Generalization in Multi-agent Collaborative Bird's Eye View Segmentation for Connected and Autonomous Driving</td>
                              <td>Senkang Hu</td>
                              <td>2023-11-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_16754v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.16754v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_17112v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Parameter Efficient Fine-tuning via Cross Block Orchestration for Segment Anything Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_17112v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_17112v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_17112v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Parameter-efficient fine-tuning (PEFT) is an effective methodology to unleash the potential of large foundation models in novel scenarios with limited training data. In the computer vision community, PEFT has shown effectiveness in image classification, but little research has studied its ability for image segmentation. Fine-tuning segmentation models usually require a heavier adjustment of parameters to align the proper projection directions in the parameter space for new scenarios. This raises a challenge to existing PEFT algorithms, as they often inject a limited number of individual parameters into each block, which prevents substantial adjustment of the projection direction of the parameter space due to the limitation of Hidden Markov Chain along blocks. In this paper, we equip PEFT with a cross-block orchestration mechanism to enable the adaptation of the Segment Anything Model (SAM) to various downstream scenarios. We introduce a novel inter-block communication module, which integrates a learnable relation matrix to facilitate communication among different coefficient sets of each PEFT block's parameter space. Moreover, we propose an intra-block enhancement module, which introduces a linear projection head whose weights are generated from a hyper-complex layer, further enhancing the impact of the adjustment of projection directions on the entire parameter space. Extensive experiments on diverse benchmarks demonstrate that our proposed approach consistently improves the segmentation performance significantly on novel scenarios with only around 1K additional parameters.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_17112v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>参数有效微调（PEFT）是在训练数据有限的新场景中释放大型基础模型潜力的有效方法。在计算机视觉领域，PEFT在图像分类方面表现出了有效性，但很少有研究对其图像分割能力进行研究。微调分割模型通常需要对参数进行更大的调整，以在新场景的参数空间中对齐适当的投影方向。这对现有的PEFT算法提出了挑战，因为它们通常将有限数量的单个参数注入到每个块中，由于隐马尔可夫链沿着块的限制，这阻止了参数空间的投影方向的实质性调整。在本文中，我们为PEFT配备了一个跨块协调机制，以使分段任意模型（SAM）能够适应各种下游场景。我们介绍了一种新的块间通信模块，该模块集成了一个可学习的关系矩阵，以促进每个PEFT块的参数空间的不同系数集之间的通信。此外，我们提出了一个块内增强模块，该模块引入了一个线性投影头，其权重由超复杂层生成，进一步增强了投影方向调整对整个参数空间的影响。在不同基准上进行的大量实验表明，我们提出的方法在只有大约1K个额外参数的新场景上持续显著提高了分割性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.17112v1" target="_blank">2311.17112v1</a>
                              </td>
                              <td>Parameter Efficient Fine-tuning via Cross Block Orchestration for Segment Anything Model</td>
                              <td>Zelin Peng</td>
                              <td>2023-11-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_17112v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.17112v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_12308v4_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Segment Anything in 3D with NeRFs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_12308v4_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_12308v4_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_12308v4_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, the Segment Anything Model (SAM) emerged as a powerful vision foundation model which is capable to segment anything in 2D images. This paper aims to generalize SAM to segment 3D objects. Rather than replicating the data acquisition and annotation procedure which is costly in 3D, we design an efficient solution, leveraging the Neural Radiance Field (NeRF) as a cheap and off-the-shelf prior that connects multi-view 2D images to the 3D space. We refer to the proposed solution as SA3D, for Segment Anything in 3D. It is only required to provide a manual segmentation prompt (e.g., rough points) for the target object in a single view, which is used to generate its 2D mask in this view with SAM. Next, SA3D alternately performs mask inverse rendering and cross-view self-prompting across various views to iteratively complete the 3D mask of the target object constructed with voxel grids. The former projects the 2D mask obtained by SAM in the current view onto 3D mask with guidance of the density distribution learned by the NeRF; The latter extracts reliable prompts automatically as the input to SAM from the NeRF-rendered 2D mask in another view. We show in experiments that SA3D adapts to various scenes and achieves 3D segmentation within minutes. Our research reveals a potential methodology to lift the ability of a 2D vision foundation model to 3D, as long as the 2D model can steadily address promptable segmentation across multiple views. Our code is available at https://github.com/Jumpat/SegmentAnythingin3D.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_12308v4_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，分割任何事物模型（SAM）作为一种强大的视觉基础模型出现了，它能够分割2D图像中的任何事物。本文旨在将SAM推广到三维物体的分割。我们没有复制3D中成本高昂的数据采集和注释程序，而是设计了一种高效的解决方案，利用神经辐射场（NeRF）作为一种廉价且现成的先验技术，将多视图2D图像连接到3D空间。我们将所提出的解决方案称为SA3D，用于3D中的任何分段。只需要在单个视图中为目标对象提供手动分割提示（例如，粗糙点），用于使用SAM在该视图中生成其2D掩模。接下来，SA3D在各个视图中交替执行掩模反向渲染和跨视图自提示，迭代完成用体素网格构建的目标对象的3D掩模。前者在NeRF学习的密度分布的指导下，将SAM在当前视图中获得的2D掩模投影到3D掩模上；后者在另一个视图中从NeRF渲染的2D掩模中自动提取可靠提示作为SAM的输入。我们在实验中表明，SA3D能够适应各种场景，并在几分钟内实现3D分割。我们的研究揭示了一种潜在的方法，可以将2D视觉基础模型的能力提升到3D，只要2D模型能够稳定地解决多个视图之间的可提示分割问题。我们的代码可在https://github.com/jumpat/segmentanythingin3d.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.12308v4" target="_blank">2304.12308v4</a>
                              </td>
                              <td>Segment Anything in 3D with NeRFs</td>
                              <td>Jiazhong Cen</td>
                              <td>2023-04-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_12308v4_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.12308v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_15111v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAMv2: A Unified Framework for Learning Appearance, Semantic and Cross-Modality Anatomical Embeddings</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_15111v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_15111v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_15111v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Identifying anatomical structures (e.g., lesions or landmarks) in medical images plays a fundamental role in medical image analysis. As an exemplar-based landmark detection method, Self-supervised Anatomical eMbedding (SAM) learns a discriminative embedding for each voxel in the image and has shown promising results on various tasks. However, SAM still faces challenges in: (1) differentiating voxels with similar appearance but different semantic meanings (\textit{e.g.}, two adjacent structures without clear borders); (2) matching voxels with similar semantics but markedly different appearance (e.g., the same vessel before and after contrast injection); and (3) cross-modality matching (e.g., CT-MRI registration). To overcome these challenges, we propose SAMv2, which is a unified framework designed to learn appearance, semantic, and cross-modality anatomical embeddings. Specifically, SAMv2 incorporates three key innovations: (1) semantic embedding learning with prototypical contrastive loss; (2) a fixed-point-based matching strategy; and (3) an iterative approach for cross-modality embedding learning. We thoroughly evaluated SAMv2 across three tasks, including one-shot landmark detection, lesion tracking on longitudinal CT scans, and CT-MRI affine/rigid registration with varying field of view. Our results suggest that SAMv2 outperforms SAM and other state-of-the-art methods, offering a robust and versatile approach for landmark based medical image analysis tasks. Code and trained models are available at: https://github.com/alibaba-damo-academy/self-supervised-anatomical-embedding-v2</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_15111v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>识别医学图像中的解剖结构（例如，病变或标志）在医学图像分析中起着重要作用。作为一种基于示例的地标检测方法，自监督解剖eMbedding（SAM）学习图像中每个体素的判别嵌入，并在各种任务中显示出有希望的结果。然而，SAM仍然面临以下挑战：（1）区分外观相似但语义不同的体素（\textit｛例如｝，两个没有明确边界的相邻结构）；（2） 匹配语义相似但外观明显不同的体素（例如造影剂注射前后相同的血管）；以及（3）跨模态匹配（例如CT-MRI注册）。为了克服这些挑战，我们提出了SAMv2，这是一个统一的框架，旨在学习外观、语义和跨模态解剖嵌入。具体而言，SAMv2包含了三个关键创新：（1）具有原型对比损失的语义嵌入学习；（2） 基于不动点的匹配策略；以及（3）用于跨模态嵌入学习的迭代方法。我们在三项任务中对SAMv2进行了全面评估，包括单次界标检测、纵向CT扫描上的病变跟踪以及不同视野下的CT-MRI仿射/刚性配准。我们的结果表明，SAMv2优于SAM和其他最先进的方法，为基于里程碑的医学图像分析任务提供了一种稳健而通用的方法。代码和经过训练的模型可在以下位置获得：https://github.com/alibaba-damo-academy/self-supervised-anatomical-embedding-v2</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.15111v2" target="_blank">2311.15111v2</a>
                              </td>
                              <td>SAMv2: A Unified Framework for Learning Appearance, Semantic and Cross-Modality Anatomical Embeddings</td>
                              <td>Xiaoyu Bai</td>
                              <td>2023-11-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_15111v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.15111v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/alibaba-damo-academy/self-supervised-anatomical-embedding-v2" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_17085v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Beyond Visual Cues: Synchronously Exploring Target-Centric Semantics for Vision-Language Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_17085v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_17085v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_17085v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Single object tracking aims to locate one specific target in video sequences, given its initial state. Classical trackers rely solely on visual cues, restricting their ability to handle challenges such as appearance variations, ambiguity, and distractions. Hence, Vision-Language (VL) tracking has emerged as a promising approach, incorporating language descriptions to directly provide high-level semantics and enhance tracking performance. However, current VL trackers have not fully exploited the power of VL learning, as they suffer from limitations such as heavily relying on off-the-shelf backbones for feature extraction, ineffective VL fusion designs, and the absence of VL-related loss functions. Consequently, we present a novel tracker that progressively explores target-centric semantics for VL tracking. Specifically, we propose the first Synchronous Learning Backbone (SLB) for VL tracking, which consists of two novel modules: the Target Enhance Module (TEM) and the Semantic Aware Module (SAM). These modules enable the tracker to perceive target-related semantics and comprehend the context of both visual and textual modalities at the same pace, facilitating VL feature extraction and fusion at different semantic levels. Moreover, we devise the dense matching loss to further strengthen multi-modal representation learning. Extensive experiments on VL tracking datasets demonstrate the superiority and effectiveness of our methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_17085v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>单目标跟踪的目标是在给定初始状态的情况下定位视频序列中的一个特定目标。传统的追踪器只依赖视觉提示，限制了它们处理诸如外观变化、模糊和分心等挑战的能力。因此，视觉语言（VL）跟踪已经成为一种很有前途的方法，它结合了语言描述来直接提供高级语义并提高跟踪性能。然而，当前的VL跟踪器还没有充分利用VL学习的能力，因为它们受到限制，例如严重依赖现成的主干进行特征提取、无效的VL融合设计以及缺乏与VL相关的损失函数。因此，我们提出了一种新的跟踪器，它逐步探索VL跟踪的以目标为中心的语义。具体来说，我们提出了第一个用于VL跟踪的同步学习主干（SLB），它由两个新模块组成：目标增强模块（TEM）和语义感知模块（SAM）。这些模块使跟踪器能够感知与目标相关的语义，并以相同的速度理解视觉和文本模态的上下文，从而促进不同语义级别的VL特征提取和融合。此外，我们设计了密集匹配损失来进一步加强多模态表示学习。在VL跟踪数据集上进行的大量实验证明了我们的方法的优越性和有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.17085v1" target="_blank">2311.17085v1</a>
                              </td>
                              <td>Beyond Visual Cues: Synchronously Exploring Target-Centric Semantics for Vision-Language Tracking</td>
                              <td>Jiawei Ge</td>
                              <td>2023-11-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_17085v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.17085v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_17081v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">I-MedSAM: Implicit Medical Image Segmentation with Segment Anything</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_17081v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_17081v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_17081v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With the development of Deep Neural Networks (DNNs), many efforts have been made to handle medical image segmentation. Traditional methods such as nnUNet train specific segmentation models on the individual datasets. Plenty of recent methods have been proposed to adapt the foundational Segment Anything Model (SAM) to medical image segmentation. However, they still focus on discrete representations to generate pixel-wise predictions, which are spatially inflexible and scale poorly to higher resolution. In contrast, implicit methods learn continuous representations for segmentation, which is crucial for medical image segmentation. In this paper, we propose I-MedSAM, which leverages the benefits of both continuous representations and SAM, to obtain better cross-domain ability and accurate boundary delineation. Since medical image segmentation needs to predict detailed segmentation boundaries, we designed a novel adapter to enhance the SAM features with high-frequency information during Parameter Efficient Fine Tuning (PEFT). To convert the SAM features and coordinates into continuous segmentation output, we utilize Implicit Neural Representation (INR) to learn an implicit segmentation decoder. We also propose an uncertainty-guided sampling strategy for efficient learning of INR. Extensive evaluations on 2D medical image segmentation tasks have shown that our proposed method with only 1.6M trainable parameters outperforms existing methods including discrete and continuous methods. The code will be released.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_17081v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着深度神经网络（DNNs）的发展，人们已经做出了许多努力来处理医学图像分割。nnUNet等传统方法在单个数据集上训练特定的分割模型。最近已经提出了许多方法来将基础的分段任意模型（SAM）应用于医学图像分割。然而，他们仍然专注于离散表示来生成逐像素预测，这在空间上是不灵活的，并且难以扩展到更高的分辨率。相反，隐式方法学习用于分割的连续表示，这对于医学图像分割至关重要。在本文中，我们提出了I-MedSAM，它利用了连续表示和SAM的优点，以获得更好的跨域能力和准确的边界描绘。由于医学图像分割需要预测详细的分割边界，我们设计了一种新的适配器，在参数有效微调（PEFT）过程中利用高频信息增强SAM特征。为了将SAM特征和坐标转换为连续分割输出，我们利用隐式神经表示（INR）来学习隐式分割解码器。我们还提出了一种不确定性引导的采样策略，用于INR的有效学习。对2D医学图像分割任务的广泛评估表明，我们提出的仅具有1.6M可训练参数的方法优于现有方法，包括离散和连续方法。代码将被发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.17081v1" target="_blank">2311.17081v1</a>
                              </td>
                              <td>I-MedSAM: Implicit Medical Image Segmentation with Segment Anything</td>
                              <td>Xiaobao Wei</td>
                              <td>2023-11-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_17081v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.17081v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_16378v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Bayesian Formulations for Graph Spectral Denoising</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_16378v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_16378v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_16378v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We consider noisy signals which are defined on the vertices of a graph and present smoothing algorithms for the cases of Gaussian, dropout, and uniformly distributed noise. The signals are assumed to follow a prior distribution defined in the frequency domain which favors signals which are smooth across the edges of the graph. By pairing this prior distribution with our three models of noise generation, we propose \textit{Maximum A Posteriori} (M.A.P.) estimates of the true signal in the presence of noisy data and provide algorithms for computing the M.A.P. Finally, we demonstrate the algorithms' ability to effectively restore white noise on image data, and from severe dropout in toy \& EHR data.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_16378v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们考虑了定义在图的顶点上的噪声信号，并提出了高斯、丢弃和均匀分布噪声情况下的平滑算法。假设信号遵循在频域中定义的先验分布，这有利于在图的边缘上平滑的信号。通过将这种先验分布与我们的三个噪声生成模型配对，我们提出了在存在噪声数据的情况下真实信号的\textit｛Maximum A Posteriori｝（M.A.P.）估计，并提供了计算M.A.P.的算法。最后，我们展示了算法有效恢复图像数据上的白噪声以及toy\&EHR数据中严重丢失的能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.16378v1" target="_blank">2311.16378v1</a>
                              </td>
                              <td>Bayesian Formulations for Graph Spectral Denoising</td>
                              <td>Sam Leone</td>
                              <td>2023-11-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_16378v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.16378v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_15941v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Tell2Design: A Dataset for Language-Guided Floor Plan Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_15941v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_15941v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_15941v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We consider the task of generating designs directly from natural language descriptions, and consider floor plan generation as the initial research area. Language conditional generative models have recently been very successful in generating high-quality artistic images. However, designs must satisfy different constraints that are not present in generating artistic images, particularly spatial and relational constraints. We make multiple contributions to initiate research on this task. First, we introduce a novel dataset, \textit{Tell2Design} (T2D), which contains more than $80k$ floor plan designs associated with natural language instructions. Second, we propose a Sequence-to-Sequence model that can serve as a strong baseline for future research. Third, we benchmark this task with several text-conditional image generation models. We conclude by conducting human evaluations on the generated samples and providing an analysis of human performance. We hope our contributions will propel the research on language-guided design generation forward.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_15941v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们考虑直接从自然语言描述中生成设计的任务，并将平面图生成视为初步研究领域。语言条件生成模型最近在生成高质量的艺术图像方面非常成功。然而，设计必须满足在生成艺术图像时不存在的不同约束，特别是空间和关系约束。我们为启动这项任务的研究做出了多项贡献。首先，我们介绍了一个新的数据集\textit｛Tell2Design｝（T2D），它包含与自然语言指令相关的超过8万美元的平面图设计。其次，我们提出了一个序列到序列的模型，可以作为未来研究的有力基线。第三，我们用几个文本条件图像生成模型对这项任务进行了基准测试。最后，我们对生成的样本进行了人为评估，并对人为表现进行了分析。我们希望我们的贡献将推动语言引导设计生成的研究向前发展。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.15941v1" target="_blank">2311.15941v1</a>
                              </td>
                              <td>Tell2Design: A Dataset for Language-Guided Floor Plan Generation</td>
                              <td>Sicong Leng</td>
                              <td>2023-11-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_15941v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.15941v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/lengsicong/tell2design" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_15939v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Unleashing the Power of Prompt-driven Nucleus Instance Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_15939v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_15939v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_15939v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Nuclear instance segmentation in histology images is crucial for a broad spectrum of clinical applications. Current prevailing nuclear instance segmentation algorithms rely on regression of nuclei contours, distance maps, watershed markers or a proxy nuclear representation of star-convex polygons. Consequently, these methods necessitate sophisticated post-processing operations to distinguish nuclei instances, which are commonly acknowledged to be error-prone and parameter-sensitive. Recently, the segment anything model (SAM) has earned attracted huge attention within the domain of medical image segmentation due to its impressive generalization ability and promptable property. Nevertheless, its potential on nuclear instance segmentation remains largely underexplored. In this paper, we present a novel prompt-driven framework that consists of a point prompter and a SAM for automatic nuclei instance segmentation. Specifically, the prompter learns to generate a unique point prompt for each nucleus while the SAM is fine tuned to output the corresponding mask of the cued nucleus. Furthermore, we propose to add adjacent nuclei as negative prompts to promote the model's ability to recognize overlapping nuclei. Without bells and whistles, our proposed method sets a new state-of-the-art performance on three challenging benchmarks. Our code is available at \textcolor{magenta}{\url{https://github.com/windygoo/PromptNucSeg}} .</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_15939v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>组织学图像中的核实例分割对于广泛的临床应用至关重要。目前流行的核实例分割算法依赖于核轮廓的回归、距离图、分水岭标记或星形凸多边形的代理核表示。因此，这些方法需要复杂的后处理操作来区分核实例，这通常被认为是容易出错和参数敏感的。近年来，分段任意模型（SAM）由于其令人印象深刻的泛化能力和可提示性，在医学图像分割领域引起了极大的关注。然而，它在核实例分割方面的潜力在很大程度上仍未得到充分挖掘。在本文中，我们提出了一种新的提示驱动框架，该框架由点提示器和SAM组成，用于自动核实例分割。具体而言，提示器学习为每个核生成唯一的点提示，同时SAM被微调以输出提示核的相应掩码。此外，我们建议添加相邻的核作为负提示，以提高模型识别重叠核的能力。在没有铃声和口哨声的情况下，我们提出的方法在三个具有挑战性的基准上设置了新的最先进的性能。我们的代码位于\textcolor｛品红色｝｛\url{https://github.com/windygoo/promptnucseg}}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.15939v1" target="_blank">2311.15939v1</a>
                              </td>
                              <td>Unleashing the Power of Prompt-driven Nucleus Instance Segmentation</td>
                              <td>Zhongyi Shui</td>
                              <td>2023-11-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_15939v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.15939v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/windygoo/promptnucseg" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_15776v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Stable Segment Anything Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_15776v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_15776v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_15776v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Segment Anything Model (SAM) achieves remarkable promptable segmentation given high-quality prompts which, however, often require good skills to specify. To make SAM robust to casual prompts, this paper presents the first comprehensive analysis on SAM's segmentation stability across a diverse spectrum of prompt qualities, notably imprecise bounding boxes and insufficient points. Our key finding reveals that given such low-quality prompts, SAM's mask decoder tends to activate image features that are biased towards the background or confined to specific object parts. To mitigate this issue, our key idea consists of adjusting the sampling locations of image feature using learnable deformable offsets, while the original SAM model architecture and weights remain unchanged. Consequently, our deformable sampling plugin (DSP) enables SAM to adaptively shift attention to the prompted target regions in a data-driven manner, facilitated by our effective robust training strategy (RTS). During inference, dynamic routing plugin (DRP) is proposed that toggles SAM between the deformable and regular grid sampling modes, conditioned on the input prompt quality. Thus, our solution, termed Stable-SAM, is one of its kind focusing on solely adjusting feature sampling locations, which offers several advantages: 1) improved SAM's segmentation stability across a wide range of prompt qualities, while 2) retaining SAM's powerful promptable segmentation efficiency and generality, with 3) minimal learnable parameters (0.08 M) and fast adaptation (by 1 training epoch). Extensive experiments across multiple datasets validate the effectiveness and advantages of our approach, underscoring Stable-SAM as a more robust solution for segmenting anything. Codes will be released upon acceptance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_15776v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分段任意模型（SAM）在给出高质量提示的情况下实现了显著的可提示分段，然而，这通常需要良好的指定技能。为了使SAM对偶然提示具有鲁棒性，本文首次全面分析了SAM在各种提示质量范围内的分割稳定性，特别是不精确的边界框和不足的点。我们的关键发现表明，在这种低质量提示的情况下，SAM的掩码解码器倾向于激活偏向背景或局限于特定对象部分的图像特征。为了缓解这个问题，我们的关键思想包括使用可学习的可变形偏移来调整图像特征的采样位置，而原始SAM模型架构和权重保持不变。因此，我们的可变形采样插件（DSP）使SAM能够以数据驱动的方式自适应地将注意力转移到提示的目标区域，这得益于我们有效的鲁棒训练策略（RTS）。在推理过程中，提出了动态路由插件（DRP），以输入提示质量为条件，在可变形网格采样模式和规则网格采样模式之间切换SAM。因此，我们的解决方案称为稳定SAM，是专注于仅调整特征采样位置的解决方案之一，它提供了几个优点：1）提高了SAM在各种提示质量上的分割稳定性，同时2）保留了SAM强大的可提示分割效率和通用性，具有3）最小可学习参数（0.08M）和快速适应（通过1个训练时期）。跨多个数据集的广泛实验验证了我们方法的有效性和优势，强调稳定SAM是一种更稳健的分割任何内容的解决方案。代码将在验收后发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.15776v1" target="_blank">2311.15776v1</a>
                              </td>
                              <td>Stable Segment Anything Model</td>
                              <td>Qi Fan</td>
                              <td>2023-11-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_15776v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.15776v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/fanq15/stable-sam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_15727v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MARIS: Referring Image Segmentation via Mutual-Aware Attention Features</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_15727v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_15727v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_15727v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Referring image segmentation (RIS) aims to segment a particular region based on a language expression prompt. Existing methods incorporate linguistic features into visual features and obtain multi-modal features for mask decoding. However, these methods may segment the visually salient entity instead of the correct referring region, as the multi-modal features are dominated by the abundant visual context. In this paper, we propose MARIS, a referring image segmentation method that leverages the Segment Anything Model (SAM) and introduces a mutual-aware attention mechanism to enhance the cross-modal fusion via two parallel branches. Specifically, our mutual-aware attention mechanism consists of Vision-Guided Attention and Language-Guided Attention, which bidirectionally model the relationship between visual and linguistic features. Correspondingly, we design a Mask Decoder to enable explicit linguistic guidance for more consistent segmentation with the language expression. To this end, a multi-modal query token is proposed to integrate linguistic information and interact with visual information simultaneously. Extensive experiments on three benchmark datasets show that our method outperforms the state-of-the-art RIS methods. Our code will be publicly available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_15727v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>参考图像分割（RIS）旨在基于语言表达提示对特定区域进行分割。现有的方法将语言特征结合到视觉特征中，并获得用于掩码解码的多模态特征。然而，这些方法可能会分割视觉上显著的实体，而不是正确的指代区域，因为多模态特征是由丰富的视觉上下文主导的。在本文中，我们提出了MARIS，这是一种参考图像分割方法，它利用了Segment Anything Model（SAM），并引入了一种相互感知的注意力机制，通过两个并行分支来增强跨模态融合。具体来说，我们的相互意识注意机制由视觉引导注意和语言引导注意组成，它们双向地模拟视觉特征和语言特征之间的关系。相应地，我们设计了一个掩码解码器，以实现明确的语言指导，使分割与语言表达更加一致。为此，提出了一种多模式查询令牌，用于整合语言信息并同时与视觉信息交互。在三个基准数据集上的大量实验表明，我们的方法优于最先进的RIS方法。我们的代码将公开。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.15727v1" target="_blank">2311.15727v1</a>
                              </td>
                              <td>MARIS: Referring Image Segmentation via Mutual-Aware Attention Features</td>
                              <td>Mengxi Zhang</td>
                              <td>2023-11-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_15727v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.15727v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_15707v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAM-6D: Segment Anything Model Meets Zero-Shot 6D Object Pose Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_15707v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_15707v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_15707v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Zero-shot 6D object pose estimation involves the detection of novel objects with their 6D poses in cluttered scenes, presenting significant challenges for model generalizability. Fortunately, the recent Segment Anything Model (SAM) has showcased remarkable zero-shot transfer performance, which provides a promising solution to tackle this task. Motivated by this, we introduce SAM-6D, a novel framework designed to realize the task through two steps, including instance segmentation and pose estimation. Given the target objects, SAM-6D employs two dedicated sub-networks, namely Instance Segmentation Model (ISM) and Pose Estimation Model (PEM), to perform these steps on cluttered RGB-D images. ISM takes SAM as an advanced starting point to generate all possible object proposals and selectively preserves valid ones through meticulously crafted object matching scores in terms of semantics, appearance and geometry. By treating pose estimation as a partial-to-partial point matching problem, PEM performs a two-stage point matching process featuring a novel design of background tokens to construct dense 3D-3D correspondence, ultimately yielding the pose estimates. Without bells and whistles, SAM-6D outperforms the existing methods on the seven core datasets of the BOP Benchmark for both instance segmentation and pose estimation of novel objects.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_15707v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>零样本6D物体姿态估计涉及在杂乱场景中检测具有6D姿态的新物体，这对模型的可推广性提出了重大挑战。幸运的是，最近的Segment Anything Model（SAM）展示了非凡的零样本转移性能，这为解决这一任务提供了一个有前景的解决方案。受此启发，我们介绍了SAM-6D，这是一种新的框架，旨在通过两个步骤实现任务，包括实例分割和姿态估计。给定目标对象，SAM-6D采用两个专用的子网络，即实例分割模型（ISM）和姿态估计模型（PEM），对杂乱的RGB-D图像执行这些步骤。ISM将SAM作为生成所有可能的对象建议的高级起点，并通过在语义、外观和几何结构方面精心编制的对象匹配分数来选择性地保留有效的对象建议。通过将姿态估计视为部分到部分点匹配问题，PEM执行两阶段点匹配过程，该过程以新颖的背景标记设计为特征，以构建密集的3D-3D对应关系，最终产生姿态估计。在BOP Benchmark的七个核心数据集上，SAM-6D在新对象的实例分割和姿态估计方面都优于现有方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.15707v1" target="_blank">2311.15707v1</a>
                              </td>
                              <td>SAM-6D: Segment Anything Model Meets Zero-Shot 6D Object Pose Estimation</td>
                              <td>Jiehong Lin</td>
                              <td>2023-11-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_15707v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.15707v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/jiehonglin/sam-6d" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_16479v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Mitigating Hallucination in Visual Language Models with Visual Supervision</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_16479v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_16479v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_16479v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large vision-language models (LVLMs) suffer from hallucination a lot, generating responses that apparently contradict to the image content occasionally. The key problem lies in its weak ability to comprehend detailed content in a multi-modal context, which can be mainly attributed to two factors in training data and loss function. The vision instruction dataset primarily focuses on global description, and the auto-regressive loss function favors text modeling rather than image understanding. In this paper, we bring more detailed vision annotations and more discriminative vision models to facilitate the training of LVLMs, so that they can generate more precise responses without encounter hallucination. On one hand, we generate image-text pairs with detailed relationship annotations in panoptic scene graph dataset (PSG). These conversations pay more attention on detailed facts in the image, encouraging the model to answer questions based on multi-modal contexts. On the other hand, we integrate SAM and mask prediction loss as auxiliary supervision, forcing the LVLMs to have the capacity to identify context-related objects, so that they can generate more accurate responses, mitigating hallucination. Moreover, to provide a deeper evaluation on the hallucination in LVLMs, we propose a new benchmark, RAH-Bench. It divides vision hallucination into three different types that contradicts the image with wrong categories, attributes or relations, and introduces False Positive Rate as detailed sub-metric for each type. In this benchmark, our approach demonstrates an +8.4% enhancement compared to original LLaVA and achieves widespread performance improvements across other models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_16479v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型视觉语言模型（LVLMs）经常产生幻觉，偶尔会产生与图像内容明显矛盾的反应。关键问题在于它在多模态环境中理解详细内容的能力较弱，这主要归因于训练数据和损失函数两个因素。视觉指令数据集主要关注全局描述，自回归损失函数倾向于文本建模而非图像理解。在本文中，我们带来了更详细的视觉注释和更具鉴别力的视觉模型，以促进LVLMs的训练，使它们能够在不产生幻觉的情况下产生更精确的反应。一方面，我们在全景场景图数据集中生成具有详细关系注释的图像-文本对。这些对话更加关注图像中的详细事实，鼓励模型基于多模态上下文回答问题。另一方面，我们将SAM和掩码预测损失集成为辅助监督，迫使LVLMs具有识别上下文相关对象的能力，以便它们能够产生更准确的响应，减轻幻觉。此外，为了对LVLMs中的幻觉进行更深入的评估，我们提出了一个新的基准，RAH Bench。它将视觉幻觉分为三种不同的类型，它们与具有错误类别、属性或关系的图像相矛盾，并引入假阳性率作为每种类型的详细子指标。在这个基准测试中，与最初的LLaVA相比，我们的方法实现了+8.4%的增强，并在其他模型中实现了广泛的性能改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.16479v1" target="_blank">2311.16479v1</a>
                              </td>
                              <td>Mitigating Hallucination in Visual Language Models with Visual Supervision</td>
                              <td>Zhiyang Chen</td>
                              <td>2023-11-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_16479v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.16479v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_12386v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Point, Segment and Count: A Generalized Framework for Object Counting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_12386v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_12386v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_12386v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Class-agnostic object counting aims to count all objects in an image with respect to example boxes or class names, \emph{a.k.a} few-shot and zero-shot counting. Current state-of-the-art methods highly rely on density maps to predict object counts, which lacks model interpretability. In this paper, we propose a generalized framework for both few-shot and zero-shot object counting based on detection. Our framework combines the superior advantages of two foundation models without compromising their zero-shot capability: (\textbf{i}) SAM to segment all possible objects as mask proposals, and (\textbf{ii}) CLIP to classify proposals to obtain accurate object counts. However, this strategy meets the obstacles of efficiency overhead and the small crowded objects that cannot be localized and distinguished. To address these issues, our framework, termed PseCo, follows three steps: point, segment, and count. Specifically, we first propose a class-agnostic object localization to provide accurate but least point prompts for SAM, which consequently not only reduces computation costs but also avoids missing small objects. Furthermore, we propose a generalized object classification that leverages CLIP image/text embeddings as the classifier, following a hierarchical knowledge distillation to obtain discriminative classifications among hierarchical mask proposals. Extensive experimental results on FSC-147 dataset demonstrate that PseCo achieves state-of-the-art performance in both few-shot/zero-shot object counting/detection, with additional results on large-scale COCO and LVIS datasets. The source code is available at \url{https://github.com/Hzzone/PseCo}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_12386v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>类不可知对象计数旨在根据示例框或类名称计数图像中的所有对象，\emph｛a.k.a｝few-shot和零样本计数。目前最先进的方法高度依赖密度图来预测物体数量，这缺乏模型的可解释性。在本文中，我们提出了一个基于检测的少热点和零样本对象计数的广义框架。我们的框架结合了两个基础模型的优越优势，而不影响它们的零样本能力：（\textbf｛i｝）SAM将所有可能的对象分割为掩码提议，以及（\textbf｛ii｝）CLIP将提议分类以获得准确的对象计数。然而，这种策略遇到了效率开销和无法定位和区分的小型拥挤对象的障碍。为了解决这些问题，我们的框架称为PseCo，遵循三个步骤：点、分段和计数。具体来说，我们首先提出了一种类不可知的对象定位方法，为SAM提供准确但最少的提示，从而不仅降低了计算成本，还避免了丢失小对象。此外，我们提出了一种广义对象分类，该分类利用CLIP图像/文本嵌入作为分类器，遵循分层知识提取，以在分层掩码建议中获得有区别的分类。在FSC-147数据集上的大量实验结果表明，PseCo在少热/零样本对象计数/检测方面都实现了最先进的性能，在大规模COCO和LVIS数据集上获得了额外的结果。源代码位于\url{https://github.com/hzzone/pseco}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.12386v2" target="_blank">2311.12386v2</a>
                              </td>
                              <td>Point, Segment and Count: A Generalized Framework for Object Counting</td>
                              <td>Zhizhong Huang</td>
                              <td>2023-11-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_12386v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.12386v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_15463v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Where to Begin? From Random to Foundation Model Instructed Initialization in Federated Learning for Medical Image Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_15463v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_15463v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_15463v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In medical image analysis, Federated Learning (FL) stands out as a key technology that enables privacy-preserved, decentralized data processing, crucial for handling sensitive medical data. Currently, most FL models employ random initialization, which has been proven effective in various instances. However, given the unique challenges posed by non-IID (independently and identically distributed) data in FL, we propose a novel perspective: exploring the impact of using the foundation model with enormous pre-trained knowledge, such as the Segment Anything Model (SAM), as an instructive teacher for FL model initialization in medical image segmentation task. This work for the first time attempts to utilize the foundation model as an instructive teacher for initialization in FL, assessing its impact on the performance of FL models, especially in non-IID data scenarios. Our empirical evaluation on chest x-ray lung segmentation showcases that FL with foundation model instructed initialization not only achieves faster convergence but also improves performance in complex data contexts. These findings offer a new perspective for model initialization in FL.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_15463v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在医学图像分析中，联合学习（FL）是一项关键技术，它能够实现隐私保护、分散的数据处理，对处理敏感的医学数据至关重要。目前，大多数FL模型都采用随机初始化，这在各种情况下都被证明是有效的。然而，考虑到FL中非IID（独立且相同分布）数据所带来的独特挑战，我们提出了一个新的视角：探索使用具有大量预先训练知识的基础模型（如分段任意模型（SAM））作为医学图像分割任务中FL模型初始化的指导教师的影响。这项工作首次尝试将基础模型作为外语初始化的指导教师，评估其对外语模型性能的影响，特别是在非IID数据场景中。我们对胸部x射线肺分割的经验评估表明，具有基础模型指示初始化的FL不仅实现了更快的收敛，而且提高了在复杂数据环境中的性能。这些发现为FL中的模型初始化提供了一个新的视角。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.15463v1" target="_blank">2311.15463v1</a>
                              </td>
                              <td>Where to Begin? From Random to Foundation Model Instructed Initialization in Federated Learning for Medical Image Segmentation</td>
                              <td>Ming Li</td>
                              <td>2023-11-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_15463v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.15463v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_15326v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Lightweight Face Recognition: An Improved MobileFaceNet Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_15326v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_15326v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_15326v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents an extensive exploration and comparative analysis of lightweight face recognition (FR) models, specifically focusing on MobileFaceNet and its modified variant, MMobileFaceNet. The need for efficient FR models on devices with limited computational resources has led to the development of models with reduced memory footprints and computational demands without sacrificing accuracy. Our research delves into the impact of dataset selection, model architecture, and optimization algorithms on the performance of FR models. We highlight our participation in the EFaR-2023 competition, where our models showcased exceptional performance, particularly in categories restricted by the number of parameters. By employing a subset of the Webface42M dataset and integrating sharpness-aware minimization (SAM) optimization, we achieved significant improvements in accuracy across various benchmarks, including those that test for cross-pose, cross-age, and cross-ethnicity performance. The results underscore the efficacy of our approach in crafting models that are not only computationally efficient but also maintain high accuracy in diverse conditions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_15326v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文对轻量级人脸识别（FR）模型进行了广泛的探索和比较分析，特别关注MobileFaceNet及其改进的变体MMobileFaceNet。在计算资源有限的设备上对高效FR模型的需求导致了在不牺牲精度的情况下开发具有减少的内存占用和计算需求的模型。我们的研究深入探讨了数据集选择、模型架构和优化算法对FR模型性能的影响。我们强调了我们参加EFaR-2023比赛的情况，在比赛中，我们的车型表现出了非凡的性能，尤其是在受参数数量限制的类别中。通过使用Webface42M数据集的子集并集成清晰度感知最小化（SAM）优化，我们在各种基准测试中实现了显著的准确性提高，包括测试跨姿势、跨年龄和跨种族性能的基准测试。结果强调了我们的方法在构建模型方面的有效性，这些模型不仅在计算上高效，而且在不同的条件下保持高精度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.15326v1" target="_blank">2311.15326v1</a>
                              </td>
                              <td>Lightweight Face Recognition: An Improved MobileFaceNet Model</td>
                              <td>Ahmad Hassanpour</td>
                              <td>2023-11-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_15326v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.15326v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_15291v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Obj-NeRF: Extract Object NeRFs from Multi-view Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_15291v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_15291v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_15291v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural Radiance Fields (NeRFs) have demonstrated remarkable effectiveness in novel view synthesis within 3D environments. However, extracting a radiance field of one specific object from multi-view images encounters substantial challenges due to occlusion and background complexity, thereby presenting difficulties in downstream applications such as NeRF editing and 3D mesh extraction. To solve this problem, in this paper, we propose Obj-NeRF, a comprehensive pipeline that recovers the 3D geometry of a specific object from multi-view images using a single prompt. This method combines the 2D segmentation capabilities of the Segment Anything Model (SAM) in conjunction with the 3D reconstruction ability of NeRF. Specifically, we first obtain multi-view segmentation for the indicated object using SAM with a single prompt. Then, we use the segmentation images to supervise NeRF construction, integrating several effective techniques. Additionally, we construct a large object-level NeRF dataset containing diverse objects, which can be useful in various downstream tasks. To demonstrate the practicality of our method, we also apply Obj-NeRF to various applications, including object removal, rotation, replacement, and recoloring.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_15291v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经辐射场（NeRF）在3D环境中的新型视图合成中表现出显著的有效性。然而，由于遮挡和背景复杂性，从多视图图像中提取一个特定对象的辐射场遇到了实质性的挑战，从而在诸如NeRF编辑和3D网格提取之类的下游应用中存在困难。为了解决这个问题，在本文中，我们提出了Obj-NeRF，这是一种综合的管道，可以使用单个提示从多视图图像中恢复特定对象的3D几何结构。该方法将Segment Anything Model（SAM）的2D分割能力与NeRF的3D重建能力相结合。具体来说，我们首先使用SAM和单个提示获得指示对象的多视图分割。然后，我们使用分割图像来监督NeRF的构建，集成了几种有效的技术。此外，我们构建了一个包含不同对象的大型对象级NeRF数据集，这在各种下游任务中都很有用。为了证明我们的方法的实用性，我们还将Obj-NeRF应用于各种应用，包括对象移除、旋转、替换和重新着色。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.15291v1" target="_blank">2311.15291v1</a>
                              </td>
                              <td>Obj-NeRF: Extract Object NeRFs from Multi-view Images</td>
                              <td>Zhiyi Li</td>
                              <td>2023-11-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_15291v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.15291v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_15138v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Can SAM recognize crops? Quantifying the zero-shot performance of a semantic segmentation foundation model on generating crop-type maps using satellite imagery for precision agriculture</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_15138v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_15138v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_15138v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Climate change is increasingly disrupting worldwide agriculture, making global food production less reliable.To tackle the growing challenges in feeding the planet, cutting-edge management strategies, such as precision agriculture, empower farmers and decision-makers with rich and actionable information to increase the efficiency and sustainability of their farming practices.Crop-type maps are key information for decision-support tools but are challenging and costly to generate.We investigate the capabilities of Meta AI's Segment Anything Model (SAM) for crop-map prediction task, acknowledging its recent successes at zero-shot image segmentation.However, SAM being limited to up-to 3 channel inputs and its zero-shot usage being class-agnostic in nature pose unique challenges in using it directly for crop-type mapping.We propose using clustering consensus metrics to assess SAM's zero-shot performance in segmenting satellite imagery and producing crop-type maps.Although direct crop-type mapping is challenging using SAM in zero-shot setting, experiments reveal SAM's potential for swiftly and accurately outlining fields in satellite images, serving as a foundation for subsequent crop classification.This paper attempts to highlight a use-case of state-of-the-art image segmentation models like SAM for crop-type mapping and related specific needs of the agriculture industry, offering a potential avenue for automatic, efficient, and cost-effective data products for precision agriculture practices.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_15138v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>气候变化正在日益扰乱全球农业，使全球粮食生产变得不那么可靠。为了应对养活地球方面日益增长的挑战，精准农业等尖端管理战略为农民和决策者提供了丰富而可操作的信息，以提高其农业实践的效率和可持续性。作物类型图是决策支持工具的关键信息，但生成起来具有挑战性且成本高昂。我们研究了Meta AI的Segment Anything Model（SAM）用于crop-map预测任务的能力，承认其最近在零样本图像分割方面的成功。然而，SAM被限制为最多3个通道输入，并且其零样本的使用本质上是类认知的，这在将其直接用于crop型映射方面带来了独特的挑战。我们建议使用聚类一致性度量来评估SAM在分割卫星图像和生成crop型地图方面的零样本性能。尽管在零样本环境中使用SAM进行直接作物类型测绘具有挑战性，但实验揭示了SAM在卫星图像中快速准确地勾勒田地轮廓的潜力，为后续作物分类奠定了基础。本文试图强调最先进的图像分割模型（如SAM）在作物类型测绘和农业相关特定需求中的使用案例，为精确农业实践提供自动化、高效和成本效益高的数据产品提供潜在途径。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.15138v1" target="_blank">2311.15138v1</a>
                              </td>
                              <td>Can SAM recognize crops? Quantifying the zero-shot performance of a semantic segmentation foundation model on generating crop-type maps using satellite imagery for precision agriculture</td>
                              <td>Rutuja Gurav</td>
                              <td>2023-11-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_15138v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.15138v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_14986v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAME++: A Self-supervised Anatomical eMbeddings Enhanced medical image registration framework using stable sampling and regularized transformation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_14986v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_14986v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_14986v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Image registration is a fundamental medical image analysis task. Ideally, registration should focus on aligning semantically corresponding voxels, i.e., the same anatomical locations. However, existing methods often optimize similarity measures computed directly on intensities or on hand-crafted features, which lack anatomical semantic information. These similarity measures may lead to sub-optimal solutions where large deformations, complex anatomical differences, or cross-modality imagery exist. In this work, we introduce a fast and accurate method for unsupervised 3D medical image registration building on top of a Self-supervised Anatomical eMbedding (SAM) algorithm, which is capable of computing dense anatomical correspondences between two images at the voxel level. We name our approach SAM-Enhanced registration (SAME++), which decomposes image registration into four steps: affine transformation, coarse deformation, deep non-parametric transformation, and instance optimization. Using SAM embeddings, we enhance these steps by finding more coherent correspondence and providing features with better semantic guidance. We extensively evaluated SAME++ using more than 50 labeled organs on three challenging inter-subject registration tasks of different body parts. As a complete registration framework, SAME++ markedly outperforms leading methods by $4.2\%$ - $8.2\%$ in terms of Dice score while being orders of magnitude faster than numerical optimization-based methods. Code is available at \url{https://github.com/alibaba-damo-academy/same}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_14986v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>图像配准是医学图像分析的一项基本任务。理想情况下，配准应侧重于对齐语义上对应的体素，即相同的解剖位置。然而，现有的方法通常优化直接根据强度或手工制作的特征计算的相似性度量，这些特征缺乏解剖学语义信息。这些相似性度量可能导致存在大变形、复杂解剖差异或跨模态图像的次优解决方案。在这项工作中，我们介绍了一种快速准确的无监督三维医学图像配准方法，该方法基于自监督解剖eMbeding（SAM）算法，能够在体素水平上计算两个图像之间的密集解剖对应关系。我们将我们的方法命名为SAM增强配准（SAME++），它将图像配准分解为四个步骤：仿射变换、粗变形、深度非参数变换和实例优化。使用SAM嵌入，我们通过找到更一致的对应关系并提供具有更好语义指导的特征来增强这些步骤。我们使用50多个标记器官对不同身体部位的三个具有挑战性的受试者间注册任务进行了广泛评估。作为一个完整的注册框架，SAME++在Dice得分方面明显优于领先的方法4.2\%%$-8.2\%%$，同时比基于数值优化的方法快几个数量级。代码位于\url{https://github.com/alibaba-damo-academy/same}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.14986v1" target="_blank">2311.14986v1</a>
                              </td>
                              <td>SAME++: A Self-supervised Anatomical eMbeddings Enhanced medical image registration framework using stable sampling and regularized transformation</td>
                              <td>Lin Tian</td>
                              <td>2023-11-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_14986v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.14986v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/alibaba-damo-academy/same" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="CLIP"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2312_02147v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rejuvenating image-GPT as Strong Visual Representation Learners</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02147v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02147v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02147v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper enhances image-GPT (iGPT), one of the pioneering works that introduce autoregressive pretraining to predict next pixels for visual representation learning. Two simple yet essential changes are made. First, we shift the prediction target from raw pixels to semantic tokens, enabling a higher-level understanding of visual content. Second, we supplement the autoregressive modeling by instructing the model to predict not only the next tokens but also the visible tokens. This pipeline is particularly effective when semantic tokens are encoded by discriminatively trained models, such as CLIP. We introduce this novel approach as D-iGPT. Extensive experiments showcase that D-iGPT excels as a strong learner of visual representations: A notable achievement of D-iGPT is its compelling performance on the ImageNet-1K dataset -- by training on publicly available datasets, D-iGPT achieves 89.5\% top-1 accuracy with a vanilla ViT-Large model. This model also shows strong generalization on the downstream task and robustness on out-of-distribution samples. Code is avaiable at \href{https://github.com/OliverRensu/D-iGPT}{https://github.com/OliverRensu/D-iGPT}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02147v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文增强了图像GPT（iGPT），这是引入自回归预训练来预测视觉表示学习的下一个像素的开创性工作之一。进行了两个简单但重要的更改。首先，我们将预测目标从原始像素转移到语义标记，从而实现对视觉内容的更高层次理解。其次，我们通过指示模型不仅预测下一个令牌，而且预测可见令牌来补充自回归建模。当语义标记由经过区别训练的模型（如CLIP）编码时，这种管道尤其有效。我们将这种新方法称为D-iGPT。大量实验表明，D-iGPT在视觉表示方面表现出色：D-iGPT的一个显著成就是其在ImageNet-1K数据集上的令人信服的性能——通过在公开可用的数据集上进行训练，D-iGPT在普通ViT-Large模型中实现了89.5\%的前1级准确率。该模型对下游任务也表现出较强的泛化能力，对分布外样本也表现出鲁棒性。代码可在\href获得{https://github.com/oliverrensu/d-igpt}{https://github.com/oliverrensu/d-igpt}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02147v1" target="_blank">2312.02147v1</a>
                              </td>
                              <td>Rejuvenating image-GPT as Strong Visual Representation Learners</td>
                              <td>Sucheng Ren</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02147v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02147v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/oliverrensu/d-igpt" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02021v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">VLTSeg: Simple Transfer of CLIP-Based Vision-Language Representations for Domain Generalized Semantic Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02021v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02021v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02021v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Domain generalization (DG) remains a significant challenge for perception based on deep neural networks (DNN), where domain shifts occur due to lighting, weather, or geolocation changes. In this work, we propose VLTSeg to enhance domain generalization in semantic segmentation, where the network is solely trained on the source domain and evaluated on unseen target domains. Our method leverages the inherent semantic robustness of vision-language models. First, by substituting traditional vision-only backbones with pre-trained encoders from CLIP and EVA-CLIP as transfer learning setting we find that in the field of DG, vision-language pre-training significantly outperforms supervised and self-supervised vision pre-training. We thus propose a new vision-language approach for domain generalized segmentation, which improves the domain generalization SOTA by 7.6% mIoU when training on the synthetic GTA5 dataset. We further show the superior generalization capabilities of vision-language segmentation models by reaching 76.48% mIoU on the popular Cityscapes-to-ACDC benchmark, outperforming the previous SOTA approach by 6.9% mIoU on the test set at the time of writing. Additionally, our approach shows strong in-domain generalization capabilities indicated by 86.1% mIoU on the Cityscapes test set, resulting in a shared first place with the previous SOTA on the current leaderboard at the time of submission.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02021v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>领域泛化（DG）仍然是基于深度神经网络（DNN）的感知的一个重大挑战，在深度神经网络中，由于照明、天气或地理位置的变化，领域会发生变化。在这项工作中，我们提出了VLTSeg来增强语义分割中的域泛化，其中网络仅在源域上进行训练，并在看不见的目标域上进行评估。我们的方法利用了视觉语言模型固有的语义稳健性。首先，通过用CLIP和EVA-CLIP的预训练编码器代替传统的纯视觉骨干作为迁移学习设置，我们发现在DG领域，视觉语言预训练显著优于监督和自监督视觉预训练。因此，我们提出了一种新的用于领域广义分割的视觉语言方法，当在合成GTA5数据集上进行训练时，该方法将领域广义SOTA提高了7.6%mIoU。我们进一步展示了视觉语言分割模型的优越泛化能力，在流行的Cityscapes到ACDC基准上达到了76.48%的mIoU，在撰写本文时的测试集上比以前的SOTA方法高出6.9%mIoU。此外，我们的方法在Cityscapes测试集中显示出86.1%mIoU所示的强大的域内泛化能力，导致在提交时与前一个SOTA在当前排行榜上并列第一。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02021v1" target="_blank">2312.02021v1</a>
                              </td>
                              <td>VLTSeg: Simple Transfer of CLIP-Based Vision-Language Representations for Domain Generalized Semantic Segmentation</td>
                              <td>Christoph Hümmer</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02021v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02021v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01998v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Language-only Efficient Training of Zero-shot Composed Image Retrieval</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01998v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01998v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01998v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Composed image retrieval (CIR) task takes a composed query of image and text, aiming to search relative images for both conditions. Conventional CIR approaches need a training dataset composed of triplets of query image, query text, and target image, which is very expensive to collect. Several recent works have worked on the zero-shot (ZS) CIR paradigm to tackle the issue without using pre-collected triplets. However, the existing ZS-CIR methods show limited backbone scalability and generalizability due to the lack of diversity of the input texts during training. We propose a novel CIR framework, only using language for its training. Our LinCIR (Language-only training for CIR) can be trained only with text datasets by a novel self-supervision named self-masking projection (SMP). We project the text latent embedding to the token embedding space and construct a new text by replacing the keyword tokens of the original text. Then, we let the new and original texts have the same latent embedding vector. With this simple strategy, LinCIR is surprisingly efficient and highly effective; LinCIR with CLIP ViT-G backbone is trained in 48 minutes and shows the best ZS-CIR performances on four different CIR benchmarks, CIRCO, GeneCIS, FashionIQ, and CIRR, even outperforming supervised method on FashionIQ. Code is available at https://github.com/navervision/lincir</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01998v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>组合图像检索（CIR）任务采用图像和文本的组合查询，旨在搜索这两种条件下的相对图像。传统的CIR方法需要由查询图像、查询文本和目标图像的三元组组成的训练数据集，这是非常昂贵的收集。最近的几项工作已经致力于零样本（ZS）CIR范式，以在不使用预先收集的三元组的情况下解决这个问题。然而，由于训练过程中输入文本缺乏多样性，现有的ZS-CIR方法显示出有限的主干可扩展性和可推广性。我们提出了一个新的CIR框架，只使用语言进行训练。我们的LinCIR（仅针对CIR的语言训练）只能通过一种新的称为自掩蔽投影（SMP）的自监督来使用文本数据集进行训练。我们将文本潜在嵌入投影到标记嵌入空间，并通过替换原始文本的关键字标记来构建新的文本。然后，我们让新文本和原始文本具有相同的潜在嵌入向量。通过这种简单的策略，LinCIR的效率和高效性令人惊讶；具有CLIP-ViT-G骨干的LinCIR在48分钟内完成训练，并在四个不同的CIR基准（CIRCO、GeneCIS、FashionIQ和CIRR）上显示出最佳的ZS-CIR性能，甚至优于Fashion智商上的监督方法。代码位于https://github.com/navervision/lincir</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01998v1" target="_blank">2312.01998v1</a>
                              </td>
                              <td>Language-only Efficient Training of Zero-shot Composed Image Retrieval</td>
                              <td>Geonmo Gu</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01998v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01998v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/navervision/lincir" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01987v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Bootstrapping SparseFormers from Vision Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01987v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01987v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01987v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The recently proposed SparseFormer architecture provides an alternative approach to visual understanding by utilizing a significantly lower number of visual tokens via adjusting RoIs, greatly reducing computational costs while still achieving promising performance. However, training SparseFormers from scratch is still expensive, and scaling up the number of parameters can be challenging. In this paper, we propose to bootstrap SparseFormers from ViT-based vision foundation models in a simple and efficient way. Since the majority of SparseFormer blocks are the standard transformer ones, we can inherit weights from large-scale pre-trained vision transformers and freeze them as much as possible. Therefore, we only need to train the SparseFormer-specific lightweight focusing transformer to adjust token RoIs and fine-tune a few early pre-trained blocks to align the final token representation. In such a way, we can bootstrap SparseFormer architectures from various large-scale pre-trained models (e.g., IN-21K pre-trained AugRegs or CLIPs) using a rather smaller amount of training samples (e.g., IN-1K) and without labels or captions within just a few hours. As a result, the bootstrapped unimodal SparseFormer (from AugReg-ViT-L/16-384) can reach 84.9% accuracy on IN-1K with only 49 tokens, and the multimodal SparseFormer from CLIPs also demonstrates notable zero-shot performance with highly reduced computational cost without seeing any caption during the bootstrapping procedure. In addition, CLIP-bootstrapped SparseFormers, which align the output space with language without seeing a word, can serve as efficient vision encoders in multimodal large language models. Code will be publicly available at https://github.com/showlab/sparseformer</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01987v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近提出的SparseFormer架构提供了一种视觉理解的替代方法，通过调整ROI来利用数量显著减少的视觉标记，大大降低了计算成本，同时仍然实现了有希望的性能。然而，从头开始训练SparseFormers仍然是昂贵的，并且扩大参数的数量可能具有挑战性。在本文中，我们建议以一种简单有效的方式从基于ViT的视觉基础模型中引导SparseFormers。由于大多数SparseFormer块都是标准的transformer块，我们可以从大规模预训练的视觉transformer中继承权重，并尽可能多地冻结它们。因此，我们只需要训练SparseFormer特定的轻量级聚焦转换器来调整令牌ROI，并微调一些早期预训练的块来对齐最终的令牌表示。通过这种方式，我们可以在短短几个小时内使用少量的训练样本（例如，In-1K）从各种大规模预训练模型（例如，In-21K预训练AugRegs或CLIP）中引导SparseFormer架构，而不需要标签或字幕。因此，自举单模态SparseFormer（来自AugReg-ViT-L/16-384）在IN-1K上仅使用49个令牌就可以达到84.9%的准确率，而来自CLIP的多模态SparseFormer也表现出显著的零样本性能，大大降低了计算成本，而在自举过程中看不到任何说明。此外，CLIP自举的SparseFormers可以在不看单词的情况下将输出空间与语言对齐，可以在多模式大型语言模型中用作高效的视觉编码器。代码将在公开https://github.com/showlab/sparseformer</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01987v1" target="_blank">2312.01987v1</a>
                              </td>
                              <td>Bootstrapping SparseFormers from Vision Foundation Models</td>
                              <td>Ziteng Gao</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01987v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01987v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/showlab/sparseformer" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01897v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Adapting Short-Term Transformers for Action Detection in Untrimmed Videos</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01897v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01897v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01897v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision transformer (ViT) has shown high potential in video recognition, owing to its flexible design, adaptable self-attention mechanisms, and the efficacy of masked pre-training. Yet, it still remains unclear how to adapt these pre-trained short-term ViTs for temporal action detection (TAD) in untrimmed videos. The existing works treat them as off-the-shelf feature extractors for each short trimmed snippet without capturing the fine-grained relation among different snippets in a broader temporal context. To mitigate this issue, this paper focuses on designing a new mechanism for adapting these pre-trained ViT models as a unified long-form video transformer to fully unleash its modeling power in capturing inter-snippet relation, while still keeping low computation overhead and memory consumption for efficient TAD. To this end, we design effective cross-snippet propagation modules to gradually exchange short-term video information among different snippets from two levels. For inner-backbone information propagation, we introduce a cross-snippet propagation strategy to enable multi-snippet temporal feature interaction inside the backbone. For post-backbone information propagation, we propose temporal transformer layers for further clip-level modeling. With the plain ViT-B pre-trained with VideoMAE, our end-to-end temporal action detector (ViT-TAD) yields a very competitive performance to previous temporal action detectors, riching up to 69.0 average mAP on THUMOS14, 37.12 average mAP on ActivityNet-1.3 and 17.20 average mAP on FineAction.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01897v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉转换器（ViT）由于其灵活的设计、适应性强的自我注意机制和掩蔽预训练的有效性，在视频识别中显示出了很高的潜力。然而，如何将这些预先训练的短期ViT应用于未经处理的视频中的时间动作检测（TAD）仍不清楚。现有的工作将它们视为每个短切片段的现成特征提取器，而没有在更广泛的时间上下文中捕捉不同片段之间的细粒度关系。为了缓解这一问题，本文重点设计了一种新的机制，将这些预先训练的ViT模型作为统一的长格式视频转换器，以充分释放其在捕捉片段间关系方面的建模能力，同时保持低计算开销和内存消耗，实现高效的TAD。为此，我们设计了有效的跨片段传播模块，从两个层面逐步在不同片段之间交换短期视频信息。对于内部主干信息传播，我们引入了一种跨片段传播策略，以实现主干内部的多片段时间特征交互。对于主干后信息传播，我们提出了用于进一步剪辑级建模的时间变换器层。使用VideoMAE预训练的普通ViT-B，我们的端到端时间动作检测器（ViT-TAD）产生了与以前的时间动作检测器相比非常有竞争力的性能，在THUMOS14上的平均mAP高达69.0，在ActivityNet-1.3上的平均mAP高达37.12，在FineAction上的平均mAP高达17.20。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01897v1" target="_blank">2312.01897v1</a>
                              </td>
                              <td>Adapting Short-Term Transformers for Action Detection in Untrimmed Videos</td>
                              <td>Min Yang</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01897v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01897v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01758v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CZL-CIAE: CLIP-driven Zero-shot Learning for Correcting Inverse Age Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01758v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01758v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01758v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Zero-shot age estimation aims to learn feature information about age from input images and make inferences about a given person's image or video frame without specific sample data. The development of zero-shot age estimation can improve the efficiency and accuracy of various applications (e.g., age verification and secure access control, etc.), while also promoting research on multi-modal and zero-shot learning in the social media field. For example, zero-sample age estimation can be used to create social networks focused on specific age groups. However, existing methods mainly focus on supervised, labeled age estimation learning, and the prediction effect of zero-shot learning is very poor. To tackle the above issues, we propose a novel CLIP-driven Zero-shot Learning for Correcting Inverse Age Estimation (CZL-CIAE). Specifically, we first introduce the CLIP model to extract image features and text semantic information respectively, and map them into a highly semantically aligned high-dimensional feature space. Next, we designed a new Transformer architecture (i.e., FourierFormer) to achieve channel evolution and spatial interaction of images, and to fuse image and text semantic information. Finally, we introduce reversible age estimation, which uses end-to-end error feedback to reduce the error rate of age predictions. Through extensive experiments on multiple data sets, CZL-CIAE has achieved better age prediction results.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01758v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>零样本年龄估计旨在从输入图像中学习关于年龄的特征信息，并在没有特定样本数据的情况下对给定人的图像或视频帧进行推断。零样本年龄估计的发展可以提高各种应用（如年龄验证和安全访问控制等）的效率和准确性，同时也促进了社交媒体领域对多模式和零样本学习的研究。例如，零样本年龄估计可以用于创建专注于特定年龄组的社交网络。然而，现有的方法主要集中在监督、标记的年龄估计学习上，零样本学习的预测效果非常差。为了解决上述问题，我们提出了一种新的CLIP驱动的零样本学习用于校正逆年龄估计（CZL-CIAE）。具体来说，我们首先引入CLIP模型来分别提取图像特征和文本语义信息，并将它们映射到语义高度对齐的高维特征空间中。接下来，我们设计了一种新的Transformer架构（即FourierFormer），以实现图像的通道进化和空间交互，并融合图像和文本的语义信息。最后，我们介绍了可逆年龄估计，它使用端到端的误差反馈来降低年龄预测的错误率。通过对多个数据集的广泛实验，CZL-CIAE取得了较好的年龄预测结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01758v1" target="_blank">2312.01758v1</a>
                              </td>
                              <td>CZL-CIAE: CLIP-driven Zero-shot Learning for Correcting Inverse Age Estimation</td>
                              <td>Yuntao Shou</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01758v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01758v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01640v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SequencePAR: Understanding Pedestrian Attributes via A Sequence Generation Paradigm</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01640v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01640v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01640v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Current pedestrian attribute recognition (PAR) algorithms are developed based on multi-label or multi-task learning frameworks, which aim to discriminate the attributes using specific classification heads. However, these discriminative models are easily influenced by imbalanced data or noisy samples. Inspired by the success of generative models, we rethink the pedestrian attribute recognition scheme and believe the generative models may perform better on modeling dependencies and complexity between human attributes. In this paper, we propose a novel sequence generation paradigm for pedestrian attribute recognition, termed SequencePAR. It extracts the pedestrian features using a pre-trained CLIP model and embeds the attribute set into query tokens under the guidance of text prompts. Then, a Transformer decoder is proposed to generate the human attributes by incorporating the visual features and attribute query tokens. The masked multi-head attention layer is introduced into the decoder module to prevent the model from remembering the next attribute while making attribute predictions during training. Extensive experiments on multiple widely used pedestrian attribute recognition datasets fully validated the effectiveness of our proposed SequencePAR. The source code and pre-trained models will be released at https://github.com/Event-AHU/OpenPAR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01640v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当前的行人属性识别（标准杆数）算法是基于多标签或多任务学习框架开发的，旨在使用特定的分类头来识别属性。然而，这些判别模型很容易受到不平衡数据或噪声样本的影响。受生成模型成功的启发，我们重新思考了行人属性识别方案，并认为生成模型可能在建模人类属性之间的依赖性和复杂性方面表现得更好。在本文中，我们提出了一种新的行人属性识别序列生成范式，称为SequencePAR。它使用预先训练的CLIP模型提取行人特征，并在文本提示的指导下将属性集嵌入到查询标记中。然后，提出了一种Transformer解码器，通过结合视觉特征和属性查询令牌来生成人类属性。在解码器模块中引入了掩蔽的多头注意力层，以防止模型在训练期间进行属性预测时记住下一个属性。在多个广泛使用的行人属性识别数据集上进行的大量实验充分验证了我们提出的SequencePAR的有效性。源代码和预训练模型将在https://github.com/event-ahu/openpar.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01640v1" target="_blank">2312.01640v1</a>
                              </td>
                              <td>SequencePAR: Understanding Pedestrian Attributes via A Sequence Generation Paradigm</td>
                              <td>Jiandong Jin</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01640v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01640v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01629v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLAMP: Contrastive LAnguage Model Prompt-tuning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01629v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01629v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01629v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have emerged as powerful general-purpose interfaces for many machine learning problems. Recent work has adapted LLMs to generative visual tasks like image captioning, visual question answering, and visual chat, using a relatively small amount of instruction-tuning data. In this paper, we explore whether modern LLMs can also be adapted to classifying an image into a set of categories. First, we evaluate multimodal LLMs that are tuned for generative tasks on zero-shot image classification and find that their performance is far below that of specialized models like CLIP. We then propose an approach for light fine-tuning of LLMs using the same contrastive image-caption matching objective as CLIP. Our results show that LLMs can, indeed, achieve good image classification performance when adapted this way. Our approach beats state-of-the-art mLLMs by 13% and slightly outperforms contrastive learning with a custom text model, while also retaining the LLM's generative abilities. LLM initialization appears to particularly help classification in domains under-represented in the visual pre-training data.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01629v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）已经成为许多机器学习问题的强大通用接口。最近的工作使用相对少量的指令调整数据，使LLM适应生成视觉任务，如图像字幕、视觉问答和视觉聊天。在本文中，我们探讨了现代LLM是否也可以适用于将图像分类为一组类别。首先，我们评估了针对零样本图像分类的生成任务调整的多模式LLM，发现它们的性能远低于CLIP等专门模型。然后，我们提出了一种使用与CLIP相同的对比图像字幕匹配目标对LLM进行光微调的方法。我们的结果表明，当采用这种方式时，LLM确实可以实现良好的图像分类性能。我们的方法以13%的优势击败了最先进的mLLM，略优于使用自定义文本模型的对比学习，同时还保留了LLM的生成能力。LLM初始化似乎特别有助于在视觉预训练数据中表现不足的领域中进行分类。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01629v1" target="_blank">2312.01629v1</a>
                              </td>
                              <td>CLAMP: Contrastive LAnguage Model Prompt-tuning</td>
                              <td>Piotr Teterwak</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01629v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01629v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01597v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01597v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01597v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01597v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in contrastive language-image pretraining (CLIP) have demonstrated strong capabilities in zero-shot classification by aligning visual representations with target text embeddings in an image level. However, in dense prediction tasks, CLIP often struggles to localize visual features within an image and fails to give accurate pixel-level predictions, which prevents it from functioning as a generalized visual foundation model. In this work, we aim to enhance CLIP's potential for semantic segmentation with minimal modifications to its pretrained models. By rethinking self-attention, we surprisingly find that CLIP can adapt to dense prediction tasks by simply introducing a novel Correlative Self-Attention (CSA) mechanism. Specifically, we replace the traditional self-attention block of CLIP vision encoder's last layer by our CSA module and reuse its pretrained projection matrices of query, key, and value, leading to a training-free adaptation approach for CLIP's zero-shot semantic segmentation. Extensive experiments show the advantage of CSA: we obtain a 38.2% average zero-shot mIoU across eight semantic segmentation benchmarks highlighted in this paper, significantly outperforming the existing SoTA's 33.9% and the vanilla CLIP's 14.1%.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01597v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对比语言图像预训练（CLIP）的最新进展表明，通过在图像级别中将视觉表示与目标文本嵌入对齐，在零样本分类中具有强大的能力。然而，在密集的预测任务中，CLIP经常难以定位图像中的视觉特征，并且无法给出准确的像素级预测，这使其无法作为通用的视觉基础模型发挥作用。在这项工作中，我们的目标是通过对其预训练模型的最小修改来增强CLIP的语义分割潜力。通过重新思考自我注意，我们惊讶地发现，CLIP可以通过简单地引入一种新的相关自我注意（CSA）机制来适应密集的预测任务。具体来说，我们用CSA模块取代了CLIP视觉编码器最后一层的传统自注意块，并重用其预先训练的查询、关键字和值的投影矩阵，从而为CLIP的零样本语义分割提供了一种无训练适配方法。广泛的实验表明了CSA的优势：我们在本文强调的八个语义分割基准中获得了38.2%的平均零样本mIoU，显著优于现有的SoTA的33.9%和普通CLIP的14.1%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01597v1" target="_blank">2312.01597v1</a>
                              </td>
                              <td>SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference</td>
                              <td>Feng Wang</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01597v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01597v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01576v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Efficient Unsupervised Satellite Image-based Building Damage Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01576v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01576v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01576v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Existing Building Damage Detection (BDD) methods always require labour-intensive pixel-level annotations of buildings and their conditions, hence largely limiting their applications. In this paper, we investigate a challenging yet practical scenario of BDD, Unsupervised Building Damage Detection (U-BDD), where only unlabelled pre- and post-disaster satellite image pairs are provided. As a pilot study, we have first proposed an advanced U-BDD baseline that leverages pre-trained vision-language foundation models (i.e., Grounding DINO, SAM and CLIP) to address the U-BDD task. However, the apparent domain gap between satellite and generic images causes low confidence in the foundation models used to identify buildings and their damages. In response, we further present a novel self-supervised framework, U-BDD++, which improves upon the U-BDD baseline by addressing domain-specific issues associated with satellite imagery. Furthermore, the new Building Proposal Generation (BPG) module and the CLIP-enabled noisy Building Proposal Selection (CLIP-BPS) module in U-BDD++ ensure high-quality self-training. Extensive experiments on the widely used building damage assessment benchmark demonstrate the effectiveness of the proposed method for unsupervised building damage detection. The presented annotation-free and foundation model-based paradigm ensures an efficient learning phase. This study opens a new direction for real-world BDD and sets a strong baseline for future research.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01576v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现有的建筑物损坏检测（BDD）方法总是需要对建筑物及其条件进行劳动密集型像素级注释，因此在很大程度上限制了其应用。在本文中，我们研究了BDD的一个具有挑战性但实用的场景，即无监督的建筑物损坏检测（U-BDD），其中只提供未标记的灾前和灾后卫星图像对。作为一项试点研究，我们首先提出了一种先进的U-BDD基线，该基线利用预先训练的视觉语言基础模型（即基础DINO、SAM和CLIP）来解决U-BDD任务。然而，卫星图像和普通图像之间明显的领域差距导致用于识别建筑物及其损坏的基础模型的置信度较低。作为回应，我们进一步提出了一个新的自我监督框架U-BDD++，该框架通过解决与卫星图像相关的特定领域问题，改进了U-BDD基线。此外，U-BDD++中新的建筑方案生成（BPG）模块和启用CLIP的嘈杂建筑方案选择（CLIP-BPS）模块确保了高质量的自我培训。在广泛使用的建筑损伤评估基准上进行的大量实验证明了所提出的无监督建筑损伤检测方法的有效性。所提出的无注释和基于基础的范式确保了有效的学习阶段。这项研究为现实世界的BDD开辟了一个新的方向，并为未来的研究奠定了坚实的基础。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01576v1" target="_blank">2312.01576v1</a>
                              </td>
                              <td>Learning Efficient Unsupervised Satellite Image-based Building Damage Detection</td>
                              <td>Yiyun Zhang</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01576v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01576v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/fzmi/ubdd" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01564v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">APoLLo: Unified Adapter and Prompt Learning for Vision Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01564v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01564v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01564v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The choice of input text prompt plays a critical role in the performance of Vision-Language Pretrained (VLP) models such as CLIP. We present APoLLo, a unified multi-modal approach that combines Adapter and Prompt learning for Vision-Language models. Our method is designed to substantially improve the generalization capabilities of VLP models when they are fine-tuned in a few-shot setting. We introduce trainable cross-attention-based adapter layers in conjunction with vision and language encoders to strengthen the alignment between the two modalities. We enforce consistency between the respective encoder branches (receiving augmented inputs) to prevent overfitting in downstream tasks. Our method is evaluated on three representative tasks: generalization to novel classes, cross-dataset evaluation, and unseen domain shifts. In practice, APoLLo achieves a relative gain up to 6.03% over MaPLe (SOTA) on novel classes for 10 diverse image recognition datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01564v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>输入文本提示的选择对CLIP等视觉语言预训练（VLP）模型的性能起着至关重要的作用。我们介绍了APoLLo，这是一种统一的多模式方法，结合了视觉语言模型的适配器和提示学习。我们的方法旨在大幅提高VLP模型的泛化能力，当它们在几个镜头设置中进行微调时。我们将可训练的基于交叉注意力的适配器层与视觉和语言编码器结合起来，以加强两种模式之间的一致性。我们加强各个编码器分支（接收增强输入）之间的一致性，以防止下游任务中的过拟合。我们的方法在三个具有代表性的任务上进行了评估：对新类的泛化、跨数据集评估和看不见的领域转移。在实践中，APoLLo在10个不同图像识别数据集的新类上实现了比MaPLe（SOTA）高达6.03%的相对增益。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01564v1" target="_blank">2312.01564v1</a>
                              </td>
                              <td>APoLLo: Unified Adapter and Prompt Learning for Vision Language Models</td>
                              <td>Sanjoy Chowdhury</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01564v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01564v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_13289v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Achieving the Minimax Optimal Sample Complexity of Offline Reinforcement Learning: A DRO-Based Approach</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_13289v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_13289v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_13289v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Offline reinforcement learning aims to learn from pre-collected datasets without active exploration. This problem faces significant challenges, including limited data availability and distributional shifts. Existing approaches adopt a pessimistic stance towards uncertainty by penalizing rewards of under-explored state-action pairs to estimate value functions conservatively. In this paper, we show that the distributionally robust optimization (DRO) based approach can also address these challenges and is minimax optimal. Specifically, we directly model the uncertainty in the transition kernel and construct an uncertainty set of statistically plausible transition kernels. We then find the policy that optimizes the worst-case performance over this uncertainty set. We first design a metric-based Hoeffding-style uncertainty set such that with high probability the true transition kernel is in this set. We prove that to achieve a sub-optimality gap of $\epsilon$, the sample complexity is $\mathcal{O}(S^2C^{\pi^*}\epsilon^{-2}(1-\gamma)^{-4})$, where $\gamma$ is the discount factor, $S$ is the number of states, and $C^{\pi^*}$ is the single-policy clipped concentrability coefficient which quantifies the distribution shift. To achieve the optimal sample complexity, we further propose a less conservative Bernstein-style uncertainty set, which, however, does not necessarily include the true transition kernel. We show that an improved sample complexity of $\mathcal{O}(SC^{\pi^*}\epsilon^{-2}(1-\gamma)^{-3})$ can be obtained, which matches with the minimax lower bound for offline reinforcement learning, and thus is minimax optimal.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_13289v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>离线强化学习旨在从预先收集的数据集中学习，而无需主动探索。这一问题面临着重大挑战，包括数据可用性有限和分布变化。现有方法对不确定性采取悲观态度，通过惩罚未充分探索的状态-动作对的奖励来保守估计值函数。在本文中，我们证明了基于分布鲁棒优化（DRO）的方法也可以解决这些挑战，并且是极小极大最优的。具体来说，我们直接对过渡核中的不确定性进行建模，并构建一个统计上合理的过渡核的不确定性集。然后，我们找到在这个不确定性集合上优化最坏情况性能的策略。我们首先设计了一个基于度量的Hoeffding式不确定性集，使得真正的转移核很有可能在这个集中。我们证明了为了实现$\epsilon$的次最优间隙，样本复杂度是$\mathcal｛O｝（S^2C^｛\pi^*｝\epsilon^｛-2｝（1-\gamma）^｛-4｝。为了实现最优样本复杂度，我们进一步提出了一个不那么保守的Bernstein风格的不确定性集，然而，它不一定包括真正的过渡核。我们证明了可以获得$\mathcal｛O｝（SC^｛\pi^*｝\epsilon ^｛-2｝（1-\gamma）^｛-3｝）$的改进样本复杂度，这与离线强化学习的极小极大下界相匹配，因此是极小极大最优的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.13289v3" target="_blank">2305.13289v3</a>
                              </td>
                              <td>Achieving the Minimax Optimal Sample Complexity of Offline Reinforcement Learning: A DRO-Based Approach</td>
                              <td>Yue Wang</td>
                              <td>2023-05-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_13289v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.13289v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01479v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">OpenVoice: Versatile Instant Voice Cloning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01479v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01479v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01479v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce OpenVoice, a versatile voice cloning approach that requires only a short audio clip from the reference speaker to replicate their voice and generate speech in multiple languages. OpenVoice represents a significant advancement in addressing the following open challenges in the field: 1) Flexible Voice Style Control. OpenVoice enables granular control over voice styles, including emotion, accent, rhythm, pauses, and intonation, in addition to replicating the tone color of the reference speaker. The voice styles are not directly copied from and constrained by the style of the reference speaker. Previous approaches lacked the ability to flexibly manipulate voice styles after cloning. 2) Zero-Shot Cross-Lingual Voice Cloning. OpenVoice achieves zero-shot cross-lingual voice cloning for languages not included in the massive-speaker training set. Unlike previous approaches, which typically require extensive massive-speaker multi-lingual (MSML) dataset for all languages, OpenVoice can clone voices into a new language without any massive-speaker training data for that language. OpenVoice is also computationally efficient, costing tens of times less than commercially available APIs that offer even inferior performance. To foster further research in the field, we have made the source code and trained model publicly accessible. We also provide qualitative results in our demo website. Prior to its public release, our internal version of OpenVoice was used tens of millions of times by users worldwide between May and October 2023, serving as the backend of MyShell.ai.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01479v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了OpenVoice，这是一种通用的语音克隆方法，只需要参考说话者的一小段音频片段就可以复制他们的语音并生成多种语言的语音。OpenVoice在解决该领域的以下公开挑战方面取得了重大进展：1）灵活的语音风格控制。OpenVoice除了复制参考说话者的音色外，还可以对语音风格进行精细控制，包括情绪、重音、节奏、停顿和语调。语音样式不会直接从参考说话者的样式复制并受其约束。以前的方法在克隆后缺乏灵活操纵声音风格的能力。2） 零样本跨语言语音克隆。OpenVoice为未包含在大量说话者训练集中的语言实现了零样本跨语言语音克隆。与以前的方法不同，以前的方法通常需要针对所有语言的大量多语言说话者（MSML）数据集，OpenVoice可以将语音克隆到一种新语言中，而无需针对该语言的任何大量说话者训练数据。OpenVoice的计算效率也很高，其成本比提供较差性能的商用API低几十倍。为了促进该领域的进一步研究，我们已经公开了源代码和训练模型。我们还在我们的演示网站上提供定性结果。在公开发布之前，我们的内部版本OpenVoice在2023年5月至10月期间被全球用户使用了数千万次，作为MyShell.ai的后端。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01479v1" target="_blank">2312.01479v1</a>
                              </td>
                              <td>OpenVoice: Versatile Instant Voice Cloning</td>
                              <td>Zengyi Qin</td>
                              <td>2023-12-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01479v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01479v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_11300v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_11300v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_11300v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_11300v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Pre-trained Vision-Language Models (VLMs) utilizing extensive image-text paired data have demonstrated unprecedented image-text association capabilities, achieving remarkable results across various downstream tasks. A critical challenge is how to make use of existing large-scale pre-trained VLMs, which are trained on common objects, to perform the domain-specific transfer for accomplishing domain-related downstream tasks. A critical challenge is how to make use of existing large-scale pre-trained VLMs, which are trained on common objects, to perform the domain-specific transfer for accomplishing domain-related downstream tasks. In this paper, we propose a new framework that includes the Domain pre-trained Vision-Language Model (DVLM), bridging the gap between the General Vision-Language Model (GVLM) and domain-specific downstream tasks. Moreover, we present an image-text paired dataset in the field of remote sensing (RS), RS5M, which has 5 million RS images with English descriptions. The dataset is obtained from filtering publicly available image-text paired datasets and captioning label-only RS datasets with pre-trained VLM. These constitute the first large-scale RS image-text paired dataset. Additionally, we fine-tuned the CLIP model and tried several Parameter-Efficient Fine-Tuning methods on RS5M to implement the DVLM. Experimental results show that our proposed dataset is highly effective for various tasks, and our model GeoRSCLIP improves upon the baseline or previous state-of-the-art model by $3\%\sim20\%$ in Zero-shot Classification (ZSC), $3\%\sim6\%$ in Remote Sensing Cross-Modal Text-Image Retrieval (RSCTIR) and $4\%\sim5\%$ in Semantic Localization (SeLo) tasks. Dataset and models have been released in: \url{https://github.com/om-ai-lab/RS5M}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_11300v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>利用大量图像-文本配对数据的预训练视觉语言模型（VLM）展示了前所未有的图像-文本关联能力，在各种下游任务中取得了显著的结果。一个关键的挑战是如何利用现有的大规模预训练VLM（在公共对象上训练）来执行特定领域的转移，以完成与领域相关的下游任务。一个关键的挑战是如何利用现有的大规模预训练VLM（在公共对象上训练）来执行特定领域的转移，以完成与领域相关的下游任务。在本文中，我们提出了一个新的框架，其中包括领域预训练视觉语言模型（DVLM），弥合了通用视觉语言模型和特定领域下游任务之间的差距。此外，我们还提出了遥感领域的图像-文本配对数据集RS5M，该数据集有500万张英文描述的遥感图像。该数据集是通过过滤公开可用的图像-文本配对数据集和使用预训练的VLM进行字幕标记的仅RS数据集而获得的。这些构成了第一个大规模的RS图像-文本配对数据集。此外，我们对CLIP模型进行了微调，并在RS5M上尝试了几种参数高效微调方法来实现DVLM。实验结果表明，我们提出的数据集对于各种任务都是非常有效的，并且我们的模型GeoRSCLIP在零样本分类（ZSC）中比基线或先前最先进的模型改进了$3\%\sim20\%$，在遥感跨模式文本图像检索（RSCTIR）中改进了$3\\%\sim6\%$和在语义定位（SeLo）任务中改进了$4\%\%\sim5\%$。数据集和模型已在以下位置发布：\url{https://github.com/om-ai-lab/rs5m}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.11300v3" target="_blank">2306.11300v3</a>
                              </td>
                              <td>RS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model</td>
                              <td>Zilun Zhang</td>
                              <td>2023-06-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_11300v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.11300v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/om-ai-lab/rs5m" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_18961v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_18961v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_18961v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_18961v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Zero-shot anomaly detection (ZSAD) requires detection models trained using auxiliary data to detect anomalies without any training sample in a target dataset. It is a crucial task when training data is not accessible due to various concerns, \eg, data privacy, yet it is challenging since the models need to generalize to anomalies across different domains where the appearance of foreground objects, abnormal regions, and background features, such as defects/tumors on different products/organs, can vary significantly. Recently large pre-trained vision-language models (VLMs), such as CLIP, have demonstrated strong zero-shot recognition ability in various vision tasks, including anomaly detection. However, their ZSAD performance is weak since the VLMs focus more on modeling the class semantics of the foreground objects rather than the abnormality/normality in the images. In this paper we introduce a novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across different domains. The key insight of AnomalyCLIP is to learn object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects. This allows our model to focus on the abnormal image regions rather than the object semantics, enabling generalized normality and abnormality recognition on diverse types of objects. Large-scale experiments on 17 real-world anomaly detection datasets show that AnomalyCLIP achieves superior zero-shot performance of detecting and segmenting anomalies in datasets of highly diverse class semantics from various defect inspection and medical imaging domains. Code will be made available at https://github.com/zqhang/AnomalyCLIP.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_18961v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>零样本异常检测（ZSAD）需要使用辅助数据训练的检测模型来检测异常，而无需在目标数据集中使用任何训练样本。当由于各种问题（例如数据隐私）而无法访问训练数据时，这是一项至关重要的任务，但这是一个挑战，因为模型需要推广到不同领域的异常，其中前景对象、异常区域和背景特征的外观，如不同产品/器官上的缺陷/肿瘤，可能会有很大差异。最近，大型预先训练的视觉语言模型（VLM），如CLIP，在包括异常检测在内的各种视觉任务中表现出强大的零样本识别能力。然而，它们的ZSAD性能较弱，因为VLM更侧重于对前景对象的类语义建模，而不是对图像中的异常/正常性建模。在本文中，我们介绍了一种新的方法，即AnomalyCLIP，以使CLIP适应不同领域的精确ZSAD。AnomalyCLIP的关键见解是学习对象不可知的文本提示，无论图像的前景对象如何，都可以捕捉图像中的一般正常和异常。这使我们的模型能够专注于异常图像区域，而不是对象语义，从而实现对不同类型对象的广义正态和异常识别。在17个真实世界异常检测数据集上进行的大规模实验表明，AnomalyCLIP在来自各种缺陷检测和医学成像领域的具有高度不同类别语义的数据集中实现了检测和分割异常的优异零样本性能。代码将在提供https://github.com/zqhang/anomalyclip.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.18961v3" target="_blank">2310.18961v3</a>
                              </td>
                              <td>AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection</td>
                              <td>Qihang Zhou</td>
                              <td>2023-10-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_18961v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.18961v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zqhang/anomalyclip" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01260v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rethinking PGD Attack: Is Sign Function Necessary?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01260v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01260v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01260v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural networks have demonstrated success in various domains, yet their performance can be significantly degraded by even a small input perturbation. Consequently, the construction of such perturbations, known as adversarial attacks, has gained significant attention, many of which fall within "white-box" scenarios where we have full access to the neural network. Existing attack algorithms, such as the projected gradient descent (PGD), commonly take the sign function on the raw gradient before updating adversarial inputs, thereby neglecting gradient magnitude information. In this paper, we present a theoretical analysis of how such sign-based update algorithm influences step-wise attack performance, as well as its caveat. We also interpret why previous attempts of directly using raw gradients failed. Based on that, we further propose a new raw gradient descent (RGD) algorithm that eliminates the use of sign. Specifically, we convert the constrained optimization problem into an unconstrained one, by introducing a new hidden variable of non-clipped perturbation that can move beyond the constraint. The effectiveness of the proposed RGD algorithm has been demonstrated extensively in experiments, outperforming PGD and other competitors in various settings, without incurring any additional computational overhead. The codes is available in https://github.com/JunjieYang97/RGD.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01260v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经网络在各个领域都取得了成功，但即使是很小的输入扰动也会显著降低其性能。因此，这种被称为对抗性攻击的扰动的构建得到了极大的关注，其中许多属于我们可以完全访问神经网络的“白盒”场景。现有的攻击算法，如投影梯度下降（PGD），通常在更新对抗性输入之前在原始梯度上取符号函数，从而忽略梯度幅度信息。在本文中，我们对这种基于符号的更新算法如何影响逐步攻击性能进行了理论分析，并提出了警告。我们还解释了为什么以前直接使用原始梯度的尝试失败了。在此基础上，我们进一步提出了一种新的原始梯度下降（RGD）算法，该算法消除了符号的使用。具体来说，我们通过引入一个新的非截断扰动隐变量，将约束优化问题转化为无约束优化问题，该隐变量可以超越约束。所提出的RGD算法的有效性已在实验中得到广泛证明，在各种设置下都优于PGD和其他竞争对手，而不会产生任何额外的计算开销。代码在中提供https://github.com/junjieyang97/rgd.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01260v1" target="_blank">2312.01260v1</a>
                              </td>
                              <td>Rethinking PGD Attack: Is Sign Function Necessary?</td>
                              <td>Junjie Yang</td>
                              <td>2023-12-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01260v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01260v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/junjieyang97/rgd" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_00119v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fewshot learning on global multimodal embeddings for earth observation tasks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_00119v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_00119v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_00119v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this work we pretrain a CLIP/ViT based model using three different modalities of satellite imagery across five AOIs covering over ~10\% of Earth's total landmass, namely Sentinel 2 RGB optical imagery, Sentinel 1 SAR radar amplitude and interferometric coherence. This model uses $\sim 250$ M parameters. Then, we use the embeddings produced for each modality with a classical machine learning method to attempt different downstream tasks for earth observation related to vegetation, built up surface, croplands and permanent water. We consistently show how we reduce the need for labeled data by 99\%, so that with ~200-500 randomly selected labeled examples (around 4K-10K km$^2$) we reach performance levels analogous to those achieved with the full labeled datasets (about 150K image chips or 3M km$^2$ in each area of interest - AOI) on all modalities, AOIs and downstream tasks. This leads us to think that the model has captured significant earth features useful in a wide variety of scenarios. To enhance our model's usability in practice, its architecture allows inference in contexts with missing modalities and even missing channels within each modality. Additionally, we visually show that this embedding space, obtained with no labels, is sensible to the different earth features represented by the labelled datasets we selected.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_00119v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项工作中，我们使用覆盖地球总陆地质量约10%的五个AOI的三种不同的卫星图像模式，即Sentinel 2 RGB光学图像、Sentinel 1 SAR雷达振幅和干涉相干性，对基于CLIP/ViT的模型进行了预训练。此模型使用$\sim 250$M参数。然后，我们使用经典机器学习方法为每种模式生成的嵌入，尝试与植被、堆积表面、农田和永久水相关的地球观测的不同下游任务。我们始终如一地展示了我们如何将对标记数据的需求减少99%，从而通过约200-500个随机选择的标记示例（约4K-10K km$^2$），我们在所有模态、AOI和下游任务上达到了与全标记数据集（约150K图像芯片或每个感兴趣区域的3M km$^2*）类似的性能水平。这让我们认为，该模型捕捉到了在各种场景中有用的重要地球特征。为了增强我们模型在实践中的可用性，其架构允许在每个模态中缺失模态甚至缺失通道的情况下进行推理。此外，我们还直观地表明，这种在没有标签的情况下获得的嵌入空间对我们选择的标记数据集所代表的不同地球特征是敏感的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.00119v2" target="_blank">2310.00119v2</a>
                              </td>
                              <td>Fewshot learning on global multimodal embeddings for earth observation tasks</td>
                              <td>Matt Allen</td>
                              <td>2023-09-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_00119v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.00119v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01163v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A New Learning Paradigm for Foundation Model-based Remote Sensing Change Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01163v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01163v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01163v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Change detection (CD) is a critical task to observe and analyze dynamic processes of land cover. Although numerous deep learning-based CD models have performed excellently, their further performance improvements are constrained by the limited knowledge extracted from the given labelled data. On the other hand, the foundation models that emerged recently contain a huge amount of knowledge by scaling up across data modalities and proxy tasks. In this paper, we propose a Bi-Temporal Adapter Network (BAN), which is a universal foundation model-based CD adaptation framework aiming to extract the knowledge of foundation models for CD. The proposed BAN contains three parts, i.e. frozen foundation model (e.g., CLIP), bitemporal adapter branch (Bi-TAB), and bridging modules between them. Specifically, the Bi-TAB can be either an existing arbitrary CD model or some hand-crafted stacked blocks. The bridging modules are designed to align the general features with the task/domain-specific features and inject the selected general knowledge into the Bi-TAB. To our knowledge, this is the first universal framework to adapt the foundation model to the CD task. Extensive experiments show the effectiveness of our BAN in improving the performance of existing CD methods (e.g., up to 4.08\% IoU improvement) with only a few additional learnable parameters. More importantly, these successful practices show us the potential of foundation models for remote sensing CD. The code is available at \url{https://github.com/likyoo/BAN} and will be supported in our Open-CD \url{https://github.com/likyoo/open-cd}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01163v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>变化检测（CD）是观测和分析土地覆盖动态过程的一项关键任务。尽管许多基于深度学习的CD模型表现出色，但它们的进一步性能改进受到从给定标记数据中提取的有限知识的限制。另一方面，最近出现的基础模型通过扩展数据模式和代理任务，包含了大量知识。在本文中，我们提出了一个双时态适配器网络（BAN），这是一个通用的基于基础模型的CD自适应框架，旨在提取CD的基础模型知识。所提出的BAN包括三个部分，即冻结基础模型（如CLIP）、双时态适配器分支（Bi-TAB）以及它们之间的桥接模块。具体来说，Bi-TAB可以是现有的任意CD模型，也可以是一些手工制作的堆叠块。桥接模块被设计为将一般特征与任务/领域特定特征对齐，并将所选的一般知识注入到Bi-TAB中。据我们所知，这是使基础模式适应裁谈会任务的第一个普遍框架。大量实验表明，我们的BAN在仅使用几个额外的可学习参数的情况下，就可以有效地提高现有CD方法的性能（例如，高达4.08\%IoU的改进）。更重要的是，这些成功的实践向我们展示了遥感CD基础模型的潜力{https://github.com/likyoo/ban}并且将在我们的Open CD\url中得到支持{https://github.com/likyoo/open-cd}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01163v1" target="_blank">2312.01163v1</a>
                              </td>
                              <td>A New Learning Paradigm for Foundation Model-based Remote Sensing Change Detection</td>
                              <td>Kaiyu Li</td>
                              <td>2023-12-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01163v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01163v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/likyoo/BAN" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/likyoo/open-cd" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01129v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ControlDreamer: Stylized 3D Generation with Multi-View ControlNet</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01129v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01129v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01129v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advancements in text-to-3D generation have significantly contributed to the automation and democratization of 3D content creation. Building upon these developments, we aim to address the limitations of current methods in generating 3D models with creative geometry and styles. We introduce multi-view ControlNet, a novel depth-aware multi-view diffusion model trained on generated datasets from a carefully curated 100K text corpus. Our multi-view ControlNet is then integrated into our two-stage pipeline, ControlDreamer, enabling text-guided generation of stylized 3D models. Additionally, we present a comprehensive benchmark for 3D style editing, encompassing a broad range of subjects, including objects, animals, and characters, to further facilitate diverse 3D generation. Our comparative analysis reveals that this new pipeline outperforms existing text-to-3D methods as evidenced by qualitative comparisons and CLIP score metrics.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01129v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文本到3D生成的最新进展对3D内容创建的自动化和民主化做出了重大贡献。在这些发展的基础上，我们旨在解决当前方法在生成具有创造性几何形状和样式的三维模型方面的局限性。我们介绍了多视图ControlNet，这是一种新颖的深度感知多视图扩散模型，在精心策划的100K文本语料库中生成的数据集上进行训练。然后，我们的多视图ControlNet集成到我们的两阶段管道ControlDreamer中，实现了以文本为导向的风格化3D模型生成。此外，我们为3D风格编辑提供了一个全面的基准，涵盖了广泛的主题，包括物体、动物和角色，以进一步促进多样化的3D生成。我们的比较分析表明，这种新的管道优于现有的文本到3D方法，定性比较和CLIP得分指标证明了这一点。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01129v1" target="_blank">2312.01129v1</a>
                              </td>
                              <td>ControlDreamer: Stylized 3D Generation with Multi-View ControlNet</td>
                              <td>Yeongtak Oh</td>
                              <td>2023-12-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01129v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01129v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/oyt9306/ControlDreamer" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01083v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Consistency Prototype Module and Motion Compensation for Few-Shot Action Recognition (CLIP-CP$\mathbf{M^2}$C)</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01083v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01083v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01083v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, few-shot action recognition has significantly progressed by learning the feature discriminability and designing suitable comparison methods. Still, there are the following restrictions. (a) Previous works are mainly based on visual mono-modal. Although some multi-modal works use labels as supplementary to construct prototypes of support videos, they can not use this information for query videos. The labels are not used efficiently. (b) Most of the works ignore the motion feature of video, although the motion features are essential for distinguishing. We proposed a Consistency Prototype and Motion Compensation Network(CLIP-CP$M^2$C) to address these issues. Firstly, we use the CLIP for multi-modal few-shot action recognition with the text-image comparison for domain adaption. Secondly, in order to make the amount of information between the prototype and the query more similar, we propose a novel method to compensate for the text(prompt) information of query videos when text(prompt) does not exist, which depends on a Consistency Loss. Thirdly, we use the differential features of the adjacent frames in two directions as the motion features, which explicitly embeds the network with motion dynamics. We also apply the Consistency Loss to the motion features. Extensive experiments on standard benchmark datasets demonstrate that the proposed method can compete with state-of-the-art results. Our code is available at the URL: https://github.com/xxx/xxx.git.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01083v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，通过学习特征可分辨性和设计合适的比较方法，少镜头动作识别取得了显著进展。尽管如此，还是有以下限制。（a） 以前的作品主要是基于视觉单模态的。尽管一些多模态作品使用标签作为补充来构建支持视频的原型，但它们不能将这些信息用于查询视频。标签使用效率不高。（b） 大多数作品忽略了视频的运动特征，尽管运动特征是区分视频的关键。我们提出了一个一致性原型和运动补偿网络（CLIP-CP$M^2$C）来解决这些问题。首先，我们使用CLIP进行多模态少镜头动作识别，并将文本图像比较用于域自适应。其次，为了使原型和查询之间的信息量更加相似，我们提出了一种新的方法，在不存在文本（提示）的情况下，补偿查询视频的文本（提醒）信息，这取决于一致性损失。第三，我们使用相邻帧在两个方向上的差分特征作为运动特征，这明确地嵌入了具有运动动力学的网络。我们还将一致性损失应用于运动特征。在标准基准数据集上进行的大量实验表明，所提出的方法可以与最先进的结果相竞争。我们的代码位于以下网址：https://github.com/xxx/xxx.git.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01083v1" target="_blank">2312.01083v1</a>
                              </td>
                              <td>Consistency Prototype Module and Motion Compensation for Few-Shot Action Recognition (CLIP-CP$\mathbf{M^2}$C)</td>
                              <td>Fei Guo</td>
                              <td>2023-12-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01083v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01083v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_13843v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CompoNeRF: Text-guided Multi-object Compositional NeRF with Editable 3D Scene Layout</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_13843v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_13843v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_13843v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances have shown promise in merging neural radiance fields (NeRFs) with pre-trained diffusion models for text-to-3D object generation. However, one enduring challenge is their inadequate capability to accurately parse and regenerate consistent multi-object environments. Specifically, these models encounter difficulties in accurately representing quantity and style prompted by multi-object texts, often resulting in a collapse of the rendering fidelity that fails to match the semantic intricacies. Moreover, amalgamating these elements into a coherent 3D scene is a substantial challenge, stemming from generic distribution inherent in diffusion models. To tackle the issue of 'guidance collapse' and enhance consistency, we propose a novel framework, dubbed CompoNeRF, by integrating an editable 3D scene layout with object specific and scene-wide guidance mechanisms. It initiates by interpreting a complex text into an editable 3D layout populated with multiple NeRFs, each paired with a corresponding subtext prompt for precise object depiction. Next, a tailored composition module seamlessly blends these NeRFs, promoting consistency, while the dual-level text guidance reduces ambiguity and boosts accuracy. Noticeably, the unique modularity of CompoNeRF permits NeRF decomposition. This enables flexible scene editing and recomposition into new scenes based on the edited layout or text prompts. Utilizing the open source Stable Diffusion model, CompoNeRF not only generates scenes with high fidelity but also paves the way for innovative multi-object composition using editable 3D layouts. Remarkably, our framework achieves up to a 54\% improvement in performance, as measured by the multi-view CLIP score metric. Code is available at https://github.com/hbai98/Componerf.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_13843v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近的进展显示出将神经辐射场（NeRF）与预先训练的扩散模型相结合以生成文本到三维对象的前景。然而，一个持久的挑战是它们不足以准确解析和重新生成一致的多对象环境。具体而言，这些模型在准确表示多对象文本提示的数量和风格方面遇到了困难，通常会导致渲染保真度的崩溃，无法与复杂的语义相匹配。此外，将这些元素合并到连贯的3D场景中是一个巨大的挑战，这源于扩散模型中固有的一般分布。为了解决“制导崩溃”的问题并增强一致性，我们提出了一种新的框架，称为CompoNeRF，通过将可编辑的3D场景布局与特定对象和全场景的制导机制相结合。它通过将复杂的文本解释为可编辑的3D布局来启动，该布局由多个NeRF填充，每个NeRF与对应的子文本提示配对，用于精确的对象描述。接下来，一个量身定制的合成模块无缝地融合了这些NeRF，提高了一致性，而双层文本指导减少了歧义，提高了准确性。值得注意的是，CompoNeRF独特的模块性允许NeRF分解。这允许灵活的场景编辑，并根据编辑后的布局或文本提示重新组合到新场景中。利用开源的稳定扩散模型，CompoNeRF不仅可以生成高保真的场景，还为使用可编辑的3D布局进行创新的多对象合成铺平了道路。值得注意的是，根据多视图CLIP评分指标，我们的框架在性能上实现了高达54%的改进。代码位于https://github.com/hbai98/componerf.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.13843v3" target="_blank">2303.13843v3</a>
                              </td>
                              <td>CompoNeRF: Text-guided Multi-object Compositional NeRF with Editable 3D Scene Layout</td>
                              <td>Haotian Bai</td>
                              <td>2023-03-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_13843v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.13843v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_19518v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Label-Retrieval-Augmented Diffusion Models for Learning from Noisy Labels</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_19518v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_19518v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_19518v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Learning from noisy labels is an important and long-standing problem in machine learning for real applications. One of the main research lines focuses on learning a label corrector to purify potential noisy labels. However, these methods typically rely on strict assumptions and are limited to certain types of label noise. In this paper, we reformulate the label-noise problem from a generative-model perspective, $\textit{i.e.}$, labels are generated by gradually refining an initial random guess. This new perspective immediately enables existing powerful diffusion models to seamlessly learn the stochastic generative process. Once the generative uncertainty is modeled, we can perform classification inference using maximum likelihood estimation of labels. To mitigate the impact of noisy labels, we propose the $\textbf{L}$abel-$\textbf{R}$etrieval-$\textbf{A}$ugmented (LRA) diffusion model, which leverages neighbor consistency to effectively construct pseudo-clean labels for diffusion training. Our model is flexible and general, allowing easy incorporation of different types of conditional information, $\textit{e.g.}$, use of pre-trained models, to further boost model performance. Extensive experiments are conducted for evaluation. Our model achieves new state-of-the-art (SOTA) results on all the standard real-world benchmark datasets. Remarkably, by incorporating conditional information from the powerful CLIP model, our method can boost the current SOTA accuracy by 10-20 absolute points in many cases.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_19518v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在实际应用的机器学习中，从噪声标签中学习是一个重要且长期存在的问题。主要的研究方向之一是学习标签校正器来净化潜在的噪声标签。然而，这些方法通常依赖于严格的假设，并且仅限于某些类型的标签噪声。在本文中，我们从生成模型的角度重新表述了标签噪声问题，$\textit｛即｝$，标签是通过逐步细化初始随机猜测来生成的。这种新的视角立即使现有强大的扩散模型能够无缝地学习随机生成过程。一旦生成不确定性被建模，我们就可以使用标签的最大似然估计来执行分类推理。为了减轻噪声标签的影响，我们提出了$\textbf｛L｝$abel-$\textbf｛R｝$etreval-$\textbf｛A｝$ugeded（LRA）扩散模型，该模型利用邻居一致性来有效地构建用于扩散训练的伪干净标签。我们的模型是灵活和通用的，允许轻松地合并不同类型的条件信息，$\textit｛例如｝$，使用预先训练的模型，以进一步提高模型性能。进行了广泛的实验进行评估。我们的模型在所有标准的真实世界基准数据集上实现了最先进的（SOTA）结果。值得注意的是，通过结合强大的CLIP模型的条件信息，我们的方法在许多情况下可以将当前的SOTA精度提高10-20个绝对点。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.19518v2" target="_blank">2305.19518v2</a>
                              </td>
                              <td>Label-Retrieval-Augmented Diffusion Models for Learning from Noisy Labels</td>
                              <td>Jian Chen</td>
                              <td>2023-05-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_19518v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.19518v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/puar-playground/lra-diffusion" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00971v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Consistent Mesh Diffusion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00971v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00971v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00971v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Given a 3D mesh with a UV parameterization, we introduce a novel approach to generating textures from text prompts. While prior work uses optimization from Text-to-Image Diffusion models to generate textures and geometry, this is slow and requires significant compute resources. Alternatively, there are projection based approaches that use the same Text-to-Image models that paint images onto a mesh, but lack consistency at different viewing angles, we propose a method that uses a single Depth-to-Image diffusion network, and generates a single consistent texture when rendered on the 3D surface by first unifying multiple 2D image's diffusion paths, and hoisting that to 3D with MultiDiffusion~\cite{multidiffusion}. We demonstrate our approach on a dataset containing 30 meshes, taking approximately 5 minutes per mesh. To evaluate the quality of our approach, we use CLIP-score~\cite{clipscore} and Frechet Inception Distance (FID)~\cite{frechet} to evaluate the quality of the rendering, and show our improvement over prior work.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00971v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>给定一个具有UV参数化的3D网格，我们介绍了一种从文本提示生成纹理的新方法。虽然先前的工作使用从文本到图像扩散模型的优化来生成纹理和几何体，但这是缓慢的，并且需要大量的计算资源。或者，有一些基于投影的方法使用相同的文本到图像模型将图像绘制到网格上，但在不同视角下缺乏一致性。我们提出了一种方法，该方法使用单个深度到图像扩散网络，并通过首先统一多个2D图像的扩散路径在3D表面上渲染时生成单个一致的纹理，并用MultiDiffusion~\cite{MultiDiffusion}将其提升到3D。我们在包含30个网格的数据集上演示了我们的方法，每个网格大约需要5分钟。为了评估我们的方法的质量，我们使用CLIP score ~\cite{clipcore}和Frechet Inception Distance（FID）~\cite{Frechet}来评估渲染的质量，并展示我们对先前工作的改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00971v1" target="_blank">2312.00971v1</a>
                              </td>
                              <td>Consistent Mesh Diffusion</td>
                              <td>Julian Knodt</td>
                              <td>2023-12-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00971v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00971v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00858v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DeepCache: Accelerating Diffusion Models for Free</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00858v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00858v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00858v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Diffusion models have recently gained unprecedented attention in the field of image synthesis due to their remarkable generative capabilities. Notwithstanding their prowess, these models often incur substantial computational costs, primarily attributed to the sequential denoising process and cumbersome model size. Traditional methods for compressing diffusion models typically involve extensive retraining, presenting cost and feasibility challenges. In this paper, we introduce DeepCache, a novel training-free paradigm that accelerates diffusion models from the perspective of model architecture. DeepCache capitalizes on the inherent temporal redundancy observed in the sequential denoising steps of diffusion models, which caches and retrieves features across adjacent denoising stages, thereby curtailing redundant computations. Utilizing the property of the U-Net, we reuse the high-level features while updating the low-level features in a very cheap way. This innovative strategy, in turn, enables a speedup factor of 2.3$\times$ for Stable Diffusion v1.5 with only a 0.05 decline in CLIP Score, and 4.1$\times$ for LDM-4-G with a slight decrease of 0.22 in FID on ImageNet. Our experiments also demonstrate DeepCache's superiority over existing pruning and distillation methods that necessitate retraining and its compatibility with current sampling techniques. Furthermore, we find that under the same throughput, DeepCache effectively achieves comparable or even marginally improved results with DDIM or PLMS. The code is available at https://github.com/horseee/DeepCache</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00858v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>扩散模型由于其卓越的生成能力，近年来在图像合成领域获得了前所未有的关注。尽管这些模型很强大，但它们往往会产生巨大的计算成本，主要归因于顺序去噪过程和繁琐的模型大小。压缩扩散模型的传统方法通常涉及广泛的再培训，这带来了成本和可行性方面的挑战。在本文中，我们介绍了DeepCache，这是一种新的无训练范式，从模型架构的角度加速扩散模型。DeepCache利用了在扩散模型的顺序去噪步骤中观察到的固有时间冗余，在相邻的去噪阶段缓存和检索特征，从而减少了冗余计算。利用U-Net的特性，我们重用高级特征，同时以非常廉价的方式更新低级特征。反过来，这种创新策略使Stable Diffusion v1.5的加速因子为2.3$\times$，CLIP得分仅下降0.05，而LDM-4-G的加速因子则为4.1$\times$，ImageNet上的FID略有下降0.22。我们的实验还证明了DeepCache优于需要重新训练的现有修剪和蒸馏方法，以及它与当前采样技术的兼容性。此外，我们发现，在相同的吞吐量下，DeepCache有效地实现了与DDIM或PLMS相当甚至略有改善的结果。代码位于https://github.com/horseee/deepcache</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00858v1" target="_blank">2312.00858v1</a>
                              </td>
                              <td>DeepCache: Accelerating Diffusion Models for Free</td>
                              <td>Xinyin Ma</td>
                              <td>2023-12-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00858v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00858v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/horseee/DeepCache" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/horseee/deepcache" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00674v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LightCLIP: Learning Multi-Level Interaction for Lightweight Vision-Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00674v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00674v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00674v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision-language pre-training like CLIP has shown promising performance on various downstream tasks such as zero-shot image classification and image-text retrieval. Most of the existing CLIP-alike works usually adopt relatively large image encoders like ResNet50 and ViT, while the lightweight counterparts are rarely discussed. In this paper, we propose a multi-level interaction paradigm for training lightweight CLIP models. Firstly, to mitigate the problem that some image-text pairs are not strictly one-to-one correspondence, we improve the conventional global instance-level alignment objective by softening the label of negative samples progressively. Secondly, a relaxed bipartite matching based token-level alignment objective is introduced for finer-grained alignment between image patches and textual words. Moreover, based on the observation that the accuracy of CLIP model does not increase correspondingly as the parameters of text encoder increase, an extra objective of masked language modeling (MLM) is leveraged for maximizing the potential of the shortened text encoder. In practice, an auxiliary fusion module injecting unmasked image embedding into masked text embedding at different network stages is proposed for enhancing the MLM. Extensive experiments show that without introducing additional computational cost during inference, the proposed method achieves a higher performance on multiple downstream tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00674v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>像CLIP这样的视觉语言预训练在各种下游任务上表现出了良好的性能，如零样本图像分类和图像文本检索。大多数现有的类似CLIP的作品通常采用相对较大的图像编码器，如ResNet50和ViT，而轻量级的编码器很少被讨论。在本文中，我们提出了一种用于训练轻量级CLIP模型的多层次交互范式。首先，为了解决某些图像-文本对不是严格一一对应的问题，我们通过逐步软化负样本的标签来改进传统的全局实例级对齐目标。其次，引入了一种基于松弛二分匹配的标记级对齐目标，用于图像块和文本单词之间的细粒度对齐。此外，基于CLIP模型的精度不会随着文本编码器参数的增加而相应增加的观察结果，利用掩蔽语言建模（MLM）的额外目标来最大化缩短的文本编码器的潜力。在实践中，为了增强MLM，提出了一种在不同网络阶段将未掩蔽图像嵌入到掩蔽文本嵌入中的辅助融合模块。大量实验表明，在推理过程中不引入额外的计算成本的情况下，该方法在多个下游任务上实现了更高的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00674v1" target="_blank">2312.00674v1</a>
                              </td>
                              <td>LightCLIP: Learning Multi-Level Interaction for Lightweight Vision-Language Models</td>
                              <td>Ying Nie</td>
                              <td>2023-12-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00674v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00674v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00364v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Benchmarking Multi-Domain Active Learning on Image Classification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00364v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00364v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00364v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Active learning aims to enhance model performance by strategically labeling informative data points. While extensively studied, its effectiveness on large-scale, real-world datasets remains underexplored. Existing research primarily focuses on single-source data, ignoring the multi-domain nature of real-world data. We introduce a multi-domain active learning benchmark to bridge this gap. Our benchmark demonstrates that traditional single-domain active learning strategies are often less effective than random selection in multi-domain scenarios. We also introduce CLIP-GeoYFCC, a novel large-scale image dataset built around geographical domains, in contrast to existing genre-based domain datasets. Analysis on our benchmark shows that all multi-domain strategies exhibit significant tradeoffs, with no strategy outperforming across all datasets or all metrics, emphasizing the need for future research.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00364v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>主动学习旨在通过战略性地标记信息数据点来提高模型性能。尽管进行了广泛的研究，但其在大规模真实世界数据集上的有效性仍有待充分探索。现有研究主要关注单一来源的数据，忽略了真实世界数据的多领域性质。我们引入了一个多领域主动学习基准来弥补这一差距。我们的基准测试表明，在多领域场景中，传统的单领域主动学习策略往往不如随机选择有效。我们还介绍了CLIP-GeoYFCC，这是一种围绕地理域构建的新型大规模图像数据集，与现有的基于类型的域数据集形成对比。对我们的基准测试的分析表明，所有多领域策略都表现出显著的权衡，没有任何策略在所有数据集或所有指标上都表现出色，这强调了未来研究的必要性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00364v1" target="_blank">2312.00364v1</a>
                              </td>
                              <td>Benchmarking Multi-Domain Active Learning on Image Classification</td>
                              <td>Jiayi Li</td>
                              <td>2023-12-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00364v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00364v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00351v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Manipulating the Label Space for In-Context Classification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00351v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00351v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00351v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>After pre-training by generating the next word conditional on previous words, the Language Model (LM) acquires the ability of In-Context Learning (ICL) that can learn a new task conditional on the context of the given in-context examples (ICEs). Similarly, visually-conditioned Language Modelling is also used to train Vision-Language Models (VLMs) with ICL ability. However, such VLMs typically exhibit weaker classification abilities compared to contrastive learning-based models like CLIP, since the Language Modelling objective does not directly contrast whether an object is paired with a text. To improve the ICL of classification, using more ICEs to provide more knowledge is a straightforward way. However, this may largely increase the selection time, and more importantly, the inclusion of additional in-context images tends to extend the length of the in-context sequence beyond the processing capacity of a VLM. To alleviate these limitations, we propose to manipulate the label space of each ICE to increase its knowledge density, allowing for fewer ICEs to convey as much information as a larger set would. Specifically, we propose two strategies which are Label Distribution Enhancement and Visual Descriptions Enhancement to improve In-context classification performance on diverse datasets, including the classic ImageNet and more fine-grained datasets like CUB-200. Specifically, using our approach on ImageNet, we increase accuracy from 74.70\% in a 4-shot setting to 76.21\% with just 2 shots. surpassing CLIP by 0.67\%. On CUB-200, our method raises 1-shot accuracy from 48.86\% to 69.05\%, 12.15\% higher than CLIP. The code is given in https://anonymous.4open.science/r/MLS_ICC.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00351v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在通过生成以先前单词为条件的下一个单词进行预训练之后，语言模型（LM）获得了上下文内学习（ICL）的能力，该能力可以学习以给定上下文内示例（ICE）的上下文为条件的新任务。同样，视觉条件语言建模也用于训练具有ICL能力的视觉语言模型。然而，与CLIP等基于对比学习的模型相比，这种VLM通常表现出较弱的分类能力，因为语言建模目标不会直接对比对象是否与文本配对。为了提高分类的ICL，使用更多的ICEs来提供更多的知识是一种简单的方法。然而，这可能在很大程度上增加选择时间，更重要的是，包含额外的上下文内图像往往会将上下文内序列的长度扩展到VLM的处理能力之外。为了缓解这些限制，我们建议操纵每个ICE的标签空间，以增加其知识密度，允许更少的ICE传达与更大集合一样多的信息。具体来说，我们提出了两种策略，即标签分布增强和视觉描述增强，以提高不同数据集上的上下文内分类性能，包括经典的ImageNet和更细粒度的数据集，如CUB-200。具体来说，使用我们在ImageNet上的方法，我们将精度从4次拍摄时的74.70\%提高到仅拍摄2次的76.21\%。在CUB-200上，我们的方法将单次射击精度从48.86%提高到69.05%，比CLIP高12.15%。代码在中给出https://anonymous.4open.science/r/mls_icc.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00351v1" target="_blank">2312.00351v1</a>
                              </td>
                              <td>Manipulating the Label Space for In-Context Classification</td>
                              <td>Haokun Chen</td>
                              <td>2023-12-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00351v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00351v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_17315v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Explaining CLIP's performance disparities on data from blind/low vision users</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_17315v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_17315v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_17315v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large multi-modal models (LMMs) hold the potential to usher in a new era of automated visual assistance for people who are blind or low vision (BLV). Yet, these models have not been systematically evaluated on data captured by BLV users. We address this by empirically assessing CLIP, a widely-used LMM likely to underpin many assistive technologies. Testing 25 CLIP variants in a zero-shot classification task, we find that their accuracy is 15 percentage points lower on average for images captured by BLV users than web-crawled images. This disparity stems from CLIP's sensitivities to 1) image content (e.g. not recognizing disability objects as well as other objects); 2) image quality (e.g. not being robust to lighting variation); and 3) text content (e.g. not recognizing objects described by tactile adjectives as well as visual ones). We delve deeper with a textual analysis of three common pre-training datasets: LAION-400M, LAION-2B and DataComp-1B, showing that disability content is rarely mentioned. We then provide three examples that illustrate how the performance disparities extend to three downstream models underpinned by CLIP: OWL-ViT, CLIPSeg and DALL-E2. We find that few-shot learning with as few as 5 images can mitigate CLIP's quality-of-service disparities for BLV users in some scenarios, which we discuss alongside a set of other possible mitigations.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_17315v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型多模态模型（LMM）有可能为盲人或低视力者开创一个自动化视觉辅助的新时代。然而，这些模型尚未根据BLV用户获取的数据进行系统评估。我们通过实证评估CLIP来解决这一问题，CLIP是一种广泛使用的LMM，可能支持许多辅助技术。在零样本分类任务中测试了25种CLIP变体，我们发现BLV用户拍摄的图像的准确率平均比网络绘制的图像低15个百分点。这种差异源于CLIP对1）图像内容的敏感性（例如，不能识别残疾对象以及其他对象）；2） 图像质量（例如对光照变化不鲁棒）；以及3）文本内容（例如，不识别由触觉形容词以及视觉形容词描述的对象）。我们对三个常见的预训练数据集：LAION-400M、LAION-2B和DataComp-1B进行了更深入的文本分析，表明残疾内容很少被提及。然后，我们提供了三个示例，说明性能差异如何扩展到以CLIP为基础的三个下游模型：OWL-ViT、CLIPSeg和DALL-E2。我们发现，在某些情况下，只有5张图像的少量镜头学习可以缓解CLIP对BLV用户的服务质量差异，我们将与一系列其他可能的缓解措施一起讨论。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.17315v2" target="_blank">2311.17315v2</a>
                              </td>
                              <td>Explaining CLIP's performance disparities on data from blind/low vision users</td>
                              <td>Daniela Massiceti</td>
                              <td>2023-11-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_17315v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.17315v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_08736v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LoSh: Long-Short Text Joint Prediction Network for Referring Video Object Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_08736v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_08736v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_08736v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Referring video object segmentation (RVOS) aims to segment the target instance referred by a given text expression in a video clip. The text expression normally contains sophisticated description of the instance's appearance, action, and relation with others. It is therefore rather difficult for a RVOS model to capture all these attributes correspondingly in the video; in fact, the model often favours more on the action- and relation-related visual attributes of the instance. This can end up with partial or even incorrect mask prediction of the target instance. We tackle this problem by taking a subject-centric short text expression from the original long text expression. The short one retains only the appearance-related information of the target instance so that we can use it to focus the model's attention on the instance's appearance. We let the model make joint predictions using both long and short text expressions; and insert a long-short cross-attention module to interact the joint features and a long-short predictions intersection loss to regulate the joint predictions. Besides the improvement on the linguistic part, we also introduce a forward-backward visual consistency loss, which utilizes optical flows to warp visual features between the annotated frames and their temporal neighbors for consistency. We build our method on top of two state of the art pipelines. Extensive experiments on A2D-Sentences, Refer-YouTube-VOS, JHMDB-Sentences and Refer-DAVIS17 show impressive improvements of our method.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_08736v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>参考视频对象分割（RVOS）旨在分割视频片段中给定文本表达式所引用的目标实例。文本表达式通常包含对实例的外观、操作以及与其他实例的关系的复杂描述。因此，RVOS模型很难在视频中相应地捕捉所有这些属性；事实上，该模型通常更倾向于实例的与动作和关系相关的视觉属性。这可能最终导致目标实例的部分甚至不正确的掩码预测。我们通过从原始的长文本表达中提取以主题为中心的短文本表达来解决这个问题。简短的一个只保留目标实例的外观相关信息，以便我们可以使用它将模型的注意力集中在实例的外观上。我们让模型使用长文本表达式和短文本表达式进行联合预测；并插入长短交叉注意力模块以交互关节特征和长短预测交叉损失以调节关节预测。除了在语言部分的改进外，我们还引入了前向-后向视觉一致性损失，该损失利用光流扭曲注释帧与其时间邻居之间的视觉特征以实现一致性。我们将我们的方法建立在两个最先进的管道之上。对A2D句子、参考YouTube VOS、JHMDB句子和参考DAVIS17的广泛实验表明，我们的方法有了令人印象深刻的改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.08736v2" target="_blank">2306.08736v2</a>
                              </td>
                              <td>LoSh: Long-Short Text Joint Prediction Network for Referring Video Object Segmentation</td>
                              <td>Linfeng Yuan</td>
                              <td>2023-06-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_08736v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.08736v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00195v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Raising the Bar of AI-generated Image Detection with CLIP</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00195v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00195v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00195v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Aim of this work is to explore the potential of pre-trained vision-language models (VLMs) for universal detection of AI-generated images. We develop a lightweight detection strategy based on CLIP features and study its performance in a wide variety of challenging scenarios. We find that, unlike previous belief, it is neither necessary nor convenient to use a large domain-specific dataset for training. On the contrary, by using only a handful of example images from a single generative model, a CLIP-based detector exhibits a surprising generalization ability and high robustness across several different architectures, including recent commercial tools such as Dalle-3, Midjourney v5, and Firefly. We match the SoTA on in-distribution data, and improve largely above it in terms of generalization to out-of-distribution data (+6% in terms of AUC) and robustness to impaired/laundered data (+13%). Our project is available at https://grip-unina.github.io/ClipBased-SyntheticImageDetection/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00195v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>这项工作的目的是探索预训练的视觉语言模型（VLM）在人工智能生成图像的通用检测中的潜力。我们开发了一种基于CLIP特征的轻量级检测策略，并研究了其在各种具有挑战性的场景中的性能。我们发现，与以前的观点不同，使用大型领域特定数据集进行训练既不必要也不方便。相反，通过仅使用来自单个生成模型的少数示例图像，基于CLIP的检测器在几种不同的架构中表现出令人惊讶的泛化能力和高鲁棒性，包括最近的商业工具，如Dalle-3、Midtravel v5和Firefly。我们在分布内数据上匹配SoTA，并在对分布外数据的泛化能力（AUC为+6%）和对受损/洗白数据的稳健性（+13%）方面大大提高。我们的项目可在https://grip-unina.github.io/clipbased-syntheticimagedetection/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00195v1" target="_blank">2312.00195v1</a>
                              </td>
                              <td>Raising the Bar of AI-generated Image Detection with CLIP</td>
                              <td>Davide Cozzolino</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00195v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00195v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_18837v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">VIDiff: Translating Videos via Multi-Modal Instructions with Diffusion Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_18837v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_18837v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_18837v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Diffusion models have achieved significant success in image and video generation. This motivates a growing interest in video editing tasks, where videos are edited according to provided text descriptions. However, most existing approaches only focus on video editing for short clips and rely on time-consuming tuning or inference. We are the first to propose Video Instruction Diffusion (VIDiff), a unified foundation model designed for a wide range of video tasks. These tasks encompass both understanding tasks (such as language-guided video object segmentation) and generative tasks (video editing and enhancement). Our model can edit and translate the desired results within seconds based on user instructions. Moreover, we design an iterative auto-regressive method to ensure consistency in editing and enhancing long videos. We provide convincing generative results for diverse input videos and written instructions, both qualitatively and quantitatively. More examples can be found at our website https://ChenHsing.github.io/VIDiff.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_18837v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>扩散模型在图像和视频生成方面取得了重大成功。这激发了人们对视频编辑任务的兴趣，在视频编辑任务中，视频是根据提供的文本描述进行编辑的。然而，大多数现有的方法只关注短剪辑的视频编辑，并且依赖于耗时的调整或推理。我们是第一个提出视频指令扩散（VIDiff）的人，这是一个为广泛的视频任务设计的统一基础模型。这些任务包括理解任务（如语言引导的视频对象分割）和生成任务（视频编辑和增强）。我们的模型可以根据用户指示在几秒钟内编辑和翻译所需的结果。此外，我们设计了一种迭代自回归方法，以确保长视频编辑和增强的一致性。我们为不同的输入视频和书面说明提供了令人信服的生成结果，无论是定性的还是定量的。更多示例可在我们的网站上找到https://chenhsing.github.io/vidiff.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.18837v1" target="_blank">2311.18837v1</a>
                              </td>
                              <td>VIDiff: Translating Videos via Multi-Modal Instructions with Diffusion Models</td>
                              <td>Zhen Xing</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_18837v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.18837v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00588v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LucidDreaming: Controllable Object-Centric 3D Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00588v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00588v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00588v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With the recent development of generative models, Text-to-3D generations have also seen significant growth. Nonetheless, achieving precise control over 3D generation continues to be an arduous task, as using text to control often leads to missing objects and imprecise locations. Contemporary strategies for enhancing controllability in 3D generation often entail the introduction of additional parameters, such as customized diffusion models. This often induces hardness in adapting to different diffusion models or creating distinct objects.   In this paper, we present LucidDreaming as an effective pipeline capable of fine-grained control over 3D generation. It requires only minimal input of 3D bounding boxes, which can be deduced from a simple text prompt using a Large Language Model. Specifically, we propose clipped ray sampling to separately render and optimize objects with user specifications. We also introduce object-centric density blob bias, fostering the separation of generated objects. With individual rendering and optimizing of objects, our method excels not only in controlled content generation from scratch but also within the pre-trained NeRF scenes. In such scenarios, existing generative approaches often disrupt the integrity of the original scene, and current editing methods struggle to synthesize new content in empty spaces. We show that our method exhibits remarkable adaptability across a spectrum of mainstream Score Distillation Sampling-based 3D generation frameworks, and achieves superior alignment of 3D content when compared to baseline approaches. We also provide a dataset of prompts with 3D bounding boxes, benchmarking 3D spatial controllability.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00588v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着生成模型的最近发展，文本到三维的几代人也有了显著的增长。尽管如此，实现对3D生成的精确控制仍然是一项艰巨的任务，因为使用文本进行控制往往会导致对象丢失和位置不精确。用于增强3D生成中的可控性的当代策略通常需要引入额外的参数，例如定制的扩散模型。这通常会在适应不同的扩散模型或创建不同的对象时产生困难。在本文中，我们提出LucidDreaming作为一种能够对3D生成进行细粒度控制的有效管道。它只需要最少的3D边界框输入，这可以使用大型语言模型从简单的文本提示中推导出来。具体而言，我们建议使用剪裁光线采样，以根据用户规范分别渲染和优化对象。我们还引入了以对象为中心的密度斑点偏差，促进了生成对象的分离。通过对对象的单独渲染和优化，我们的方法不仅在从头开始的受控内容生成方面表现出色，而且在预先训练的NeRF场景中也表现出色。在这种情况下，现有的生成方法往往会破坏原始场景的完整性，而当前的编辑方法很难在空白处合成新内容。我们表明，与基线方法相比，我们的方法在一系列主流的基于分数蒸馏采样的3D生成框架中表现出显著的适应性，并实现了3D内容的卓越对齐。我们还提供了一个具有三维边界框的提示数据集，以基准测试三维空间可控性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00588v1" target="_blank">2312.00588v1</a>
                              </td>
                              <td>LucidDreaming: Controllable Object-Centric 3D Generation</td>
                              <td>Zhaoning Wang</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00588v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00588v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00110v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLIP-QDA: An Explainable Concept Bottleneck Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00110v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00110v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00110v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we introduce an explainable algorithm designed from a multi-modal foundation model, that performs fast and explainable image classification. Drawing inspiration from CLIP-based Concept Bottleneck Models (CBMs), our method creates a latent space where each neuron is linked to a specific word. Observing that this latent space can be modeled with simple distributions, we use a Mixture of Gaussians (MoG) formalism to enhance the interpretability of this latent space. Then, we introduce CLIP-QDA, a classifier that only uses statistical values to infer labels from the concepts. In addition, this formalism allows for both local and global explanations. These explanations come from the inner design of our architecture, our work is part of a new family of greybox models, combining performances of opaque foundation models and the interpretability of transparent models. Our empirical findings show that in instances where the MoG assumption holds, CLIP-QDA achieves similar accuracy with state-of-the-art methods CBMs. Our explanations compete with existing XAI methods while being faster to compute.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00110v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们介绍了一种由多模态基础模型设计的可解释算法，该算法可以执行快速且可解释的图像分类。从基于CLIP的概念瓶颈模型（CBM）中汲取灵感，我们的方法创建了一个潜在空间，每个神经元都与特定的单词相连。注意到这个潜在空间可以用简单的分布来建模，我们使用高斯混合（MoG）形式来增强这个潜在空间的可解释性。然后，我们介绍了CLIP-QDA，这是一种只使用统计值从概念中推断标签的分类器。此外，这种形式主义允许局部和全局解释。这些解释来自我们架构的内部设计，我们的工作是灰盒模型家族的一部分，结合了不透明基础模型的性能和透明模型的可解释性。我们的经验发现表明，在MoG假设成立的情况下，CLIP-QDA与最先进的CBM方法实现了类似的准确性。我们的解释与现有的XAI方法竞争，同时计算速度更快。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00110v1" target="_blank">2312.00110v1</a>
                              </td>
                              <td>CLIP-QDA: An Explainable Concept Bottleneck Model</td>
                              <td>Rémi Kazmierczak</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00110v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00110v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_14631v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CatVersion: Concatenating Embeddings for Diffusion-Based Text-to-Image Personalization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_14631v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_14631v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_14631v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose CatVersion, an inversion-based method that learns the personalized concept through a handful of examples. Subsequently, users can utilize text prompts to generate images that embody the personalized concept, thereby achieving text-to-image personalization. In contrast to existing approaches that emphasize word embedding learning or parameter fine-tuning for the diffusion model, which potentially causes concept dilution or overfitting, our method concatenates embeddings on the feature-dense space of the text encoder in the diffusion model to learn the gap between the personalized concept and its base class, aiming to maximize the preservation of prior knowledge in diffusion models while restoring the personalized concepts. To this end, we first dissect the text encoder's integration in the image generation process to identify the feature-dense space of the encoder. Afterward, we concatenate embeddings on the Keys and Values in this space to learn the gap between the personalized concept and its base class. In this way, the concatenated embeddings ultimately manifest as a residual on the original attention output. To more accurately and unbiasedly quantify the results of personalized image generation, we improve the CLIP image alignment score based on masks. Qualitatively and quantitatively, CatVersion helps to restore personalization concepts more faithfully and enables more robust editing.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_14631v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了CatVersion，这是一种基于倒置的方法，通过几个例子来学习个性化概念。随后，用户可以利用文本提示来生成体现个性化概念的图像，从而实现文本到图像的个性化。与现有的强调单词嵌入学习或扩散模型的参数微调的方法不同，这可能会导致概念稀释或过拟合，我们的方法将文本编码器的特征密集空间上的嵌入连接到扩散模型中，以学习个性化概念与其基类之间的差距，旨在最大限度地保留扩散模型中的先验知识，同时恢复个性化概念。为此，我们首先剖析了文本编码器在图像生成过程中的集成，以识别编码器的特征密集空间。然后，我们在这个空间中连接键和值上的嵌入，以了解个性化概念与其基类之间的差距。通过这种方式，连接的嵌入最终表现为原始注意力输出上的残差。为了更准确、无偏地量化个性化图像生成的结果，我们改进了基于掩模的CLIP图像对齐分数。从质量和数量上讲，CatVersion有助于更忠实地恢复个性化概念，并实现更稳健的编辑。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.14631v2" target="_blank">2311.14631v2</a>
                              </td>
                              <td>CatVersion: Concatenating Embeddings for Diffusion-Based Text-to-Image Personalization</td>
                              <td>Ruoyu Zhao</td>
                              <td>2023-11-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_14631v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.14631v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_18592v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Semantic-Aware Frame-Event Fusion based Pattern Recognition via Large Vision-Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_18592v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_18592v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_18592v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Pattern recognition through the fusion of RGB frames and Event streams has emerged as a novel research area in recent years. Current methods typically employ backbone networks to individually extract the features of RGB frames and event streams, and subsequently fuse these features for pattern recognition. However, we posit that these methods may suffer from key issues like sematic gaps and small-scale backbone networks. In this study, we introduce a novel pattern recognition framework that consolidates the semantic labels, RGB frames, and event streams, leveraging pre-trained large-scale vision-language models. Specifically, given the input RGB frames, event streams, and all the predefined semantic labels, we employ a pre-trained large-scale vision model (CLIP vision encoder) to extract the RGB and event features. To handle the semantic labels, we initially convert them into language descriptions through prompt engineering, and then obtain the semantic features using the pre-trained large-scale language model (CLIP text encoder). Subsequently, we integrate the RGB/Event features and semantic features using multimodal Transformer networks. The resulting frame and event tokens are further amplified using self-attention layers. Concurrently, we propose to enhance the interactions between text tokens and RGB/Event tokens via cross-attention. Finally, we consolidate all three modalities using self-attention and feed-forward layers for recognition. Comprehensive experiments on the HARDVS and PokerEvent datasets fully substantiate the efficacy of our proposed SAFE model. The source code will be made available at https://github.com/Event-AHU/SAFE_LargeVLM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_18592v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>通过RGB帧和事件流的融合进行模式识别是近年来出现的一个新的研究领域。当前的方法通常使用骨干网络来单独提取RGB帧和事件流的特征，并随后将这些特征融合用于模式识别。然而，我们假设这些方法可能会遇到关键问题，如信号间隙和小规模骨干网络。在这项研究中，我们引入了一种新的模式识别框架，该框架利用预先训练的大规模视觉语言模型，整合了语义标签、RGB帧和事件流。具体来说，给定输入的RGB帧、事件流和所有预定义的语义标签，我们使用预先训练的大规模视觉模型（CLIP视觉编码器）来提取RGB和事件特征。为了处理语义标签，我们首先通过提示工程将其转换为语言描述，然后使用预先训练的大规模语言模型（CLIP文本编码器）获得语义特征。随后，我们使用多模式Transformer网络集成RGB/Event特征和语义特征。使用自关注层进一步放大所得到的帧和事件标记。同时，我们建议通过交叉关注来增强文本标记和RGB/Event标记之间的交互。最后，我们使用自我注意和前馈层来整合所有三种模式进行识别。在HARDVS和PokerEvent数据集上的综合实验充分证明了我们提出的SAFE模型的有效性。源代码将在提供https://github.com/event-ahu/safe_largevlm.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.18592v1" target="_blank">2311.18592v1</a>
                              </td>
                              <td>Semantic-Aware Frame-Event Fusion based Pattern Recognition via Large Vision-Language Models</td>
                              <td>Dong Li</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_18592v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.18592v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/event-ahu/safe_largevlm" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_18537v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MaXTron: Mask Transformer with Trajectory Attention for Video Panoptic Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_18537v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_18537v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_18537v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video panoptic segmentation requires consistently segmenting (for both `thing' and `stuff' classes) and tracking objects in a video over time. In this work, we present MaXTron, a general framework that exploits Mask XFormer with Trajectory Attention to tackle the task. MaXTron enriches an off-the-shelf mask transformer by leveraging trajectory attention. The deployed mask transformer takes as input a short clip consisting of only a few frames and predicts the clip-level segmentation. To enhance the temporal consistency, MaXTron employs within-clip and cross-clip tracking modules, efficiently utilizing trajectory attention. Originally designed for video classification, trajectory attention learns to model the temporal correspondences between neighboring frames and aggregates information along the estimated motion paths. However, it is nontrivial to directly extend trajectory attention to the per-pixel dense prediction tasks due to its quadratic dependency on input size. To alleviate the issue, we propose to adapt the trajectory attention for both the dense pixel features and object queries, aiming to improve the short-term and long-term tracking results, respectively. Particularly, in our within-clip tracking module, we propose axial-trajectory attention that effectively computes the trajectory attention for tracking dense pixels sequentially along the height- and width-axes. The axial decomposition significantly reduces the computational complexity for dense pixel features. In our cross-clip tracking module, since the object queries in mask transformer are learned to encode the object information, we are able to capture the long-term temporal connections by applying trajectory attention to object queries, which learns to track each object across different clips. Without bells and whistles, MaXTron demonstrates state-of-the-art performances on video segmentation benchmarks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_18537v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频全景分割需要一致地分割（对于“thing”和“stuff”类）并随着时间的推移跟踪视频中的对象。在这项工作中，我们介绍了MaXTron，这是一个通用框架，它利用具有轨迹注意力的Mask XFormer来处理任务。MaXTron通过利用轨迹注意力丰富了现成的口罩转换器。所部署的掩码转换器将仅由几个帧组成的短剪辑作为输入，并预测剪辑级别的分割。为了增强时间一致性，MaXTron采用了片段内和跨片段跟踪模块，有效地利用了轨迹注意力。轨迹注意力最初是为视频分类而设计的，它学习对相邻帧之间的时间对应关系进行建模，并沿着估计的运动路径聚合信息。然而，由于其对输入大小的二次依赖性，将轨迹注意力直接扩展到每像素密集预测任务是不重要的。为了缓解这一问题，我们建议将轨迹注意力调整为密集像素特征和对象查询，旨在分别提高短期和长期跟踪结果。特别是，在我们的片段内跟踪模块中，我们提出了轴向轨迹注意力，该模块有效地计算了沿高度轴和宽度轴顺序跟踪密集像素的轨迹注意力。轴向分解显著降低了密集像素特征的计算复杂度。在我们的跨剪辑跟踪模块中，由于掩码转换器中的对象查询是为了对对象信息进行编码而学习的，因此我们能够通过将轨迹注意力应用于对象查询来捕捉长期的时间连接，从而学习在不同的剪辑中跟踪每个对象。MaXTron在视频分割基准测试上展示了最先进的性能，无需大惊小怪。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.18537v1" target="_blank">2311.18537v1</a>
                              </td>
                              <td>MaXTron: Mask Transformer with Trajectory Attention for Video Panoptic Segmentation</td>
                              <td>Ju He</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_18537v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.18537v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/tacju/maxtron" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_17812v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DAP: Domain-aware Prompt Learning for Vision-and-Language Navigation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_17812v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_17812v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_17812v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Following language instructions to navigate in unseen environments is a challenging task for autonomous embodied agents. With strong representation capabilities, pretrained vision-and-language models are widely used in VLN. However, most of them are trained on web-crawled general-purpose datasets, which incurs a considerable domain gap when used for VLN tasks. To address the problem, we propose a novel and model-agnostic domain-aware prompt learning (DAP) framework. For equipping the pretrained models with specific object-level and scene-level cross-modal alignment in VLN tasks, DAP applies a low-cost prompt tuning paradigm to learn soft visual prompts for extracting in-domain image semantics. Specifically, we first generate a set of in-domain image-text pairs with the help of the CLIP model. Then we introduce soft visual prompts in the input space of the visual encoder in a pretrained model. DAP injects in-domain visual knowledge into the visual encoder of the pretrained model in an efficient way. Experimental results on both R2R and REVERIE show the superiority of DAP compared to existing state-of-the-art methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_17812v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>遵循语言指令在看不见的环境中导航对自主的具体代理来说是一项具有挑战性的任务。预训练的视觉和语言模型具有强大的表示能力，在VLN中得到了广泛的应用。然而，它们中的大多数都是在web爬网的通用数据集上训练的，这在用于VLN任务时会产生相当大的域间隙。为了解决这个问题，我们提出了一个新的、模型不可知的领域感知即时学习（DAP）框架。为了在VLN任务中为预训练的模型配备特定的对象级和场景级跨模态对齐，DAP应用低成本的提示调整范式来学习用于提取域内图像语义的软视觉提示。具体来说，我们首先在CLIP模型的帮助下生成一组域内图像-文本对。然后，我们在预训练模型中的视觉编码器的输入空间中引入软视觉提示。DAP以一种有效的方式将域内视觉知识注入到预训练模型的视觉编码器中。R2R和REVERIE的实验结果表明，与现有的最先进的方法相比，DAP具有优越性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.17812v2" target="_blank">2311.17812v2</a>
                              </td>
                              <td>DAP: Domain-aware Prompt Learning for Vision-and-Language Navigation</td>
                              <td>Ting Liu</td>
                              <td>2023-11-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_17812v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.17812v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_18402v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MV-CLIP: Multi-View CLIP for Zero-shot 3D Shape Recognition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_18402v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_18402v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_18402v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large-scale pre-trained models have demonstrated impressive performance in vision and language tasks within open-world scenarios. Due to the lack of comparable pre-trained models for 3D shapes, recent methods utilize language-image pre-training to realize zero-shot 3D shape recognition. However, due to the modality gap, pretrained language-image models are not confident enough in the generalization to 3D shape recognition. Consequently, this paper aims to improve the confidence with view selection and hierarchical prompts. Leveraging the CLIP model as an example, we employ view selection on the vision side by identifying views with high prediction confidence from multiple rendered views of a 3D shape. On the textual side, the strategy of hierarchical prompts is proposed for the first time. The first layer prompts several classification candidates with traditional class-level descriptions, while the second layer refines the prediction based on function-level descriptions or further distinctions between the candidates. Remarkably, without the need for additional training, our proposed method achieves impressive zero-shot 3D classification accuracies of 84.44\%, 91.51\%, and 66.17\% on ModelNet40, ModelNet10, and ShapeNet Core55, respectively. Furthermore, we will make the code publicly available to facilitate reproducibility and further research in this area.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_18402v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大规模预训练模型在开放世界场景中的视觉和语言任务中表现出了令人印象深刻的性能。由于缺乏可比较的三维形状预训练模型，最近的方法利用语言图像预训练来实现零样本三维形状识别。然而，由于模态的差距，预训练的语言图像模型对三维形状识别的泛化能力不够自信。因此，本文旨在通过视图选择和层次提示来提高置信度。以CLIP模型为例，我们通过从3D形状的多个渲染视图中识别具有高预测置信度的视图，在视觉侧采用视图选择。在文本方面，首次提出了分层提示策略。第一层用传统的类级描述提示几个分类候选者，而第二层基于功能级描述或候选者之间的进一步区分来细化预测。值得注意的是，在不需要额外训练的情况下，我们提出的方法在ModelNet40、ModelNet10和ShapeNet Core55上分别实现了令人印象深刻的零样本3D分类准确率84.44%、91.51%和66.17%。此外，我们将公开该代码，以促进该领域的再现性和进一步研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.18402v1" target="_blank">2311.18402v1</a>
                              </td>
                              <td>MV-CLIP: Multi-View CLIP for Zero-shot 3D Shape Recognition</td>
                              <td>Dan Song</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_18402v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.18402v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_11293v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Beyond the Field-of-View: Enhancing Scene Visibility and Perception with Clip-Recurrent Transformer</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_11293v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_11293v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_11293v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision sensors are widely applied in vehicles, robots, and roadside infrastructure. However, due to limitations in hardware cost and system size, camera Field-of-View (FoV) is often restricted and may not provide sufficient coverage. Nevertheless, from a spatiotemporal perspective, it is possible to obtain information beyond the camera's physical FoV from past video streams. In this paper, we propose the concept of online video inpainting for autonomous vehicles to expand the field of view, thereby enhancing scene visibility, perception, and system safety. To achieve this, we introduce the FlowLens architecture, which explicitly employs optical flow and implicitly incorporates a novel clip-recurrent transformer for feature propagation. FlowLens offers two key features: 1) FlowLens includes a newly designed Clip-Recurrent Hub with 3D-Decoupled Cross Attention (DDCA) to progressively process global information accumulated over time. 2) It integrates a multi-branch Mix Fusion Feed Forward Network (MixF3N) to enhance the precise spatial flow of local features. To facilitate training and evaluation, we derive the KITTI360 dataset with various FoV mask, which covers both outer- and inner FoV expansion scenarios. We also conduct quantitative assessments of beyond-FoV semantics across different models and perform qualitative comparisons of beyond-FoV object detection. We illustrate that employing FlowLens to reconstruct unseen scenes even enhances perception within the field of view by providing reliable semantic context. Extensive experiments and user studies involving offline and online video inpainting, as well as beyond-FoV perception tasks, demonstrate that FlowLens achieves state-of-the-art performance. The source code and dataset are made publicly available at https://github.com/MasterHow/FlowLens.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_11293v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉传感器广泛应用于车辆、机器人和路边基础设施。然而，由于硬件成本和系统尺寸的限制，相机视场（FoV）经常受到限制，并且可能无法提供足够的覆盖。然而，从时空的角度来看，有可能从过去的视频流中获得超出相机物理FoV的信息。在本文中，我们提出了自动驾驶汽车在线视频修复的概念，以扩大视野，从而提高场景的可见性、感知能力和系统安全性。为了实现这一点，我们引入了FlowLens架构，该架构显式地使用光流，并隐式地结合了一种用于特征传播的新型剪辑递归变换器。FlowLens提供了两个关键功能：1）FlowLens包括一个新设计的具有3D解耦交叉注意力（DDCA）的剪辑递归集线器，以逐步处理随着时间的推移积累的全局信息。2） 它集成了多分支混合融合前馈网络（MixF3N），以增强局部特征的精确空间流。为了便于训练和评估，我们导出了具有各种FoV掩码的KITTI360数据集，该数据集涵盖了外部和内部FoV扩展场景。我们还对不同模型的超FoV语义进行了定量评估，并对超FoV对象检测进行了定性比较。我们说明，使用FlowLens重建看不见的场景甚至可以通过提供可靠的语义上下文来增强视野内的感知。涉及离线和在线视频修复以及FoV感知任务之外的大量实验和用户研究表明，FlowLens实现了最先进的性能。源代码和数据集公开于https://github.com/masterhow/flowlens.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.11293v2" target="_blank">2211.11293v2</a>
                              </td>
                              <td>Beyond the Field-of-View: Enhancing Scene Visibility and Perception with Clip-Recurrent Transformer</td>
                              <td>Hao Shi</td>
                              <td>2022-11-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_11293v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.11293v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/masterhow/flowlens" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_18303v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">OmniMotionGPT: Animal Motion Generation with Limited Data</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_18303v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_18303v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_18303v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Our paper aims to generate diverse and realistic animal motion sequences from textual descriptions, without a large-scale animal text-motion dataset. While the task of text-driven human motion synthesis is already extensively studied and benchmarked, it remains challenging to transfer this success to other skeleton structures with limited data. In this work, we design a model architecture that imitates Generative Pretraining Transformer (GPT), utilizing prior knowledge learned from human data to the animal domain. We jointly train motion autoencoders for both animal and human motions and at the same time optimize through the similarity scores among human motion encoding, animal motion encoding, and text CLIP embedding. Presenting the first solution to this problem, we are able to generate animal motions with high diversity and fidelity, quantitatively and qualitatively outperforming the results of training human motion generation baselines on animal data. Additionally, we introduce AnimalML3D, the first text-animal motion dataset with 1240 animation sequences spanning 36 different animal identities. We hope this dataset would mediate the data scarcity problem in text-driven animal motion generation, providing a new playground for the research community.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_18303v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们的论文旨在从文本描述中生成多样化和逼真的动物运动序列，而不需要大规模的动物文本运动数据集。虽然文本驱动的人体运动合成任务已经得到了广泛的研究和基准测试，但将这一成功转移到数据有限的其他骨骼结构中仍然具有挑战性。在这项工作中，我们设计了一个模仿生成预训练转换器（GPT）的模型架构，利用从人类数据中学习到的动物领域的先验知识。我们联合训练动物和人类运动的运动自动编码器，同时通过人类运动编码、动物运动编码和文本CLIP嵌入之间的相似性得分进行优化。提出了这个问题的第一个解决方案，我们能够以高多样性和保真度生成动物运动，在数量和质量上都优于基于动物数据训练人类运动生成基线的结果。此外，我们还介绍了AnimalML3D，这是第一个文本动物运动数据集，包含1240个跨越36种不同动物身份的动画序列。我们希望这个数据集能解决文本驱动的动物运动生成中的数据稀缺问题，为研究界提供一个新的平台。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.18303v1" target="_blank">2311.18303v1</a>
                              </td>
                              <td>OmniMotionGPT: Animal Motion Generation with Limited Data</td>
                              <td>Zhangsihao Yang</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_18303v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.18303v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_18291v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TLDR: Text Based Last-layer Retraining for Debiasing Image Classifiers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_18291v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_18291v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_18291v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A classifier may depend on incidental features stemming from a strong correlation between the feature and the classification target in the training dataset. Recently, Last Layer Retraining (LLR) with group-balanced datasets is known to be efficient in mitigating the spurious correlation of classifiers. However, the acquisition of group-balanced datasets is costly, which hinders the applicability of the LLR method. In this work, we propose to perform LLR based on text datasets built with large language models for a general image classifier. We demonstrate that text can be a proxy for its corresponding image beyond the image-text joint embedding space, such as CLIP. Based on this, we use generated texts to train the final layer in the embedding space of the arbitrary image classifier. In addition, we propose a method of filtering the generated words to get rid of noisy, imprecise words, which reduces the effort of inspecting each word. We dub these procedures as TLDR (\textbf{T}ext-based \textbf{L}ast layer retraining for \textbf{D}ebiasing image classifie\textbf{R}s) and show our method achieves the performance that is comparable to those of the LLR methods that also utilize group-balanced image dataset for retraining. Furthermore, TLDR outperforms other baselines that involve training the last linear layer without a group annotated dataset.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_18291v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分类器可以依赖于源自训练数据集中的特征和分类目标之间的强相关性的附带特征。最近，使用组平衡数据集的最后一层再训练（LLR）被认为在减轻分类器的虚假相关性方面是有效的。然而，组平衡数据集的获取成本高昂，这阻碍了LLR方法的适用性。在这项工作中，我们建议对通用图像分类器基于用大型语言模型构建的文本数据集执行LLR。我们证明了文本可以在图像-文本联合嵌入空间之外作为其相应图像的代理，例如CLIP。基于此，我们使用生成的文本来训练任意图像分类器嵌入空间中的最终层。此外，我们还提出了一种对生成的单词进行过滤的方法，以去除嘈杂、不精确的单词，从而减少了检查每个单词的工作量。我们将这些过程称为TLDR（\textbf{t}ext-based\textbf{l}ast\textbf的层再培训{d}ebiasing图像分类\textbf{r}s)并表明我们的方法实现了与同样利用组平衡图像数据集进行再训练的LLR方法相当的性能。此外，TLDR优于其他基线，这些基线涉及在没有组注释数据集的情况下训练最后一个线性层。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.18291v1" target="_blank">2311.18291v1</a>
                              </td>
                              <td>TLDR: Text Based Last-layer Retraining for Debiasing Image Classifiers</td>
                              <td>Juhyeon Park</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_18291v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.18291v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/tmlabonte/last-layer-retraining" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_18237v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Label-efficient Training of Small Task-specific Models by Leveraging Vision Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_18237v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_18237v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_18237v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Vision Foundation Models (VFMs) pretrained on massive datasets exhibit impressive performance on various downstream tasks, especially with limited labeled target data. However, due to their high memory and compute requirements, these models cannot be deployed in resource constrained settings. This raises an important question: How can we utilize the knowledge from a large VFM to train a small task-specific model for a new target task with limited labeled training data? In this work, we answer this question by proposing a simple and highly effective task-oriented knowledge transfer approach to leverage pretrained VFMs for effective training of small task-specific models. Our experimental results on four target tasks under limited labeled data settings show that the proposed knowledge transfer approach outperforms task-agnostic VFM distillation, web-scale CLIP pretraining and supervised ImageNet pretraining by 1-10.5%, 2-22% and 2-14%, respectively. We also show that the dataset used for transferring knowledge has a significant effect on the final target task performance, and propose an image retrieval-based approach for curating effective transfer sets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_18237v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在海量数据集上预训练的大型视觉基础模型（VFM）在各种下游任务上表现出令人印象深刻的性能，尤其是在标记的目标数据有限的情况下。然而，由于这些模型对内存和计算的要求很高，因此无法在资源受限的环境中进行部署。这就提出了一个重要的问题：我们如何利用来自大型VFM的知识，在标记训练数据有限的情况下，为新的目标任务训练小型任务专用模型？在这项工作中，我们通过提出一种简单而高效的面向任务的知识转移方法来回答这个问题，以利用预训练的VFM来有效训练小任务特定模型。我们在有限标记数据设置下对四个目标任务的实验结果表明，所提出的知识转移方法分别比任务无关的VFM提取、网络级CLIP预训练和监督ImageNet预训练好1-10.5%、2-22%和2-14%。我们还表明，用于传递知识的数据集对最终目标任务的性能有显著影响，并提出了一种基于图像检索的有效传递集管理方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.18237v1" target="_blank">2311.18237v1</a>
                              </td>
                              <td>Label-efficient Training of Small Task-specific Models by Leveraging Vision Foundation Models</td>
                              <td>Raviteja Vemulapalli</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_18237v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.18237v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_18168v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Probabilistic Speech-Driven 3D Facial Motion Synthesis: New Benchmarks, Methods, and Applications</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_18168v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_18168v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_18168v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We consider the task of animating 3D facial geometry from speech signal. Existing works are primarily deterministic, focusing on learning a one-to-one mapping from speech signal to 3D face meshes on small datasets with limited speakers. While these models can achieve high-quality lip articulation for speakers in the training set, they are unable to capture the full and diverse distribution of 3D facial motions that accompany speech in the real world. Importantly, the relationship between speech and facial motion is one-to-many, containing both inter-speaker and intra-speaker variations and necessitating a probabilistic approach. In this paper, we identify and address key challenges that have so far limited the development of probabilistic models: lack of datasets and metrics that are suitable for training and evaluating them, as well as the difficulty of designing a model that generates diverse results while remaining faithful to a strong conditioning signal as speech. We first propose large-scale benchmark datasets and metrics suitable for probabilistic modeling. Then, we demonstrate a probabilistic model that achieves both diversity and fidelity to speech, outperforming other methods across the proposed benchmarks. Finally, we showcase useful applications of probabilistic models trained on these large-scale datasets: we can generate diverse speech-driven 3D facial motion that matches unseen speaker styles extracted from reference clips; and our synthetic meshes can be used to improve the performance of downstream audio-visual models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_18168v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们考虑从语音信号中动画化3D面部几何形状的任务。现有的工作主要是确定性的，专注于在扬声器有限的小数据集上学习从语音信号到3D人脸网格的一对一映射。虽然这些模型可以为训练集中的说话者实现高质量的嘴唇清晰度，但它们无法捕捉真实世界中伴随语音的3D面部运动的完整而多样的分布。重要的是，语音和面部运动之间的关系是一对多的，既包含说话者之间的变化，也包含说话者内部的变化，因此需要采用概率方法。在本文中，我们确定并解决了迄今为止限制概率模型发展的关键挑战：缺乏适合训练和评估它们的数据集和指标，以及设计一个在忠实于语音等强条件信号的同时产生不同结果的模型的困难。我们首先提出了适用于概率建模的大规模基准数据集和度量。然后，我们展示了一个概率模型，该模型实现了语音的多样性和保真度，在所提出的基准测试中优于其他方法。最后，我们展示了在这些大规模数据集上训练的概率模型的有用应用：我们可以生成各种语音驱动的3D面部运动，这些运动与从参考片段中提取的看不见的说话者风格相匹配；并且我们的合成网格可以用于提高下游视听模型的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.18168v1" target="_blank">2311.18168v1</a>
                              </td>
                              <td>Probabilistic Speech-Driven 3D Facial Motion Synthesis: New Benchmarks, Methods, and Applications</td>
                              <td>Karren D. Yang</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_18168v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.18168v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_16102v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Diffusion-TTA: Test-time Adaptation of Discriminative Models via Generative Feedback</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_16102v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_16102v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_16102v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The advancements in generative modeling, particularly the advent of diffusion models, have sparked a fundamental question: how can these models be effectively used for discriminative tasks? In this work, we find that generative models can be great test-time adapters for discriminative models. Our method, Diffusion-TTA, adapts pre-trained discriminative models such as image classifiers, segmenters and depth predictors, to each unlabelled example in the test set using generative feedback from a diffusion model. We achieve this by modulating the conditioning of the diffusion model using the output of the discriminative model. We then maximize the image likelihood objective by backpropagating the gradients to discriminative model's parameters. We show Diffusion-TTA significantly enhances the accuracy of various large-scale pre-trained discriminative models, such as, ImageNet classifiers, CLIP models, image pixel labellers and image depth predictors. Diffusion-TTA outperforms existing test-time adaptation methods, including TTT-MAE and TENT, and particularly shines in online adaptation setups, where the discriminative model is continually adapted to each example in the test set. We provide access to code, results, and visualizations on our website: https://diffusion-tta.github.io/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_16102v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>生成建模的进步，特别是扩散模型的出现，引发了一个根本问题：如何有效地将这些模型用于判别任务？在这项工作中，我们发现生成模型可以成为判别模型的很好的测试时间适配器。我们的方法Diffusion TTA使用来自扩散模型的生成反馈，将预先训练的判别模型（如图像分类器、分割器和深度预测器）适应于测试集中的每个未标记的例子。我们通过使用判别模型的输出来调节扩散模型的条件来实现这一点。然后，我们通过将梯度反向传播到判别模型的参数来最大化图像似然目标。我们展示了Diffusion TTA显著提高了各种大规模预训练判别模型的准确性，如ImageNet分类器、CLIP模型、图像像素标记器和图像深度预测器。扩散TTA优于现有的测试时间自适应方法，包括TTT-MAE和TENT，尤其在在线自适应设置中大放异彩，其中判别模型不断适应测试集中的每个例子。我们在我们的网站上提供对代码、结果和可视化的访问：https://diffusion-tta.github.io/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.16102v2" target="_blank">2311.16102v2</a>
                              </td>
                              <td>Diffusion-TTA: Test-time Adaptation of Discriminative Models via Generative Feedback</td>
                              <td>Mihir Prabhudesai</td>
                              <td>2023-11-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_16102v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.16102v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_17922v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Simple Recipe for Language-guided Domain Generalized Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_17922v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_17922v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_17922v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Generalization to new domains not seen during training is one of the long-standing goals and challenges in deploying neural networks in real-world applications. Existing generalization techniques necessitate substantial data augmentation, potentially sourced from external datasets, and aim at learning invariant representations by imposing various alignment constraints. Large-scale pretraining has recently shown promising generalization capabilities, along with the potential of bridging different modalities. For instance, the recent advent of vision-language models like CLIP has opened the doorway for vision models to exploit the textual modality. In this paper, we introduce a simple framework for generalizing semantic segmentation networks by employing language as the source of randomization. Our recipe comprises three key ingredients: i) the preservation of the intrinsic CLIP robustness through minimal fine-tuning, ii) language-driven local style augmentation, and iii) randomization by locally mixing the source and augmented styles during training. Extensive experiments report state-of-the-art results on various generalization benchmarks. The code will be made available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_17922v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>将神经网络推广到训练过程中没有看到的新领域是在现实世界应用中部署神经网络的长期目标和挑战之一。现有的泛化技术需要大量的数据扩充，可能来自外部数据集，并旨在通过施加各种对齐约束来学习不变的表示。大规模预训练最近显示出了很有希望的泛化能力，以及桥接不同模态的潜力。例如，最近出现的像CLIP这样的视觉语言模型为视觉模型利用文本模态打开了大门。在本文中，我们介绍了一个简单的框架，通过使用语言作为随机化的来源来推广语义分割网络。我们的配方包括三个关键成分：i）通过最小的微调来保持固有的CLIP稳健性，ii）语言驱动的局部风格增强，以及iii）在训练期间通过局部混合源风格和增强风格来随机化。广泛的实验报告了在各种泛化基准上的最先进的结果。代码将可用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.17922v1" target="_blank">2311.17922v1</a>
                              </td>
                              <td>A Simple Recipe for Language-guided Domain Generalized Segmentation</td>
                              <td>Mohammad Fahes</td>
                              <td>2023-11-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_17922v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.17922v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_17833v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Analyzing and Explaining Image Classifiers via Diffusion Guidance</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_17833v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_17833v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_17833v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While deep learning has led to huge progress in complex image classification tasks like ImageNet, unexpected failure modes, e.g. via spurious features, call into question how reliably these classifiers work in the wild. Furthermore, for safety-critical tasks the black-box nature of their decisions is problematic, and explanations or at least methods which make decisions plausible are needed urgently. In this paper, we address these problems by generating images that optimize a classifier-derived objective using a framework for guided image generation. We analyze the behavior and decisions of image classifiers by visual counterfactual explanations (VCEs), detection of systematic mistakes by analyzing images where classifiers maximally disagree, and visualization of neurons to verify potential spurious features. In this way, we validate existing observations, e.g. the shape bias of adversarially robust models, as well as novel failure modes, e.g. systematic errors of zero-shot CLIP classifiers, or identify harmful spurious features. Moreover, our VCEs outperform previous work while being more versatile.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_17833v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>虽然深度学习在ImageNet等复杂的图像分类任务中取得了巨大进展，但意外的失败模式，例如通过虚假特征，让人质疑这些分类器在野外工作的可靠性。此外，对于安全关键任务，其决策的黑匣子性质是有问题的，迫切需要解释或至少需要使决策合理的方法。在本文中，我们通过使用引导图像生成框架生成优化分类器导出目标的图像来解决这些问题。我们通过视觉反事实解释（VCE）分析图像分类器的行为和决策，通过分析分类器最大程度不一致的图像检测系统错误，以及可视化神经元以验证潜在的虚假特征。通过这种方式，我们验证了现有的观察结果，例如对抗性鲁棒模型的形状偏差，以及新的故障模式，例如零样本CLIP分类器的系统误差，或识别有害的虚假特征。此外，我们的VCE在更通用的同时，也优于以前的工作。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.17833v1" target="_blank">2311.17833v1</a>
                              </td>
                              <td>Analyzing and Explaining Image Classifiers via Diffusion Guidance</td>
                              <td>Maximilian Augustin</td>
                              <td>2023-11-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_17833v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.17833v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_17041v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Efficient In-Context Learning in Vision-Language Models for Egocentric Videos</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_17041v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_17041v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_17041v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advancements in text-only large language models (LLMs) have highlighted the benefit of in-context learning for adapting to new tasks with a few demonstrations. However, extending in-context learning to large vision-language models (VLMs) using a huge amount of naturalistic vision-language data has shown limited success, particularly for egocentric videos, due to high data collection costs. We propose a novel training method $\mathbb{E}$fficient $\mathbb{I}$n-context $\mathbb{L}$earning on $\mathbb{E}$gocentric $\mathbb{V}$ideos ($\mathbb{EILEV}$), which elicits in-context learning in VLMs for egocentric videos without requiring massive, naturalistic egocentric video datasets. $\mathbb{EILEV}$ involves architectural and training data adaptations to allow the model to process contexts interleaved with video clips and narrations, sampling of in-context examples with clusters of similar verbs and nouns, use of data with skewed marginal distributions with a long tail of infrequent verbs and nouns, as well as homonyms and synonyms. Our evaluations show that $\mathbb{EILEV}$-trained models outperform larger VLMs trained on a huge amount of naturalistic data in in-context learning. Furthermore, they can generalize to not only out-of-distribution, but also novel, rare egocentric videos and texts via in-context learning, demonstrating potential for applications requiring cost-effective training, and rapid post-deployment adaptability. Our code and demo are available at \url{https://github.com/yukw777/EILEV}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_17041v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>纯文本大语言模型（LLM）的最新进展通过一些演示突出了上下文学习在适应新任务方面的好处。然而，由于数据收集成本高，使用大量自然主义视觉语言数据将上下文学习扩展到大型视觉语言模型（VLM）的成功有限，尤其是在以自我为中心的视频中。我们提出了一种新的训练方法$\mathbb｛E｝$efficient$\mathbb｛I｝$n-context$\mathbb{L｝$在$\mathbb{E｝$gocentric$\mathB{V｝$ideos（$\mathb{EILEV｝$）上的收益，该方法在VLM中引发以自我为中心的视频的上下文学习，而不需要大规模的、自然主义的以自我为核心的视频数据集$\mathb{EILEV}$涉及架构和训练数据调整，以使模型能够处理与视频片段和叙述交织的上下文，对具有相似动词和名词簇的上下文中的示例进行采样，使用具有偏斜边缘分布的数据，该数据具有不常见动词和名词的长尾，以及同音异义词和同义词。我们的评估表明，在上下文学习中，$\mathbb｛EILEV｝$训练的模型优于在大量自然数据上训练的较大VLM。此外，它们不仅可以推广到分发外的视频和文本，还可以通过上下文学习推广到新颖、罕见的以自我为中心的视频和文字，展示了需要成本效益高的培训和快速部署后适应性的应用潜力。我们的代码和演示可在\url上获得{https://github.com/yukw777/eilev}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>46</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.17041v2" target="_blank">2311.17041v2</a>
                              </td>
                              <td>Efficient In-Context Learning in Vision-Language Models for Egocentric Videos</td>
                              <td>Keunwoo Peter Yu</td>
                              <td>2023-11-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_17041v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.17041v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/yukw777/eilev" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_13562v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Soulstyler: Using Large Language Model to Guide Image Style Transfer for Target Object</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_13562v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_13562v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_13562v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Image style transfer occupies an important place in both computer graphics and computer vision. However, most current methods require reference to stylized images and cannot individually stylize specific objects. To overcome this limitation, we propose the "Soulstyler" framework, which allows users to guide the stylization of specific objects in an image through simple textual descriptions. We introduce a large language model to parse the text and identify stylization goals and specific styles. Combined with a CLIP-based semantic visual embedding encoder, the model understands and matches text and image content. We also introduce a novel localized text-image block matching loss that ensures that style transfer is performed only on specified target objects, while non-target regions remain in their original style. Experimental results demonstrate that our model is able to accurately perform style transfer on target objects according to textual descriptions without affecting the style of background regions. Our code will be available at https://github.com/yisuanwang/Soulstyler.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_13562v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>图像风格转换在计算机图形学和计算机视觉中都占有重要的地位。然而，大多数当前的方法都需要参考样式化的图像，并且不能单独设置特定对象的样式。为了克服这一限制，我们提出了“Soulstyler”框架，该框架允许用户通过简单的文本描述来指导图像中特定对象的风格化。我们引入了一个大型语言模型来解析文本，并确定风格化目标和特定风格。该模型与基于CLIP的语义视觉嵌入编码器相结合，可以理解和匹配文本和图像内容。我们还引入了一种新颖的本地化文本图像块匹配损失，该损失确保仅对指定的目标对象执行样式转换，而非目标区域保持其原始样式。实验结果表明，我们的模型能够根据文本描述准确地对目标对象进行风格转换，而不会影响背景区域的风格。我们的代码将在https://github.com/yisuanwang/soulstyler.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>47</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.13562v2" target="_blank">2311.13562v2</a>
                              </td>
                              <td>Soulstyler: Using Large Language Model to Guide Image Style Transfer for Target Object</td>
                              <td>Junhao Chen</td>
                              <td>2023-11-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_13562v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.13562v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/yisuanwang/soulstyler" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="DINO"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2312_01677v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-task Image Restoration Guided By Robust DINO Features</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01677v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01677v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01677v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multi-task image restoration has gained significant interest due to its inherent versatility and efficiency compared to its single-task counterpart. Despite its potential, performance degradation is observed with an increase in the number of tasks, primarily attributed to the distinct nature of each restoration task. Addressing this challenge, we introduce \mbox{\textbf{DINO-IR}}, a novel multi-task image restoration approach leveraging robust features extracted from DINOv2. Our empirical analysis shows that while shallow features of DINOv2 capture rich low-level image characteristics, the deep features ensure a robust semantic representation insensitive to degradations while preserving high-frequency contour details. Building on these features, we devise specialized components, including multi-layer semantic fusion module, DINO-Restore adaption and fusion module, and DINO perception contrastive loss, to integrate DINOv2 features into the restoration paradigm. Equipped with the aforementioned components, our DINO-IR performs favorably against existing multi-task image restoration approaches in various tasks by a large margin, indicating the superiority and necessity of reinforcing the robust features for multi-task image restoration.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01677v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>与单任务图像恢复相比，多任务图像恢复由于其固有的多功能性和效率而引起了人们的极大兴趣。尽管有其潜力，但随着任务数量的增加，性能会下降，这主要归因于每个恢复任务的不同性质。为了应对这一挑战，我们引入了\box｛\textbf｛DINO-IR｝｝，这是一种利用从DINOv2中提取的鲁棒特征的新的多任务图像恢复方法。我们的实证分析表明，虽然DINOv2的浅层特征捕捉到了丰富的低层图像特征，但深层特征确保了对退化不敏感的鲁棒语义表示，同时保留了高频轮廓细节。基于这些特征，我们设计了专门的组件，包括多层语义融合模块、DINO恢复适应和融合模块以及DINO感知对比损失，以将DINOv2特征整合到恢复范式中。配备了上述组件，我们的DINO-IR在各种任务中与现有的多任务图像恢复方法相比表现良好，这表明了增强多任务图像修复的鲁棒性特征的优越性和必要性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01677v1" target="_blank">2312.01677v1</a>
                              </td>
                              <td>Multi-task Image Restoration Guided By Robust DINO Features</td>
                              <td>Xin Lin</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01677v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01677v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01576v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Efficient Unsupervised Satellite Image-based Building Damage Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01576v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01576v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01576v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Existing Building Damage Detection (BDD) methods always require labour-intensive pixel-level annotations of buildings and their conditions, hence largely limiting their applications. In this paper, we investigate a challenging yet practical scenario of BDD, Unsupervised Building Damage Detection (U-BDD), where only unlabelled pre- and post-disaster satellite image pairs are provided. As a pilot study, we have first proposed an advanced U-BDD baseline that leverages pre-trained vision-language foundation models (i.e., Grounding DINO, SAM and CLIP) to address the U-BDD task. However, the apparent domain gap between satellite and generic images causes low confidence in the foundation models used to identify buildings and their damages. In response, we further present a novel self-supervised framework, U-BDD++, which improves upon the U-BDD baseline by addressing domain-specific issues associated with satellite imagery. Furthermore, the new Building Proposal Generation (BPG) module and the CLIP-enabled noisy Building Proposal Selection (CLIP-BPS) module in U-BDD++ ensure high-quality self-training. Extensive experiments on the widely used building damage assessment benchmark demonstrate the effectiveness of the proposed method for unsupervised building damage detection. The presented annotation-free and foundation model-based paradigm ensures an efficient learning phase. This study opens a new direction for real-world BDD and sets a strong baseline for future research.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01576v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现有的建筑物损坏检测（BDD）方法总是需要对建筑物及其条件进行劳动密集型像素级注释，因此在很大程度上限制了其应用。在本文中，我们研究了BDD的一个具有挑战性但实用的场景，即无监督的建筑物损坏检测（U-BDD），其中只提供未标记的灾前和灾后卫星图像对。作为一项试点研究，我们首先提出了一种先进的U-BDD基线，该基线利用预先训练的视觉语言基础模型（即基础DINO、SAM和CLIP）来解决U-BDD任务。然而，卫星图像和普通图像之间明显的领域差距导致用于识别建筑物及其损坏的基础模型的置信度较低。作为回应，我们进一步提出了一个新的自我监督框架U-BDD++，该框架通过解决与卫星图像相关的特定领域问题，改进了U-BDD基线。此外，U-BDD++中新的建筑方案生成（BPG）模块和启用CLIP的嘈杂建筑方案选择（CLIP-BPS）模块确保了高质量的自我培训。在广泛使用的建筑损伤评估基准上进行的大量实验证明了所提出的无监督建筑损伤检测方法的有效性。所提出的无注释和基于基础的范式确保了有效的学习阶段。这项研究为现实世界的BDD开辟了一个新的方向，并为未来的研究奠定了坚实的基础。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01576v1" target="_blank">2312.01576v1</a>
                              </td>
                              <td>Learning Efficient Unsupervised Satellite Image-based Building Damage Detection</td>
                              <td>Yiyun Zhang</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01576v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01576v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/fzmi/ubdd" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_03513v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exploring DINO: Emergent Properties and Limitations for Synthetic Aperture Radar Imagery</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_03513v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_03513v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_03513v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised learning (SSL) models have recently demonstrated remarkable performance across various tasks, including image segmentation. This study delves into the emergent characteristics of the Self-Distillation with No Labels (DINO) algorithm and its application to Synthetic Aperture Radar (SAR) imagery. We pre-train a vision transformer (ViT)-based DINO model using unlabeled SAR data, and later fine-tune the model to predict high-resolution land cover maps. We rigorously evaluate the utility of attention maps generated by the ViT backbone and compare them with the model's token embedding space. We observe a small improvement in model performance with pre-training compared to training from scratch and discuss the limitations and opportunities of SSL for remote sensing and land cover segmentation. Beyond small performance increases, we show that ViT attention maps hold great intrinsic value for remote sensing, and could provide useful inputs to other algorithms. With this, our work lays the groundwork for bigger and better SSL models for Earth Observation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_03513v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督学习（SSL）模型最近在包括图像分割在内的各种任务中表现出了显著的性能。本研究探讨了无标签自蒸馏（DINO）算法的涌现特性及其在合成孔径雷达（SAR）图像中的应用。我们使用未标记的SAR数据预训练基于视觉变换器（ViT）的DINO模型，然后对模型进行微调，以预测高分辨率的土地覆盖图。我们严格评估了ViT主干生成的注意力图的效用，并将其与模型的令牌嵌入空间进行了比较。我们观察到，与从头开始的训练相比，预训练的模型性能略有提高，并讨论了SSL在遥感和土地覆盖分割方面的局限性和机会。除了小幅的性能提高外，我们还表明，ViT注意力图对遥感具有巨大的内在价值，并可以为其他算法提供有用的输入。有了这一点，我们的工作为更大更好的地球观测SSL模型奠定了基础。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.03513v2" target="_blank">2310.03513v2</a>
                              </td>
                              <td>Exploring DINO: Emergent Properties and Limitations for Synthetic Aperture Radar Imagery</td>
                              <td>Joseph A. Gallego-Mejia</td>
                              <td>2023-10-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_03513v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.03513v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_02048v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exploring Generalisability of Self-Distillation with No Labels for SAR-Based Vegetation Prediction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_02048v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_02048v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_02048v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this work we pre-train a DINO-ViT based model using two Synthetic Aperture Radar datasets (S1GRD or GSSIC) across three regions (China, Conus, Europe). We fine-tune the models on smaller labeled datasets to predict vegetation percentage, and empirically study the connection between the embedding space of the models and their ability to generalize across diverse geographic regions and to unseen data. For S1GRD, embedding spaces of different regions are clearly separated, while GSSIC's overlaps. Positional patterns remain during fine-tuning, and greater distances in embeddings often result in higher errors for unfamiliar regions. With this, our work increases our understanding of generalizability for self-supervised models applied to remote sensing.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_02048v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项工作中，我们使用三个地区（中国、美国和欧洲）的两个合成孔径雷达数据集（S1GRD或GSSIC）预训练了一个基于DINO-ViT的模型。我们在较小的标记数据集上微调模型，以预测植被百分比，并实证研究模型的嵌入空间与其在不同地理区域和看不见的数据之间的联系。对于S1GRD，不同区域的嵌入空间明显分离，而GSSIC的嵌入空间重叠。在微调过程中，位置模式仍然存在，嵌入中距离越大，通常会导致不熟悉区域的误差越大。通过这一点，我们的工作增加了我们对应用于遥感的自监督模型的可推广性的理解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.02048v2" target="_blank">2310.02048v2</a>
                              </td>
                              <td>Exploring Generalisability of Self-Distillation with No Labels for SAR-Based Vegetation Prediction</td>
                              <td>Laura Martínez-Ferrer</td>
                              <td>2023-10-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_02048v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.02048v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_03940v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Hard View Selection for Self-Supervised Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_03940v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_03940v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_03940v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Many Self-Supervised Learning (SSL) methods train their models to be invariant to different "views" of an image input for which a good data augmentation pipeline is crucial. While considerable efforts were directed towards improving pre-text tasks, architectures, or robustness (e.g., Siamese networks or teacher-softmax centering), the majority of these methods remain strongly reliant on the random sampling of operations within the image augmentation pipeline, such as the random resized crop or color distortion operation. In this paper, we argue that the role of the view generation and its effect on performance has so far received insufficient attention. To address this, we propose an easy, learning-free, yet powerful Hard View Selection (HVS) strategy designed to extend the random view generation to expose the pretrained model to harder samples during SSL training. It encompasses the following iterative steps: 1) randomly sample multiple views and create pairs of two views, 2) run forward passes for each view pair on the currently trained model, 3) adversarially select the pair yielding the worst loss, and 4) run the backward pass with the selected pair. In our empirical analysis we show that under the hood, HVS increases task difficulty by controlling the Intersection over Union of views during pretraining. With only 300-epoch pretraining, HVS is able to closely rival the 800-epoch DINO baseline which remains very favorable even when factoring in the slowdown induced by the additional forwards of HVS. Additionally, HVS consistently achieves accuracy improvements on ImageNet between 0.4% and 1.9% on linear evaluation and similar improvements on transfer tasks across multiple SSL methods, such as DINO, SimSiam, iBOT, and SimCLR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_03940v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>许多自监督学习（SSL）方法将其模型训练为对图像输入的不同“视图”保持不变，而良好的数据增强管道对图像输入至关重要。虽然在改进文本前任务、架构或稳健性（例如，连体网络或教师softmax居中）方面做出了相当大的努力，但这些方法中的大多数仍然强烈依赖于图像增强管道内的操作的随机采样，例如随机调整大小的裁剪或颜色失真操作。在本文中，我们认为到目前为止，视图生成的作用及其对性能的影响还没有得到足够的关注。为了解决这一问题，我们提出了一种简单、无需学习但功能强大的硬视图选择（HVS）策略，旨在扩展随机视图生成，以便在SSL训练期间将预训练的模型暴露给更硬的样本。它包括以下迭代步骤：1）随机采样多个视图并创建两个视图对，2）在当前训练的模型上为每个视图对运行前向通道，3）对抗性地选择产生最差损失的一对，以及4）使用所选的一对运行后向通道。在我们的实证分析中，我们发现在引擎盖下，HVS通过在预训练过程中控制视图并集上的交集来增加任务难度。只有300个历元的预训练，HVS能够与800个历元DINO基线相媲美，即使考虑到HVS额外前锋导致的速度减慢，这一基线仍然非常有利。此外，HVS在ImageNet上的线性评估准确率持续提高0.4%至1.9%，在多种SSL方法（如DINO、SimSiam、iBOT和SimCLR）的传输任务上也实现了类似的提高。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.03940v3" target="_blank">2310.03940v3</a>
                              </td>
                              <td>Hard View Selection for Self-Supervised Learning</td>
                              <td>Fabio Ferreira</td>
                              <td>2023-10-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_03940v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.03940v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_18809v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FoundPose: Unseen Object Pose Estimation with Foundation Features</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_18809v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_18809v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_18809v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose FoundPose, a method for 6D pose estimation of unseen rigid objects from a single RGB image. The method assumes that 3D models of the objects are available but does not require any object-specific training. This is achieved by building upon DINOv2, a recent vision foundation model with impressive generalization capabilities. An online pose estimation stage is supported by a minimal object representation that is built during a short onboarding stage from DINOv2 patch features extracted from rendered object templates. Given a query image with an object segmentation mask, FoundPose first rapidly retrieves a handful of similarly looking templates by a DINOv2-based bag-of-words approach. Pose hypotheses are then generated from 2D-3D correspondences established by matching DINOv2 patch features between the query image and a retrieved template, and finally optimized by featuremetric refinement. The method can handle diverse objects, including challenging ones with symmetries and without any texture, and noticeably outperforms existing RGB methods for coarse pose estimation in both accuracy and speed on the standard BOP benchmark. With the featuremetric and additional MegaPose refinement, which are demonstrated complementary, the method outperforms all RGB competitors. Source code is at: evinpinar.github.io/foundpose.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_18809v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了FoundPose，这是一种从单个RGB图像中对看不见的刚性物体进行6D姿态估计的方法。该方法假设对象的3D模型是可用的，但不需要任何特定于对象的训练。这是在DINOv2的基础上实现的，DINOv2是一个最近的视觉基础模型，具有令人印象深刻的泛化能力。在线姿态估计阶段由最小对象表示支持，该最小对象表示是在短期入职阶段根据从渲染对象模板中提取的DINOv2补丁特征构建的。给定一个带有对象分割掩码的查询图像，FoundPose首先通过基于DINOv2的单词袋方法快速检索一些外观相似的模板。然后，从通过匹配查询图像和检索到的模板之间的DINOv2补丁特征而建立的2D-3D对应关系中生成姿势假设，并最终通过特征度量细化进行优化。该方法可以处理不同的对象，包括具有对称性且没有任何纹理的具有挑战性的对象，并且在标准BOP基准上，在精度和速度方面明显优于现有的RGB方法进行粗略姿态估计。该方法的特征度量和额外的MegaPose细化被证明是互补的，优于所有RGB竞争对手。源代码位于：evinpinar.github.io/foundpose。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.18809v1" target="_blank">2311.18809v1</a>
                              </td>
                              <td>FoundPose: Unseen Object Pose Estimation with Foundation Features</td>
                              <td>Evin Pınar Örnek</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_18809v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.18809v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00079v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">HiFi Tuner: High-Fidelity Subject-Driven Fine-Tuning for Diffusion Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00079v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00079v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00079v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper explores advancements in high-fidelity personalized image generation through the utilization of pre-trained text-to-image diffusion models. While previous approaches have made significant strides in generating versatile scenes based on text descriptions and a few input images, challenges persist in maintaining the subject fidelity within the generated images. In this work, we introduce an innovative algorithm named HiFi Tuner to enhance the appearance preservation of objects during personalized image generation. Our proposed method employs a parameter-efficient fine-tuning framework, comprising a denoising process and a pivotal inversion process. Key enhancements include the utilization of mask guidance, a novel parameter regularization technique, and the incorporation of step-wise subject representations to elevate the sample fidelity. Additionally, we propose a reference-guided generation approach that leverages the pivotal inversion of a reference image to mitigate unwanted subject variations and artifacts. We further extend our method to a novel image editing task: substituting the subject in an image through textual manipulations. Experimental evaluations conducted on the DreamBooth dataset using the Stable Diffusion model showcase promising results. Fine-tuning solely on textual embeddings improves CLIP-T score by 3.6 points and improves DINO score by 9.6 points over Textual Inversion. When fine-tuning all parameters, HiFi Tuner improves CLIP-T score by 1.2 points and improves DINO score by 1.2 points over DreamBooth, establishing a new state of the art.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00079v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文通过利用预先训练的文本到图像扩散模型，探索了高保真个性化图像生成的进展。虽然以前的方法在基于文本描述和一些输入图像生成多功能场景方面取得了重大进展，但在保持生成图像中的主题保真度方面仍然存在挑战。在这项工作中，我们引入了一种名为HiFi Tuner的创新算法，以在个性化图像生成过程中增强对象的外观保护。我们提出的方法采用了一个参数有效的微调框架，包括去噪过程和关键反演过程。关键的增强包括利用掩模引导、一种新的参数正则化技术，以及结合逐步主题表示来提高样本保真度。此外，我们提出了一种参考引导生成方法，该方法利用参考图像的关键反转来减轻不必要的受试者变化和伪影。我们进一步将我们的方法扩展到一个新颖的图像编辑任务：通过文本操作替换图像中的主体。使用稳定扩散模型在DreamBooth数据集上进行的实验评估显示了有希望的结果。与文本反转相比，仅对文本嵌入进行微调可将CLIP-T分数提高3.6分，将DINO分数提高9.6分。当对所有参数进行微调时，HiFi Tuner将CLIP-T分数提高了1.2分，并将DINO分数提高了比DreamBooth高1.2分，建立了新的技术水平。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00079v1" target="_blank">2312.00079v1</a>
                              </td>
                              <td>HiFi Tuner: High-Fidelity Subject-Driven Fine-Tuning for Diffusion Models</td>
                              <td>Zhonghao Wang</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00079v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00079v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_17893v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Betrayed by Attention: A Simple yet Effective Approach for Self-supervised Video Object Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_17893v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_17893v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_17893v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we propose a simple yet effective approach for self-supervised video object segmentation (VOS). Our key insight is that the inherent structural dependencies present in DINO-pretrained Transformers can be leveraged to establish robust spatio-temporal correspondences in videos. Furthermore, simple clustering on this correspondence cue is sufficient to yield competitive segmentation results. Previous self-supervised VOS techniques majorly resort to auxiliary modalities or utilize iterative slot attention to assist in object discovery, which restricts their general applicability and imposes higher computational requirements. To deal with these challenges, we develop a simplified architecture that capitalizes on the emerging objectness from DINO-pretrained Transformers, bypassing the need for additional modalities or slot attention. Specifically, we first introduce a single spatio-temporal Transformer block to process the frame-wise DINO features and establish spatio-temporal dependencies in the form of self-attention. Subsequently, utilizing these attention maps, we implement hierarchical clustering to generate object segmentation masks. To train the spatio-temporal block in a fully self-supervised manner, we employ semantic and dynamic motion consistency coupled with entropy normalization. Our method demonstrates state-of-the-art performance across multiple unsupervised VOS benchmarks and particularly excels in complex real-world multi-object video segmentation tasks such as DAVIS-17-Unsupervised and YouTube-VIS-19. The code and model checkpoints will be released at https://github.com/shvdiwnkozbw/SSL-UVOS.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_17893v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了一种简单而有效的自监督视频对象分割（VOS）方法。我们的关键见解是，DINO预训练的变形金刚中存在的固有结构依赖性可以用来在视频中建立强大的时空对应关系。此外，在该对应线索上的简单聚类足以产生有竞争力的分割结果。以前的自监督VOS技术主要采用辅助模态或利用迭代槽注意力来辅助对象发现，这限制了它们的普遍适用性，并提出了更高的计算要求。为了应对这些挑战，我们开发了一种简化的架构，该架构利用了DINO预训练变压器中出现的对象性，绕过了对额外模式或插槽关注的需求。具体来说，我们首先引入单个时空变换器块来处理逐帧的DINO特征，并以自注意的形式建立时空依赖关系。随后，利用这些注意力图，我们实现了分层聚类来生成对象分割掩码。为了以完全自监督的方式训练时空块，我们使用语义和动态运动一致性以及熵归一化。我们的方法在多个无监督VOS基准测试中展示了最先进的性能，尤其擅长复杂的真实世界多对象视频分割任务，如DAVIS-17-无监督和YouTube-VIS-19。代码和模型检查点将在发布https://github.com/shvdiwnkozbw/ssl-uvos.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.17893v1" target="_blank">2311.17893v1</a>
                              </td>
                              <td>Betrayed by Attention: A Simple yet Effective Approach for Self-supervised Video Object Segmentation</td>
                              <td>Shuangrui Ding</td>
                              <td>2023-11-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_17893v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.17893v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_15347v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_15347v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_15347v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_15347v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Text-to-image diffusion models have made significant advances in generating and editing high-quality images. As a result, numerous approaches have explored the ability of diffusion model features to understand and process single images for downstream tasks, e.g., classification, semantic segmentation, and stylization. However, significantly less is known about what these features reveal across multiple, different images and objects. In this work, we exploit Stable Diffusion (SD) features for semantic and dense correspondence and discover that with simple post-processing, SD features can perform quantitatively similar to SOTA representations. Interestingly, the qualitative analysis reveals that SD features have very different properties compared to existing representation learning features, such as the recently released DINOv2: while DINOv2 provides sparse but accurate matches, SD features provide high-quality spatial information but sometimes inaccurate semantic matches. We demonstrate that a simple fusion of these two features works surprisingly well, and a zero-shot evaluation using nearest neighbors on these fused features provides a significant performance gain over state-of-the-art methods on benchmark datasets, e.g., SPair-71k, PF-Pascal, and TSS. We also show that these correspondences can enable interesting applications such as instance swapping in two images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_15347v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文本到图像的扩散模型在生成和编辑高质量图像方面取得了重大进展。因此，许多方法已经探索了扩散模型特征理解和处理下游任务的单个图像的能力，例如分类、语义分割和风格化。然而，人们对这些特征在多个不同的图像和对象中所揭示的内容知之甚少。在这项工作中，我们利用稳定扩散（SD）特征进行语义和密集对应，并发现通过简单的后处理，SD特征可以在数量上与SOTA表示相似。有趣的是，定性分析表明，与现有的表示学习特征（如最近发布的DINOv2）相比，SD特征具有非常不同的特性：虽然DINOv2提供了稀疏但准确的匹配，但SD特征提供了高质量的空间信息，但有时语义匹配不准确。我们证明，这两个特征的简单融合效果令人惊讶地好，并且在这些融合特征上使用最近邻居的零样本评估在基准数据集（例如，SPair-71k、PF-Pascal和TSS）上提供了比现有技术方法显著的性能增益。我们还展示了这些对应关系可以实现有趣的应用程序，例如两个图像中的实例交换。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.15347v2" target="_blank">2305.15347v2</a>
                              </td>
                              <td>A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence</td>
                              <td>Junyi Zhang</td>
                              <td>2023-05-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_15347v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.15347v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_15937v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Optimal Transport Aggregation for Visual Place Recognition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_15937v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_15937v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_15937v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The task of Visual Place Recognition (VPR) aims to match a query image against references from an extensive database of images from different places, relying solely on visual cues. State-of-the-art pipelines focus on the aggregation of features extracted from a deep backbone, in order to form a global descriptor for each image. In this context, we introduce SALAD (Sinkhorn Algorithm for Locally Aggregated Descriptors), which reformulates NetVLAD's soft-assignment of local features to clusters as an optimal transport problem. In SALAD, we consider both feature-to-cluster and cluster-to-feature relations and we also introduce a 'dustbin' cluster, designed to selectively discard features deemed non-informative, enhancing the overall descriptor quality. Additionally, we leverage and fine-tune DINOv2 as a backbone, which provides enhanced description power for the local features, and dramatically reduces the required training time. As a result, our single-stage method not only surpasses single-stage baselines in public VPR datasets, but also surpasses two-stage methods that add a re-ranking with significantly higher cost. Code and models are available at https://github.com/serizba/salad.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_15937v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉位置识别（VPR）的任务旨在仅依靠视觉提示，将查询图像与来自不同位置的图像的广泛数据库中的参考进行匹配。最先进的管道专注于从深层主干提取的特征的聚合，以便为每个图像形成全局描述符。在这种情况下，我们引入了SALAD（局部聚合描述符的Sinkhorn算法），它将NetVLAD的局部特征到簇的软分配重新表述为最优传输问题。在SALAD中，我们考虑了特征到聚类和聚类到特征的关系，我们还引入了一个“垃圾箱”聚类，旨在选择性地丢弃被视为非信息性的特征，提高整体描述符质量。此外，我们利用并微调DINOv2作为主干，它为局部特征提供了增强的描述能力，并显著减少了所需的训练时间。因此，我们的单阶段方法不仅超过了公共VPR数据集中的单阶段基线，而且还超过了以显著更高的成本添加重新排序的两阶段方法。代码和型号可在https://github.com/serizba/salad.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.15937v1" target="_blank">2311.15937v1</a>
                              </td>
                              <td>Optimal Transport Aggregation for Visual Place Recognition</td>
                              <td>Sergio Izquierdo</td>
                              <td>2023-11-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_15937v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.15937v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/serizba/salad" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_14665v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Understanding Self-Supervised Features for Learning Unsupervised Instance Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_14665v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_14665v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_14665v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised learning (SSL) can be used to solve complex visual tasks without human labels. Self-supervised representations encode useful semantic information about images, and as a result, they have already been used for tasks such as unsupervised semantic segmentation. In this paper, we investigate self-supervised representations for instance segmentation without any manual annotations. We find that the features of different SSL methods vary in their level of instance-awareness. In particular, DINO features, which are known to be excellent semantic descriptors, lack behind MAE features in their sensitivity for separating instances.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_14665v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督学习（SSL）可以用于解决没有人为标签的复杂视觉任务。自监督表示对图像的有用语义信息进行编码，因此，它们已经被用于无监督语义分割等任务。在本文中，我们研究了在没有任何手动注释的情况下进行实例分割的自监督表示。我们发现，不同SSL方法的特性在实例感知级别上有所不同。特别是，众所周知，DINO特征是优秀的语义描述符，但在分离实例的敏感性方面却落后于MAE特征。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.14665v1" target="_blank">2311.14665v1</a>
                              </td>
                              <td>Understanding Self-Supervised Features for Learning Unsupervised Instance Segmentation</td>
                              <td>Paul Engstler</td>
                              <td>2023-11-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_14665v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.14665v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_13110v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_13110v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_13110v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_13110v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we contend that a natural objective of representation learning is to compress and transform the distribution of the data, say sets of tokens, towards a low-dimensional Gaussian mixture supported on incoherent subspaces. The goodness of such a representation can be evaluated by a principled measure, called sparse rate reduction, that simultaneously maximizes the intrinsic information gain and extrinsic sparsity of the learned representation. From this perspective, popular deep network architectures, including transformers, can be viewed as realizing iterative schemes to optimize this measure. Particularly, we derive a transformer block from alternating optimization on parts of this objective: the multi-head self-attention operator compresses the representation by implementing an approximate gradient descent step on the coding rate of the features, and the subsequent multi-layer perceptron sparsifies the features. This leads to a family of white-box transformer-like deep network architectures, named CRATE, which are mathematically fully interpretable. We show, by way of a novel connection between denoising and compression, that the inverse to the aforementioned compressive encoding can be realized by the same class of CRATE architectures. Thus, the so-derived white-box architectures are universal to both encoders and decoders. Experiments show that these networks, despite their simplicity, indeed learn to compress and sparsify representations of large-scale real-world image and text datasets, and achieve performance very close to highly engineered transformer-based models: ViT, MAE, DINO, BERT, and GPT2. We believe the proposed computational framework demonstrates great potential in bridging the gap between theory and practice of deep learning, from a unified perspective of data compression. Code is available at: https://ma-lab-berkeley.github.io/CRATE .</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_13110v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们认为表示学习的一个自然目标是将数据的分布（比如令牌集）压缩和转换为非相干子空间上支持的低维高斯混合。这种表示的优度可以通过一种称为稀疏率降低的原则性度量来评估，该度量同时最大化学习表示的内在信息增益和外在稀疏性。从这个角度来看，包括transformer在内的流行的深度网络架构可以被视为实现了优化这一措施的迭代方案。特别地，我们从该目标部分的交替优化中推导出一个变换器块：多头自注意算子通过对特征的编码率实现近似梯度下降步骤来压缩表示，随后的多层感知器对特征进行稀疏化。这导致了一系列类似白盒变压器的深度网络架构，称为CRATE，在数学上是完全可解释的。我们通过去噪和压缩之间的新连接表明，上述压缩编码的逆编码可以通过同一类CRATE架构来实现。因此，如此导出的白盒架构对于编码器和解码器都是通用的。实验表明，尽管这些网络很简单，但它们确实学会了压缩和稀疏大规模真实世界图像和文本数据集的表示，并实现了非常接近高度工程化的基于转换器的模型的性能：ViT、MAE、DINO、BERT和GPT2。我们认为，从数据压缩的统一角度来看，所提出的计算框架在弥合深度学习理论和实践之间的差距方面显示出巨大的潜力。代码位于：https://ma-lab-berkeley.github.io/crate。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.13110v2" target="_blank">2311.13110v2</a>
                              </td>
                              <td>White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?</td>
                              <td>Yaodong Yu</td>
                              <td>2023-11-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_13110v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.13110v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_13717v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Importance of Feature Extraction in the Calculation of Fréchet Distance for Medical Imaging</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_13717v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_13717v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_13717v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Fr\'echet Inception Distance is a widely used metric for evaluating synthetic image quality that utilizes an ImageNet-trained InceptionV3 network as a feature extractor. However, its application in medical imaging lacks a standard feature extractor, leading to biased and inconsistent comparisons. This study aimed to compare state-of-the-art feature extractors for computing Fr\'echet Distances (FDs) in medical imaging. A StyleGAN2 network was trained with data augmentation techniques tailored for limited data domains on datasets comprising three medical imaging modalities and four anatomical locations. Human evaluation of generative quality (via a visual Turing test) was compared to FDs calculated using ImageNet-trained InceptionV3, ResNet50, SwAV, DINO, and Swin Transformer architectures, in addition to an InceptionV3 network trained on a large medical dataset, RadImageNet. All ImageNet-based extractors were consistent with each other, but only SwAV was significantly correlated with medical expert judgment. The RadImageNet-based FD showed volatility and lacked correlation with human judgment. Caution is advised when using medical image-trained extraction networks in the FD calculation. These networks should be rigorously evaluated on the imaging modality under consideration and publicly released. ImageNet-based extractors, while imperfect, are consistent and widely understood. Training extraction networks with SwAV is a promising approach for synthetic medical image evaluation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_13717v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Fr’echet Inception Distance是一种广泛用于评估合成图像质量的指标，它利用ImageNet训练的InceptionV3网络作为特征提取器。然而，它在医学成像中的应用缺乏标准的特征提取器，导致了有偏差和不一致的比较。本研究旨在比较用于计算医学成像中Fr’chet距离（FD）的最先进的特征提取器。StyleGAN2网络使用针对数据集上的有限数据域量身定制的数据增强技术进行训练，数据集包括三种医学成像模式和四个解剖位置。将人类对生成质量的评估（通过视觉图灵测试）与使用ImageNet训练的InceptionV3、ResNet50、SwAV、DINO和Swin Transformer架构以及在大型医学数据集RadImageNet上训练的InceptV3网络计算的FD进行比较。所有基于ImageNet的提取器彼此一致，但只有SwAV与医学专家的判断显著相关。基于RadImageNet的FD显示出波动性，并且缺乏与人类判断的相关性。当在FD计算中使用医学图像训练的提取网络时，建议谨慎。这些网络应根据正在考虑的成像模式进行严格评估并公开发布。基于ImageNet的提取器虽然不完善，但却是一致的，并得到了广泛的理解。用SwAV训练提取网络是一种很有前途的合成医学图像评估方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.13717v1" target="_blank">2311.13717v1</a>
                              </td>
                              <td>Importance of Feature Extraction in the Calculation of Fréchet Distance for Medical Imaging</td>
                              <td>McKell Woodland</td>
                              <td>2023-11-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_13717v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.13717v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/mckellwoodland/fid-med-eval" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_12969v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Detect Every Thing with Few Examples</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_12969v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_12969v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_12969v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Open-set object detection aims at detecting arbitrary categories beyond those seen during training. Most recent advancements have adopted the open-vocabulary paradigm, utilizing vision-language backbones to represent categories with language. In this paper, we introduce DE-ViT, an open-set object detector that employs vision-only DINOv2 backbones and learns new categories through example images instead of language. To improve general detection ability, we transform multi-classification tasks into binary classification tasks while bypassing per-class inference, and propose a novel region propagation technique for localization. We evaluate DE-ViT on open-vocabulary, few-shot, and one-shot object detection benchmark with COCO and LVIS. For COCO, DE-ViT outperforms the open-vocabulary SoTA by 6.9 AP50 and achieves 50 AP50 in novel classes. DE-ViT surpasses the few-shot SoTA by 15 mAP on 10-shot and 7.2 mAP on 30-shot and one-shot SoTA by 2.8 AP50. For LVIS, DE-ViT outperforms the open-vocabulary SoTA by 2.2 mask AP and reaches 34.3 mask APr. Code is available at https://github.com/mlzxy/devit.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_12969v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>开集对象检测的目的是检测训练中发现的任意类别之外的任意类别。最近的进展采用了开放词汇范式，利用视觉语言主干来用语言表示类别。在本文中，我们介绍了DE ViT，这是一种开放集对象检测器，它使用仅视觉的DINOv2主干，并通过示例图像而不是语言来学习新的类别。为了提高一般检测能力，我们将多分类任务转换为二进制分类任务，同时绕过每类推理，并提出了一种新的区域传播定位技术。我们使用COCO和LVIS在开放词汇、少镜头和单镜头物体检测基准上评估了DE-ViT。对于COCO，DE ViT比开放词汇SoTA高6.9 AP50，并在新类中达到50 AP50。DE ViT在10次发射时以15毫安时的容量超过了少数发射的SoTA，在30次发射时则以7.2毫安时的体积超过了SoTA，而在一次发射时仅以2.8 AP50的容量超过SoTA。对于LVIS，DE ViT比开放词汇表SoTA高2.2掩码AP，达到34.3掩码AP。代码可在https://github.com/mlzxy/devit.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.12969v2" target="_blank">2309.12969v2</a>
                              </td>
                              <td>Detect Every Thing with Few Examples</td>
                              <td>Xinyu Zhang</td>
                              <td>2023-09-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_12969v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.12969v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/mlzxy/devit" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2006_01236v7_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Aperiodicity, Star-freeness, and First-order Logic Definability of Operator Precedence Languages</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2006_01236v7_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2006_01236v7_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2006_01236v7_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A classic result in formal language theory is the equivalence among non-counting, or aperiodic, regular languages, and languages defined through star-free regular expressions, or first-order logic. Past attempts to extend this result beyond the realm of regular languages have met with difficulties: for instance it is known that star-free tree languages may violate the non-counting property and there are aperiodic tree languages that cannot be defined through first-order logic. We extend such classic equivalence results to a significant family of deterministic context-free languages, the operator-precedence languages (OPL), which strictly includes the widely investigated visibly pushdown, alias input-driven, family and other structured context-free languages. The OP model originated in the '60s for defining programming languages and is still used by high performance compilers; its rich algebraic properties have been investigated initially in connection with grammar learning and recently completed with further closure properties and with monadic second order logic definition. We introduce an extension of regular expressions, the OP-expressions (OPE) which define the OPLs and, under the star-free hypothesis, define first-order definable and non-counting OPLs. Then, we prove, through a fairly articulated grammar transformation, that aperiodic OPLs are first-order definable. Thus, the classic equivalence of star-freeness, aperiodicity, and first-order definability is established for the large and powerful class of OPLs. We argue that the same approach can be exploited to obtain analogous results for visibly pushdown languages too.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2006_01236v7_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>形式语言理论中的一个经典结果是不计数或非周期的正则语言与通过无星正则表达式或一阶逻辑定义的语言之间的等价性。过去试图将这一结果扩展到正则语言之外的尝试遇到了困难：例如，已知无星树语言可能违反不计数性质，并且存在无法通过一阶逻辑定义的非周期树语言。我们将这种经典等价结果扩展到一个重要的确定上下文无关语言族，即算子优先语言（OPL），它严格包括广泛研究的可见下推、别名输入驱动、族和其他结构化上下文无关语言。OP模型起源于60年代，用于定义编程语言，目前仍被高性能编译器使用；它丰富的代数性质最初与语法学习有关，最近又与进一步的闭包性质和一元二阶逻辑定义有关。我们引入了正则表达式的一个扩展，即OP表达式（OPE），它定义了OPL，并在无星假设下定义了一阶可定义和不计数的OPL。然后，我们通过一个相当清晰的语法转换证明了非周期OPL是一阶可定义的。因此，对于大而有力的OPL类，建立了星自由度、非周期性和一阶可定义性的经典等价性。我们认为，同样的方法也可以用于获得明显下推语言的类似结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2006.01236v7" target="_blank">2006.01236v7</a>
                              </td>
                              <td>Aperiodicity, Star-freeness, and First-order Logic Definability of Operator Precedence Languages</td>
                              <td>Dino Mandrioli</td>
                              <td>2020-06-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2006_01236v7_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2006.01236v7" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11754v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Large-Scale Car Parts (LSCP) Dataset for Lightweight Fine-Grained Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11754v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11754v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11754v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Automotive related datasets have previously been used for training autonomous driving systems or vehicle classification tasks. However, there is a lack of datasets in the field of automotive AI for car parts detection, and most available datasets are limited in size and scope, struggling to cover diverse scenarios. To address this gap, this paper presents a large-scale and fine-grained automotive dataset consisting of 84,162 images for detecting 12 different types of car parts. This dataset was collected from natural cameras and online websites which covers various car brands, scenarios, and shooting angles. To alleviate the burden of manual annotation, we propose a novel semi-supervised auto-labeling method that leverages state-of-the-art pre-trained detectors. Moreover, we study the limitations of the Grounding DINO approach for zero-shot labeling. Finally, we evaluate the effectiveness of our proposed dataset through fine-grained car parts detection by training several lightweight YOLO-series detectors.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11754v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>汽车相关数据集先前已用于训练自动驾驶系统或车辆分类任务。然而，用于汽车零部件检测的汽车人工智能领域缺乏数据集，大多数可用数据集的大小和范围都有限，难以覆盖各种场景。为了解决这一差距，本文提出了一个由84162张图像组成的大规模细粒度汽车数据集，用于检测12种不同类型的汽车零件。该数据集是从自然相机和在线网站收集的，涵盖了各种汽车品牌、场景和拍摄角度。为了减轻手动注释的负担，我们提出了一种新的半监督自动标记方法，该方法利用了最先进的预训练检测器。此外，我们还研究了用于零样本标记的Grounding DINO方法的局限性。最后，我们通过训练几个轻量级YOLO系列检测器，通过细粒度的汽车零件检测来评估我们提出的数据集的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11754v1" target="_blank">2311.11754v1</a>
                              </td>
                              <td>A Large-Scale Car Parts (LSCP) Dataset for Lightweight Fine-Grained Detection</td>
                              <td>Wang Jie</td>
                              <td>2023-11-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11754v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11754v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_12079v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FreeKD: Knowledge Distillation via Semantic Frequency Prompt</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_12079v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_12079v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_12079v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Knowledge distillation (KD) has been applied to various tasks successfully, and mainstream methods typically boost the student model via spatial imitation losses. However, the consecutive downsamplings induced in the spatial domain of teacher model is a type of corruption, hindering the student from analyzing what specific information needs to be imitated, which results in accuracy degradation. To better understand the underlying pattern of corrupted feature maps, we shift our attention to the frequency domain. During frequency distillation, we encounter a new challenge: the low-frequency bands convey general but minimal context, while the high are more informative but also introduce noise. Not each pixel within the frequency bands contributes equally to the performance. To address the above problem: (1) We propose the Frequency Prompt plugged into the teacher model, absorbing the semantic frequency context during finetuning. (2) During the distillation period, a pixel-wise frequency mask is generated via Frequency Prompt, to localize those pixel of interests (PoIs) in various frequency bands. Additionally, we employ a position-aware relational frequency loss for dense prediction tasks, delivering a high-order spatial enhancement to the student model. We dub our Frequency Knowledge Distillation method as FreeKD, which determines the optimal localization and extent for the frequency distillation. Extensive experiments demonstrate that FreeKD not only outperforms spatial-based distillation methods consistently on dense prediction tasks (e.g., FreeKD brings 3.8 AP gains for RepPoints-R50 on COCO2017 and 4.55 mIoU gains for PSPNet-R18 on Cityscapes), but also conveys more robustness to the student. Notably, we also validate the generalization of our approach on large-scale vision models (e.g., DINO and SAM).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_12079v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>知识提取（KD）已成功应用于各种任务，主流方法通常通过空间模仿损失来提升学生模型。然而，在教师模型的空间域中引发的连续下采样是一种腐败，阻碍了学生分析需要模仿的特定信息，从而导致准确性下降。为了更好地理解损坏特征图的潜在模式，我们将注意力转移到频域。在频率提取过程中，我们遇到了一个新的挑战：低频带传达了一般但最小的上下文，而高频带信息量更大，但也引入了噪声。并非频带内的每个像素对性能的贡献都相等。为了解决上述问题：（1）我们提出了插入教师模型的频率提示，在微调过程中吸收语义频率上下文。（2） 在提取期间，通过频率提示生成逐像素的频率掩码，以在各个频带中定位那些感兴趣的像素（PoI）。此外，我们在密集预测任务中使用了位置感知的关系频率损失，为学生模型提供了高阶空间增强。我们将我们的频率知识提取方法称为FreeKD，它确定了频率提取的最佳定位和范围。大量实验表明，FreeKD不仅在密集的预测任务上始终优于基于空间的蒸馏方法（例如，FreeKD在COCO2017上为RepPoints-R50带来了3.8的AP增益，在Cityscapes上为PSPNet-R18带来了4.55 mIoU增益），而且还向学生传达了更大的鲁棒性。值得注意的是，我们还验证了我们的方法在大规模视觉模型（例如，DINO和SAM）上的通用性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.12079v1" target="_blank">2311.12079v1</a>
                              </td>
                              <td>FreeKD: Knowledge Distillation via Semantic Frequency Prompt</td>
                              <td>Yuan Zhang</td>
                              <td>2023-11-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_12079v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.12079v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11125v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SecondPose: SE(3)-Consistent Dual-Stream Feature Fusion for Category-Level Pose Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11125v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11125v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11125v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Category-level object pose estimation, aiming to predict the 6D pose and 3D size of objects from known categories, typically struggles with large intra-class shape variation. Existing works utilizing mean shapes often fall short of capturing this variation. To address this issue, we present SecondPose, a novel approach integrating object-specific geometric features with semantic category priors from DINOv2. Leveraging the advantage of DINOv2 in providing SE(3)-consistent semantic features, we hierarchically extract two types of SE(3)-invariant geometric features to further encapsulate local-to-global object-specific information. These geometric features are then point-aligned with DINOv2 features to establish a consistent object representation under SE(3) transformations, facilitating the mapping from camera space to the pre-defined canonical space, thus further enhancing pose estimation. Extensive experiments on NOCS-REAL275 demonstrate that SecondPose achieves a 12.4% leap forward over the state-of-the-art. Moreover, on a more complex dataset HouseCat6D which provides photometrically challenging objects, SecondPose still surpasses other competitors by a large margin. The code will be released soon.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11125v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>类别级物体姿态估计旨在从已知类别中预测物体的6D姿态和3D尺寸，通常难以应对大的类内形状变化。利用平均形状的现有作品往往无法捕捉到这种变化。为了解决这个问题，我们提出了SecondPose，这是一种将DINOv2中特定于对象的几何特征与语义类别先验相结合的新方法。利用DINOv2在提供SE（3）一致语义特征方面的优势，我们分层提取了两种类型的SE（3（3）不变几何特征，以进一步封装局部到全局的特定对象信息。然后，这些几何特征与DINOv2特征点对齐，以在SE（3）变换下建立一致的对象表示，促进从相机空间到预定义规范空间的映射，从而进一步增强姿态估计。在NOCS-REAL275上进行的大量实验表明，SecondPose比最先进的技术进步了12.4%。此外，在更复杂的数据集HouseCat6D上，SecondPose仍然以很大的优势超过了其他竞争对手，该数据集提供了具有光度挑战性的物体。代码将很快发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11125v1" target="_blank">2311.11125v1</a>
                              </td>
                              <td>SecondPose: SE(3)-Consistent Dual-Stream Feature Fusion for Category-Level Pose Estimation</td>
                              <td>Yamei Chen</td>
                              <td>2023-11-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11125v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11125v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_09770v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DINO-VITS: Data-Efficient Noise-Robust Zero-Shot Voice Cloning via Multi-Tasking with Self-Supervised Speaker Verification Loss</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_09770v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_09770v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_09770v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent progress in self-supervised representation learning has opened up new opportunities for training from unlabeled data and has been a growing trend in voice conversion. However, unsupervised training of voice cloning seems to remain a challenging task. In this paper we propose a semi-supervised zero-shot voice cloning approach that works by adapting a HuBERT-based voice conversion system to the voice cloning task and shows the robustness of such a system to noises both in training data (we add noises resulting in up to 0db signal-to-noise-ratio to 35% of training data with no significant degradation of evaluation metrics) and in the target speaker reference audio at inference. Moreover, such a method does not require any type of denoising or noise-labeling of training data. Finally, we introduce a novel multi-tasking approach by incorporating self-supervised DINO loss into joint training of a CAM++ based speaker verification system and a unit-based VITS cloning system. We show that it significantly improves the quality of generated audio over baselines, especially for noisy target speaker references.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_09770v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督表示学习的最新进展为从未标记数据中进行训练开辟了新的机会，并成为语音转换的一个日益增长的趋势。然而，语音克隆的无监督训练似乎仍然是一项具有挑战性的任务。在本文中，我们提出了一种半监督的零样本语音克隆方法，该方法通过使基于HuBERT的语音转换系统适应语音克隆任务来工作，并显示了这种系统对训练数据中的噪声（我们将导致高达0db信号噪声比的噪声添加到35%的训练数据中，而评估指标没有显著退化）和目标中的噪声的鲁棒性说话者在推理时参考音频。此外，这种方法不需要对训练数据进行任何类型的去噪或噪声标记。最后，我们介绍了一种新的多任务方法，将自我监督的DINO损失纳入基于CAM++的说话人验证系统和基于单元的VITS克隆系统的联合训练中。我们发现，它显著提高了生成的音频质量，特别是对于有噪声的目标扬声器参考。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.09770v2" target="_blank">2311.09770v2</a>
                              </td>
                              <td>DINO-VITS: Data-Efficient Noise-Robust Zero-Shot Voice Cloning via Multi-Tasking with Self-Supervised Speaker Verification Loss</td>
                              <td>Vikentii Pankov</td>
                              <td>2023-11-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_09770v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.09770v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_10125v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">UnifiedVisionGPT: Streamlining Vision-Oriented AI through Generalized Multimodal Framework</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_10125v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_10125v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_10125v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In the current landscape of artificial intelligence, foundation models serve as the bedrock for advancements in both language and vision domains. OpenAI GPT-4 has emerged as the pinnacle in large language models (LLMs), while the computer vision (CV) domain boasts a plethora of state-of-the-art (SOTA) models such as Meta's SAM and DINO, and YOLOS. However, the financial and computational burdens of training new models from scratch remain a significant barrier to progress. In response to this challenge, we introduce UnifiedVisionGPT, a novel framework designed to consolidate and automate the integration of SOTA vision models, thereby facilitating the development of vision-oriented AI. UnifiedVisionGPT distinguishes itself through four key features: (1) provides a versatile multimodal framework adaptable to a wide range of applications, building upon the strengths of multimodal foundation models; (2) seamlessly integrates various SOTA vision models to create a comprehensive multimodal platform, capitalizing on the best components of each model; (3) prioritizes vision-oriented AI, ensuring a more rapid progression in the CV domain compared to the current trajectory of LLMs; and (4) introduces automation in the selection of SOTA vision models, generating optimal results based on diverse multimodal inputs such as text prompts and images. This paper outlines the architecture and capabilities of UnifiedVisionGPT, demonstrating its potential to revolutionize the field of computer vision through enhanced efficiency, versatility, generalization, and performance. Our implementation, along with the unified multimodal framework and comprehensive dataset, is made publicly available at https://github.com/LHBuilder/SA-Segment-Anything.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_10125v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在当前的人工智能领域，基础模型是语言和视觉领域进步的基石。OpenAI GPT-4已成为大型语言模型（LLM）的巅峰，而计算机视觉（CV）领域拥有大量最先进的（SOTA）模型，如Meta的SAM、DINO和YOLOS。然而，从头开始训练新模型的财务和计算负担仍然是取得进展的重大障碍。为了应对这一挑战，我们引入了UnifiedVisionGPT，这是一种新颖的框架，旨在整合和自动化SOTA视觉模型的集成，从而促进面向视觉的人工智能的发展。UnifiedVision通过四个关键功能脱颖而出：（1）提供了一个适用于广泛应用的多模式框架，利用多模态基础模型的优势；（2） 无缝集成各种SOTA愿景模型，利用每个模型的最佳组件，创建一个全面的多模式平台；（3） 优先考虑面向视觉的人工智能，确保与LLM的当前轨迹相比，CV领域的进展更快；以及（4）在SOTA视觉模型的选择中引入自动化，基于不同的多模式输入（如文本提示和图像）生成最佳结果。本文概述了UnifiedVisionGPT的体系结构和功能，展示了其通过提高效率、多功能性、通用性和性能来彻底改变计算机视觉领域的潜力。我们的实现，以及统一的多模式框架和全面的数据集，在https://github.com/lhbuilder/sa-segment-anything.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.10125v1" target="_blank">2311.10125v1</a>
                              </td>
                              <td>UnifiedVisionGPT: Streamlining Vision-Oriented AI through Generalized Multimodal Framework</td>
                              <td>Chris Kelly</td>
                              <td>2023-11-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_10125v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.10125v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/lhbuilder/sa-segment-anything" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_09350v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Generalizable Imitation Learning Through Pre-Trained Representations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_09350v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_09350v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_09350v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we leverage self-supervised vision transformer models and their emergent semantic abilities to improve the generalization abilities of imitation learning policies. We introduce BC-ViT, an imitation learning algorithm that leverages rich DINO pre-trained Visual Transformer (ViT) patch-level embeddings to obtain better generalization when learning through demonstrations. Our learner sees the world by clustering appearance features into semantic concepts, forming stable keypoints that generalize across a wide range of appearance variations and object types. We show that this representation enables generalized behaviour by evaluating imitation learning across a diverse dataset of object manipulation tasks. Our method, data and evaluation approach are made available to facilitate further study of generalization in Imitation Learners.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_09350v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们利用自监督视觉转换器模型及其涌现的语义能力来提高模仿学习策略的泛化能力。我们介绍了BC-ViT，这是一种模仿学习算法，它利用丰富的DINO预训练的Visual Transformer（ViT）补丁级嵌入，在通过演示进行学习时获得更好的泛化能力。我们的学习者通过将外观特征聚类为语义概念来看待世界，形成稳定的关键点，这些关键点在广泛的外观变化和对象类型中进行概括。我们展示了这种表示通过评估对象操作任务的不同数据集的模仿学习来实现广义行为。我们提供的方法、数据和评估方法有助于进一步研究模仿学习者的泛化能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.09350v1" target="_blank">2311.09350v1</a>
                              </td>
                              <td>Generalizable Imitation Learning Through Pre-Trained Representations</td>
                              <td>Wei-Di Chang</td>
                              <td>2023-11-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_09350v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.09350v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_09118v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">WildlifeDatasets: An open-source toolkit for animal re-identification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_09118v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_09118v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_09118v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we present WildlifeDatasets (https://github.com/WildlifeDatasets/wildlife-datasets) - an open-source toolkit intended primarily for ecologists and computer-vision / machine-learning researchers. The WildlifeDatasets is written in Python, allows straightforward access to publicly available wildlife datasets, and provides a wide variety of methods for dataset pre-processing, performance analysis, and model fine-tuning. We showcase the toolkit in various scenarios and baseline experiments, including, to the best of our knowledge, the most comprehensive experimental comparison of datasets and methods for wildlife re-identification, including both local descriptors and deep learning approaches. Furthermore, we provide the first-ever foundation model for individual re-identification within a wide range of species - MegaDescriptor - that provides state-of-the-art performance on animal re-identification datasets and outperforms other pre-trained models such as CLIP and DINOv2 by a significant margin. To make the model available to the general public and to allow easy integration with any existing wildlife monitoring applications, we provide multiple MegaDescriptor flavors (i.e., Small, Medium, and Large) through the HuggingFace hub (https://huggingface.co/BVRA).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_09118v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了WildlifeDataset(https://github.com/wildlifedatasets/wildlife-datasets)-主要面向生态学家和计算机视觉/机器学习研究人员的开源工具包。WildlifeDataset是用Python编写的，允许直接访问公开可用的野生动物数据集，并为数据集预处理、性能分析和模型微调提供了多种方法。我们在各种场景和基线实验中展示了该工具包，据我们所知，包括对野生动物重新识别的数据集和方法进行最全面的实验比较，包括局部描述符和深度学习方法。此外，我们提供了第一个用于广泛物种内个体重新识别的基础模型MegaDescriptor，该模型在动物重新识别数据集上提供了最先进的性能，并显著优于其他预先训练的模型，如CLIP和DINOv2。为了向公众提供该模型，并允许与任何现有的野生动物监测应用程序轻松集成，我们通过HuggingFace中心提供多种MegaDescriptor风格（即小型、中型和大型）(https://huggingface.co/bvra)。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.09118v1" target="_blank">2311.09118v1</a>
                              </td>
                              <td>WildlifeDatasets: An open-source toolkit for animal re-identification</td>
                              <td>Vojtěch Čermák</td>
                              <td>2023-11-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_09118v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.09118v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/wildlifedatasets/wildlife-datasets" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_19522v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Are Natural Domain Foundation Models Useful for Medical Image Classification?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_19522v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_19522v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_19522v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The deep learning field is converging towards the use of general foundation models that can be easily adapted for diverse tasks. While this paradigm shift has become common practice within the field of natural language processing, progress has been slower in computer vision. In this paper we attempt to address this issue by investigating the transferability of various state-of-the-art foundation models to medical image classification tasks. Specifically, we evaluate the performance of five foundation models, namely SAM, SEEM, DINOv2, BLIP, and OpenCLIP across four well-established medical imaging datasets. We explore different training settings to fully harness the potential of these models. Our study shows mixed results. DINOv2 consistently outperforms the standard practice of ImageNet pretraining. However, other foundation models failed to consistently beat this established baseline indicating limitations in their transferability to medical image classification tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_19522v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>深度学习领域正朝着使用通用基础模型的方向发展，这些模型可以很容易地适应不同的任务。虽然这种范式转变已经成为自然语言处理领域的普遍做法，但计算机视觉的进展却较慢。在本文中，我们试图通过研究各种最先进的基础模型对医学图像分类任务的可转移性来解决这个问题。具体而言，我们评估了五个基础模型的性能，即SAM、SEEM、DINOv2、BLIP和OpenCLIP，它们跨越了四个成熟的医学成像数据集。我们探索了不同的训练环境，以充分利用这些模型的潜力。我们的研究结果喜忧参半。DINOv2始终优于ImageNet预训练的标准实践。然而，其他基础模型未能始终超过这一既定基线，这表明它们在医学图像分类任务中的可转移性存在局限性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.19522v2" target="_blank">2310.19522v2</a>
                              </td>
                              <td>Are Natural Domain Foundation Models Useful for Medical Image Classification?</td>
                              <td>Joana Palés Huix</td>
                              <td>2023-10-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_19522v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.19522v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_04010v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Excision And Recovery: Visual Defect Obfuscation Based Self-Supervised Anomaly Detection Strategy</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_04010v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_04010v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_04010v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Due to scarcity of anomaly situations in the early manufacturing stage, an unsupervised anomaly detection (UAD) approach is widely adopted which only uses normal samples for training. This approach is based on the assumption that the trained UAD model will accurately reconstruct normal patterns but struggles with unseen anomalous patterns. To enhance the UAD performance, reconstruction-by-inpainting based methods have recently been investigated, especially on the masking strategy of suspected defective regions. However, there are still issues to overcome: 1) time-consuming inference due to multiple masking, 2) output inconsistency by random masking strategy, and 3) inaccurate reconstruction of normal patterns when the masked area is large. Motivated by this, we propose a novel reconstruction-by-inpainting method, dubbed Excision And Recovery (EAR), that features single deterministic masking based on the ImageNet pre-trained DINO-ViT and visual obfuscation for hint-providing. Experimental results on the MVTec AD dataset show that deterministic masking by pre-trained attention effectively cuts out suspected defective regions and resolve the aforementioned issues 1 and 2. Also, hint-providing by mosaicing proves to enhance the UAD performance than emptying those regions by binary masking, thereby overcomes issue 3. Our approach achieves a high UAD performance without any change of the neural network structure. Thus, we suggest that EAR be adopted in various manufacturing industries as a practically deployable solution.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_04010v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于制造早期异常情况的稀缺性，一种仅使用正态样本进行训练的无监督异常检测（UAD）方法被广泛采用。这种方法基于这样的假设，即经过训练的无人机模型将准确地重建正常模式，但要与看不见的异常模式作斗争。为了提高UAD的性能，最近研究了通过基于修复的方法进行重建，特别是在可疑缺陷区域的掩蔽策略上。然而，仍有一些问题需要克服：1）由于多次掩蔽而导致的耗时推理，2）通过随机掩蔽策略的输出不一致，以及3）当掩蔽区域较大时，对正常模式的重建不准确。受此启发，我们提出了一种新的修复重建方法，称为切除和恢复（EAR），其特征是基于ImageNet预训练的DINO ViT的单一确定性掩蔽和用于提示提供的视觉模糊。在MVTec AD数据集上的实验结果表明，通过预先训练的注意力进行的确定性掩蔽有效地去除了可疑的缺陷区域，并解决了上述问题1和2。此外，通过镶嵌提供的提示被证明比通过二进制掩码清空那些区域增强了UAD性能，从而克服了问题3。我们的方法在不改变神经网络结构的情况下实现了高UAD性能。因此，我们建议在各种制造业中采用EAR作为一种可实际部署的解决方案。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.04010v2" target="_blank">2310.04010v2</a>
                              </td>
                              <td>Excision And Recovery: Visual Defect Obfuscation Based Self-Supervised Anomaly Detection Strategy</td>
                              <td>YeongHyeon Park</td>
                              <td>2023-10-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_04010v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.04010v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03570v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Cal-DETR: Calibrated Detection Transformer</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03570v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03570v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03570v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Albeit revealing impressive predictive performance for several computer vision tasks, deep neural networks (DNNs) are prone to making overconfident predictions. This limits the adoption and wider utilization of DNNs in many safety-critical applications. There have been recent efforts toward calibrating DNNs, however, almost all of them focus on the classification task. Surprisingly, very little attention has been devoted to calibrating modern DNN-based object detectors, especially detection transformers, which have recently demonstrated promising detection performance and are influential in many decision-making systems. In this work, we address the problem by proposing a mechanism for calibrated detection transformers (Cal-DETR), particularly for Deformable-DETR, UP-DETR and DINO. We pursue the train-time calibration route and make the following contributions. First, we propose a simple yet effective approach for quantifying uncertainty in transformer-based object detectors. Second, we develop an uncertainty-guided logit modulation mechanism that leverages the uncertainty to modulate the class logits. Third, we develop a logit mixing approach that acts as a regularizer with detection-specific losses and is also complementary to the uncertainty-guided logit modulation technique to further improve the calibration performance. Lastly, we conduct extensive experiments across three in-domain and four out-domain scenarios. Results corroborate the effectiveness of Cal-DETR against the competing train-time methods in calibrating both in-domain and out-domain detections while maintaining or even improving the detection performance. Our codebase and pre-trained models can be accessed at \url{https://github.com/akhtarvision/cal-detr}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03570v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管深度神经网络在一些计算机视觉任务中表现出令人印象深刻的预测性能，但它容易做出过于自信的预测。这限制了DNN在许多安全关键应用中的采用和更广泛的利用。然而，最近一直在努力校准DNN，几乎所有的努力都集中在分类任务上。令人惊讶的是，很少有人关注校准现代基于DNN的目标检测器，尤其是检测变压器，它们最近表现出了很好的检测性能，并在许多决策系统中具有影响力。在这项工作中，我们通过提出一种校准检测变压器（Cal-DETR）的机制来解决这个问题，特别是对于可变形DETR、UP-DETR和DINO。我们追求列车时间校准路线，并做出以下贡献。首先，我们提出了一种简单而有效的方法来量化基于变换器的物体检测器的不确定性。其次，我们开发了一种不确定性引导的logit调制机制，该机制利用不确定性来调制类logit。第三，我们开发了一种logit混合方法，该方法作为具有检测特定损耗的正则化子，也是对不确定性引导的logit调制技术的补充，以进一步提高校准性能。最后，我们在三个域内和四个域外场景中进行了广泛的实验。结果证实了Cal DETR在校准域内和域外检测方面的有效性，同时保持甚至提高了检测性能。我们的代码库和预训练模型可以访问\url{https://github.com/akhtarvision/cal-detr}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03570v1" target="_blank">2311.03570v1</a>
                              </td>
                              <td>Cal-DETR: Calibrated Detection Transformer</td>
                              <td>Muhammad Akhtar Munir</td>
                              <td>2023-11-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03570v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03570v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/akhtarvision/cal-detr" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_06203v4_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FLSL: Feature-level Self-supervised Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_06203v4_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_06203v4_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_06203v4_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Current self-supervised learning (SSL) methods (e.g., SimCLR, DINO, VICReg,MOCOv3) target primarily on representations at instance level and do not generalize well to dense prediction tasks, such as object detection and segmentation.Towards aligning SSL with dense predictions, this paper demonstrates for the first time the underlying mean-shift clustering process of Vision Transformers (ViT), which aligns well with natural image semantics (e.g., a world of objects and stuffs). By employing transformer for joint embedding and clustering, we propose a two-level feature clustering SSL method, coined Feature-Level Self-supervised Learning (FLSL). We present the formal definition of the FLSL problem and construct the objectives from the mean-shift and k-means perspectives. We show that FLSL promotes remarkable semantic cluster representations and learns an embedding scheme amenable to intra-view and inter-view feature clustering. Experiments show that FLSL yields significant improvements in dense prediction tasks, achieving 44.9 (+2.8)% AP and 46.5% AP in object detection, as well as 40.8 (+2.3)% AP and 42.1% AP in instance segmentation on MS-COCO, using Mask R-CNN with ViT-S/16 and ViT-S/8 as backbone, respectively. FLSL consistently outperforms existing SSL methods across additional benchmarks, including UAV17 object detection on UAVDT, and video instance segmentation on DAVIS 2017.We conclude by presenting visualization and various ablation studies to better understand the success of FLSL. The source code is available at https://github.com/ISL-CV/FLSL.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_06203v4_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当前的自监督学习（SSL）方法（例如，SimCLR、DINO、VICReg、MOCOv3）主要针对实例级别的表示，并且不能很好地推广到密集的预测任务，例如对象检测和分割。为了使SSL与密集预测相一致，本文首次演示了视觉变换器（ViT）的基本均值偏移聚类过程，该过程与自然图像语义（例如，对象和填充物的世界）非常一致。通过使用transformer进行联合嵌入和聚类，我们提出了一种两级特征聚类SSL方法，称为特征级自监督学习（FLSL）。我们给出了FLSL问题的形式化定义，并从均值转移和k-均值的角度构建了目标。我们表明，FLSL促进了显著的语义聚类表示，并学习了一种适用于视图内和视图间特征聚类的嵌入方案。实验表明，使用以ViT-S/16和ViT-S/8为骨干的Mask R-CNN，FLSL在密集预测任务中产生了显著的改进，在MS-COCO上分别实现了44.9（+2.8）%AP和46.5%AP，以及40.8（+2.3）%AP、42.1%AP的实例分割。FLSL在其他基准测试中始终优于现有的SSL方法，包括UAVDT上的UAV17对象检测和DAVIS 2017上的视频实例分割。我们通过展示可视化和各种消融研究来更好地了解FLSL的成功。源代码位于https://github.com/isl-cv/flsl.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.06203v4" target="_blank">2306.06203v4</a>
                              </td>
                              <td>FLSL: Feature-level Self-supervised Learning</td>
                              <td>Qing Su</td>
                              <td>2023-06-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_06203v4_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.06203v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03053v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Masking Hyperspectral Imaging Data with Pretrained Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03053v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03053v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03053v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The presence of undesired background areas associated with potential noise and unknown spectral characteristics degrades the performance of hyperspectral data processing. Masking out unwanted regions is key to addressing this issue. Processing only regions of interest yields notable improvements in terms of computational costs, required memory, and overall performance. The proposed processing pipeline encompasses two fundamental parts: regions of interest mask generation, followed by the application of hyperspectral data processing techniques solely on the newly masked hyperspectral cube. The novelty of our work lies in the methodology adopted for the preliminary image segmentation. We employ the Segment Anything Model (SAM) to extract all objects within the dataset, and subsequently refine the segments with a zero-shot Grounding Dino object detector, followed by intersection and exclusion filtering steps, without the need for fine-tuning or retraining. To illustrate the efficacy of the masking procedure, the proposed method is deployed on three challenging applications scenarios that demand accurate masking; shredded plastics characterization, drill core scanning, and litter monitoring. The numerical evaluation of the proposed masking method on the three applications is provided along with the used hyperparameters. The scripts for the method will be available at https://github.com/hifexplo/Masking.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03053v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>与潜在噪声和未知光谱特性相关联的不期望的背景区域的存在降低了高光谱数据处理的性能。掩盖不需要的区域是解决这一问题的关键。仅处理感兴趣的区域在计算成本、所需内存和总体性能方面产生了显著的改进。所提出的处理管道包括两个基本部分：感兴趣区域掩模生成，然后仅在新掩模的高光谱立方体上应用高光谱数据处理技术。我们工作的新颖之处在于用于初步图像分割的方法。我们使用Segment Anything Model（SAM）来提取数据集中的所有对象，然后使用零样本Grounding Dino对象检测器来细化片段，然后执行交集和排除过滤步骤，而无需进行微调或重新训练。为了说明掩蔽过程的有效性，将所提出的方法部署在三个需要精确掩蔽的具有挑战性的应用场景中；碎塑料表征、岩芯扫描和垃圾监测。提供了所提出的掩蔽方法对三个应用的数值评估以及所使用的超参数。该方法的脚本将在https://github.com/hifexplo/masking.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03053v1" target="_blank">2311.03053v1</a>
                              </td>
                              <td>Masking Hyperspectral Imaging Data with Pretrained Models</td>
                              <td>Elias Arbash</td>
                              <td>2023-11-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03053v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03053v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/hifexplo/masking" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_01584v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Secured Fiscal Credit Model: Multi-Agent Systems And Decentralized Autonomous Organisations For Tax Credit's Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_01584v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_01584v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_01584v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Tax incentives and fiscal bonuses have had a significant impact on the Italian economy over the past decade. In particular, the "Superbonus 110" tax relief in 2020, offering a generous 110% deduction for expenses related to energy efficiency improvements and seismic risk reduction in buildings, has played a pivotal role. However, the surge in construction activities has also brought about an unfortunate increase in fraudulent activities. To address this challenge, our research introduces a practical system for monitoring and managing the entire process of the Superbonus 110 tax credit, from its initiation to redemption. This system leverages artificial intelligence and blockchain technology to streamline tax credit management and incorporates controllers based on a Decentralised Autonomous Organisation architecture, bolstered by a Multi-agent System. The outcome of our work is a system capable of establishing a tokenomics framework that caters to the needs and functionalities of both investors and operators. Moreover, it features a robust control system to prevent inadvertent errors like double spending, overspending, and deceitful practices such as false claims of completed work. The collaborative approach between the Decentralised Autonomous Organisation and the Multi-agent System enhances trust and security levels among participants in a competitive environment where potential fraudsters might attempt to exploit the system. It also enables comprehensive tracking and monitoring of the entire Superbonus process. In the realm of engineering, our project represents an innovative fusion of blockchain technology and Multi-agent Systems, advancing the application of artificial intelligence. This integration guarantees the validation, recording, and execution of transactions with a remarkable level of trust and transparency.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_01584v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>过去十年，税收优惠和财政奖金对意大利经济产生了重大影响。特别是，2020年的“超级奖金110”税收减免发挥了关键作用，为提高建筑能效和降低地震风险的相关费用提供了110%的慷慨减免。然而，建筑活动的激增也带来了欺诈活动的不幸增加。为了应对这一挑战，我们的研究引入了一个实用的系统，用于监控和管理超级奖金110税收抵免的整个过程，从启动到赎回。该系统利用人工智能和区块链技术简化税收抵免管理，并结合了基于分散自治组织架构的控制器，由多代理系统支持。我们的工作成果是建立一个能够建立一个符合投资者和运营商需求和功能的标记基因组学框架的系统。此外，它还具有一个强大的控制系统，以防止意外的错误，如重复支出、超支和欺诈行为，如对已完成工作的虚假声明。分散自治组织和多代理系统之间的协作方法提高了参与者之间的信任和安全水平，在竞争环境中，潜在的欺诈者可能会试图利用该系统。它还可以全面跟踪和监控整个超级奖金流程。在工程领域，我们的项目代表了区块链技术和多智能体系统的创新融合，推动了人工智能的应用。这种集成保证了交易的验证、记录和执行具有显著的信任和透明度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.01584v2" target="_blank">2311.01584v2</a>
                              </td>
                              <td>Secured Fiscal Credit Model: Multi-Agent Systems And Decentralized Autonomous Organisations For Tax Credit's Tracking</td>
                              <td>Giovanni De Gasperis</td>
                              <td>2023-11-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_01584v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.01584v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_11092v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Universal Domain Adaptation from Foundation Models: A Baseline Study</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_11092v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_11092v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_11092v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Foundation models (e.g., CLIP or DINOv2) have shown their impressive learning and transfer capabilities in a wide range of visual tasks, by training on a large corpus of data and adapting to specific downstream tasks. It is, however, interesting that foundation models have not been fully explored for universal domain adaptation (UniDA), which is to learn models using labeled data in a source domain and unlabeled data in a target one, such that the learned models can successfully adapt to the target data. In this paper, we make comprehensive empirical studies of state-of-the-art UniDA methods using foundation models. We first observe that, unlike fine-tuning from ImageNet pre-trained models, as previous methods do, fine-tuning from foundation models yields significantly poorer results, sometimes even worse than training from scratch. While freezing the backbones, we demonstrate that although the foundation models greatly improve the performance of the baseline method that trains the models on the source data alone, existing UniDA methods generally fail to improve over the baseline. This suggests that new research efforts are very necessary for UniDA using foundation models. Based on these findings, we introduce \textit{CLIP distillation}, a parameter-free method specifically designed to distill target knowledge from CLIP models. The core of our \textit{CLIP distillation} lies in a self-calibration technique for automatic temperature scaling, a feature that significantly enhances the baseline's out-class detection capability. Although simple, our method outperforms previous approaches in most benchmark tasks, excelling in evaluation metrics including H-score/H$^3$-score and the newly proposed universal classification rate (UCR) metric. We hope that our investigation and the proposed simple framework can serve as a strong baseline to facilitate future studies in this field.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_11092v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基础模型（例如CLIP或DINOv2）通过在大量数据库上进行训练并适应特定的下游任务，在广泛的视觉任务中显示出了令人印象深刻的学习和迁移能力。然而，有趣的是，基础模型还没有被充分探索用于通用域自适应（UniDA），即使用源域中的标记数据和目标域中的未标记数据来学习模型，以便学习的模型能够成功地适应目标数据。在本文中，我们使用基础模型对最先进的UniDA方法进行了全面的实证研究。我们首先观察到，与之前的方法一样，从ImageNet预训练模型进行微调不同，从基础模型进行微调会产生明显较差的结果，有时甚至比从头开始训练更差。在冻结主干的同时，我们证明，尽管基础模型大大提高了仅在源数据上训练模型的基线方法的性能，但现有的UniDA方法通常无法在基线上改进。这表明，对于使用基础模型的UniDA来说，新的研究工作是非常必要的。基于这些发现，我们介绍了\textit｛CLIP蒸馏｝，这是一种专门设计用于从CLIP模型中提取目标知识的无参数方法。我们的\textit｛CLIP蒸馏｝的核心在于自动温度标度的自校准技术，这一功能显著增强了基线的超一流检测能力。尽管简单，但我们的方法在大多数基准任务中都优于以前的方法，在评估指标方面表现出色，包括H-核心/H$^3$-得分和新提出的通用分类率（UCR）指标。我们希望我们的调查和拟议的简单框架能够作为一个强有力的基线，促进该领域未来的研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.11092v2" target="_blank">2305.11092v2</a>
                              </td>
                              <td>Universal Domain Adaptation from Foundation Models: A Baseline Study</td>
                              <td>Bin Deng</td>
                              <td>2023-05-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_11092v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.11092v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/szubing/uniood" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_08854v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rank-DETR for High Quality Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_08854v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_08854v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_08854v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Modern detection transformers (DETRs) use a set of object queries to predict a list of bounding boxes, sort them by their classification confidence scores, and select the top-ranked predictions as the final detection results for the given input image. A highly performant object detector requires accurate ranking for the bounding box predictions. For DETR-based detectors, the top-ranked bounding boxes suffer from less accurate localization quality due to the misalignment between classification scores and localization accuracy, thus impeding the construction of high-quality detectors. In this work, we introduce a simple and highly performant DETR-based object detector by proposing a series of rank-oriented designs, combinedly called Rank-DETR. Our key contributions include: (i) a rank-oriented architecture design that can prompt positive predictions and suppress the negative ones to ensure lower false positive rates, as well as (ii) a rank-oriented loss function and matching cost design that prioritizes predictions of more accurate localization accuracy during ranking to boost the AP under high IoU thresholds. We apply our method to improve the recent SOTA methods (e.g., H-DETR and DINO-DETR) and report strong COCO object detection results when using different backbones such as ResNet-$50$, Swin-T, and Swin-L, demonstrating the effectiveness of our approach. Code is available at \url{https://github.com/LeapLabTHU/Rank-DETR}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_08854v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现代检测变换器（DETR）使用一组对象查询来预测边界框的列表，根据它们的分类置信度得分对它们进行排序，并选择排名靠前的预测作为给定输入图像的最终检测结果。高性能的对象检测器需要边界框预测的准确排序。对于基于DETR的检测器，由于分类分数和定位精度之间的不对准，排名靠前的边界框的定位质量较差，从而阻碍了高质量检测器的构建。在这项工作中，我们通过提出一系列面向秩的设计，结合称为秩DETR，介绍了一种简单且高性能的基于DETR的对象检测器。我们的主要贡献包括：（i）一种面向秩的架构设计，它可以提示正预测并抑制负预测，以确保更低的假阳性率，以及（ii）一种基于秩的损失函数和匹配成本设计，它在排序过程中优先考虑更准确定位精度的预测，以在高IoU阈值下提高AP。我们应用我们的方法改进了最近的SOTA方法（例如，H-DETR和DINO-DETR），并在使用不同的主干（如ResNet-$50$、Swin-T和Swin-L）时报告了强大的COCO对象检测结果，证明了我们方法的有效性。代码位于\url{https://github.com/leaplabthu/rank-detr}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.08854v3" target="_blank">2310.08854v3</a>
                              </td>
                              <td>Rank-DETR for High Quality Object Detection</td>
                              <td>Yifan Pu</td>
                              <td>2023-10-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_08854v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.08854v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_01646v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SemiGPC: Distribution-Aware Label Refinement for Imbalanced Semi-Supervised Learning Using Gaussian Processes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_01646v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_01646v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_01646v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we introduce SemiGPC, a distribution-aware label refinement strategy based on Gaussian Processes where the predictions of the model are derived from the labels posterior distribution. Differently from other buffer-based semi-supervised methods such as CoMatch and SimMatch, our SemiGPC includes a normalization term that addresses imbalances in the global data distribution while maintaining local sensitivity. This explicit control allows SemiGPC to be more robust to confirmation bias especially under class imbalance. We show that SemiGPC improves performance when paired with different Semi-Supervised methods such as FixMatch, ReMixMatch, SimMatch and FreeMatch and different pre-training strategies including MSN and Dino. We also show that SemiGPC achieves state of the art results under different degrees of class imbalance on standard CIFAR10-LT/CIFAR100-LT especially in the low data-regime. Using SemiGPC also results in about 2% avg.accuracy increase compared to a new competitive baseline on the more challenging benchmarks SemiAves, SemiCUB, SemiFungi and Semi-iNat.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_01646v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们介绍了SemiGPC，这是一种基于高斯过程的分布感知标签细化策略，其中模型的预测是从标签的后验分布导出的。与其他基于缓冲区的半监督方法（如CoMatch和SimMatch）不同，我们的SemiGPC包括一个归一化项，该项可以解决全局数据分布的不平衡问题，同时保持局部敏感性。这种显式控制允许SemiGPC对确认偏差更具鲁棒性，尤其是在类不平衡的情况下。我们发现，当与不同的半监督方法（如FixMatch、ReMixMatch、SimMatch和FreeMatch）以及不同的预训练策略（包括MSN和Dino）配对时，SemiGPC可以提高性能。我们还表明，SemiGPC在标准CIFAR10-LT/CIFAR100-LT的不同程度的类不平衡下，尤其是在低数据状态下，实现了最先进的结果。在更具挑战性的基准SemiAves、SemiCUB、SemiFormics和Semi-iNat上，与新的竞争基线相比，使用SemiGPC还导致平均准确率增加约2%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.01646v1" target="_blank">2311.01646v1</a>
                              </td>
                              <td>SemiGPC: Distribution-Aware Label Refinement for Imbalanced Semi-Supervised Learning Using Gaussian Processes</td>
                              <td>Abdelhak Lemkhenter</td>
                              <td>2023-11-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_01646v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.01646v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_10726v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Few-Shot Panoptic Segmentation With Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_10726v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_10726v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_10726v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Current state-of-the-art methods for panoptic segmentation require an immense amount of annotated training data that is both arduous and expensive to obtain posing a significant challenge for their widespread adoption. Concurrently, recent breakthroughs in visual representation learning have sparked a paradigm shift leading to the advent of large foundation models that can be trained with completely unlabeled images. In this work, we propose to leverage such task-agnostic image features to enable few-shot panoptic segmentation by presenting Segmenting Panoptic Information with Nearly 0 labels (SPINO). In detail, our method combines a DINOv2 backbone with lightweight network heads for semantic segmentation and boundary estimation. We show that our approach, albeit being trained with only ten annotated images, predicts high-quality pseudo-labels that can be used with any existing panoptic segmentation method. Notably, we demonstrate that SPINO achieves competitive results compared to fully supervised baselines while using less than 0.3% of the ground truth labels, paving the way for learning complex visual recognition tasks leveraging foundation models. To illustrate its general applicability, we further deploy SPINO on real-world robotic vision systems for both outdoor and indoor environments. To foster future research, we make the code and trained models publicly available at http://spino.cs.uni-freiburg.de.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_10726v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>目前最先进的全景分割方法需要大量的带注释的训练数据，这既困难又昂贵，对其广泛采用构成了重大挑战。与此同时，视觉表示学习的最新突破引发了范式的转变，导致了可以用完全未标记的图像训练的大型基础模型的出现。在这项工作中，我们建议利用这种任务不可知的图像特征，通过提供具有近0个标签的分割全景信息（SPINO）来实现少镜头全景分割。详细地说，我们的方法将DINOv2主干与轻量级网络头相结合，用于语义分割和边界估计。我们表明，尽管我们的方法仅用10幅带注释的图像进行训练，但它预测了可用于任何现有全景分割方法的高质量伪标签。值得注意的是，我们证明，与完全监督的基线相比，SPINO在使用不到0.3%的基本事实标签的情况下取得了有竞争力的结果，为利用基础模型学习复杂的视觉识别任务铺平了道路。为了说明其普遍适用性，我们进一步将SPINO部署在室外和室内环境的真实世界机器人视觉系统上。为了促进未来的研究，我们在http://spino.cs.uni-freiburg.de.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.10726v2" target="_blank">2309.10726v2</a>
                              </td>
                              <td>Few-Shot Panoptic Segmentation With Foundation Models</td>
                              <td>Markus Käppeler</td>
                              <td>2023-09-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_10726v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.10726v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/robot-learning-freiburg/SPINO" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_00230v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DINO-Mix: Enhancing Visual Place Recognition with Foundational Vision Model and Feature Mixing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_00230v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_00230v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_00230v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Utilizing visual place recognition (VPR) technology to ascertain the geographical location of publicly available images is a pressing issue for real-world VPR applications. Although most current VPR methods achieve favorable results under ideal conditions, their performance in complex environments, characterized by lighting variations, seasonal changes, and occlusions caused by moving objects, is generally unsatisfactory. In this study, we utilize the DINOv2 model as the backbone network for trimming and fine-tuning to extract robust image features. We propose a novel VPR architecture called DINO-Mix, which combines a foundational vision model with feature aggregation. This architecture relies on the powerful image feature extraction capabilities of foundational vision models. We employ an MLP-Mixer-based mix module to aggregate image features, resulting in globally robust and generalizable descriptors that enable high-precision VPR. We experimentally demonstrate that the proposed DINO-Mix architecture significantly outperforms current state-of-the-art (SOTA) methods. In test sets having lighting variations, seasonal changes, and occlusions (Tokyo24/7, Nordland, SF-XL-Testv1), our proposed DINO-Mix architecture achieved Top-1 accuracy rates of 91.75%, 80.18%, and 82%, respectively. Compared with SOTA methods, our architecture exhibited an average accuracy improvement of 5.14%.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_00230v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>利用视觉位置识别（VPR）技术来确定公开可用图像的地理位置是现实世界VPR应用的一个紧迫问题。尽管目前大多数VPR方法在理想条件下都能获得良好的结果，但它们在复杂环境中的性能通常不令人满意，这些环境的特点是光照变化、季节变化和移动物体引起的遮挡。在这项研究中，我们利用DINOv2模型作为骨干网络进行修剪和微调，以提取稳健的图像特征。我们提出了一种新的VPR架构，称为DINO Mix，它将基础视觉模型与特征聚合相结合。该架构依赖于基础视觉模型强大的图像特征提取能力。我们使用基于MLP Mixer的混合模块来聚合图像特征，从而产生全局鲁棒和可推广的描述符，从而实现高精度的VPR。我们通过实验证明，所提出的DINO-Mix架构显著优于当前最先进的（SOTA）方法。在具有光照变化、季节变化和遮挡的测试集（Tokyo24/7，Nordland，SF-XL-Testv1）中，我们提出的DINO Mix架构分别实现了91.75%、80.18%和82%的Top-1准确率。与SOTA方法相比，我们的体系结构的平均准确度提高了5.14%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.00230v1" target="_blank">2311.00230v1</a>
                              </td>
                              <td>DINO-Mix: Enhancing Visual Place Recognition with Foundational Vision Model and Feature Mixing</td>
                              <td>Gaoshuang Huang</td>
                              <td>2023-11-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_00230v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.00230v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/GaoShuang98/DINO-Mix" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04675v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04675v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04675v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04675v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We systematically study a wide variety of generative models spanning semantically-diverse image datasets to understand and improve the feature extractors and metrics used to evaluate them. Using best practices in psychophysics, we measure human perception of image realism for generated samples by conducting the largest experiment evaluating generative models to date, and find that no existing metric strongly correlates with human evaluations. Comparing to 17 modern metrics for evaluating the overall performance, fidelity, diversity, rarity, and memorization of generative models, we find that the state-of-the-art perceptual realism of diffusion models as judged by humans is not reflected in commonly reported metrics such as FID. This discrepancy is not explained by diversity in generated samples, though one cause is over-reliance on Inception-V3. We address these flaws through a study of alternative self-supervised feature extractors, find that the semantic information encoded by individual networks strongly depends on their training procedure, and show that DINOv2-ViT-L/14 allows for much richer evaluation of generative models. Next, we investigate data memorization, and find that generative models do memorize training examples on simple, smaller datasets like CIFAR10, but not necessarily on more complex datasets like ImageNet. However, our experiments show that current metrics do not properly detect memorization: none in the literature is able to separate memorization from other phenomena such as underfitting or mode shrinkage. To facilitate further development of generative models and their evaluation we release all generated image datasets, human evaluation data, and a modular library to compute 17 common metrics for 9 different encoders at https://github.com/layer6ai-labs/dgm-eval.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04675v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们系统地研究了跨越语义多样的图像数据集的各种生成模型，以理解和改进用于评估它们的特征提取器和度量。利用心理物理学中的最佳实践，我们通过进行迄今为止最大的实验来评估生成模型，从而测量生成样本的人类对图像真实感的感知，并发现没有任何现有的指标与人类评估密切相关。与用于评估生成模型的整体性能、保真度、多样性、稀有性和记忆性的17个现代指标相比，我们发现，人类判断的扩散模型的最先进的感知真实性没有反映在常见的指标中，如FID。这种差异并不能用生成样本的多样性来解释，尽管其中一个原因是过度依赖Inception-V3。我们通过对替代自监督特征提取器的研究来解决这些缺陷，发现单个网络编码的语义信息在很大程度上取决于它们的训练过程，并表明DINOv2-ViT-L/14允许对生成模型进行更丰富的评估。接下来，我们研究了数据记忆，发现生成模型确实在像CIFAR10这样的简单、较小的数据集上记忆训练示例，但不一定在像ImageNet这样的更复杂数据集上。然而，我们的实验表明，目前的指标并不能正确地检测记忆：文献中没有一个能够将记忆与其他现象（如填充不足或模式收缩）区分开来。为了促进生成模型及其评估的进一步发展，我们发布了所有生成的图像数据集、人类评估数据和模块化库，以计算9个不同编码器的17个通用指标https://github.com/layer6ai-labs/dgm-eval.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04675v2" target="_blank">2306.04675v2</a>
                              </td>
                              <td>Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models</td>
                              <td>George Stein</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04675v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04675v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_19257v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A High-Resolution Dataset for Instance Detection with Multi-View Instance Capture</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_19257v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_19257v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_19257v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Instance detection (InsDet) is a long-lasting problem in robotics and computer vision, aiming to detect object instances (predefined by some visual examples) in a cluttered scene. Despite its practical significance, its advancement is overshadowed by Object Detection, which aims to detect objects belonging to some predefined classes. One major reason is that current InsDet datasets are too small in scale by today's standards. For example, the popular InsDet dataset GMU (published in 2016) has only 23 instances, far less than COCO (80 classes), a well-known object detection dataset published in 2014. We are motivated to introduce a new InsDet dataset and protocol. First, we define a realistic setup for InsDet: training data consists of multi-view instance captures, along with diverse scene images allowing synthesizing training images by pasting instance images on them with free box annotations. Second, we release a real-world database, which contains multi-view capture of 100 object instances, and high-resolution (6k x 8k) testing images. Third, we extensively study baseline methods for InsDet on our dataset, analyze their performance and suggest future work. Somewhat surprisingly, using the off-the-shelf class-agnostic segmentation model (Segment Anything Model, SAM) and the self-supervised feature representation DINOv2 performs the best, achieving >10 AP better than end-to-end trained InsDet models that repurpose object detectors (e.g., FasterRCNN and RetinaNet).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_19257v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>实例检测（InsDet）是机器人和计算机视觉中一个长期存在的问题，旨在检测杂乱场景中的对象实例（由一些视觉实例预定义）。尽管它具有实际意义，但它的进步被对象检测所掩盖，对象检测旨在检测属于某些预定义类的对象。一个主要原因是，按照今天的标准，当前的InsDet数据集规模太小。例如，流行的InsDet数据集GMU（2016年发布）只有23个实例，远远少于2014年发布的著名对象检测数据集COCO（80个类）。我们有动力引入一个新的InsDet数据集和协议。首先，我们为InsDet定义了一个逼真的设置：训练数据由多视图实例捕获以及不同的场景图像组成，允许通过在其上粘贴带有自由框注释的实例图像来合成训练图像。其次，我们发布了一个真实世界的数据库，其中包含100个对象实例的多视图捕获和高分辨率（6k x 8k）测试图像。第三，我们在数据集上广泛研究了InsDet的基线方法，分析了它们的性能，并提出了未来的工作建议。令人惊讶的是，使用现成的类不可知分割模型（Segment Anything model，SAM）和自监督特征表示DINOv2表现最好，比重新利用对象检测器的端到端训练的InsDet模型（例如，FasterRCNN和RetinaNet）更好地实现了>10 AP。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.19257v1" target="_blank">2310.19257v1</a>
                              </td>
                              <td>A High-Resolution Dataset for Instance Detection with Multi-View Instance Capture</td>
                              <td>Qianqian Shen</td>
                              <td>2023-10-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_19257v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.19257v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_14736v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAMCLR: Contrastive pre-training on complex scenes using SAM for view sampling</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_14736v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_14736v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_14736v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In Computer Vision, self-supervised contrastive learning enforces similar representations between different views of the same image. The pre-training is most often performed on image classification datasets, like ImageNet, where images mainly contain a single class of objects. However, when dealing with complex scenes with multiple items, it becomes very unlikely for several views of the same image to represent the same object category. In this setting, we propose SAMCLR, an add-on to SimCLR which uses SAM to segment the image into semantic regions, then sample the two views from the same region. Preliminary results show empirically that when pre-training on Cityscapes and ADE20K, then evaluating on classification on CIFAR-10, STL10 and ImageNette, SAMCLR performs at least on par with, and most often significantly outperforms not only SimCLR, but also DINO and MoCo.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_14736v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在计算机视觉中，自监督对比学习在同一图像的不同视图之间强制执行相似的表示。预训练通常在图像分类数据集上执行，如ImageNet，其中图像主要包含一类对象。然而，当处理具有多个项目的复杂场景时，同一图像的多个视图不太可能表示同一对象类别。在这种设置中，我们提出了SAMCLR，这是SimCLR的一个附加组件，它使用SAM将图像分割成语义区域，然后对同一区域的两个视图进行采样。初步结果实证表明，当在Cityscapes和ADE20K上进行预训练，然后在CIFAR-10、STL10和ImageNette上进行分类评估时，SAMCLR的表现至少与SimCLR不相上下，而且通常显著优于SimCLR，还优于DINO和MoCo。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.14736v2" target="_blank">2310.14736v2</a>
                              </td>
                              <td>SAMCLR: Contrastive pre-training on complex scenes using SAM for view sampling</td>
                              <td>Benjamin Missaoui</td>
                              <td>2023-10-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_14736v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.14736v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_18642v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">One-shot Localization and Segmentation of Medical Images with Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_18642v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_18642v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_18642v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in Vision Transformers (ViT) and Stable Diffusion (SD) models with their ability to capture rich semantic features of the image have been used for image correspondence tasks on natural images. In this paper, we examine the ability of a variety of pre-trained ViT (DINO, DINOv2, SAM, CLIP) and SD models, trained exclusively on natural images, for solving the correspondence problems on medical images. While many works have made a case for in-domain training, we show that the models trained on natural images can offer good performance on medical images across different modalities (CT,MR,Ultrasound) sourced from various manufacturers, over multiple anatomical regions (brain, thorax, abdomen, extremities), and on wide variety of tasks. Further, we leverage the correspondence with respect to a template image to prompt a Segment Anything (SAM) model to arrive at single shot segmentation, achieving dice range of 62%-90% across tasks, using just one image as reference. We also show that our single-shot method outperforms the recently proposed few-shot segmentation method - UniverSeg (Dice range 47%-80%) on most of the semantic segmentation tasks(six out of seven) across medical imaging modalities.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_18642v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉变换器（ViT）和稳定扩散（SD）模型的最新进展及其捕捉图像丰富语义特征的能力已被用于自然图像上的图像对应任务。在本文中，我们检验了各种预训练的ViT（DINO、DINOv2、SAM、CLIP）和SD模型（仅在自然图像上训练）解决医学图像上的对应问题的能力。虽然许多工作已经为域内训练提供了理由，但我们表明，在自然图像上训练的模型可以在来自不同制造商的不同模态（CT、MR、超声）、多个解剖区域（大脑、胸部、腹部、四肢）和各种任务的医学图像上提供良好的性能。此外，我们利用与模板图像的对应关系，提示Segment Anything（SAM）模型实现单次分割，仅使用一张图像作为参考，即可在任务中实现62%-90%的骰子范围。我们还表明，在医学成像模态的大多数语义分割任务（七分之六）上，我们的单镜头方法优于最近提出的少镜头分割方法UniverSeg（Dice范围47%-80%）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.18642v1" target="_blank">2310.18642v1</a>
                              </td>
                              <td>One-shot Localization and Segmentation of Medical Images with Foundation Models</td>
                              <td>Deepa Anand</td>
                              <td>2023-10-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_18642v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.18642v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_18251v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Self-Supervised Approach to Land Cover Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_18251v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_18251v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_18251v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Land use/land cover change (LULC) maps are integral resources in earth science and agricultural research. Due to the nature of such maps, the creation of LULC maps is often constrained by the time and human resources necessary to accurately annotate satellite imagery and remote sensing data. While computer vision models that perform semantic segmentation to create detailed labels from such data are not uncommon, litle research has been done on self-supervised and unsupervised approaches to labelling LULC maps without the use of ground-truth masks. Here, we demonstrate a self-supervised method of land cover segmentation that has no need for high-quality ground truth labels. The proposed deep learning employs a frozen pre-trained ViT backbone transferred from DINO in a STEGO architecture and is fine-tuned using a custom dataset consisting of very high resolution (VHR) sattelite imagery. After only 10 epochs of fine-tuning, an accuracy of roughly 52% was observed across 5 samples, signifying the feasibility of self-supervised models for the automated labelling of VHR LULC maps.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_18251v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>土地利用/土地覆盖变化图是地球科学和农业研究中不可或缺的资源。由于这类地图的性质，土地利用和土地利用法地图的制作往往受到准确注释卫星图像和遥感数据所需的时间和人力资源的限制。虽然执行语义分割以从这些数据中创建详细标签的计算机视觉模型并不罕见，但很少有人对在不使用地面实况掩码的情况下标记LULC地图的自监督和无监督方法进行研究。在这里，我们展示了一种自监督的土地覆盖分割方法，该方法不需要高质量的地面实况标签。所提出的深度学习采用了从STEGO架构中的DINO转移的冷冻预训练ViT骨干，并使用由超高分辨率（VHR）卫星图像组成的自定义数据集进行微调。仅经过10个时期的微调，在5个样本中观察到约52%的准确率，这表明自监督模型用于VHR-LULC图的自动标记的可行性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.18251v1" target="_blank">2310.18251v1</a>
                              </td>
                              <td>A Self-Supervised Approach to Land Cover Segmentation</td>
                              <td>Charles Moore</td>
                              <td>2023-10-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_18251v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.18251v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_13552v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Squared Neural Families: A New Class of Tractable Density Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_13552v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_13552v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_13552v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Flexible models for probability distributions are an essential ingredient in many machine learning tasks. We develop and investigate a new class of probability distributions, which we call a Squared Neural Family (SNEFY), formed by squaring the 2-norm of a neural network and normalising it with respect to a base measure. Following the reasoning similar to the well established connections between infinitely wide neural networks and Gaussian processes, we show that SNEFYs admit closed form normalising constants in many cases of interest, thereby resulting in flexible yet fully tractable density models. SNEFYs strictly generalise classical exponential families, are closed under conditioning, and have tractable marginal distributions. Their utility is illustrated on a variety of density estimation, conditional density estimation, and density estimation with missing data tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_13552v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>概率分布的灵活模型是许多机器学习任务的重要组成部分。我们开发并研究了一类新的概率分布，我们称之为平方神经家族（SNEFY），它是通过对神经网络的2-范数进行平方并相对于基本测度对其进行归一化而形成的。根据类似于无限宽神经网络和高斯过程之间已建立的良好连接的推理，我们表明，在许多感兴趣的情况下，SNEFY允许闭合形式的归一化常数，从而产生灵活但完全可处理的密度模型。SNEFY严格推广经典指数族，在条件作用下是封闭的，并且具有可处理的边缘分布。它们在各种密度估计、条件密度估计和具有缺失数据任务的密度估计中的效用得到了说明。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.13552v2" target="_blank">2305.13552v2</a>
                              </td>
                              <td>Squared Neural Families: A New Class of Tractable Density Models</td>
                              <td>Russell Tsuchida</td>
                              <td>2023-05-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_13552v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.13552v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_08825v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_08825v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_08825v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_08825v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multi-modal Large Language Models (MLLMs) have made significant strides in expanding the capabilities of Large Language Models (LLMs) through the incorporation of visual perception interfaces. Despite the emergence of exciting applications and the availability of diverse instruction tuning data, existing approaches often rely on CLIP or its variants as the visual branch, and merely extract features from the deep layers. However, these methods lack a comprehensive analysis of the visual encoders in MLLMs. In this paper, we conduct an extensive investigation into the effectiveness of different vision encoders within MLLMs. Our findings reveal that the shallow layer features of CLIP offer particular advantages for fine-grained tasks such as grounding and region understanding. Surprisingly, the vision-only model DINO, which is not pretrained with text-image alignment, demonstrates promising performance as a visual branch within MLLMs. By simply equipping it with an MLP layer for alignment, DINO surpasses CLIP in fine-grained related perception tasks. Building upon these observations, we propose a simple yet effective feature merging strategy, named COMM, that integrates CLIP and DINO with Multi-level features Merging, to enhance the visual capabilities of MLLMs. We evaluate COMM through comprehensive experiments on a wide range of benchmarks, including image captioning, visual question answering, visual grounding, and object hallucination. Experimental results demonstrate the superior performance of COMM compared to existing methods, showcasing its enhanced visual capabilities within MLLMs. Code will be made available at https://github.com/YuchenLiu98/COMM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_08825v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多模态大语言模型（MLLMs）通过引入视觉感知接口，在扩展大语言模型的能力方面取得了重大进展。尽管出现了令人兴奋的应用程序和各种指令调整数据，但现有的方法往往依赖于CLIP或其变体作为视觉分支，而只是从深层提取特征。然而，这些方法缺乏对MLLMs中的视觉编码器的全面分析。在本文中，我们对MLLMs中不同视觉编码器的有效性进行了广泛的研究。我们的研究结果表明，CLIP的浅层特征为细粒度任务（如接地和区域理解）提供了特别的优势。令人惊讶的是，未经文本图像对齐预训练的仅视觉模型DINO作为MLLMs中的视觉分支表现出了良好的性能。通过简单地为其配备MLP层进行对齐，DINO在细粒度相关感知任务中超越了CLIP。基于这些观察结果，我们提出了一种简单而有效的特征合并策略，称为COMM，将CLIP和DINO与多级特征合并相结合，以增强MLLM的视觉能力。我们通过在广泛的基准上进行综合实验来评估COMM，包括图像字幕、视觉问答、视觉基础和对象幻觉。实验结果表明，与现有方法相比，COMM具有优越的性能，显示了其在MLLM中增强的视觉能力。代码将在提供https://github.com/yuchenliu98/comm.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.08825v2" target="_blank">2310.08825v2</a>
                              </td>
                              <td>From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models</td>
                              <td>Dongsheng Jiang</td>
                              <td>2023-10-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_08825v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.08825v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/yuchenliu98/comm" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </div>
  <div class="text-muted text-center py-2">
    <div class="container">
      <div class="row">
        <div class="col-md-12 my-4">
          <p class="mb-1">© 2023 Li Yingping - Powered by Arxiv API.</p>
        </div>
      </div>
    </div>
  </div>
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
    integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
    crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
    integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
    crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js"
    integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy"
    crossorigin="anonymous"></script>
  <pingendo onclick="window.open('https://pingendo.com/', '_blank')"
    style="cursor:pointer;position: fixed;bottom: 20px;right:20px;padding:4px;background-color: #00b0eb;border-radius: 8px; width:220px;display:flex;flex-direction:row;align-items:center;justify-content:center;font-size:14px;color:white">
    Made with Pingendo Free&nbsp;&nbsp;<img src="https://pingendo.com/site-assets/Pingendo_logo_big.png" class="d-block"
      alt="Pingendo logo" height="16"></pingendo>
</body>

</html>